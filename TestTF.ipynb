{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start in Init: 36\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "\n",
    "# Define the environment\n",
    "class TSPEnv(gym.Env):\n",
    "  def __init__(self, n_cities=100, show_debug_data = False):\n",
    "    self.n_cities = n_cities\n",
    "    self.xy = (np.random.rand(self.n_cities,2)*100).round(2)\n",
    "    self.x=self.xy[:,0]\n",
    "    self.y=self.xy[:,1]\n",
    "    self.step_counter = 0\n",
    "    self.show_debug_data = show_debug_data\n",
    "\n",
    "    #print(f'genrated stops xy: {self.xy}')\n",
    "    self.distance_matrix = cdist(self.xy,self.xy,'euclidean').round(0)\n",
    "\n",
    "    # self.distance_matrix = np.array([[0, 2448, 1434, 1260, 2045],\n",
    "    #                   [2448, 0, 2546, 959, 2367],\n",
    "    #                   [1434, 2546, 0, 2408, 1745],\n",
    "    #                   [1260, 959, 2408, 0, 2295],\n",
    "    #                   [2045, 2367, 1745, 2295, 0]])\n",
    "    self.current_city = np.random.randint(n_cities)\n",
    "    self.visited_cities = [self.current_city]\n",
    "    self.remaining_cities = [i for i in range(n_cities) if i != self.current_city]\n",
    "    # Define the action space\n",
    "    self.action_space = gym.spaces.Discrete(n_cities)\n",
    "\n",
    "    # Define the observation space\n",
    "    self.observation_space = gym.spaces.Box(low=0, high=1, shape=(n_cities,), dtype=np.float32)\n",
    "\n",
    "    print(f'Start in Init: {self.current_city}')\n",
    "\n",
    "  def reset(self):\n",
    "    self.step_counter = 0\n",
    "    self.current_city = np.random.randint(self.n_cities)\n",
    "    self.visited_cities = [self.current_city]\n",
    "    self.remaining_cities = [i for i in range(self.n_cities) if i != self.current_city]\n",
    "    return self._get_observation()\n",
    "\n",
    "  def step(self, action):\n",
    "    self.step_counter += 1\n",
    "    if action < len(self.remaining_cities):\n",
    "      next_city = self.remaining_cities[action]\n",
    "      reward = -self.distance_matrix[self.current_city][next_city]\n",
    "      self.remaining_cities.remove(next_city)\n",
    "      self.visited_cities.append(next_city)\n",
    "      self.current_city = next_city\n",
    "      done = len(self.remaining_cities) == 0\n",
    "      \n",
    "      if(self.show_debug_data):\n",
    "        print(f'Action in step: {action}')\n",
    "        print(f'Reward in step: {reward}')\n",
    "        print(f'Current city in step: {self.current_city}')\n",
    "        print(f'Remaining city in step: {self.remaining_cities}')\n",
    "        print(f'Visited city in step: {self.visited_cities}')\n",
    "        print(f'Stepcounter in step: {self.step_counter}')\n",
    "      return self._get_observation(), reward, done, {}\n",
    "    else:\n",
    "      return self._get_observation(), 0, False, {}\n",
    "\n",
    "\n",
    "  def _get_observation(self):\n",
    "    observation = np.zeros(self.n_cities)\n",
    "    observation[self.current_city] = 1\n",
    "    return observation\n",
    "\n",
    "  \n",
    "  def _test_distance(self,CurrentCity, NextCity):\n",
    "    return -self.distance_matrix[CurrentCity][NextCity]\n",
    "\n",
    "\n",
    "env = TSPEnv() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"sequential_9\" \"                 f\"(type Sequential).\n\nInput 0 of layer \"dense_40\" is incompatible with the layer: expected min_ndim=2, found ndim=1. Full shape received: (1,)\n\nCall arguments received by layer \"sequential_9\" \"                 f\"(type Sequential):\n  • inputs=tf.Tensor(shape=(1,), dtype=int32)\n  • training=None\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [30], line 49\u001b[0m\n\u001b[0;32m     46\u001b[0m       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mfit(state, q_values, verbose\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m     48\u001b[0m \u001b[39m# Initialize the agent\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m agent \u001b[39m=\u001b[39m DQNAgent(n_actions\u001b[39m=\u001b[39;49menv\u001b[39m.\u001b[39;49maction_space\u001b[39m.\u001b[39;49mn)\n\u001b[0;32m     51\u001b[0m \u001b[39m# Train the agent\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   \u001b[39m# Reset the environment\u001b[39;00m\n",
      "Cell \u001b[1;32mIn [30], line 11\u001b[0m, in \u001b[0;36mDQNAgent.__init__\u001b[1;34m(self, n_actions, learning_rate, discount_factor, epsilon, batch_size)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepsilon \u001b[39m=\u001b[39m epsilon\n\u001b[0;32m     10\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplay_memory \u001b[39m=\u001b[39m []\n\u001b[1;32m---> 11\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_build_model(env\u001b[39m.\u001b[39;49mobservation_space\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m], env\u001b[39m.\u001b[39;49maction_space\u001b[39m.\u001b[39;49mn)\n\u001b[0;32m     12\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size \u001b[39m=\u001b[39m batch_size\n",
      "Cell \u001b[1;32mIn [30], line 22\u001b[0m, in \u001b[0;36mDQNAgent._build_model\u001b[1;34m(self, input_dim, action_size)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_build_model\u001b[39m(\u001b[39mself\u001b[39m, input_dim, action_size):\n\u001b[0;32m     15\u001b[0m   model \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mSequential(\n\u001b[0;32m     16\u001b[0m     [\n\u001b[0;32m     17\u001b[0m       tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mDense(units\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m'\u001b[39m, input_shape\u001b[39m=\u001b[39m(input_dim,)),\n\u001b[0;32m     18\u001b[0m       tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mDense(units\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msoftmax\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[0;32m     19\u001b[0m       tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mDense(action_size, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlinear\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[0;32m     20\u001b[0m     ])\n\u001b[1;32m---> 22\u001b[0m   model(tf\u001b[39m.\u001b[39;49mones(\u001b[39m1\u001b[39;49m,\u001b[39m3\u001b[39;49m))\n\u001b[0;32m     23\u001b[0m   \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mc:\\Users\\fabian.woellenweber\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\fabian.woellenweber\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\input_spec.py:250\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    248\u001b[0m     ndim \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mshape\u001b[39m.\u001b[39mrank\n\u001b[0;32m    249\u001b[0m     \u001b[39mif\u001b[39;00m ndim \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m ndim \u001b[39m<\u001b[39m spec\u001b[39m.\u001b[39mmin_ndim:\n\u001b[1;32m--> 250\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    251\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mInput \u001b[39m\u001b[39m{\u001b[39;00minput_index\u001b[39m}\u001b[39;00m\u001b[39m of layer \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mlayer_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    252\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mis incompatible with the layer: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    253\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mexpected min_ndim=\u001b[39m\u001b[39m{\u001b[39;00mspec\u001b[39m.\u001b[39mmin_ndim\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    254\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfound ndim=\u001b[39m\u001b[39m{\u001b[39;00mndim\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    255\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFull shape received: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtuple\u001b[39m(shape)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    256\u001b[0m         )\n\u001b[0;32m    257\u001b[0m \u001b[39m# Check dtype.\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[39mif\u001b[39;00m spec\u001b[39m.\u001b[39mdtype \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling layer \"sequential_9\" \"                 f\"(type Sequential).\n\nInput 0 of layer \"dense_40\" is incompatible with the layer: expected min_ndim=2, found ndim=1. Full shape received: (1,)\n\nCall arguments received by layer \"sequential_9\" \"                 f\"(type Sequential):\n  • inputs=tf.Tensor(shape=(1,), dtype=int32)\n  • training=None\n  • mask=None"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class DQNAgent:\n",
    "\n",
    "  def __init__(self, n_actions, learning_rate=0.01, discount_factor=0.95, epsilon=0.1, batch_size=64):\n",
    "    self.n_actions = n_actions\n",
    "    self.learning_rate = learning_rate\n",
    "    self.discount_factor = discount_factor\n",
    "    self.epsilon = epsilon\n",
    "    self.replay_memory = []\n",
    "    self.model = self._build_model(env.observation_space.shape[0], env.action_space.n)\n",
    "    self.batch_size = batch_size\n",
    "\n",
    "  def _build_model(self, input_dim, action_size):\n",
    "    model = tf.keras.Sequential(\n",
    "      [\n",
    "        tf.keras.layers.Dense(units=32, activation='relu', input_shape=(input_dim,)),\n",
    "        tf.keras.layers.Dense(units=10, activation='softmax'),\n",
    "        tf.keras.layers.Dense(action_size, activation='linear'),\n",
    "      ])\n",
    "    \n",
    "    model(tf.ones(1,3))\n",
    "    return model\n",
    "\n",
    "  def act(self, state):\n",
    "    if np.random.rand() < self.epsilon:\n",
    "      # Explore: choose a random action\n",
    "      return np.random.randint(self.n_actions)\n",
    "    else:\n",
    "      # Exploit: choose the action with the highest Q value\n",
    "      q_values = self.model.predict(state)\n",
    "      return np.argmax(q_values[0])\n",
    "\n",
    "  def remember(self, state, action, reward, next_state, done):\n",
    "    self.replay_memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "  def train(self):\n",
    "    # Sample a minibatch from the replay memory\n",
    "    minibatch = np.random.choice(self.replay_memory, self.batch_size)\n",
    "    for state, action, reward, next_state, done in minibatch:\n",
    "      q_update = reward\n",
    "      if not done:\n",
    "        q_update = (reward + self.discount_factor * np.amax(self.model.predict(next_state)[0]))\n",
    "      q_values = self.model.predict(state)\n",
    "      q_values[0][action] = q_update\n",
    "      self.model.fit(state, q_values, verbose=0)\n",
    "\n",
    "# Initialize the agent\n",
    "agent = DQNAgent(n_actions=env.action_space.n)\n",
    "\n",
    "# Train the agent\n",
    "while True:\n",
    "  # Reset the environment\n",
    "  state = env.reset()\n",
    "  # Run the episode\n",
    "  while True:\n",
    "    # Choose an action\n",
    "    action = agent.act(state)\n",
    "    # Take a step in the environment\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    # Remember the experience\n",
    "    agent.remember(state, action, reward, next_state, done)\n",
    "    # Update the state\n",
    "    state = next_state\n",
    "    # If the episode is done, exit the loop\n",
    "    if done:\n",
    "      break\n",
    "  # Train the agent\n",
    "  agent.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Define the DQN model\n",
    "class DQN(tf.keras.Model):\n",
    "  def __init__(self, input_dim, output_dim):\n",
    "    super().__init__()\n",
    "    self.fc1 = tf.keras.layers.Dense(32, input_shape=(input_dim,), activation=\"relu\")\n",
    "    self.fc2 = tf.keras.layers.Dense(output_dim)\n",
    "\n",
    "  # def call(self, x):\n",
    "  #   x = self.fc1(x)\n",
    "  #   x = self.fc2(x)\n",
    "  #   return x\n",
    "\n",
    "  def call(self, inputs):\n",
    "    # Concatenate the inputs into a single tensor\n",
    "    x = tf.concat(inputs, axis=-1)\n",
    "    # Pass the tensor through the model\n",
    "    return self.model(x)\n",
    "\n",
    "# Define the DQNAgent\n",
    "class DQNAgent:\n",
    "  def __init__(self, env, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01, \n",
    "               alpha=1e-3, alpha_decay=0.01, gamma=0.99, memory_size=10000, \n",
    "               batch_size=64):\n",
    "    self.env = env\n",
    "    self.epsilon = epsilon\n",
    "    self.epsilon_decay = epsilon_decay\n",
    "    self.epsilon_min = epsilon_min\n",
    "    self.alpha = alpha\n",
    "    self.alpha_decay = alpha_decay\n",
    "    self.gamma = gamma\n",
    "    self.memory = []\n",
    "    self.batch_size = batch_size\n",
    "\n",
    "    # Define the model and the target model\n",
    "    self.model = DQN(env.observation_space.shape[0], env.action_space.n)\n",
    "    self.target_model = DQN(env.observation_space.shape[0], env.action_space.n)\n",
    "    self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    # Define the optimizer\n",
    "    self.optimizer = tf.keras.optimizers.Adam(learning_rate=alpha)\n",
    "\n",
    "  def remember(self, state, action, reward, next_state, done):\n",
    "    self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "  def act(self, state):\n",
    "    if np.random.rand() <= self.epsilon:\n",
    "      return self.env.action_space.sample()\n",
    "    else:\n",
    "      state = np.expand_dims(state, axis=0)\n",
    "      q_values = self.model(state)\n",
    "      return np.argmax(q_values[0]).numpy()\n",
    "\n",
    "  def update(self):\n",
    "    # Don't update if there are not enough samples in the memory\n",
    "    if len(self.memory) < self.batch_size:\n",
    "      return\n",
    "\n",
    "    # Sample a batch from the memory\n",
    "    samples = random.sample(self.memory, self.batch_size)\n",
    "\n",
    "    # Split the batch into separate variables\n",
    "    states, actions, rewards, next_states, dones = zip(*samples)\n",
    "\n",
    "    # Convert variables to arrays\n",
    "    states = np.vstack(states)\n",
    "    actions = np.vstack(actions)\n",
    "    rewards = np.vstack(rewards)\n",
    "\n",
    "        # Calculate the Q values for the current states\n",
    "    q_values = self.model(np.array(states))\n",
    "    q_values = tf.gather(q_values, actions, axis=1)\n",
    "\n",
    "    # Calculate the Q values for the next states\n",
    "    next_q_values = self.target_model(next_states).max(1)[0]\n",
    "\n",
    "    # Calculate the target Q values\n",
    "    target_q_values = rewards + (self.gamma * next_q_values * (1 - dones))\n",
    "\n",
    "    # Calculate the loss\n",
    "    loss = tf.losses.mean_squared_error(target_q_values, q_values)\n",
    "\n",
    "    # Perform backpropagation\n",
    "    with tf.GradientTape() as tape:\n",
    "      grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "      self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "\n",
    "    # Update the target model\n",
    "    self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    # Update the epsilon value\n",
    "    self.epsilon = max(self.epsilon * self.epsilon_decay, self.epsilon_min)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Instantiate the environment\n",
    "env = TSPEnv()\n",
    "\n",
    "# Instantiate the agent\n",
    "agent = DQNAgent(env)\n",
    "\n",
    "# Set the number of episodes to run\n",
    "n_episodes = 10\n",
    "\n",
    "# Set the initial reward to 0\n",
    "total_reward = 0\n",
    "\n",
    "# Run the episodes\n",
    "for episode in range(n_episodes):\n",
    "  # Reset the environment and get the initial state\n",
    "  state = env.reset()\n",
    "\n",
    "  while True:\n",
    "    # Take an action\n",
    "    action = agent.act(state)\n",
    "\n",
    "    # Step the environment\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "    # Remember the experience\n",
    "    agent.remember(state, action, reward, next_state, done)\n",
    "\n",
    "    # Update the state and the reward\n",
    "    state = next_state\n",
    "    total_reward += reward\n",
    "\n",
    "    # Update the agent\n",
    "    agent.update()\n",
    "\n",
    "    # If the episode is done, break the loop\n",
    "    if done:\n",
    "      break\n",
    "\n",
    "  # Print the total reward for the episode\n",
    "  print(f\"Episode: {episode+1}, Reward: {total_reward}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5a312273ffa6240539e020f48a5de0b17350fba060a017bafd7c66c880b86767"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
