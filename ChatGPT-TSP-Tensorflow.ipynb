{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fabian.woellenweber\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gym\\envs\\registration.py:498: UserWarning: \u001b[33mWARN: Overriding environment TSP-v0 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "# Define the TSP environment\n",
    "class TSPEnv(gym.Env):\n",
    "  # Initialize the environment\n",
    "  def __init__(self, distances):\n",
    "    self.distances = distances\n",
    "    self.num_cities = len(distances)\n",
    "    self.action_space = gym.spaces.Discrete(self.num_cities)\n",
    "    self.observation_space = gym.spaces.Discrete(1)\n",
    "\n",
    "  # Reset the environment\n",
    "  def reset(self):\n",
    "    self.route = None\n",
    "    return 0\n",
    "\n",
    "  # Step the environment\n",
    "  def step(self, action):\n",
    "    if self.route is None:\n",
    "      self.route = [action]\n",
    "      return 0, 0, False, {}\n",
    "    else:\n",
    "      self.route.append(action)\n",
    "      if len(self.route) == self.num_cities:\n",
    "        return 0, self.evaluate(self.route), True, {}\n",
    "      else:\n",
    "        return 0, 0, False, {}\n",
    "\n",
    "  # Render the environment\n",
    "  def render(self, mode='human'):\n",
    "    if self.route is None:\n",
    "      print(\"Route: []\")\n",
    "    else:\n",
    "      print(\"Route:\", self.route)\n",
    "\n",
    "  # Evaluate the fitness of an individual\n",
    "  def evaluate(self, individual):\n",
    "    fitness = 0\n",
    "    for i in range(self.num_cities-1):\n",
    "      fitness += self.distances[individual[i]][individual[i+1]]\n",
    "    # Add the distance between the last city and the starting city\n",
    "    fitness += self.distances[individual[-1]][individual[0]]\n",
    "    return fitness\n",
    "\n",
    "# Register the TSP environment\n",
    "gym.envs.register(id='TSP-v0', entry_point=TSPEnv, kwargs={'distances': distances})\n",
    "\n",
    "# Create an instance of the TSP environment\n",
    "env = gym.make('TSP-v0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No gradients provided for any variable, check your graph for ops that do not support gradients, between variables [\"<tf.Variable 'Variable:0' shape=(4,) dtype=int32>\"] and loss Tensor(\"Sum_2:0\", shape=(), dtype=float32).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [11], line 68\u001b[0m\n\u001b[0;32m     65\u001b[0m       \u001b[39m# Print the best fitness of the current generation\u001b[39;00m\n\u001b[0;32m     66\u001b[0m       \u001b[39mprint\u001b[39m(\u001b[39mmin\u001b[39m(fitnesses))\n\u001b[1;32m---> 68\u001b[0m solve_tsp()\n",
      "Cell \u001b[1;32mIn [11], line 48\u001b[0m, in \u001b[0;36msolve_tsp\u001b[1;34m()\u001b[0m\n\u001b[0;32m     45\u001b[0m objective \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mreduce_sum(distance)\n\u001b[0;32m     47\u001b[0m \u001b[39m# Use the Adam optimizer to minimize the objective\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m optimizer \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mcompat\u001b[39m.\u001b[39;49mv1\u001b[39m.\u001b[39;49mtrain\u001b[39m.\u001b[39;49mAdamOptimizer(learning_rate)\u001b[39m.\u001b[39;49mminimize(objective)\n\u001b[0;32m     50\u001b[0m \u001b[39m# Initialize variables\u001b[39;00m\n\u001b[0;32m     51\u001b[0m sess\u001b[39m.\u001b[39mrun(tf\u001b[39m.\u001b[39mcompat\u001b[39m.\u001b[39mv1\u001b[39m.\u001b[39mglobal_variables_initializer())\n",
      "File \u001b[1;32mc:\\Users\\fabian.woellenweber\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py:485\u001b[0m, in \u001b[0;36mOptimizer.minimize\u001b[1;34m(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\u001b[0m\n\u001b[0;32m    483\u001b[0m vars_with_grad \u001b[39m=\u001b[39m [v \u001b[39mfor\u001b[39;00m g, v \u001b[39min\u001b[39;00m grads_and_vars \u001b[39mif\u001b[39;00m g \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m]\n\u001b[0;32m    484\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m vars_with_grad:\n\u001b[1;32m--> 485\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    486\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mNo gradients provided for any variable, check your graph for ops\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    487\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39m that do not support gradients, between variables \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m and loss \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m    488\u001b[0m       ([\u001b[39mstr\u001b[39m(v) \u001b[39mfor\u001b[39;00m _, v \u001b[39min\u001b[39;00m grads_and_vars], loss))\n\u001b[0;32m    490\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_gradients(grads_and_vars, global_step\u001b[39m=\u001b[39mglobal_step,\n\u001b[0;32m    491\u001b[0m                             name\u001b[39m=\u001b[39mname)\n",
      "\u001b[1;31mValueError\u001b[0m: No gradients provided for any variable, check your graph for ops that do not support gradients, between variables [\"<tf.Variable 'Variable:0' shape=(4,) dtype=int32>\"] and loss Tensor(\"Sum_2:0\", shape=(), dtype=float32)."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# The distance between each city\n",
    "distances = [[0, 2, 9, 10],\n",
    "             [1, 0, 6, 4],\n",
    "             [8, 9, 0, 7],\n",
    "             [6, 3, 12, 0]]\n",
    "\n",
    "# The number of cities\n",
    "num_cities = len(distances)\n",
    "\n",
    "# The size of the population\n",
    "pop_size = 20\n",
    "\n",
    "# The number of generations\n",
    "num_generations = 1000\n",
    "\n",
    "# The learning rate\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Create the TSP environment\n",
    "env = TSPEnv(distances)\n",
    "\n",
    "# The main function\n",
    "def solve_tsp():\n",
    "  # Use TensorFlow to optimize the route\n",
    "  with tf.compat.v1.Session() as sess:\n",
    "    # Define the route variable\n",
    "    route = tf.Variable(tf.random.uniform([env.num_cities], minval=0, maxval=env.num_cities, dtype=tf.int32))\n",
    "\n",
    "    # Placeholder for the individual\n",
    "    individual = tf.compat.v1.placeholder(tf.int32, [env.num_cities])\n",
    "\n",
    "    # One-hot encoding of the individual\n",
    "    one_hot = tf.compat.v1.one_hot(individual, env.num_cities)\n",
    "\n",
    "    # The distance between each pair of cities\n",
    "    distance_matrix = tf.constant(env.distances, dtype=tf.float32)\n",
    "\n",
    "    # The distance traveled by the individual\n",
    "    distance = tf.matmul(tf.matmul(one_hot, distance_matrix),\n",
    "                         tf.transpose(one_hot))\n",
    "\n",
    "    # The optimization objective is to minimize the distance traveled\n",
    "    objective = tf.reduce_sum(distance)\n",
    "\n",
    "    # Use the Adam optimizer to minimize the objective\n",
    "    optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate).minimize(objective)\n",
    "\n",
    "    # Initialize variables\n",
    "    sess.run(tf.compat.v1.global_variables_initializer())\n",
    "\n",
    "    # Iteratively optimize the route\n",
    "    for generation in range(num_generations):\n",
    "      # Generate a random population\n",
    "      population = [env.action_space.sample() for _ in range(pop_size)]\n",
    "\n",
    "      # Optimize the route for each individual\n",
    "      for individual in population:\n",
    "        sess.run(optimizer, feed_dict={individual: individual})\n",
    "      \n",
    "      # Calculate the fitness of each individual\n",
    "      fitnesses = [env.evaluate(x) for x in population]\n",
    "\n",
    "      # Print the best fitness of the current generation\n",
    "      print(min(fitnesses))\n",
    "\n",
    "solve_tsp()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No gradients provided for any variable, check your graph for ops that do not support gradients, between variables [\"<tf.Variable 'Variable:0' shape=(4,) dtype=int32>\", \"<tf.Variable 'Variable_1:0' shape=(4,) dtype=int32>\", \"<tf.Variable 'Variable_2:0' shape=(4,) dtype=int32>\"] and loss Tensor(\"Sum_4:0\", shape=(), dtype=float32).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [13], line 50\u001b[0m\n\u001b[0;32m     47\u001b[0m       \u001b[39mprint\u001b[39m(\u001b[39mmin\u001b[39m(fitnesses))\n\u001b[0;32m     49\u001b[0m \u001b[39m# Run the TSP solver\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m solve_tsp()\n",
      "Cell \u001b[1;32mIn [13], line 22\u001b[0m, in \u001b[0;36msolve_tsp\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m objective \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mreduce_sum(distance)\n\u001b[0;32m     21\u001b[0m \u001b[39m# Use the Adam optimizer to minimize the objective\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m optimizer \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mcompat\u001b[39m.\u001b[39;49mv1\u001b[39m.\u001b[39;49mtrain\u001b[39m.\u001b[39;49mAdamOptimizer(learning_rate)\u001b[39m.\u001b[39;49mminimize(objective)\n\u001b[0;32m     24\u001b[0m \u001b[39m# Initialize variables\u001b[39;00m\n\u001b[0;32m     25\u001b[0m sess\u001b[39m.\u001b[39mrun(tf\u001b[39m.\u001b[39mcompat\u001b[39m.\u001b[39mv1\u001b[39m.\u001b[39mglobal_variables_initializer())\n",
      "File \u001b[1;32mc:\\Users\\fabian.woellenweber\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py:485\u001b[0m, in \u001b[0;36mOptimizer.minimize\u001b[1;34m(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\u001b[0m\n\u001b[0;32m    483\u001b[0m vars_with_grad \u001b[39m=\u001b[39m [v \u001b[39mfor\u001b[39;00m g, v \u001b[39min\u001b[39;00m grads_and_vars \u001b[39mif\u001b[39;00m g \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m]\n\u001b[0;32m    484\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m vars_with_grad:\n\u001b[1;32m--> 485\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    486\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mNo gradients provided for any variable, check your graph for ops\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    487\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39m that do not support gradients, between variables \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m and loss \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m    488\u001b[0m       ([\u001b[39mstr\u001b[39m(v) \u001b[39mfor\u001b[39;00m _, v \u001b[39min\u001b[39;00m grads_and_vars], loss))\n\u001b[0;32m    490\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_gradients(grads_and_vars, global_step\u001b[39m=\u001b[39mglobal_step,\n\u001b[0;32m    491\u001b[0m                             name\u001b[39m=\u001b[39mname)\n",
      "\u001b[1;31mValueError\u001b[0m: No gradients provided for any variable, check your graph for ops that do not support gradients, between variables [\"<tf.Variable 'Variable:0' shape=(4,) dtype=int32>\", \"<tf.Variable 'Variable_1:0' shape=(4,) dtype=int32>\", \"<tf.Variable 'Variable_2:0' shape=(4,) dtype=int32>\"] and loss Tensor(\"Sum_4:0\", shape=(), dtype=float32)."
     ]
    }
   ],
   "source": [
    "# The main function\n",
    "def solve_tsp():\n",
    "  # Use TensorFlow to optimize the route\n",
    "  with tf.compat.v1.Session() as sess:\n",
    "    # Define the route variable\n",
    "    route = tf.Variable(tf.random.uniform([env.num_cities], minval=0, maxval=env.num_cities, dtype=tf.int32))\n",
    "\n",
    "    # One-hot encoding of the route\n",
    "    one_hot = tf.one_hot(route, env.num_cities)\n",
    "\n",
    "    # The distance between each pair of cities\n",
    "    distance_matrix = tf.constant(env.distances, dtype=tf.float32)\n",
    "\n",
    "    # The distance traveled by the route\n",
    "    distance = tf.matmul(tf.matmul(one_hot, distance_matrix),\n",
    "                         tf.transpose(one_hot))\n",
    "\n",
    "    # The optimization objective is to minimize the distance traveled\n",
    "    objective = tf.reduce_sum(distance)\n",
    "\n",
    "    # Use the Adam optimizer to minimize the objective\n",
    "    optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate).minimize(objective)\n",
    "\n",
    "    # Initialize variables\n",
    "    sess.run(tf.compat.v1.global_variables_initializer())\n",
    "\n",
    "    # Run the optimization for a fixed number of generations\n",
    "    for i in range(num_generations):\n",
    "      # Generate a random population\n",
    "      population = [env.reset() for _ in range(pop_size)]\n",
    "      # Evaluate the fitness of each individual\n",
    "      fitnesses = [env.evaluate(individual) for individual in population]\n",
    "      # Select the best individuals for breeding\n",
    "      parents = [population[i] for i in np.argsort(fitnesses)[:num_parents]]\n",
    "      # Crossover and mutation to generate the children\n",
    "      children = []\n",
    "      for _ in range(pop_size - num_parents):\n",
    "        child = crossover(*np.random.choice(parents, 2))\n",
    "        child = mutate(child, mutation_rate)\n",
    "        children.append(child)\n",
    "      # Update the population\n",
    "      population = parents + children\n",
    "      # Use TensorFlow to optimize the route\n",
    "      for individual in population:\n",
    "        sess.run(optimizer, feed_dict={route: individual})\n",
    "      # Print the best fitness of the current generation\n",
    "      print(min(fitnesses))\n",
    "\n",
    "# Run the TSP solver\n",
    "solve_tsp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 0. 0. 0.]\n",
      "-2448\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "class TSPEnv(gym.Env):\n",
    "    def __init__(self, cities, distances):\n",
    "        self.cities = cities\n",
    "        self.distances = distances\n",
    "        self.num_cities = len(cities)\n",
    "        self.action_space = gym.spaces.Discrete(self.num_cities)\n",
    "        self.observation_space = gym.spaces.Box(0, self.num_cities, shape=(self.num_cities,))\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = np.zeros(self.num_cities)\n",
    "        self.state[0] = 1\n",
    "        self.visited = [0]\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action):\n",
    "        self.state[action] = 1\n",
    "        self.visited.append(action)\n",
    "        reward = -self.distances[self.visited[-2], action]\n",
    "        done = len(self.visited) == self.num_cities\n",
    "        return self.state, reward, done, {}\n",
    "\n",
    "# Create the TSP environment with a set of cities and distances between them\n",
    "cities = ['New York', 'Chicago', 'Denver', 'San Francisco', 'Los Angeles']\n",
    "distances = np.array([[0, 2448, 1434, 1260, 2045],\n",
    "                      [2448, 0, 2546, 959, 2367],\n",
    "                      [1434, 2546, 0, 2408, 1745],\n",
    "                      [1260, 959, 2408, 0, 2295],\n",
    "                      [2045, 2367, 1745, 2295, 0]])\n",
    "env = TSPEnv(cities, distances)\n",
    "\n",
    "# Reset the environment and take an action\n",
    "state = env.reset()\n",
    "state, reward, done, _ = env.step(1)\n",
    "\n",
    "print(state)  # should print [1, 1, 0, 0, 0]\n",
    "print(reward) # should print -2448\n",
    "print(done)   # should print False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "arrays used as indices must be of integer (or boolean) type",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [15], line 39\u001b[0m\n\u001b[0;32m     35\u001b[0m state \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mreset()\n\u001b[0;32m     37\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     38\u001b[0m     \u001b[39m# Select an action using the action selection function\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m     action \u001b[39m=\u001b[39m select_action(state)\n\u001b[0;32m     41\u001b[0m     \u001b[39m# Take the action and observe the reward and next state\u001b[39;00m\n\u001b[0;32m     42\u001b[0m     next_state, reward, done, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n",
      "Cell \u001b[1;32mIn [15], line 25\u001b[0m, in \u001b[0;36mselect_action\u001b[1;34m(state, epsilon)\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandint(num_actions)\n\u001b[0;32m     23\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[39m# Exploit: choose the action with the highest Q-value\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39margmax(Q[state, :])\n",
      "\u001b[1;31mIndexError\u001b[0m: arrays used as indices must be of integer (or boolean) type"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "# Create the TSP environment with a set of cities and distances between them\n",
    "cities = ['New York', 'Chicago', 'Denver', 'San Francisco', 'Los Angeles']\n",
    "distances = np.array([[0, 2448, 1434, 1260, 2045],\n",
    "                      [2448, 0, 2546, 959, 2367],\n",
    "                      [1434, 2546, 0, 2408, 1745],\n",
    "                      [1260, 959, 2408, 0, 2295],\n",
    "                      [2045, 2367, 1745, 2295, 0]])\n",
    "env = TSPEnv(cities, distances)\n",
    "\n",
    "# Initialize the Q-table\n",
    "num_states = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.n\n",
    "Q = np.zeros((num_states, num_actions))\n",
    "\n",
    "# Define the action selection function\n",
    "def select_action(state, epsilon=0.1):\n",
    "    if np.random.rand() < epsilon:\n",
    "        # Explore: choose a random action\n",
    "        return np.random.randint(num_actions)\n",
    "    else:\n",
    "        # Exploit: choose the action with the highest Q-value\n",
    "        return np.argmax(Q[state, :])\n",
    "\n",
    "# Define the Q-learning update function\n",
    "def update_Q(state, action, reward, next_state, alpha=0.1, gamma=0.9):\n",
    "    Q[state, action] = (1 - alpha) * Q[state, action] + alpha * (reward + gamma * np.max(Q[next_state, :]))\n",
    "\n",
    "# Run the TSP loop\n",
    "num_episodes = 1000\n",
    "for episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    state = env.reset()\n",
    "\n",
    "    while True:\n",
    "        # Select an action using the action selection function\n",
    "        action = select_action(state)\n",
    "\n",
    "        # Take the action and observe the reward and next state\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # Update the Q-table using the Q-learning update function\n",
    "        update_Q(state, action, reward, next_state)\n",
    "\n",
    "        # Update the state to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # If the TSP has been completed, break out of the inner loop\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "# Print the final Q-table\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "class TSPEnv(gym.Env):\n",
    "    def __init__(self, distances):\n",
    "        # distances is a 4x4 matrix of the distances between each pair of cities\n",
    "        self.distances = distances\n",
    "        self.state = 0  # current city\n",
    "        self.done = False  # whether the episode is finished\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = 0\n",
    "        self.done = False\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        # action is the index of the city to move to\n",
    "        if self.done:\n",
    "            raise ValueError(\"Episode is finished\")\n",
    "        reward = -self.distances[self.state, action]  # negative of the distance traveled\n",
    "        self.state = action\n",
    "        if self.state == 0:  # returned to starting city\n",
    "            self.done = True\n",
    "        return self.state, reward, self.done, {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Keras symbolic inputs/outputs do not implement `__len__`. You may be trying to pass Keras symbolic inputs/outputs to a TF API that does not register dispatching, preventing Keras from automatically converting the API call to a lambda layer in the Functional Model. This error will also get raised if you try asserting a symbolic input/output directly.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [22], line 25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39m# define the reinforcement learning agent\u001b[39;00m\n\u001b[0;32m     24\u001b[0m policy \u001b[39m=\u001b[39m BoltzmannQPolicy()  \u001b[39m# Boltzmann policy with exploration\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m agent \u001b[39m=\u001b[39m DQNAgent(model\u001b[39m=\u001b[39;49mmodel, nb_actions\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m, memory\u001b[39m=\u001b[39;49mmemory, policy\u001b[39m=\u001b[39;49mpolicy)\n\u001b[0;32m     27\u001b[0m \u001b[39m# compile the agent\u001b[39;00m\n\u001b[0;32m     28\u001b[0m agent\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mmae\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\fabian.woellenweber\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\rl\\agents\\dqn.py:108\u001b[0m, in \u001b[0;36mDQNAgent.__init__\u001b[1;34m(self, model, policy, test_policy, enable_double_dqn, enable_dueling_network, dueling_type, *args, **kwargs)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[39msuper\u001b[39m(DQNAgent, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    107\u001b[0m \u001b[39m# Validate (important) input.\u001b[39;00m\n\u001b[1;32m--> 108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(model\u001b[39m.\u001b[39moutput, \u001b[39m'\u001b[39m\u001b[39m__len__\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39;49m(model\u001b[39m.\u001b[39;49moutput) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    109\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mModel \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m has more than one output. DQN expects a model that has a single output.\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(model))\n\u001b[0;32m    110\u001b[0m \u001b[39mif\u001b[39;00m model\u001b[39m.\u001b[39moutput\u001b[39m.\u001b[39m_keras_shape \u001b[39m!=\u001b[39m (\u001b[39mNone\u001b[39;00m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnb_actions):\n",
      "File \u001b[1;32mc:\\Users\\fabian.woellenweber\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\keras_tensor.py:244\u001b[0m, in \u001b[0;36mKerasTensor.__len__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__len__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 244\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m    245\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mKeras symbolic inputs/outputs do not \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    246\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mimplement `__len__`. You may be \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    247\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtrying to pass Keras symbolic inputs/outputs \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    248\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mto a TF API that does not register dispatching, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    249\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpreventing Keras from automatically \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    250\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mconverting the API call to a lambda layer \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    251\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39min the Functional Model. This error will also get raised \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    252\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mif you try asserting a symbolic input/output directly.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    253\u001b[0m     )\n",
      "\u001b[1;31mTypeError\u001b[0m: Keras symbolic inputs/outputs do not implement `__len__`. You may be trying to pass Keras symbolic inputs/outputs to a TF API that does not register dispatching, preventing Keras from automatically converting the API call to a lambda layer in the Functional Model. This error will also get raised if you try asserting a symbolic input/output directly."
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "\n",
    "# define the distances between the cities\n",
    "distances = np.array([[0, 10, 15, 20],\n",
    "                      [10, 0, 35, 25],\n",
    "                      [15, 35, 0, 30],\n",
    "                      [20, 25, 30, 0]])\n",
    "\n",
    "# create the environment\n",
    "env = TSPEnv(distances)\n",
    "\n",
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=1, activation='relu'))  # input is the current city\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(4, activation='linear'))  # output is a distribution over the actions (cities)\n",
    "\n",
    "# define the reinforcement learning agent\n",
    "policy = BoltzmannQPolicy()  # Boltzmann policy with exploration\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "agent = DQNAgent(model=model, nb_actions=4, memory=memory, policy=policy)\n",
    "\n",
    "# compile the agent\n",
    "agent.compile(optimizer='adam', metrics=['mae'])\n",
    "\n",
    "# fit the agent to the environment\n",
    "agent.fit(env, nb_steps=10000, visualize=False, verbose=2)\n",
    "\n",
    "# test the agent\n",
    "state = env.reset()\n",
    "for i in range(3):\n",
    "    action, _, _ = agent.forward(state)\n",
    "    state, reward, done, _ = env.step(np.argmax(action))\n",
    "    print(f\"State: {state}, Reward: {reward}, Done: {done}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "AbstractDQNAgent.__init__() missing 2 required positional arguments: 'nb_actions' and 'memory'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [23], line 25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39m# define the reinforcement learning agent\u001b[39;00m\n\u001b[0;32m     24\u001b[0m policy \u001b[39m=\u001b[39m BoltzmannQPolicy()  \u001b[39m# Boltzmann policy with exploration\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m agent \u001b[39m=\u001b[39m DQNAgent(model\u001b[39m=\u001b[39;49mmodel, policy\u001b[39m=\u001b[39;49mpolicy)\n\u001b[0;32m     27\u001b[0m \u001b[39m# compile the agent\u001b[39;00m\n\u001b[0;32m     28\u001b[0m agent\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mmae\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\fabian.woellenweber\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\rl\\agents\\dqn.py:105\u001b[0m, in \u001b[0;36mDQNAgent.__init__\u001b[1;34m(self, model, policy, test_policy, enable_double_dqn, enable_dueling_network, dueling_type, *args, **kwargs)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, model, policy\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, test_policy\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, enable_double_dqn\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, enable_dueling_network\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    104\u001b[0m              dueling_type\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mavg\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 105\u001b[0m     \u001b[39msuper\u001b[39m(DQNAgent, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    107\u001b[0m     \u001b[39m# Validate (important) input.\u001b[39;00m\n\u001b[0;32m    108\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(model\u001b[39m.\u001b[39moutput, \u001b[39m'\u001b[39m\u001b[39m__len__\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(model\u001b[39m.\u001b[39moutput) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "\u001b[1;31mTypeError\u001b[0m: AbstractDQNAgent.__init__() missing 2 required positional arguments: 'nb_actions' and 'memory'"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "\n",
    "# define the distances between the cities\n",
    "distances = np.array([[0, 10, 15, 20],\n",
    "                      [10, 0, 35, 25],\n",
    "                      [15, 35, 0, 30],\n",
    "                      [20, 25, 30, 0]])\n",
    "\n",
    "# create the environment\n",
    "env = TSPEnv(distances)\n",
    "\n",
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=1, activation='relu'))  # input is the current city\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(4, activation='linear'))  # output is a distribution over the actions (cities)\n",
    "\n",
    "# define the reinforcement learning agent\n",
    "policy = BoltzmannQPolicy()  # Boltzmann policy with exploration\n",
    "agent = DQNAgent(model=model, policy=policy)\n",
    "\n",
    "# compile the agent\n",
    "agent.compile(optimizer='adam', metrics=['mae'])\n",
    "\n",
    "# fit the agent to the environment\n",
    "agent.fit(env, nb_steps=10000, visualize=False, verbose=2)\n",
    "\n",
    "# test the agent\n",
    "state = env.reset()\n",
    "for i in range(3):\n",
    "    action, _, _ = agent.forward(state)\n",
    "    state, reward, done, _ = env.step(np.argmax(action))\n",
    "    print(f\"State: {state}, Reward: {reward}, Done: {done}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5a312273ffa6240539e020f48a5de0b17350fba060a017bafd7c66c880b86767"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
