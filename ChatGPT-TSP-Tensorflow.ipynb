{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Agent' from 'rl.agents' (c:\\Users\\fabian.woellenweber\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\rl\\agents\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [27], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m Sequential\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m \u001b[39mimport\u001b[39;00m Dense\n\u001b[1;32m----> 4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mrl\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magents\u001b[39;00m \u001b[39mimport\u001b[39;00m Agent\n\u001b[0;32m      6\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mTSPAgent\u001b[39;00m(Agent):\n\u001b[0;32m      7\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, model, nb_actions, memory, policy\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, test_policy\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, enable_double_dqn\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, enable_dueling_network\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, dueling_type\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mavg\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'Agent' from 'rl.agents' (c:\\Users\\fabian.woellenweber\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\rl\\agents\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from rl.agents import \n",
    "\n",
    "class TSPAgent(Agent):\n",
    "    def __init__(self, model, nb_actions, memory, policy=None, test_policy=None, enable_double_dqn=True, enable_dueling_network=False, dueling_type='avg', *args, **kwargs):\n",
    "        super(TSPAgent, self).__init__(*args, **kwargs)\n",
    "        self.model = model\n",
    "        self.nb_actions = nb_actions\n",
    "        self.memory = memory\n",
    "        self.policy = policy\n",
    "        self.test_policy = test_policy\n",
    "        self.enable_double_dqn = enable_double_dqn\n",
    "        self.enable_dueling_network = enable_dueling_network\n",
    "        self.dueling_type = dueling_type\n",
    "\n",
    "    def forward(self, observation):\n",
    "        # predict the action distribution from the model\n",
    "        q_values = self.model.predict(observation)\n",
    "        action = np.argmax(q_values[0])  # choose the action with the highest Q-value\n",
    "        return action, q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "class TSPEnv(gym.Env):\n",
    "    def __init__(self, cities, distances):\n",
    "        self.cities = cities\n",
    "        self.distances = distances\n",
    "        self.num_cities = len(cities)\n",
    "        self.action_space = gym.spaces.Discrete(self.num_cities)\n",
    "        self.observation_space = gym.spaces.Box(0, self.num_cities, shape=(self.num_cities,))\n",
    "        print(self.observation_space)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = np.zeros(self.num_cities)\n",
    "        print(self.state)\n",
    "        self.state[0] = 1\n",
    "        self.visited = [0]\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action):\n",
    "        self.state[action] = 1\n",
    "        if(action not in self.visited):\n",
    "            self.visited.append(action)\n",
    "        reward = -self.distances[self.visited[-2], action]\n",
    "        done = len(self.visited) == self.num_cities\n",
    "        return self.state, reward, done, {}\n",
    "\n",
    "\n",
    "\n",
    "# print(state)  # should print [1, 1, 0, 0, 0]\n",
    "# print(reward) # should print -2448\n",
    "# print(done)   # should print False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(0.0, 5.0, (5,), float32)\n",
      "[0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cities = ['New York', 'Chicago', 'Denver', 'San Francisco', 'Los Angeles']\n",
    "distances = np.array([[0, 2448, 1434, 1260, 2045],\n",
    "                      [2448, 0, 2546, 959, 2367],\n",
    "                      [1434, 2546, 0, 2408, 1745],\n",
    "                      [1260, 959, 2408, 0, 2295],\n",
    "                      [2045, 2367, 1745, 2295, 0]])\n",
    "\n",
    "                      \n",
    "env = TSPEnv(cities, distances)\n",
    "env.reset()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Error converting shape to a TensorShape: Dimension value must be integer or None or have an __index__ method, got value 'Box(0.0, 5.0, (5,), float32)' with type '<class 'gym.spaces.box.Box'>'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [4], line 81\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[39mreturn\u001b[39;00m agent\n\u001b[0;32m     80\u001b[0m \u001b[39m# Create the TSP model\u001b[39;00m\n\u001b[1;32m---> 81\u001b[0m model \u001b[39m=\u001b[39m build_model(state_size, action_size)\n\u001b[0;32m     82\u001b[0m model\u001b[39m.\u001b[39moutput\n\u001b[0;32m     83\u001b[0m \u001b[39m# Create the TSP agent\u001b[39;00m\n",
      "Cell \u001b[1;32mIn [4], line 61\u001b[0m, in \u001b[0;36mbuild_model\u001b[1;34m(state_size, action_size)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbuild_model\u001b[39m(state_size, action_size):\n\u001b[0;32m     60\u001b[0m     model \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mSequential()\n\u001b[1;32m---> 61\u001b[0m     model\u001b[39m.\u001b[39;49madd(Dense(\u001b[39m256\u001b[39;49m, input_dim\u001b[39m=\u001b[39;49mstate_size, activation\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mrelu\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[0;32m     62\u001b[0m     model\u001b[39m.\u001b[39madd(Dense(\u001b[39m256\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m     63\u001b[0m     model\u001b[39m.\u001b[39madd(Dense(action_size, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlinear\u001b[39m\u001b[39m'\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\fabian.woellenweber\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\trackable\\base.py:205\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m   result \u001b[39m=\u001b[39m method(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    206\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m previous_value  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\fabian.woellenweber\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\fabian.woellenweber\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:204\u001b[0m, in \u001b[0;36mmake_shape\u001b[1;34m(v, arg_name)\u001b[0m\n\u001b[0;32m    202\u001b[0m   shape \u001b[39m=\u001b[39m tensor_shape\u001b[39m.\u001b[39mas_shape(v)\n\u001b[0;32m    203\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m--> 204\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mError converting \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m to a TensorShape: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (arg_name, e))\n\u001b[0;32m    205\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    206\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mError converting \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m to a TensorShape: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m    207\u001b[0m                    (arg_name, e))\n",
      "\u001b[1;31mTypeError\u001b[0m: Error converting shape to a TensorShape: Dimension value must be integer or None or have an __index__ method, got value 'Box(0.0, 5.0, (5,), float32)' with type '<class 'gym.spaces.box.Box'>'."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from rl.agents import DQNAgent, DDPGAgent, ContinuousDQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy, BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from tensorflow.keras.layers import Lambda\n",
    "\n",
    "\n",
    "# Learning factors...\n",
    "EPISODES = 50\n",
    "discount_factor = 0.99\n",
    "learning_rate = 0.001\n",
    "epsilon_start = 1.0\n",
    "epsilon_decay_steps = 0.999\n",
    "epsilon_end = 0.01\n",
    "batch_size = 64\n",
    "train_start = 1000\n",
    "\n",
    "# create replay memory using deque\n",
    "#memory = deque(maxlen=2000)\n",
    "\n",
    "# Create gym environment\n",
    "# Create the TSP environment with a set of cities and distances between them\n",
    "cities = ['New York', 'Chicago', 'Denver', 'San Francisco', 'Los Angeles']\n",
    "distances = np.array([[0, 2448, 1434, 1260, 2045],\n",
    "                      [2448, 0, 2546, 959, 2367],\n",
    "                      [1434, 2546, 0, 2408, 1745],\n",
    "                      [1260, 959, 2408, 0, 2295],\n",
    "                      [2045, 2367, 1745, 2295, 0]])\n",
    "\n",
    "env = TSPEnv(cities, distances)\n",
    "\n",
    "# Reset the environment and take an action\n",
    "state = env.reset()\n",
    "state, reward, done, _ = env.step(2)\n",
    "\n",
    "state_size = env.observation_space\n",
    "action_size = env.action_space\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "loss_fn = tf.keras.losses.mean_squared_error\n",
    "\n",
    "# Create a replay buffer to store transitions\n",
    "replay_buffer = []\n",
    "\n",
    "# Define the update frequency and batch size\n",
    "update_frequency = 10\n",
    "batch_size = 10\n",
    "\n",
    "# The following function creates a neural network which is used as an \n",
    "# approximate Q function\n",
    "# Input: state \n",
    "# Output: Q Value of each action\n",
    "def build_model(state_size, action_size):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(Dense(256, input_dim=state_size, activation='relu'))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(action_size, activation='linear'))\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "# Create the TSP agent\n",
    "def build_agent(model, action_size):\n",
    "    # Use Epsilon-Greedy policy for exploration\n",
    "    policy = BoltzmannQPolicy()\n",
    "    # Create memory for storing transitions\n",
    "    memory = SequentialMemory(limit=50000, window_length=1)\n",
    "    # Create the DQN agent\n",
    "    agent = DQNAgent(model, memory=memory, policy=policy, nb_actions=action_size, nb_steps_warmup=10, target_model_update=1e-2)\n",
    "    \n",
    "    return agent\n",
    "\n",
    "\n",
    "\n",
    "# Create the TSP model\n",
    "model = build_model(state_size, action_size)\n",
    "model.output\n",
    "# Create the TSP agent\n",
    "agent = build_agent(model, action_size)\n",
    "# Train the agent\n",
    "agent.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "agent.fit(env, nb_steps=5000, visualize=False, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "## Initialize important variables\n",
    "env_name = 'TSPEnv'\n",
    "bRender = False\n",
    "load_model = False\n",
    "\n",
    "# Learning factors...\n",
    "EPISODES = 99\n",
    "discount_factor = 0.99\n",
    "learning_rate = 0.001\n",
    "epsilon_start = 1.0\n",
    "epsilon_decay_steps = 0.999\n",
    "epsilon_end = 0.01\n",
    "batch_size = 64\n",
    "train_start = 1000\n",
    "\n",
    "# create replay memory using deque\n",
    "#memory = deque(maxlen=2000)\n",
    "\n",
    "# Create gym environment\n",
    "env = TSPEnvironment(EPISODES)\n",
    "\n",
    "state_size = env.observation_space.n\n",
    "action_size = env.action_space.n\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "loss_fn = tf.keras.losses.mean_squared_error\n",
    "\n",
    "# Create a replay buffer to store transitions\n",
    "replay_buffer = []\n",
    "\n",
    "# Define the update frequency and batch size\n",
    "update_frequency = 10\n",
    "batch_size = 10\n",
    "\n",
    "# The following function creates a neural network which is used as an \n",
    "# approximate Q function\n",
    "# Input: state \n",
    "# Output: Q Value of each action\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, input_dim=state_size, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(32, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(action_size, activation='linear', kernel_initializer='he_uniform'))\n",
    "    model.summary()\n",
    "    model.compile(loss='mse', optimizer=Adam(learning_rate=learning_rate))\n",
    "    return model\n",
    "\n",
    "# after some time interval update the target model to be same with model\n",
    "# This is done using the following function\n",
    "def update_target_model():\n",
    "    target_model.set_weights(model.get_weights())\n",
    "\n",
    "# create main model and target model\n",
    "model = build_model()\n",
    "target_model = build_model()\n",
    "\n",
    "# We call that function in the beginning to make sure model and target_model have the same weights initally\n",
    "update_target_model()\n",
    "\n",
    "# Run the TSP loop\n",
    "num_episodes = 50\n",
    "for episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    state = env.reset()\n",
    "\n",
    "    # Calculate the epsilon for this episode\n",
    "    epsilon = epsilon_end + (epsilon_start - epsilon_end) * np.exp(-episode / epsilon_decay_steps)\n",
    "\n",
    "    while True:\n",
    "        # Select an action using the DQN model and an epsilon-greedy policy\n",
    "        if np.random.rand() < epsilon:\n",
    "            # Explore: choose a random action\n",
    "            action = np.random.randint(env.action_space.n)\n",
    "        else:\n",
    "            # Exploit: choose the action with the highest Q-value\n",
    "            action = np.argmax(model(np.expand_dims(state, axis=0)).numpy())\n",
    "\n",
    "        # Take the action and observe the reward and next state\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # Store the transition in the replay buffer\n",
    "        replay_buffer.append((state, action, reward, next_state))\n",
    "\n",
    "        # If the replay buffer has reached a certain size, sample a batch of transitions and update the model\n",
    "        if len(replay_buffer) >= batch_size:\n",
    "            # Sample a batch of transitions\n",
    "            transitions = np.random.sample(replay_buffer,batch_size)\n",
    "            states, actions, rewards, next_states = zip(*transitions)\n",
    "\n",
    "            # Convert the states and next_states to tensors\n",
    "            states = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "            next_states = tf.convert_to_tensor(next_states, dtype=tf.float32)\n",
    "            \n",
    "            print(f'States: {states}')\n",
    "            print(f'States: {next_states}')\n",
    "\n",
    "            # Compute the Q-values for the current states and the next states\n",
    "            current_q_values = model(states)\n",
    "            next_q_values = target_model(next_states)\n",
    "\n",
    "            # Compute the expected Q-values\n",
    "            expected_q_values = rewards + np.max(next_q_values.numpy(), axis=1)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = loss_fn(expected_q_values, current_q_values)\n",
    "\n",
    "            # Update the model\n",
    "            optimizer.minimize(loss, model.trainable_variables)\n",
    "\n",
    "            # Clear the replay buffer\n",
    "            replay_buffer = []\n",
    "\n",
    "        # Update the target model every `update_frequency` steps\n",
    "        if episode % update_frequency == 0:\n",
    "            update_target_model()\n",
    "\n",
    "        # Update the state\n",
    "        state = next_state\n",
    "\n",
    "        # If the episode is done, break out of the inner loop\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5a312273ffa6240539e020f48a5de0b17350fba060a017bafd7c66c880b86767"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
