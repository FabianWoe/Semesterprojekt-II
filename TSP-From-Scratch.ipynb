{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabian.woellenweber\\AppData\\Local\\Temp\\ipykernel_27520\\1645299263.py:11: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
      "  plt.style.use(\"seaborn-dark\")\n"
     ]
    }
   ],
   "source": [
    "## Create the TSP Environment\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "import pygame\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.collections import PatchCollection\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"seaborn-dark\")\n",
    "\n",
    "class TSPEnvironment(gym.Env):\n",
    "    def __init__ (self, n_stops = 100, version=True, debugInfo=False):\n",
    "        print(f\"TSP-Environment initialized with {n_stops} random stops\")\n",
    "\n",
    "        #True (V1) = Discrete Space / False = Array space\n",
    "        self.version = version\n",
    "        self._debugInfo = debugInfo\n",
    "\n",
    "        # Initialization\n",
    "        #Number of stops\n",
    "        self.n_stops = n_stops\n",
    "        #Coordinates of stops\n",
    "        self.xy = []\n",
    "        self._visitedStops = []\n",
    "        self._notVisitedStops = list(range(0,self.n_stops))\n",
    "        \n",
    "        #if(self.version):\n",
    "        \n",
    "        #self.action_space = self._notVisitedStops\n",
    "        self.episode_length = 0\n",
    "        self.step_count = 0\n",
    "        self.distances = np.array\n",
    "\n",
    "        #set starting point (state)\n",
    "        \n",
    "        self.array_visitedStops = np.zeros(n_stops)\n",
    "\n",
    "        #Generate stops\n",
    "        self._generate_stops()\n",
    "        #self._generate_q_values()\n",
    "\n",
    "        self.action_space = spaces.Discrete(n_stops)\n",
    "        #else:\n",
    "            #self.action_space = spaces.Box(np.array(range(0,self.n_stops))\n",
    "            \n",
    "        \n",
    "        self.array1 = np.ones(n_stops)\n",
    "\n",
    "\n",
    "        high = np.array(\n",
    "            [\n",
    "                np.zeros(4),\n",
    "                np.zeros(4),\n",
    "            ],\n",
    "            dtype=np.int32,\n",
    "        )\n",
    "        \n",
    "        if(self.version):\n",
    "            self.observation_space = spaces.Box(-high, high, dtype=np.int32)\n",
    "        else:\n",
    "            self.observation_space = spaces.MultiBinary(self.n_stops)\n",
    "\n",
    "\n",
    "    def _generate_stops(self):\n",
    "        self.xy = (np.random.rand(self.n_stops,2)*100).round(2)\n",
    "        self.x=self.xy[:,0]\n",
    "        self.y=self.xy[:,1]\n",
    "\n",
    "        #print(f'genrated stops xy: {self.xy}')\n",
    "        self.distances = cdist(self.xy,self.xy,'euclidean').round(0)\n",
    "        \n",
    "        #pick random StartPoint\n",
    "        self._visitedStops.append(np.random.randint(0,self.n_stops))\n",
    "        self._debugInfo\n",
    "        print(f'Starting Point: {self._visitedStops}')\n",
    "\n",
    "\n",
    "    #return stops[-1]\n",
    "    #Gibt die aktuelle Position des Agenten zurÃ¼ck\n",
    "    def _get_state(self):\n",
    "        if( len(self._visitedStops)>0):\n",
    "            return self._visitedStops[-1]\n",
    "        else:\n",
    "            return 0 \n",
    "\n",
    "\n",
    "    def render(self):\n",
    "        pass\n",
    "\n",
    "    #Resets StartingPoint\n",
    "    def reset(self):\n",
    "        self._visitedStops.clear()\n",
    "        self.array_visitedStops = np.zeros(self.n_stops)\n",
    "        #self._notVisitedStops = list(range(0,self.n_stops))\n",
    "\n",
    "        first_stop = np.random.randint(self.n_stops)\n",
    "        #self._notVisitedStops.remove(first_stop)\n",
    "        self._visitedStops.append(first_stop)\n",
    "        self.array_visitedStops[first_stop] = True\n",
    "        self.step_count = 1\n",
    "\n",
    "        if(self.version):\n",
    "            return self._get_state()\n",
    "        else:\n",
    "            return np.array(self.xy), {}\n",
    "\n",
    "    def step(self,destination):\n",
    "        done = False\n",
    "        self.step_count +=1\n",
    "        reward = -self.n_stops*1000\n",
    "\n",
    "        self.episode_length += 1\n",
    "        if(self.episode_length < 1000):\n",
    "            if(np.random.rand(1,1) < 0.1):\n",
    "                destination = np.random.randint(0,self.n_stops)\n",
    "\n",
    "        #Validize Step\n",
    "        if(self._get_state() != destination & destination not in self._visitedStops):\n",
    "            #Get reward for such a move\n",
    "            reward = -self.distances[self._get_state(), destination]\n",
    "            \n",
    "            # Append state (new position)\n",
    "            self._visitedStops.append(destination)\n",
    "            self.array_visitedStops[destination] = True\n",
    "\n",
    "        if(self._debugInfo):\n",
    "            print(f'State in step: {self._get_state()}')\n",
    "            print(f'Destination in step: {destination}')\n",
    "            print(f'Length visited stops: {len(self._visitedStops)}')\n",
    "            print(f'Visited Stops in step: {self._visitedStops}')\n",
    "            print(f'Reward in step: {reward}')\n",
    "            print(f'Stepcounter: {self.step_count}')\n",
    "        \n",
    "        #exploration = np.random.random_sample()\n",
    "        #print(f'Exploration: {exploration}')\n",
    "        #if(exploration >= 0.05):\n",
    "            #destination = np.random.randint(self.action_space.n)\n",
    "            #print(f'Random Destination: {destination}')\n",
    "\n",
    "        # if(self.step_count >= self.n_stops*5):\n",
    "        #     done = True\n",
    "        #     reward = -2000000\n",
    "        #     if(self._debugInfo):\n",
    "        #         print('Too much steps')\n",
    "\n",
    "        if(len(self._visitedStops) == self.n_stops):\n",
    "            if(len(self._visitedStops) <= self.step_count*2):\n",
    "                reward += 500\n",
    "            reward += 200    \n",
    "            done = True\n",
    "            print(f'Done = True')\n",
    "            print(f'Length visited stops: {len(self._visitedStops)}')\n",
    "            print(f'Visited Stops in step: {self._visitedStops}')\n",
    "            \n",
    "        info = {}\n",
    "\n",
    "        #print(f'Agent position: {self._get_state()}')\n",
    "\n",
    "        if(self.version):\n",
    "            return self._get_state(), reward, done, {}\n",
    "        else:\n",
    "            return np.array(self.array_visitedStops, reward, done, {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSP-Environment initialized with 4 random stops\n",
      "Starting Point: [2]\n",
      "(array([[54.57, 41.98],\n",
      "       [ 5.33, 86.36],\n",
      "       [64.55, 61.67],\n",
      "       [77.52, 62.32]]), {})\n"
     ]
    }
   ],
   "source": [
    "env = TSPEnvironment(4, False)\n",
    "print(env.reset())\n",
    "# env.step(2)\n",
    "# print(env.observation_space.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 10 \n",
    "\n",
    "env = TSPEnvironment(episodes, False)\n",
    "print(env.array_visitedStops)\n",
    "print(f'Step: {env.step(2)}')\n",
    "print(f'Step: {env.step(3)}')\n",
    "print(env.array_visitedStops.shape)\n",
    "print(f'Reset: {env.reset()}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mkeras\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[0;32m      4\u001b[0m model \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mSequential(\n\u001b[0;32m      5\u001b[0m         [\n\u001b[0;32m      6\u001b[0m     \u001b[39m#model.add(Input(shape=(3,)))\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m         ]\n\u001b[0;32m     12\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\fabian.woellenweber\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\__init__.py:20\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39m\"\"\"Implementation of the Keras API, the high-level API of TensorFlow.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[39mDetailed documentation and user guides are available at\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[39m[keras.io](https://keras.io).\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m distribute\n\u001b[0;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m models\n\u001b[0;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minput_layer\u001b[39;00m \u001b[39mimport\u001b[39;00m Input\n",
      "File \u001b[1;32mc:\\Users\\fabian.woellenweber\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\distribute\\__init__.py:18\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39m\"\"\"Keras' Distribution Strategy library.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistribute\u001b[39;00m \u001b[39mimport\u001b[39;00m sidecar_evaluator\n",
      "File \u001b[1;32mc:\\Users\\fabian.woellenweber\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\distribute\\sidecar_evaluator.py:17\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39m\"\"\"Python module for evaluation loop.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[39m# isort: off\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mplatform\u001b[39;00m \u001b[39mimport\u001b[39;00m tf_logging \u001b[39mas\u001b[39;00m logging\n",
      "File \u001b[1;32mc:\\Users\\fabian.woellenweber\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\__init__.py:37\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_sys\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_typing\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtools\u001b[39;00m \u001b[39mimport\u001b[39;00m module_util \u001b[39mas\u001b[39;00m _module_util\n\u001b[0;32m     38\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlazy_loader\u001b[39;00m \u001b[39mimport\u001b[39;00m LazyLoader \u001b[39mas\u001b[39;00m _LazyLoader\n\u001b[0;32m     40\u001b[0m \u001b[39m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\fabian.woellenweber\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\__init__.py:36\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtraceback\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[39m# We aim to keep this file minimal and ideally remove completely.\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[39m# If you are adding a new file with @tf_export decorators,\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[39m# import it in modules_with_exports.py instead.\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \n\u001b[0;32m     33\u001b[0m \u001b[39m# go/tf-wildcard-import\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[39m# pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m pywrap_tensorflow \u001b[39mas\u001b[39;00m _pywrap_tensorflow\n\u001b[0;32m     37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39meager\u001b[39;00m \u001b[39mimport\u001b[39;00m context\n\u001b[0;32m     39\u001b[0m \u001b[39m# pylint: enable=wildcard-import\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \n\u001b[0;32m     41\u001b[0m \u001b[39m# Bring in subpackages.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\fabian.woellenweber\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtraceback\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mplatform\u001b[39;00m \u001b[39mimport\u001b[39;00m self_check\n\u001b[0;32m     23\u001b[0m \u001b[39m# TODO(mdan): Cleanup antipattern: import for side effects.\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \n\u001b[0;32m     25\u001b[0m \u001b[39m# Perform pre-load sanity checks in order to produce a more actionable error.\u001b[39;00m\n\u001b[0;32m     26\u001b[0m self_check\u001b[39m.\u001b[39mpreload_check()\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:879\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:975\u001b[0m, in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1074\u001b[0m, in \u001b[0;36mget_data\u001b[1;34m(self, path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.Sequential(\n",
    "        [\n",
    "    #model.add(Input(shape=(3,)))\n",
    "    \n",
    "            keras.layers.Dense(512 , activation='relu', kernel_initializer='he_uniform'),\n",
    "            keras.layers.Dense(256 ,activation='relu', kernel_initializer='he_uniform'),\n",
    "            keras.layers.Dense(1, activation='linear', kernel_initializer='he_uniform'),\n",
    "        ]\n",
    ")\n",
    "    \n",
    "x = tf.ones((1,3))\n",
    "y = model(x)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 10 \n",
    "\n",
    "env = TSPEnvironment(episodes)\n",
    "\n",
    "\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    Loops = 0\n",
    "    \n",
    "\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score+=reward\n",
    "        Loops += 1\n",
    "    print('Epsiode: {} Score: {} Episodes: {}'.format(episode,score.round(2),Loops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "class TSPAgent:\n",
    "  def __init__(self, n_stops):\n",
    "    self.n_stops = n_stops\n",
    "    self.model = self._build_model()\n",
    "    \n",
    "  # def _build_model(self):\n",
    "  #   model = tf.keras.Sequential()\n",
    "  #   # Add layers to your model\n",
    "  #   model.add(tf.keras.layers.Dense(units=64, activation='relu', input_shape=(self.n_stops,), kernel_initializer='he_uniform'))\n",
    "  #   model.add(tf.keras.layers.Dense(units=32, activation='relu', kernel_initializer='he_uniform'))\n",
    "  #   model.add(tf.keras.layers.Dense(units=1, activation='linear', kernel_initializer='he_uniform'))\n",
    "  #   return model\n",
    "\n",
    "  \n",
    "\n",
    "    x = tf.ones((1,3))\n",
    "    model(x)\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "    \n",
    "  def act(self, state):\n",
    "    # Convert the state from an integer to an array\n",
    "    state = np.array([state])\n",
    "    \n",
    "    # Reshape the state array to have the correct shape\n",
    "    # state = state.reshape(-1, self.n_stops)\n",
    "    \n",
    "    # Use the model to predict the action given the state\n",
    "    action = self.model.predict(state)\n",
    "    return action\n",
    "\n",
    "    \n",
    "  def train(self, state, action, reward, done):\n",
    "    # Use the reward and next state to update the model\n",
    "    self.model.fit(state, action, reward, done)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Set hyperparameters\n",
    "learning_rate = 0.001\n",
    "n_episodes = 1000\n",
    "\n",
    "# Create the TSP environment and agent\n",
    "env = TSPEnvironment(50)\n",
    "agent = TSPAgent(n_stops=100)\n",
    "\n",
    "# Set the optimizer for the agent\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for episode in range(n_episodes):\n",
    "  # Reset the environment at the beginning of each episode\n",
    "  state = env.reset()\n",
    "  done = False\n",
    "  \n",
    "  while not done:\n",
    "    # Have the agent act on the current state\n",
    "    #action = agent.act(state)\n",
    "    \n",
    "    # Take a step in the environment\n",
    "    #next_state, reward, done, _ = env.step(action)\n",
    "    \n",
    "    # Train the agent\n",
    "    agent.train(state, action, reward, done)\n",
    "    \n",
    "    # Update the current state\n",
    "    #state = next_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSP-Environment initialized with 10 random stops\n",
      "Starting Point: [1]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"sequential\" is incompatible with the layer: expected shape=(None, 4), found shape=(1, 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [20], line 63\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[39mreturn\u001b[39;00m agent\n\u001b[0;32m     62\u001b[0m \u001b[39m# Create the TSP model\u001b[39;00m\n\u001b[1;32m---> 63\u001b[0m model \u001b[39m=\u001b[39m build_model(state_size, action_size)\n\u001b[0;32m     64\u001b[0m \u001b[39m# Create the TSP agent\u001b[39;00m\n\u001b[0;32m     65\u001b[0m agent \u001b[39m=\u001b[39m build_agent(model, action_size)\n",
      "Cell \u001b[1;32mIn [20], line 43\u001b[0m, in \u001b[0;36mbuild_model\u001b[1;34m(state_size, action_size)\u001b[0m\n\u001b[0;32m     32\u001b[0m model \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mSequential(\n\u001b[0;32m     33\u001b[0m     [\n\u001b[0;32m     34\u001b[0m \u001b[39m#model.add(Input(shape=(3,)))\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     39\u001b[0m     ]\n\u001b[0;32m     40\u001b[0m )\n\u001b[0;32m     42\u001b[0m x \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mones((\u001b[39m1\u001b[39m,\u001b[39m3\u001b[39m))\n\u001b[1;32m---> 43\u001b[0m model(x)\n\u001b[0;32m     44\u001b[0m model\u001b[39m.\u001b[39msummary()\n\u001b[0;32m     45\u001b[0m model\u001b[39m.\u001b[39mcompile(loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmse\u001b[39m\u001b[39m'\u001b[39m, optimizer\u001b[39m=\u001b[39mAdam(learning_rate\u001b[39m=\u001b[39mlearning_rate))\n",
      "File \u001b[1;32mc:\\Users\\fabian.woellenweber\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\fabian.woellenweber\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\input_spec.py:295\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[39mif\u001b[39;00m spec_dim \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m dim \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    294\u001b[0m     \u001b[39mif\u001b[39;00m spec_dim \u001b[39m!=\u001b[39m dim:\n\u001b[1;32m--> 295\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    296\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mInput \u001b[39m\u001b[39m{\u001b[39;00minput_index\u001b[39m}\u001b[39;00m\u001b[39m of layer \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mlayer_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m is \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    297\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mincompatible with the layer: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    298\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mexpected shape=\u001b[39m\u001b[39m{\u001b[39;00mspec\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    299\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfound shape=\u001b[39m\u001b[39m{\u001b[39;00mdisplay_shape(x\u001b[39m.\u001b[39mshape)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    300\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 of layer \"sequential\" is incompatible with the layer: expected shape=(None, 4), found shape=(1, 3)"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from rl.agents import DQNAgent, DDPGAgent, ContinuousDQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy, BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from tensorflow.keras.layers import Lambda\n",
    "\n",
    "# Learning factors...\n",
    "GeneratedStops = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "# create replay memory using deque\n",
    "#memory = deque(maxlen=2000)\n",
    "\n",
    "# Create gym environment\n",
    "env = TSPEnvironment(GeneratedStops, False)\n",
    "\n",
    "\n",
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space.shape\n",
    "\n",
    "\n",
    "# The following function creates a neural network which is used as an \n",
    "# approximate Q function\n",
    "# Input: state \n",
    "# Output: Q Value of each action\n",
    "def build_model(state_size, action_size):\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "    #model.add(Input(shape=(3,)))\n",
    "    \n",
    "            keras.layers.Dense(8 , input_dim=4, activation='relu', kernel_initializer='he_uniform'),\n",
    "            keras.layers.Dense(8 ,activation='relu', kernel_initializer='he_uniform'),\n",
    "            keras.layers.Dense(action_size, activation='linear', kernel_initializer='he_uniform'),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    #x = tf.ones((1,3))\n",
    "    #model(x)\n",
    "    model.summary()\n",
    "    model.compile(loss='mse', optimizer=Adam(learning_rate=learning_rate))\n",
    "    return model\n",
    "    #input_shape=(input_dim,)\n",
    "\n",
    "# Create the TSP agent\n",
    "def build_agent(model, action_size):\n",
    "    # Use Epsilon-Greedy policy for exploration\n",
    "    policy = BoltzmannQPolicy()\n",
    "    # Create memory for storing transitions\n",
    "    memory = SequentialMemory(limit=50000, window_length=1)\n",
    "    # Create the DQN agent\n",
    "    agent = DQNAgent(model, memory=memory, policy=policy, nb_actions=action_size, nb_steps_warmup=100, target_model_update=1e-2)\n",
    "    \n",
    "    agent.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "    return agent\n",
    "\n",
    "\n",
    "# Create the TSP model\n",
    "model = build_model(state_size, action_size)\n",
    "# Create the TSP agent\n",
    "agent = build_agent(model, action_size)\n",
    "#agent = DeliveryQAgent(env.observation_space.n,env.action_space.n)\n",
    "# Train the agent\n",
    "\n",
    "\n",
    "\n",
    "#print(model.output)\n",
    "print(env.observation_space.shape[0])\n",
    "\n",
    "#agent.fit(env, nb_steps=5000, visualize=False, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores=agent.test(env, nb_episodes=1000, visualize=False)\n",
    "print(np.mean(scores.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TSPEnvironment(GeneratedStops, False)\n",
    "\n",
    "\n",
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space.shape\n",
    "\n",
    "print(action_size)\n",
    "print(state_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5a312273ffa6240539e020f48a5de0b17350fba060a017bafd7c66c880b86767"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
