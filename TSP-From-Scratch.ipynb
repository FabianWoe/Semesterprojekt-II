{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import Env\n",
    "from gym.spaces import Discrete, box\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeliveryQAgent(QAgent):\n",
    "\n",
    "    def __init__(self,*args,**kwargs):\n",
    "        super().__init__(*args,**kwargs)\n",
    "        self.reset_memory()\n",
    "\n",
    "    def act(self,s):\n",
    "\n",
    "        # Get Q Vector\n",
    "        q = np.copy(self.Q[s,:])\n",
    "\n",
    "        # Avoid already visited states\n",
    "        q[self.states_memory] = -np.inf\n",
    "\n",
    "        if np.random.rand() > self.epsilon:\n",
    "            a = np.argmax(q)\n",
    "        else:\n",
    "            a = np.random.choice([x for x in range(self.actions_size) if x not in self.states_memory])\n",
    "\n",
    "        return a\n",
    "\n",
    "\n",
    "    def remember_state(self,s):\n",
    "        self.states_memory.append(s)\n",
    "\n",
    "    def reset_memory(self):\n",
    "        self.states_memory = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabian.woellenweber\\AppData\\Local\\Temp\\ipykernel_15728\\4100043469.py:11: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
      "  plt.style.use(\"seaborn-dark\")\n"
     ]
    }
   ],
   "source": [
    "## Create the TSP Environment\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "import pygame\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.collections import PatchCollection\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"seaborn-dark\")\n",
    "\n",
    "class TSPEnvironment(gym.Env):\n",
    "    def __init__ (self, n_stops = 100):\n",
    "        print(f\"TSP-Environment initialized with {n_stops} random stops\")\n",
    "\n",
    "        # Initialization\n",
    "        #Number of stops\n",
    "        self.n_stops = n_stops\n",
    "        #Coordinates of stops\n",
    "        self.xy = []\n",
    "        \n",
    "        self.action_space = spaces.Discrete(n_stops)\n",
    "        self.observation_space = spaces.Discrete(n_stops)\n",
    "        self.episode_length = n_stops\n",
    "        self._visitedStops = []\n",
    "\n",
    "        #set starting point (state)\n",
    "        \n",
    "        #Generate stops\n",
    "        self._generate_stops()\n",
    "        self._generate_q_values()\n",
    "                \n",
    "        \n",
    "\n",
    "    def _get_obs(self):\n",
    "        return {\"agent\": self._agent_location, \"target\": self._target_location}\n",
    "\n",
    "    def _get_info(self):\n",
    "        return {\"distance\": np.linalg.normal(self._agent_location - self._target_location, ord=1)} \n",
    "\n",
    "    def _generate_stops(self):\n",
    "        self.xy = (np.random.rand(self.n_stops,2)*100).round(2)\n",
    "        self.x=self.xy[:,0]\n",
    "        self.y=self.xy[:,1]\n",
    "\n",
    "        print(f'genrated stops xy: {self.xy}')\n",
    "        \n",
    "        #pick random StartPoint\n",
    "        self._visitedStops.append(np.random.randint(0,self.n_stops))\n",
    "        print(f'Starting Point: {self._visitedStops}')\n",
    "\n",
    "\n",
    "    def _generate_q_values(self,box_size = 0.2):\n",
    "        self.q_stops = cdist(self.xy,self.xy,'euclidean').round(2)\n",
    "        print(f'Distance: \\n {self.q_stops}')\n",
    "\n",
    "\n",
    "    def render(self):\n",
    "        pass\n",
    "\n",
    "    #Resets StartingPoint\n",
    "    def reset(self):\n",
    "        self._visitedStops = []\n",
    "\n",
    "        first_stop = np.random.randint(self.n_stops)\n",
    "        self._visitedStops.append(first_stop)\n",
    "        return first_stop\n",
    "\n",
    "    def step(self,destination):\n",
    "        #Get reward for such a move\n",
    "        reward = self._get_reward(self._get_state(), destination)\n",
    "\n",
    "        #set new position of agent\n",
    "        #self.state = destination\n",
    "        self._visitedStops.append(destination)\n",
    "        print(f'Visited Stops: {self._visitedStops}')\n",
    "\n",
    "        done = len(self._visitedStops) == self.n_stops\n",
    "\n",
    "        info = {}\n",
    "\n",
    "        print(f'Agent position: {self._get_state()}')\n",
    "\n",
    "        return self._get_state(), reward, done, info\n",
    "\n",
    "\n",
    "    #return stops[-1]\n",
    "    #Gibt die aktuelle Position des Agenten zurÃ¼ck\n",
    "    def _get_state(self):\n",
    "        if( len(self._visitedStops)>0):\n",
    "            return self._visitedStops[-1]\n",
    "        else:\n",
    "            return 0 \n",
    "\n",
    "    def _get_reward(self, state, destination):\n",
    "        return self.q_stops[self._get_state(), destination]\n",
    "\n",
    "    def _get_xy(self):\n",
    "        return self.xy[self._get_state()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSP-Environment initialized with 5 random stops\n",
      "genrated stops xy: [[91.97 56.92]\n",
      " [71.08 91.64]\n",
      " [82.89 90.14]\n",
      " [95.84 45.98]\n",
      " [11.78 23.56]]\n",
      "Starting Point: [1]\n",
      "Distance: \n",
      " [[ 0.   40.52 34.44 11.6  86.85]\n",
      " [40.52  0.   11.9  51.94 90.28]\n",
      " [34.44 11.9   0.   46.02 97.41]\n",
      " [11.6  51.94 46.02  0.   87.  ]\n",
      " [86.85 90.28 97.41 87.    0.  ]]\n",
      "Visited Stops: [1, 3]\n",
      "Agent position: 3\n"
     ]
    }
   ],
   "source": [
    "episodes = 5 \n",
    "\n",
    "env = TSPEnvironment(episodes)\n",
    "action = env.action_space.sample()\n",
    "len(env._visitedStops)\n",
    "n_state, reward, done, info = env.step(action)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSP-Environment initialized with 10 random stops\n",
      "genrated stops xy: [[78.73 61.46]\n",
      " [53.33 80.51]\n",
      " [36.27  4.91]\n",
      " [34.98 26.58]\n",
      " [ 1.62 49.81]\n",
      " [76.22 40.21]\n",
      " [83.39 34.32]\n",
      " [34.19 69.32]\n",
      " [95.21 63.72]\n",
      " [84.87 24.43]]\n",
      "Starting Point: [6]\n",
      " [[ 0.   31.75 70.72 55.95 77.99 21.4  27.54 45.23 16.63 37.54]\n",
      " [31.75  0.   77.5  56.97 60.14 46.35 55.11 22.17 45.12 64.34]\n",
      " [70.72 77.5   0.   21.71 56.72 53.31 55.54 64.44 83.26 52.37]\n",
      " [55.95 56.97 21.71  0.   40.65 43.43 49.02 42.75 70.76 49.94]\n",
      " [77.99 60.14 56.72 40.65  0.   75.22 83.22 37.97 94.62 87.03]\n",
      " [21.4  46.35 53.31 43.43 75.22  0.    9.28 51.13 30.22 18.  ]\n",
      " [27.54 55.11 55.54 49.02 83.22  9.28  0.   60.38 31.69 10.  ]\n",
      " [45.23 22.17 64.44 42.75 37.97 51.13 60.38  0.   61.28 67.7 ]\n",
      " [16.63 45.12 83.26 70.76 94.62 30.22 31.69 61.28  0.   40.63]\n",
      " [37.54 64.34 52.37 49.94 87.03 18.   10.   67.7  40.63  0.  ]]\n",
      "Visited Stops: [1, 3]\n",
      "Agent position: 3\n",
      "Visited Stops: [1, 3, 5]\n",
      "Agent position: 5\n",
      "Visited Stops: [1, 3, 5, 4]\n",
      "Agent position: 4\n",
      "Visited Stops: [1, 3, 5, 4, 2]\n",
      "Agent position: 2\n",
      "Visited Stops: [1, 3, 5, 4, 2, 1]\n",
      "Agent position: 1\n",
      "Visited Stops: [1, 3, 5, 4, 2, 1, 5]\n",
      "Agent position: 5\n",
      "Visited Stops: [1, 3, 5, 4, 2, 1, 5, 7]\n",
      "Agent position: 7\n",
      "Visited Stops: [1, 3, 5, 4, 2, 1, 5, 7, 2]\n",
      "Agent position: 2\n",
      "Visited Stops: [1, 3, 5, 4, 2, 1, 5, 7, 2, 8]\n",
      "Agent position: 8\n",
      "Epsiode: 1 Score: 555.0200000000001\n",
      "Visited Stops: [4, 6]\n",
      "Agent position: 6\n",
      "Visited Stops: [4, 6, 5]\n",
      "Agent position: 5\n",
      "Visited Stops: [4, 6, 5, 5]\n",
      "Agent position: 5\n",
      "Visited Stops: [4, 6, 5, 5, 4]\n",
      "Agent position: 4\n",
      "Visited Stops: [4, 6, 5, 5, 4, 9]\n",
      "Agent position: 9\n",
      "Visited Stops: [4, 6, 5, 5, 4, 9, 8]\n",
      "Agent position: 8\n",
      "Visited Stops: [4, 6, 5, 5, 4, 9, 8, 7]\n",
      "Agent position: 7\n",
      "Visited Stops: [4, 6, 5, 5, 4, 9, 8, 7, 0]\n",
      "Agent position: 0\n",
      "Visited Stops: [4, 6, 5, 5, 4, 9, 8, 7, 0, 7]\n",
      "Agent position: 7\n",
      "Epsiode: 2 Score: 447.12\n",
      "Visited Stops: [6, 9]\n",
      "Agent position: 9\n",
      "Visited Stops: [6, 9, 7]\n",
      "Agent position: 7\n",
      "Visited Stops: [6, 9, 7, 4]\n",
      "Agent position: 4\n",
      "Visited Stops: [6, 9, 7, 4, 7]\n",
      "Agent position: 7\n",
      "Visited Stops: [6, 9, 7, 4, 7, 8]\n",
      "Agent position: 8\n",
      "Visited Stops: [6, 9, 7, 4, 7, 8, 2]\n",
      "Agent position: 2\n",
      "Visited Stops: [6, 9, 7, 4, 7, 8, 2, 0]\n",
      "Agent position: 0\n",
      "Visited Stops: [6, 9, 7, 4, 7, 8, 2, 0, 8]\n",
      "Agent position: 8\n",
      "Visited Stops: [6, 9, 7, 4, 7, 8, 2, 0, 8, 2]\n",
      "Agent position: 2\n",
      "Epsiode: 3 Score: 468.78999999999996\n",
      "Visited Stops: [2, 6]\n",
      "Agent position: 6\n",
      "Visited Stops: [2, 6, 8]\n",
      "Agent position: 8\n",
      "Visited Stops: [2, 6, 8, 3]\n",
      "Agent position: 3\n",
      "Visited Stops: [2, 6, 8, 3, 5]\n",
      "Agent position: 5\n",
      "Visited Stops: [2, 6, 8, 3, 5, 8]\n",
      "Agent position: 8\n",
      "Visited Stops: [2, 6, 8, 3, 5, 8, 5]\n",
      "Agent position: 5\n",
      "Visited Stops: [2, 6, 8, 3, 5, 8, 5, 3]\n",
      "Agent position: 3\n",
      "Visited Stops: [2, 6, 8, 3, 5, 8, 5, 3, 0]\n",
      "Agent position: 0\n",
      "Visited Stops: [2, 6, 8, 3, 5, 8, 5, 3, 0, 6]\n",
      "Agent position: 6\n",
      "Epsiode: 4 Score: 388.78000000000003\n",
      "Visited Stops: [7, 7]\n",
      "Agent position: 7\n",
      "Visited Stops: [7, 7, 6]\n",
      "Agent position: 6\n",
      "Visited Stops: [7, 7, 6, 4]\n",
      "Agent position: 4\n",
      "Visited Stops: [7, 7, 6, 4, 1]\n",
      "Agent position: 1\n",
      "Visited Stops: [7, 7, 6, 4, 1, 3]\n",
      "Agent position: 3\n",
      "Visited Stops: [7, 7, 6, 4, 1, 3, 8]\n",
      "Agent position: 8\n",
      "Visited Stops: [7, 7, 6, 4, 1, 3, 8, 2]\n",
      "Agent position: 2\n",
      "Visited Stops: [7, 7, 6, 4, 1, 3, 8, 2, 5]\n",
      "Agent position: 5\n",
      "Visited Stops: [7, 7, 6, 4, 1, 3, 8, 2, 5, 9]\n",
      "Agent position: 9\n",
      "Epsiode: 5 Score: 486.04\n",
      "Visited Stops: [0, 1]\n",
      "Agent position: 1\n",
      "Visited Stops: [0, 1, 4]\n",
      "Agent position: 4\n",
      "Visited Stops: [0, 1, 4, 4]\n",
      "Agent position: 4\n",
      "Visited Stops: [0, 1, 4, 4, 3]\n",
      "Agent position: 3\n",
      "Visited Stops: [0, 1, 4, 4, 3, 1]\n",
      "Agent position: 1\n",
      "Visited Stops: [0, 1, 4, 4, 3, 1, 3]\n",
      "Agent position: 3\n",
      "Visited Stops: [0, 1, 4, 4, 3, 1, 3, 4]\n",
      "Agent position: 4\n",
      "Visited Stops: [0, 1, 4, 4, 3, 1, 3, 4, 0]\n",
      "Agent position: 0\n",
      "Visited Stops: [0, 1, 4, 4, 3, 1, 3, 4, 0, 5]\n",
      "Agent position: 5\n",
      "Epsiode: 6 Score: 386.52\n",
      "Visited Stops: [1, 9]\n",
      "Agent position: 9\n",
      "Visited Stops: [1, 9, 5]\n",
      "Agent position: 5\n",
      "Visited Stops: [1, 9, 5, 2]\n",
      "Agent position: 2\n",
      "Visited Stops: [1, 9, 5, 2, 8]\n",
      "Agent position: 8\n",
      "Visited Stops: [1, 9, 5, 2, 8, 2]\n",
      "Agent position: 2\n",
      "Visited Stops: [1, 9, 5, 2, 8, 2, 1]\n",
      "Agent position: 1\n",
      "Visited Stops: [1, 9, 5, 2, 8, 2, 1, 7]\n",
      "Agent position: 7\n",
      "Visited Stops: [1, 9, 5, 2, 8, 2, 1, 7, 7]\n",
      "Agent position: 7\n",
      "Visited Stops: [1, 9, 5, 2, 8, 2, 1, 7, 7, 2]\n",
      "Agent position: 2\n",
      "Epsiode: 7 Score: 466.28000000000003\n",
      "Visited Stops: [7, 3]\n",
      "Agent position: 3\n",
      "Visited Stops: [7, 3, 6]\n",
      "Agent position: 6\n",
      "Visited Stops: [7, 3, 6, 9]\n",
      "Agent position: 9\n",
      "Visited Stops: [7, 3, 6, 9, 4]\n",
      "Agent position: 4\n",
      "Visited Stops: [7, 3, 6, 9, 4, 1]\n",
      "Agent position: 1\n",
      "Visited Stops: [7, 3, 6, 9, 4, 1, 5]\n",
      "Agent position: 5\n",
      "Visited Stops: [7, 3, 6, 9, 4, 1, 5, 2]\n",
      "Agent position: 2\n",
      "Visited Stops: [7, 3, 6, 9, 4, 1, 5, 2, 4]\n",
      "Agent position: 4\n",
      "Visited Stops: [7, 3, 6, 9, 4, 1, 5, 2, 4, 1]\n",
      "Agent position: 1\n",
      "Epsiode: 8 Score: 465.46000000000004\n",
      "Visited Stops: [4, 8]\n",
      "Agent position: 8\n",
      "Visited Stops: [4, 8, 2]\n",
      "Agent position: 2\n",
      "Visited Stops: [4, 8, 2, 2]\n",
      "Agent position: 2\n",
      "Visited Stops: [4, 8, 2, 2, 5]\n",
      "Agent position: 5\n",
      "Visited Stops: [4, 8, 2, 2, 5, 2]\n",
      "Agent position: 2\n",
      "Visited Stops: [4, 8, 2, 2, 5, 2, 4]\n",
      "Agent position: 4\n",
      "Visited Stops: [4, 8, 2, 2, 5, 2, 4, 7]\n",
      "Agent position: 7\n",
      "Visited Stops: [4, 8, 2, 2, 5, 2, 4, 7, 2]\n",
      "Agent position: 2\n",
      "Visited Stops: [4, 8, 2, 2, 5, 2, 4, 7, 2, 3]\n",
      "Agent position: 3\n",
      "Epsiode: 9 Score: 465.34000000000003\n",
      "Visited Stops: [8, 6]\n",
      "Agent position: 6\n",
      "Visited Stops: [8, 6, 6]\n",
      "Agent position: 6\n",
      "Visited Stops: [8, 6, 6, 5]\n",
      "Agent position: 5\n",
      "Visited Stops: [8, 6, 6, 5, 0]\n",
      "Agent position: 0\n",
      "Visited Stops: [8, 6, 6, 5, 0, 3]\n",
      "Agent position: 3\n",
      "Visited Stops: [8, 6, 6, 5, 0, 3, 9]\n",
      "Agent position: 9\n",
      "Visited Stops: [8, 6, 6, 5, 0, 3, 9, 3]\n",
      "Agent position: 3\n",
      "Visited Stops: [8, 6, 6, 5, 0, 3, 9, 3, 1]\n",
      "Agent position: 1\n",
      "Visited Stops: [8, 6, 6, 5, 0, 3, 9, 3, 1, 6]\n",
      "Agent position: 6\n",
      "Epsiode: 10 Score: 330.28\n"
     ]
    }
   ],
   "source": [
    "episodes = 10 \n",
    "\n",
    "env = TSPEnvironment(episodes)\n",
    "\n",
    "\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score+=reward\n",
    "    print('Epsiode: {} Score: {}'.format(episode,score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = env.observation_space.shape\n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(states, actions):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(24,activation='relu', input_shapes=states))\n",
    "    model.add(Dense(24,activation='relu'))\n",
    "    model.add(Dense(action,activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "('Keyword argument not understood:', 'input_shapes')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [35], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m build_model(states, actions)\n\u001b[0;32m      2\u001b[0m model\u001b[39m.\u001b[39msummary()\n",
      "Cell \u001b[1;32mIn [34], line 3\u001b[0m, in \u001b[0;36mbuild_model\u001b[1;34m(states, actions)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbuild_model\u001b[39m(states, actions):\n\u001b[0;32m      2\u001b[0m     model \u001b[39m=\u001b[39m Sequential()\n\u001b[1;32m----> 3\u001b[0m     model\u001b[39m.\u001b[39madd(Dense(\u001b[39m24\u001b[39;49m,activation\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mrelu\u001b[39;49m\u001b[39m'\u001b[39;49m, input_shapes\u001b[39m=\u001b[39;49mstates))\n\u001b[0;32m      4\u001b[0m     model\u001b[39m.\u001b[39madd(Dense(\u001b[39m24\u001b[39m,activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m      5\u001b[0m     model\u001b[39m.\u001b[39madd(Dense(action,activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlinear\u001b[39m\u001b[39m'\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\fabian.woellenweber\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\dtensor\\utils.py:96\u001b[0m, in \u001b[0;36mallow_initializer_layout.<locals>._wrap_function\u001b[1;34m(layer_instance, *args, **kwargs)\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[39mif\u001b[39;00m layout:\n\u001b[0;32m     94\u001b[0m             layout_args[variable_name \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m_layout\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m layout\n\u001b[1;32m---> 96\u001b[0m init_method(layer_instance, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     98\u001b[0m \u001b[39m# Inject the layout parameter after the invocation of __init__()\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[39mfor\u001b[39;00m layout_param_name, layout \u001b[39min\u001b[39;00m layout_args\u001b[39m.\u001b[39mitems():\n",
      "File \u001b[1;32mc:\\Users\\fabian.woellenweber\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\layers\\core\\dense.py:117\u001b[0m, in \u001b[0;36mDense.__init__\u001b[1;34m(self, units, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, **kwargs)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[39m@utils\u001b[39m\u001b[39m.\u001b[39mallow_initializer_layout\n\u001b[0;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m    104\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    116\u001b[0m ):\n\u001b[1;32m--> 117\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(activity_regularizer\u001b[39m=\u001b[39mactivity_regularizer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    119\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munits \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(units) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(units, \u001b[39mint\u001b[39m) \u001b[39melse\u001b[39;00m units\n\u001b[0;32m    120\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munits \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\fabian.woellenweber\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\trackable\\base.py:205\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m   result \u001b[39m=\u001b[39m method(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    206\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m previous_value  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\fabian.woellenweber\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\base_layer.py:335\u001b[0m, in \u001b[0;36mLayer.__init__\u001b[1;34m(self, trainable, name, dtype, dynamic, **kwargs)\u001b[0m\n\u001b[0;32m    324\u001b[0m allowed_kwargs \u001b[39m=\u001b[39m {\n\u001b[0;32m    325\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39minput_dim\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    326\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39minput_shape\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    332\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mimplementation\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    333\u001b[0m }\n\u001b[0;32m    334\u001b[0m \u001b[39m# Validate optional keyword arguments.\u001b[39;00m\n\u001b[1;32m--> 335\u001b[0m generic_utils\u001b[39m.\u001b[39;49mvalidate_kwargs(kwargs, allowed_kwargs)\n\u001b[0;32m    337\u001b[0m \u001b[39m# Mutable properties\u001b[39;00m\n\u001b[0;32m    338\u001b[0m \u001b[39m# Indicates whether the layer's weights are updated during training\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \u001b[39m# and whether the layer's updates are run during training.\u001b[39;00m\n\u001b[0;32m    340\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\n\u001b[0;32m    341\u001b[0m     \u001b[39misinstance\u001b[39m(trainable, \u001b[39mbool\u001b[39m)\n\u001b[0;32m    342\u001b[0m     \u001b[39mor\u001b[39;00m (\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    345\u001b[0m     )\n\u001b[0;32m    346\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\fabian.woellenweber\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\generic_utils.py:1269\u001b[0m, in \u001b[0;36mvalidate_kwargs\u001b[1;34m(kwargs, allowed_kwargs, error_message)\u001b[0m\n\u001b[0;32m   1267\u001b[0m \u001b[39mfor\u001b[39;00m kwarg \u001b[39min\u001b[39;00m kwargs:\n\u001b[0;32m   1268\u001b[0m     \u001b[39mif\u001b[39;00m kwarg \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m allowed_kwargs:\n\u001b[1;32m-> 1269\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(error_message, kwarg)\n",
      "\u001b[1;31mTypeError\u001b[0m: ('Keyword argument not understood:', 'input_shapes')"
     ]
    }
   ],
   "source": [
    "model = build_model(states, actions)\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5a312273ffa6240539e020f48a5de0b17350fba060a017bafd7c66c880b86767"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
