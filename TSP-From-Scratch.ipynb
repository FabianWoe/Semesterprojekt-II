{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*- \n",
    "\n",
    "\n",
    "\"\"\"--------------------------------------------------------------------\n",
    "REINFORCEMENT LEARNING\n",
    "\n",
    "Started on the 25/08/2017\n",
    "\n",
    "\n",
    "theo.alves.da.costa@gmail.com\n",
    "https://github.com/theolvs\n",
    "------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import sys\n",
    "import random\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.memory import Memory\n",
    "\n",
    "class Agent(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def expand_state_vector(self,state):\n",
    "        if len(state.shape) == 1 or len(state.shape)==3:\n",
    "            return np.expand_dims(state,axis = 0)\n",
    "        else:\n",
    "            return state\n",
    "\n",
    "\n",
    "\n",
    "    def remember(self,*args):\n",
    "        self.memory.save(args)\n",
    "\n",
    "class QAgent(Agent):\n",
    "    def __init__(self,states_size,actions_size,epsilon = 1.0,epsilon_min = 0.01,epsilon_decay = 0.999,gamma = 0.95,lr = 0.8):\n",
    "        self.states_size = states_size\n",
    "        self.actions_size = actions_size\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.gamma = gamma\n",
    "        self.lr = lr\n",
    "        self.Q = self.build_model(states_size,actions_size)\n",
    "\n",
    "\n",
    "    def build_model(self,states_size,actions_size):\n",
    "        Q = np.zeros([states_size,actions_size])\n",
    "        return Q\n",
    "\n",
    "\n",
    "    def train(self,s,a,r,s_next):\n",
    "        self.Q[s,a] = self.Q[s,a] + self.lr * (r + self.gamma*np.max(self.Q[s_next,a]) - self.Q[s,a])\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "\n",
    "    def act(self,s):\n",
    "\n",
    "        q = self.Q[s,:]\n",
    "\n",
    "        if np.random.rand() > self.epsilon:\n",
    "            a = np.argmax(q)\n",
    "        else:\n",
    "            a = np.random.randint(self.actions_size)\n",
    "\n",
    "        return a\n",
    "\n",
    "class DQNAgentTSP(Agent):\n",
    "    def __init__(self,states_size,actions_size,epsilon = 1.0,epsilon_min = 0.01,epsilon_decay = 0.995,gamma = 0.95,lr = 0.001,low = 0,high = 1,max_memory = 2000,observation_type = \"discrete\"):\n",
    "        assert observation_type in [\"discrete\",\"continuous\"]\n",
    "        self.states_size = states_size\n",
    "        self.actions_size = actions_size\n",
    "        self.memory = Memory(max_memory = max_memory)\n",
    "        self.epsilon = epsilon\n",
    "        self.low = low\n",
    "        self.high = high\n",
    "        self.observation_type = observation_type\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.gamma = gamma\n",
    "        self.lr = lr\n",
    "        self.model = self.build_model(states_size,actions_size)\n",
    "\n",
    "    def build_model(self,states_size,actions_size):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24,input_dim = states_size,activation = \"relu\"))\n",
    "        model.add(Dense(24,activation = \"relu\"))\n",
    "        model.add(Dense(actions_size,activation = \"linear\"))\n",
    "        model.compile(loss='mse',\n",
    "                      optimizer=Adam(lr=self.lr))\n",
    "        return model\n",
    "\n",
    "    def train(self,batch_size = 32):\n",
    "        if len(self.memory.cache) > batch_size:\n",
    "            batch = random.sample(self.memory.cache, batch_size)\n",
    "        else:\n",
    "            batch = self.memory.cache\n",
    "\n",
    "        for state,action,reward,next_state,done in batch:\n",
    "            state = self.expand_state_vector(state)\n",
    "            next_state = self.expand_state_vector(next_state)\n",
    "\n",
    "\n",
    "            targets = self.model.predict(state)\n",
    "\n",
    "            if not done:\n",
    "                target = reward + self.gamma * np.max(self.model.predict(next_state))\n",
    "            else:\n",
    "                target = reward\n",
    "\n",
    "            targets[0][action] = target\n",
    "\n",
    "            self.model.fit(state,targets,epochs = 1,verbose = 0)\n",
    "\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def act(self,state):\n",
    "        state = self.expand_state_vector(state)\n",
    "\n",
    "\n",
    "        if np.random.rand() > self.epsilon:\n",
    "            q = self.model.predict(state)\n",
    "\n",
    "            if self.observation_type == \"discrete\":\n",
    "                a = np.argmax(q[0])\n",
    "            elif self.observation_type == \"continuous\":\n",
    "                a = np.squeeze(np.clip(q,self.low,self.high))\n",
    "\n",
    "        else:\n",
    "            if self.observation_type == \"discrete\":\n",
    "                a = np.random.randint(self.actions_size)\n",
    "            elif self.observation_type == \"continuous\":\n",
    "                a = np.random.uniform(self.low,self.high,self.actions_size)\n",
    "        return a \n",
    "\n",
    "class DeliveryQAgent(QAgent):\n",
    "\n",
    "    def __init__(self,*args,**kwargs):\n",
    "        super().__init__(*args,**kwargs)\n",
    "        self.reset_memory()\n",
    "\n",
    "    def act(self,s):\n",
    "\n",
    "        # Get Q Vector\n",
    "        q = np.copy(self.Q[s,:])\n",
    "\n",
    "        # Avoid already visited states\n",
    "        q[self.states_memory] = -np.inf\n",
    "\n",
    "        if np.random.rand() > self.epsilon:\n",
    "            a = np.argmax(q)\n",
    "        else:\n",
    "            a = np.random.choice([x for x in range(self.actions_size) if x not in self.states_memory])\n",
    "\n",
    "        return a\n",
    "\n",
    "\n",
    "    def remember_state(self,s):\n",
    "        self.states_memory.append(s)\n",
    "\n",
    "    def reset_memory(self):\n",
    "        self.states_memory = []\n",
    "\n",
    "\n",
    "\n",
    "def run_n_episodes(env,agent,name=\"training.gif\",n_episodes=1000,render_each=10,fps=10):\n",
    "\n",
    "    # Store the rewards\n",
    "    rewards = []\n",
    "    imgs = []\n",
    "\n",
    "    # Experience replay\n",
    "    for i in tqdm_notebook(range(n_episodes)):\n",
    "\n",
    "        # Run the episode\n",
    "        env,agent,episode_reward = run_episode(env,agent,verbose = 0)\n",
    "        rewards.append(episode_reward)\n",
    "        \n",
    "        if i % render_each == 0:\n",
    "            img = env.render(return_img = True)\n",
    "            imgs.append(img)\n",
    "\n",
    "    # Show rewards\n",
    "    plt.figure(figsize = (15,3))\n",
    "    plt.title(\"Rewards over training\")\n",
    "    plt.plot(rewards)\n",
    "    plt.show()\n",
    "\n",
    "    # Save imgs as gif\n",
    "    imageio.mimsave(name,imgs,fps = fps)\n",
    "\n",
    "    return env,agent\n",
    "\n",
    "\n",
    "class DeliveryQAgent(QAgent):\n",
    "\n",
    "    def __init__(self,*args,**kwargs):\n",
    "        super().__init__(*args,**kwargs)\n",
    "        self.reset_memory()\n",
    "\n",
    "    def act(self,s):\n",
    "\n",
    "        # Get Q Vector\n",
    "        q = np.copy(self.Q[s,:])\n",
    "\n",
    "        # Avoid already visited states\n",
    "        q[self.states_memory] = -np.inf\n",
    "\n",
    "        if np.random.rand() > self.epsilon:\n",
    "            a = np.argmax(q)\n",
    "        else:\n",
    "            a = np.random.choice([x for x in range(self.actions_size) if x not in self.states_memory])\n",
    "\n",
    "        return a\n",
    "\n",
    "\n",
    "    def remember_state(self,s):\n",
    "        self.states_memory.append(s)\n",
    "\n",
    "    def reset_memory(self):\n",
    "        self.states_memory = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabian.woellenweber\\AppData\\Local\\Temp\\ipykernel_19148\\1146908951.py:11: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
      "  plt.style.use(\"seaborn-dark\")\n"
     ]
    }
   ],
   "source": [
    "## Create the TSP Environment\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "import pygame\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.collections import PatchCollection\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"seaborn-dark\")\n",
    "\n",
    "class TSPEnvironment(gym.Env):\n",
    "    def __init__ (self, n_stops = 100, version=True):\n",
    "        print(f\"TSP-Environment initialized with {n_stops} random stops\")\n",
    "\n",
    "        #True (V1) = Discrete Space / False = Array space\n",
    "        self.version = version\n",
    "\n",
    "        # Initialization\n",
    "        #Number of stops\n",
    "        self.n_stops = n_stops\n",
    "        #Coordinates of stops\n",
    "        self.xy = []\n",
    "        self._visitedStops = []\n",
    "        self._notVisitedStops = list(range(0,self.n_stops))\n",
    "        \n",
    "        #if(self.version):\n",
    "        self.action_space = spaces.Discrete(n_stops)\n",
    "        #else:\n",
    "            #self.action_space = spaces.Box(np.array(range(0,self.n_stops)))\n",
    "        \n",
    "        if(self.version):\n",
    "            self.observation_space = spaces.Box(low= 0, high = self.n_stops)\n",
    "        else:\n",
    "            self.observation_space = spaces.Box(low=np.zeros(self.n_stops), high=np.ones(self.n_stops))\n",
    "        #self.action_space = self._notVisitedStops\n",
    "        self.episode_length = 0\n",
    "        self.step_count = 0\n",
    "        self.distances = np.array\n",
    "\n",
    "        self.array_visitedStops = np.zeros(n_stops)\n",
    "        print(f'Shape Array:{self.array_visitedStops.shape}')\n",
    "\n",
    "        \n",
    "\n",
    "        #set starting point (state)\n",
    "        \n",
    "        #Generate stops\n",
    "        self._generate_stops()\n",
    "        #self._generate_q_values()\n",
    "\n",
    "\n",
    "    def _generate_stops(self):\n",
    "        self.xy = (np.random.rand(self.n_stops,2)*100).round(2)\n",
    "        self.x=self.xy[:,0]\n",
    "        self.y=self.xy[:,1]\n",
    "\n",
    "        #print(f'genrated stops xy: {self.xy}')\n",
    "        self.distances = cdist(self.xy,self.xy,'euclidean').round(0)\n",
    "        \n",
    "        #pick random StartPoint\n",
    "        self._visitedStops.append(np.random.randint(0,self.n_stops))\n",
    "        print(f'Starting Point: {self._visitedStops}')\n",
    "\n",
    "\n",
    "    #return stops[-1]\n",
    "    #Gibt die aktuelle Position des Agenten zurück\n",
    "    def _get_state(self):\n",
    "        if( len(self._visitedStops)>0):\n",
    "            return self._visitedStops[-1]\n",
    "        else:\n",
    "            return 0 \n",
    "\n",
    "\n",
    "    def render(self):\n",
    "        pass\n",
    "\n",
    "    #Resets StartingPoint\n",
    "    def reset(self):\n",
    "        self._visitedStops.clear()\n",
    "        self.array_visitedStops = np.zeros(self.n_stops)\n",
    "        #self._notVisitedStops = list(range(0,self.n_stops))\n",
    "\n",
    "        first_stop = np.random.randint(self.n_stops)\n",
    "        #self._notVisitedStops.remove(first_stop)\n",
    "        self._visitedStops.append(first_stop)\n",
    "        self.array_visitedStops[first_stop] = True\n",
    "        self.step_count = 1\n",
    "\n",
    "        if(self.version):\n",
    "            return self._get_state()\n",
    "        else:\n",
    "            return self.array_visitedStops\n",
    "\n",
    "    def step(self,destination):\n",
    "        done = False\n",
    "        self.step_count +=1\n",
    "        reward = -self.n_stops*1000\n",
    "\n",
    "        self.episode_length += 1\n",
    "        if(self.episode_length < 1000):\n",
    "            if(np.random.rand(1,1) < 0.1):\n",
    "                destination = np.random.randint(0,self.n_stops)\n",
    "\n",
    "        #Validize Step\n",
    "        if(self._get_state() != destination & destination not in self._visitedStops):\n",
    "            #Get reward for such a move\n",
    "            reward = -self.distances[self._get_state(), destination]\n",
    "            \n",
    "            # Append state (new position)\n",
    "            self._visitedStops.append(destination)\n",
    "            self.array_visitedStops[destination] = True\n",
    "\n",
    "        print(f'State in step: {self._get_state()}')\n",
    "        print(f'Destination in step: {destination}')\n",
    "        print(f'Length visited stops: {len(self._visitedStops)}')\n",
    "        print(f'Visited Stops in step: {self._visitedStops}')\n",
    "        print(f'Reward in step: {reward}')\n",
    "        print(f'Stepcounter: {self.step_count}')\n",
    "        \n",
    "        #exploration = np.random.random_sample()\n",
    "        #print(f'Exploration: {exploration}')\n",
    "        #if(exploration >= 0.05):\n",
    "            #destination = np.random.randint(self.action_space.n)\n",
    "            #print(f'Random Destination: {destination}')\n",
    "\n",
    "        if(self.step_count >= self.n_stops*5):\n",
    "            done = True\n",
    "            reward = -2000000\n",
    "            print('Too much steps')\n",
    "\n",
    "        if(len(self._visitedStops) == self.n_stops):\n",
    "            if(len(self._visitedStops) <= self.step_count*2):\n",
    "                reward += 500\n",
    "            reward += 200    \n",
    "            done = True\n",
    "            print(f'Done = True')\n",
    "            print(f'Length visited stops: {len(self._visitedStops)}')\n",
    "            print(f'Visited Stops in step: {self._visitedStops}')\n",
    "            \n",
    "        info = {}\n",
    "\n",
    "        #print(f'Agent position: {self._get_state()}')\n",
    "\n",
    "        if(self.version):\n",
    "            return self._get_state(), reward, done, {}\n",
    "        else:\n",
    "            return self.array_visitedStops, reward, done, {}\n",
    "\n",
    "\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSP-Environment initialized with 10 random stops\n",
      "Shape Array:(10,)\n",
      "Starting Point: [2]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "State in step: 2\n",
      "Destination in step: 2\n",
      "Length visited stops: 1\n",
      "Visited Stops in step: [2]\n",
      "Reward in step: -10000\n",
      "Stepcounter: 1\n",
      "Step: (array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), -10000, False, {})\n",
      "State in step: 5\n",
      "Destination in step: 5\n",
      "Length visited stops: 2\n",
      "Visited Stops in step: [2, 5]\n",
      "Reward in step: -72.0\n",
      "Stepcounter: 2\n",
      "Step: (array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]), -72.0, False, {})\n",
      "(10,)\n",
      "Reset: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fabian.woellenweber\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gym\\spaces\\box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    }
   ],
   "source": [
    "episodes = 10 \n",
    "\n",
    "env = TSPEnvironment(episodes, False)\n",
    "print(env.array_visitedStops)\n",
    "print(f'Step: {env.step(2)}')\n",
    "print(f'Step: {env.step(3)}')\n",
    "print(env.array_visitedStops.shape)\n",
    "print(f'Reset: {env.reset()}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 10 \n",
    "\n",
    "env = TSPEnvironment(episodes)\n",
    "\n",
    "\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    Loops = 0\n",
    "    \n",
    "\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score+=reward\n",
    "        Loops += 1\n",
    "    print('Epsiode: {} Score: {} Episodes: {}'.format(episode,score.round(2),Loops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSP-Environment initialized with 10 random stops\n",
      "Shape Array:(10,)\n",
      "Starting Point: [4]\n",
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_42 (Dense)            (None, 512)               5632      \n",
      "                                                                 \n",
      " dense_43 (Dense)            (None, 512)               262656    \n",
      "                                                                 \n",
      " dense_44 (Dense)            (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 273,418\n",
      "Trainable params: 273,418\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'Box' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [39], line 62\u001b[0m\n\u001b[0;32m     59\u001b[0m model\u001b[39m.\u001b[39moutput\n\u001b[0;32m     60\u001b[0m \u001b[39m# Create the TSP agent\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[39m#agent = build_agent(model, action_size)\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m agent \u001b[39m=\u001b[39m DeliveryQAgent(env\u001b[39m.\u001b[39;49mobservation_space,env\u001b[39m.\u001b[39;49maction_space)\n\u001b[0;32m     63\u001b[0m \u001b[39m# Train the agent\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[39mprint\u001b[39m(model\u001b[39m.\u001b[39moutput)\n",
      "Cell \u001b[1;32mIn [36], line 215\u001b[0m, in \u001b[0;36mDeliveryQAgent.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m,\u001b[39m*\u001b[39margs,\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 215\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39m*\u001b[39margs,\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    216\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreset_memory()\n",
      "Cell \u001b[1;32mIn [36], line 57\u001b[0m, in \u001b[0;36mQAgent.__init__\u001b[1;34m(self, states_size, actions_size, epsilon, epsilon_min, epsilon_decay, gamma, lr)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgamma \u001b[39m=\u001b[39m gamma\n\u001b[0;32m     56\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlr \u001b[39m=\u001b[39m lr\n\u001b[1;32m---> 57\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mQ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbuild_model(states_size,actions_size)\n",
      "Cell \u001b[1;32mIn [36], line 61\u001b[0m, in \u001b[0;36mQAgent.build_model\u001b[1;34m(self, states_size, actions_size)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbuild_model\u001b[39m(\u001b[39mself\u001b[39m,states_size,actions_size):\n\u001b[1;32m---> 61\u001b[0m     Q \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mzeros([states_size,actions_size])\n\u001b[0;32m     62\u001b[0m     \u001b[39mreturn\u001b[39;00m Q\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Box' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from rl.agents import DQNAgent, DDPGAgent, ContinuousDQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy, BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from tensorflow.keras.layers import Lambda\n",
    "\n",
    "# Learning factors...\n",
    "GeneratedStops = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "# create replay memory using deque\n",
    "#memory = deque(maxlen=2000)\n",
    "\n",
    "# Create gym environment\n",
    "env = TSPEnvironment(GeneratedStops, False)\n",
    "\n",
    "\n",
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space.shape[0]\n",
    "\n",
    "\n",
    "# The following function creates a neural network which is used as an \n",
    "# approximate Q function\n",
    "# Input: state \n",
    "# Output: Q Value of each action\n",
    "def build_model(self, state_size, action_size):\n",
    "    model = tf.keras.Sequential()\n",
    "    #model.add(Input(shape=(3,)))\n",
    "    model.add(Dense(512 ,input_shape=(state_size,), activation='relu'))\n",
    "    # model.add(Dense(512 ,activation='relu'))\n",
    "    model.add(Dense(action_size, activation='linear'))\n",
    "    model.summary()\n",
    "    model.compile(loss='mse', optimizer=Adam(learning_rate=learning_rate))\n",
    "    return model\n",
    "\n",
    "\n",
    "    #input_shape=(input_dim,)\n",
    "\n",
    "# Create the TSP agent\n",
    "def build_agent(model, action_size):\n",
    "    # Use Epsilon-Greedy policy for exploration\n",
    "    policy = BoltzmannQPolicy()\n",
    "    # Create memory for storing transitions\n",
    "    memory = SequentialMemory(limit=50000, window_length=1)\n",
    "    # Create the DQN agent\n",
    "    #agent = DQNAgentTSP(model, memory=memory, policy=policy, nb_actions=action_size, nb_steps_warmup=100, target_model_update=1e-2)\n",
    "    \n",
    "    #agent.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "    return agent\n",
    "\n",
    "\n",
    "# Create the TSP model\n",
    "model = build_model(True,state_size, action_size)\n",
    "model.output\n",
    "# Create the TSP agent\n",
    "#agent = build_agent(model, action_size)\n",
    "agent = DeliveryQAgent(env.observation_space,env.action_space)\n",
    "# Train the agent\n",
    "\n",
    "\n",
    "\n",
    "print(model.output)\n",
    "\n",
    "#agent.fit(env, nb_steps=5000, visualize=False, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores=agent.test(env, nb_episodes=1000, visualize=False)\n",
    "print(np.mean(scores.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TSPEnvironment(GeneratedStops, False)\n",
    "\n",
    "\n",
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space.shape\n",
    "\n",
    "print(action_size)\n",
    "print(state_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5a312273ffa6240539e020f48a5de0b17350fba060a017bafd7c66c880b86767"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
