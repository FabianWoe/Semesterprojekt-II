{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabian.woellenweber\\AppData\\Local\\Temp\\ipykernel_19148\\871156789.py:11: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
      "  plt.style.use(\"seaborn-dark\")\n"
     ]
    }
   ],
   "source": [
    "## Create the TSP Environment\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "import pygame\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.collections import PatchCollection\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"seaborn-dark\")\n",
    "\n",
    "class TSPEnvironment(gym.Env):\n",
    "    def __init__ (self, n_stops = 100, version=True, debugInfo=False):\n",
    "        print(f\"TSP-Environment initialized with {n_stops} random stops\")\n",
    "\n",
    "        #True (V1) = Discrete Space / False = Array space\n",
    "        self.version = version\n",
    "        self._debugInfo = debugInfo\n",
    "\n",
    "        # Initialization\n",
    "        #Number of stops\n",
    "        self.n_stops = n_stops\n",
    "        #Coordinates of stops\n",
    "        self.xy = []\n",
    "        self._visitedStops = []\n",
    "        self._notVisitedStops = list(range(0,self.n_stops))\n",
    "        \n",
    "        #if(self.version):\n",
    "        self.action_space = spaces.Discrete(n_stops)\n",
    "        #else:\n",
    "            #self.action_space = spaces.Box(np.array(range(0,self.n_stops))\n",
    "            \n",
    "        self.array_visitedStops = np.zeros(n_stops)\n",
    "        self.array1 = np.ones(n_stops)\n",
    "        self.array2 = np.ones(n_stops)\n",
    "        self.array3 = np.ones(n_stops)\n",
    "\n",
    "        high = np.array(\n",
    "            [\n",
    "                self.array_visitedStops,\n",
    "                self.array1,\n",
    "                self.array2,\n",
    "                self.array3\n",
    "            ],\n",
    "            dtype=np.int32,\n",
    "        )\n",
    "        \n",
    "        if(self.version):\n",
    "            self.observation_space = spaces.Box(-high, high, dtype=np.int32)\n",
    "        else:\n",
    "            self.observation_space = spaces.MultiBinary(self.n_stops)\n",
    "        #self.action_space = self._notVisitedStops\n",
    "        self.episode_length = 0\n",
    "        self.step_count = 0\n",
    "        self.distances = np.array\n",
    "\n",
    "        \n",
    "        print(f'Shape Array:{self.array_visitedStops.shape}')\n",
    "\n",
    "        #set starting point (state)\n",
    "        \n",
    "        #Generate stops\n",
    "        self._generate_stops()\n",
    "        #self._generate_q_values()\n",
    "\n",
    "\n",
    "    def _generate_stops(self):\n",
    "        self.xy = (np.random.rand(self.n_stops,2)*100).round(2)\n",
    "        self.x=self.xy[:,0]\n",
    "        self.y=self.xy[:,1]\n",
    "\n",
    "        #print(f'genrated stops xy: {self.xy}')\n",
    "        self.distances = cdist(self.xy,self.xy,'euclidean').round(0)\n",
    "        \n",
    "        #pick random StartPoint\n",
    "        self._visitedStops.append(np.random.randint(0,self.n_stops))\n",
    "        self._debugInfo\n",
    "        print(f'Starting Point: {self._visitedStops}')\n",
    "\n",
    "\n",
    "    #return stops[-1]\n",
    "    #Gibt die aktuelle Position des Agenten zurÃ¼ck\n",
    "    def _get_state(self):\n",
    "        if( len(self._visitedStops)>0):\n",
    "            return self._visitedStops[-1]\n",
    "        else:\n",
    "            return 0 \n",
    "\n",
    "\n",
    "    def render(self):\n",
    "        pass\n",
    "\n",
    "    #Resets StartingPoint\n",
    "    def reset(self):\n",
    "        self._visitedStops.clear()\n",
    "        self.array_visitedStops = np.zeros(self.n_stops)\n",
    "        #self._notVisitedStops = list(range(0,self.n_stops))\n",
    "\n",
    "        first_stop = np.random.randint(self.n_stops)\n",
    "        #self._notVisitedStops.remove(first_stop)\n",
    "        self._visitedStops.append(first_stop)\n",
    "        self.array_visitedStops[first_stop] = True\n",
    "        self.step_count = 1\n",
    "\n",
    "        if(self.version):\n",
    "            return self._get_state()\n",
    "        else:\n",
    "            return np.array(self.array_visitedStops, dtype=np.int32), {}\n",
    "\n",
    "    def step(self,destination):\n",
    "        done = False\n",
    "        self.step_count +=1\n",
    "        reward = -self.n_stops*1000\n",
    "\n",
    "        self.episode_length += 1\n",
    "        if(self.episode_length < 1000):\n",
    "            if(np.random.rand(1,1) < 0.1):\n",
    "                destination = np.random.randint(0,self.n_stops)\n",
    "\n",
    "        #Validize Step\n",
    "        if(self._get_state() != destination & destination not in self._visitedStops):\n",
    "            #Get reward for such a move\n",
    "            reward = -self.distances[self._get_state(), destination]\n",
    "            \n",
    "            # Append state (new position)\n",
    "            self._visitedStops.append(destination)\n",
    "            self.array_visitedStops[destination] = True\n",
    "\n",
    "        if(self._debugInfo):\n",
    "            print(f'State in step: {self._get_state()}')\n",
    "            print(f'Destination in step: {destination}')\n",
    "            print(f'Length visited stops: {len(self._visitedStops)}')\n",
    "            print(f'Visited Stops in step: {self._visitedStops}')\n",
    "            print(f'Reward in step: {reward}')\n",
    "            print(f'Stepcounter: {self.step_count}')\n",
    "        \n",
    "        #exploration = np.random.random_sample()\n",
    "        #print(f'Exploration: {exploration}')\n",
    "        #if(exploration >= 0.05):\n",
    "            #destination = np.random.randint(self.action_space.n)\n",
    "            #print(f'Random Destination: {destination}')\n",
    "\n",
    "        # if(self.step_count >= self.n_stops*5):\n",
    "        #     done = True\n",
    "        #     reward = -2000000\n",
    "        #     if(self._debugInfo):\n",
    "        #         print('Too much steps')\n",
    "\n",
    "        if(len(self._visitedStops) == self.n_stops):\n",
    "            if(len(self._visitedStops) <= self.step_count*2):\n",
    "                reward += 500\n",
    "            reward += 200    \n",
    "            done = True\n",
    "            print(f'Done = True')\n",
    "            print(f'Length visited stops: {len(self._visitedStops)}')\n",
    "            print(f'Visited Stops in step: {self._visitedStops}')\n",
    "            \n",
    "        info = {}\n",
    "\n",
    "        #print(f'Agent position: {self._get_state()}')\n",
    "\n",
    "        if(self.version):\n",
    "            return self._get_state(), reward, done, {}\n",
    "        else:\n",
    "            return np.array(self.array_visitedStops, reward, done, {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TSPEnvironment(10, True)\n",
    "env.reset()\n",
    "env.step(2)\n",
    "print(env.observation_space.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 10 \n",
    "\n",
    "env = TSPEnvironment(episodes, False)\n",
    "print(env.array_visitedStops)\n",
    "print(f'Step: {env.step(2)}')\n",
    "print(f'Step: {env.step(3)}')\n",
    "print(env.array_visitedStops.shape)\n",
    "print(f'Reset: {env.reset()}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_87\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_240 (Dense)           multiple                  2048      \n",
      "                                                                 \n",
      " dense_241 (Dense)           multiple                  131328    \n",
      "                                                                 \n",
      " dense_242 (Dense)           multiple                  257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 133,633\n",
      "Trainable params: 133,633\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.Sequential(\n",
    "        [\n",
    "    #model.add(Input(shape=(3,)))\n",
    "    \n",
    "            keras.layers.Dense(512 , activation='relu', kernel_initializer='he_uniform'),\n",
    "            keras.layers.Dense(256 ,activation='relu', kernel_initializer='he_uniform'),\n",
    "            keras.layers.Dense(1, activation='linear', kernel_initializer='he_uniform'),\n",
    "        ]\n",
    ")\n",
    "    \n",
    "x = tf.ones((1,3))\n",
    "y = model(x)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSP-Environment initialized with 10 random stops\n",
      "Shape Array:(10,)\n",
      "Starting Point: [9]\n",
      "Done = True\n",
      "Length visited stops: 10\n",
      "Visited Stops in step: [8, 9, 4, 0, 6, 1, 3, 5, 2, 7]\n",
      "Epsiode: 1 Score: -159695.0 Episodes: 25\n",
      "Done = True\n",
      "Length visited stops: 10\n",
      "Visited Stops in step: [7, 6, 4, 8, 2, 9, 5, 3, 0, 1]\n",
      "Epsiode: 2 Score: -179735.0 Episodes: 27\n",
      "Done = True\n",
      "Length visited stops: 10\n",
      "Visited Stops in step: [0, 1, 8, 6, 2, 7, 9, 3, 5, 4]\n",
      "Epsiode: 3 Score: -189733.0 Episodes: 28\n",
      "Done = True\n",
      "Length visited stops: 10\n",
      "Visited Stops in step: [7, 6, 4, 0, 5, 9, 3, 8, 1, 2]\n",
      "Epsiode: 4 Score: -269801.0 Episodes: 36\n",
      "Done = True\n",
      "Length visited stops: 10\n",
      "Visited Stops in step: [8, 9, 7, 4, 5, 2, 0, 3, 1, 6]\n",
      "Epsiode: 5 Score: -89800.0 Episodes: 18\n",
      "Epsiode: 6 Score: -2400395.0 Episodes: 49\n",
      "Done = True\n",
      "Length visited stops: 10\n",
      "Visited Stops in step: [0, 9, 6, 2, 8, 1, 7, 3, 5, 4]\n",
      "Epsiode: 7 Score: -229657.0 Episodes: 32\n",
      "Done = True\n",
      "Length visited stops: 10\n",
      "Visited Stops in step: [8, 3, 2, 9, 4, 1, 0, 5, 7, 6]\n",
      "Epsiode: 8 Score: -129718.0 Episodes: 22\n",
      "Done = True\n",
      "Length visited stops: 10\n",
      "Visited Stops in step: [9, 6, 4, 2, 8, 0, 5, 3, 1, 7]\n",
      "Epsiode: 9 Score: -349627.0 Episodes: 44\n",
      "Done = True\n",
      "Length visited stops: 10\n",
      "Visited Stops in step: [3, 4, 8, 1, 6, 2, 7, 9, 5, 0]\n",
      "Epsiode: 10 Score: -119806.0 Episodes: 21\n"
     ]
    }
   ],
   "source": [
    "episodes = 10 \n",
    "\n",
    "env = TSPEnvironment(episodes)\n",
    "\n",
    "\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    Loops = 0\n",
    "    \n",
    "\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score+=reward\n",
    "        Loops += 1\n",
    "    print('Epsiode: {} Score: {} Episodes: {}'.format(episode,score.round(2),Loops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "class TSPAgent:\n",
    "  def __init__(self, n_stops):\n",
    "    self.n_stops = n_stops\n",
    "    self.model = self._build_model()\n",
    "    \n",
    "  # def _build_model(self):\n",
    "  #   model = tf.keras.Sequential()\n",
    "  #   # Add layers to your model\n",
    "  #   model.add(tf.keras.layers.Dense(units=64, activation='relu', input_shape=(self.n_stops,), kernel_initializer='he_uniform'))\n",
    "  #   model.add(tf.keras.layers.Dense(units=32, activation='relu', kernel_initializer='he_uniform'))\n",
    "  #   model.add(tf.keras.layers.Dense(units=1, activation='linear', kernel_initializer='he_uniform'))\n",
    "  #   return model\n",
    "\n",
    "  \n",
    "\n",
    "    x = tf.ones((1,3))\n",
    "    model(x)\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "    \n",
    "  def act(self, state):\n",
    "    # Convert the state from an integer to an array\n",
    "    state = np.array([state])\n",
    "    \n",
    "    # Reshape the state array to have the correct shape\n",
    "    # state = state.reshape(-1, self.n_stops)\n",
    "    \n",
    "    # Use the model to predict the action given the state\n",
    "    action = self.model.predict(state)\n",
    "    return action\n",
    "\n",
    "    \n",
    "  def train(self, state, action, reward, done):\n",
    "    # Use the reward and next state to update the model\n",
    "    self.model.fit(state, action, reward, done)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSP-Environment initialized with 50 random stops\n",
      "Shape Array:(50,)\n",
      "Starting Point: [17]\n",
      "Model: \"sequential_86\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_237 (Dense)           multiple                  2048      \n",
      "                                                                 \n",
      " dense_238 (Dense)           multiple                  131328    \n",
      "                                                                 \n",
      " dense_239 (Dense)           multiple                  257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 133,633\n",
      "Trainable params: 133,633\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "TSPAgent.train() missing 1 required positional argument: 'done'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [238], line 28\u001b[0m\n\u001b[0;32m     18\u001b[0m done \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m done:\n\u001b[0;32m     21\u001b[0m   \u001b[39m# Have the agent act on the current state\u001b[39;00m\n\u001b[0;32m     22\u001b[0m   \u001b[39m#action = agent.act(state)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m   \n\u001b[0;32m     27\u001b[0m   \u001b[39m# Train the agent\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m   agent\u001b[39m.\u001b[39;49mtrain(state, action, reward, done)\n\u001b[0;32m     30\u001b[0m   \u001b[39m# Update the current state\u001b[39;00m\n\u001b[0;32m     31\u001b[0m   \u001b[39m#state = next_state\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: TSPAgent.train() missing 1 required positional argument: 'done'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Set hyperparameters\n",
    "learning_rate = 0.001\n",
    "n_episodes = 1000\n",
    "\n",
    "# Create the TSP environment and agent\n",
    "env = TSPEnvironment(50)\n",
    "agent = TSPAgent(n_stops=100)\n",
    "\n",
    "# Set the optimizer for the agent\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for episode in range(n_episodes):\n",
    "  # Reset the environment at the beginning of each episode\n",
    "  state = env.reset()\n",
    "  done = False\n",
    "  \n",
    "  while not done:\n",
    "    # Have the agent act on the current state\n",
    "    #action = agent.act(state)\n",
    "    \n",
    "    # Take a step in the environment\n",
    "    #next_state, reward, done, _ = env.step(action)\n",
    "    \n",
    "    # Train the agent\n",
    "    agent.train(state, action, reward, done)\n",
    "    \n",
    "    # Update the current state\n",
    "    #state = next_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSP-Environment initialized with 10 random stops\n",
      "Shape Array:(10,)\n",
      "Starting Point: [0]\n",
      "Model: \"sequential_81\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_222 (Dense)           multiple                  2048      \n",
      "                                                                 \n",
      " dense_223 (Dense)           multiple                  131328    \n",
      "                                                                 \n",
      " dense_224 (Dense)           multiple                  2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 135,946\n",
      "Trainable params: 135,946\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Sequential' object has no attribute '_nested_outputs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [232], line 65\u001b[0m\n\u001b[0;32m     63\u001b[0m model \u001b[39m=\u001b[39m build_model(state_size, action_size)\n\u001b[0;32m     64\u001b[0m \u001b[39m# Create the TSP agent\u001b[39;00m\n\u001b[1;32m---> 65\u001b[0m agent \u001b[39m=\u001b[39m build_agent(model, action_size)\n\u001b[0;32m     66\u001b[0m \u001b[39m#agent = DeliveryQAgent(env.observation_space.n,env.action_space.n)\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[39m# Train the agent\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \n\u001b[0;32m     69\u001b[0m \n\u001b[0;32m     70\u001b[0m \n\u001b[0;32m     71\u001b[0m \u001b[39m#print(model.output)\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[39mprint\u001b[39m(env\u001b[39m.\u001b[39mobservation_space\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])\n",
      "Cell \u001b[1;32mIn [232], line 56\u001b[0m, in \u001b[0;36mbuild_agent\u001b[1;34m(model, action_size)\u001b[0m\n\u001b[0;32m     54\u001b[0m memory \u001b[39m=\u001b[39m SequentialMemory(limit\u001b[39m=\u001b[39m\u001b[39m50000\u001b[39m, window_length\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     55\u001b[0m \u001b[39m# Create the DQN agent\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m agent \u001b[39m=\u001b[39m DQNAgent(model, memory\u001b[39m=\u001b[39;49mmemory, policy\u001b[39m=\u001b[39;49mpolicy, nb_actions\u001b[39m=\u001b[39;49maction_size, nb_steps_warmup\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, target_model_update\u001b[39m=\u001b[39;49m\u001b[39m1e-2\u001b[39;49m)\n\u001b[0;32m     58\u001b[0m agent\u001b[39m.\u001b[39mcompile(Adam(lr\u001b[39m=\u001b[39m\u001b[39m1e-3\u001b[39m), metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mmae\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     59\u001b[0m \u001b[39mreturn\u001b[39;00m agent\n",
      "File \u001b[1;32mc:\\Users\\fabian.woellenweber\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\rl\\agents\\dqn.py:106\u001b[0m, in \u001b[0;36mDQNAgent.__init__\u001b[1;34m(self, model, policy, test_policy, enable_double_dqn, enable_dueling_network, dueling_type, *args, **kwargs)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    105\u001b[0m \u001b[39m# Validate (important) input.\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlist\u001b[39m(model\u001b[39m.\u001b[39;49moutput\u001b[39m.\u001b[39mshape) \u001b[39m!=\u001b[39m \u001b[39mlist\u001b[39m((\u001b[39mNone\u001b[39;00m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnb_actions)):\n\u001b[0;32m    107\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mModel output \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmodel\u001b[39m.\u001b[39moutput\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m has invalid shape. DQN expects a model that has one dimension for each action, in this case \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnb_actions\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    109\u001b[0m \u001b[39m# Parameters.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\fabian.woellenweber\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\functional.py:393\u001b[0m, in \u001b[0;36mFunctional.output\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    378\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[0;32m    379\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moutput\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    380\u001b[0m     \u001b[39m\"\"\"Retrieves the output tensor(s) of a layer.\u001b[39;00m\n\u001b[0;32m    381\u001b[0m \n\u001b[0;32m    382\u001b[0m \u001b[39m    Only applicable if the layer has exactly one output,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    391\u001b[0m \u001b[39m      RuntimeError: if called in Eager mode.\u001b[39;00m\n\u001b[0;32m    392\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 393\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_nested_outputs\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Sequential' object has no attribute '_nested_outputs'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from rl.agents import DQNAgent, DDPGAgent, ContinuousDQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy, BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from tensorflow.keras.layers import Lambda\n",
    "\n",
    "# Learning factors...\n",
    "GeneratedStops = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "# create replay memory using deque\n",
    "#memory = deque(maxlen=2000)\n",
    "\n",
    "# Create gym environment\n",
    "env = TSPEnvironment(GeneratedStops, False)\n",
    "\n",
    "\n",
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space.shape\n",
    "\n",
    "\n",
    "# The following function creates a neural network which is used as an \n",
    "# approximate Q function\n",
    "# Input: state \n",
    "# Output: Q Value of each action\n",
    "def build_model(state_size, action_size):\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "    #model.add(Input(shape=(3,)))\n",
    "    \n",
    "            keras.layers.Dense(512 , activation='relu', kernel_initializer='he_uniform'),\n",
    "            keras.layers.Dense(256 ,activation='relu', kernel_initializer='he_uniform'),\n",
    "            keras.layers.Dense(action_size, activation='linear', kernel_initializer='he_uniform'),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    x = tf.ones((1,3))\n",
    "    model(x)\n",
    "    model.summary()\n",
    "    model.compile(loss='mse', optimizer=Adam(learning_rate=learning_rate))\n",
    "    return model\n",
    "    #input_shape=(input_dim,)\n",
    "\n",
    "# Create the TSP agent\n",
    "def build_agent(model, action_size):\n",
    "    # Use Epsilon-Greedy policy for exploration\n",
    "    policy = BoltzmannQPolicy()\n",
    "    # Create memory for storing transitions\n",
    "    memory = SequentialMemory(limit=50000, window_length=1)\n",
    "    # Create the DQN agent\n",
    "    agent = DQNAgent(model, memory=memory, policy=policy, nb_actions=action_size, nb_steps_warmup=100, target_model_update=1e-2)\n",
    "    \n",
    "    agent.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "    return agent\n",
    "\n",
    "\n",
    "# Create the TSP model\n",
    "model = build_model(state_size, action_size)\n",
    "# Create the TSP agent\n",
    "agent = build_agent(model, action_size)\n",
    "#agent = DeliveryQAgent(env.observation_space.n,env.action_space.n)\n",
    "# Train the agent\n",
    "\n",
    "\n",
    "\n",
    "#print(model.output)\n",
    "print(env.observation_space.shape[0])\n",
    "\n",
    "#agent.fit(env, nb_steps=5000, visualize=False, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores=agent.test(env, nb_episodes=1000, visualize=False)\n",
    "print(np.mean(scores.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TSPEnvironment(GeneratedStops, False)\n",
    "\n",
    "\n",
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space.shape\n",
    "\n",
    "print(action_size)\n",
    "print(state_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5a312273ffa6240539e020f48a5de0b17350fba060a017bafd7c66c880b86767"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
