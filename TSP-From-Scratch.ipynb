{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import Env\n",
    "from gym.spaces import Discrete, box\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'QAgent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mDeliveryQAgent\u001b[39;00m(QAgent):\n\u001b[0;32m      3\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m,\u001b[39m*\u001b[39margs,\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m      4\u001b[0m         \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39m*\u001b[39margs,\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'QAgent' is not defined"
     ]
    }
   ],
   "source": [
    "class DeliveryQAgent(QAgent):\n",
    "\n",
    "    def __init__(self,*args,**kwargs):\n",
    "        super().__init__(*args,**kwargs)\n",
    "        self.reset_memory()\n",
    "\n",
    "    def act(self,s):\n",
    "\n",
    "        # Get Q Vector\n",
    "        q = np.copy(self.Q[s,:])\n",
    "\n",
    "        # Avoid already visited states\n",
    "        q[self.states_memory] = -np.inf\n",
    "\n",
    "        if np.random.rand() > self.epsilon:\n",
    "            a = np.argmax(q)\n",
    "        else:\n",
    "            a = np.random.choice([x for x in range(self.actions_size) if x not in self.states_memory])\n",
    "\n",
    "        return a\n",
    "\n",
    "\n",
    "    def remember_state(self,s):\n",
    "        self.states_memory.append(s)\n",
    "\n",
    "    def reset_memory(self):\n",
    "        self.states_memory = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabian.woellenweber\\AppData\\Local\\Temp\\ipykernel_40488\\1267304800.py:11: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
      "  plt.style.use(\"seaborn-dark\")\n"
     ]
    }
   ],
   "source": [
    "## Create the TSP Environment\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "import pygame\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.collections import PatchCollection\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"seaborn-dark\")\n",
    "\n",
    "class TSPEnvironment(gym.Env):\n",
    "    def __init__ (self, n_stops = 100):\n",
    "        print(f\"TSP-Environment initialized with {n_stops} random stops\")\n",
    "\n",
    "        # Initialization\n",
    "        #Number of stops\n",
    "        self.n_stops = n_stops\n",
    "        #Coordinates of stops\n",
    "        self.xy = []\n",
    "        \n",
    "        self.action_space = spaces.Discrete(n_stops)\n",
    "        self.observation_space = spaces.Discrete(n_stops)\n",
    "        self.episode_length = n_stops\n",
    "        self._visitedStops = []\n",
    "\n",
    "        #set starting point (state)\n",
    "        \n",
    "        #Generate stops\n",
    "        self._generate_stops()\n",
    "        self._generate_q_values()\n",
    "                \n",
    "        \n",
    "\n",
    "    def _get_obs(self):\n",
    "        return {\"agent\": self._agent_location, \"target\": self._target_location}\n",
    "\n",
    "    def _get_info(self):\n",
    "        return {\"distance\": np.linalg.normal(self._agent_location - self._target_location, ord=1)} \n",
    "\n",
    "    def _generate_stops(self):\n",
    "        self.xy = (np.random.rand(100,2)*100).round(2)\n",
    "        self.x=self.xy[:,0]\n",
    "        self.y=self.xy[:,1]\n",
    "\n",
    "        #print(f'genrated stops xy: {self.xy}')\n",
    "        \n",
    "        #pick random StartPoint\n",
    "        self._visitedStops.append(np.random.randint(0,self.n_stops))\n",
    "        print(f'Starting Point: {self._visitedStops}')\n",
    "\n",
    "\n",
    "    def _generate_q_values(self,box_size = 0.2):\n",
    "        self.q_stops = -cdist(self.xy,self.xy,'euclidean').round(2)\n",
    "        #print(f'Distance: \\n {self.q_stops}')\n",
    "\n",
    "\n",
    "    def render(self):\n",
    "        pass\n",
    "\n",
    "    #Resets StartingPoint\n",
    "    def reset(self):\n",
    "        self._visitedStops = []\n",
    "\n",
    "        first_stop = np.random.randint(self.n_stops)\n",
    "        self._visitedStops.append(first_stop)\n",
    "        return first_stop\n",
    "\n",
    "    def step(self,destination):\n",
    "        #Get reward for such a move\n",
    "        reward = self._get_reward(self._get_state(), destination)\n",
    "\n",
    "        #set new position of agent\n",
    "        #self.state = destination\n",
    "        if(destination not in self._visitedStops):\n",
    "            self._visitedStops.append(destination)\n",
    "\n",
    "        print(f'Visited Stops: {self._visitedStops}')\n",
    "\n",
    "        done = len(self._visitedStops) == self.n_stops\n",
    "        if(done):\n",
    "            reward+=200\n",
    "            \n",
    "        info = {}\n",
    "\n",
    "        #print(f'Agent position: {self._get_state()}')\n",
    "\n",
    "        return self._get_state(), reward, done, info\n",
    "\n",
    "\n",
    "    #return stops[-1]\n",
    "    #Gibt die aktuelle Position des Agenten zurÃ¼ck\n",
    "    def _get_state(self):\n",
    "        if( len(self._visitedStops)>0):\n",
    "            return self._visitedStops[-1]\n",
    "        else:\n",
    "            return 0 \n",
    "\n",
    "    def _get_reward(self, state, destination):\n",
    "        return self.q_stops[self._get_state(), destination]\n",
    "\n",
    "    def _get_xy(self):\n",
    "        return self.xy[self._get_state()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSP-Environment initialized with 10 random stops\n",
      "Starting Point: [2]\n",
      "Visited Stops: [8]\n",
      "Visited Stops: [8, 6]\n",
      "Visited Stops: [8, 6, 7]\n",
      "Visited Stops: [8, 6, 7, 0]\n",
      "Visited Stops: [8, 6, 7, 0]\n",
      "Visited Stops: [8, 6, 7, 0, 3]\n",
      "Visited Stops: [8, 6, 7, 0, 3, 1]\n",
      "Visited Stops: [8, 6, 7, 0, 3, 1]\n",
      "Visited Stops: [8, 6, 7, 0, 3, 1]\n",
      "Visited Stops: [8, 6, 7, 0, 3, 1, 5]\n",
      "Visited Stops: [8, 6, 7, 0, 3, 1, 5, 9]\n",
      "Visited Stops: [8, 6, 7, 0, 3, 1, 5, 9]\n",
      "Visited Stops: [8, 6, 7, 0, 3, 1, 5, 9, 4]\n",
      "Visited Stops: [8, 6, 7, 0, 3, 1, 5, 9, 4]\n",
      "Visited Stops: [8, 6, 7, 0, 3, 1, 5, 9, 4]\n",
      "Visited Stops: [8, 6, 7, 0, 3, 1, 5, 9, 4]\n",
      "Visited Stops: [8, 6, 7, 0, 3, 1, 5, 9, 4]\n",
      "Visited Stops: [8, 6, 7, 0, 3, 1, 5, 9, 4, 2]\n",
      "Epsiode: 1 Score: -424.01 Episodes: 18\n",
      "Visited Stops: [5, 3]\n",
      "Visited Stops: [5, 3, 6]\n",
      "Visited Stops: [5, 3, 6, 8]\n",
      "Visited Stops: [5, 3, 6, 8, 1]\n",
      "Visited Stops: [5, 3, 6, 8, 1]\n",
      "Visited Stops: [5, 3, 6, 8, 1]\n",
      "Visited Stops: [5, 3, 6, 8, 1, 7]\n",
      "Visited Stops: [5, 3, 6, 8, 1, 7]\n",
      "Visited Stops: [5, 3, 6, 8, 1, 7]\n",
      "Visited Stops: [5, 3, 6, 8, 1, 7]\n",
      "Visited Stops: [5, 3, 6, 8, 1, 7, 2]\n",
      "Visited Stops: [5, 3, 6, 8, 1, 7, 2]\n",
      "Visited Stops: [5, 3, 6, 8, 1, 7, 2]\n",
      "Visited Stops: [5, 3, 6, 8, 1, 7, 2]\n",
      "Visited Stops: [5, 3, 6, 8, 1, 7, 2]\n",
      "Visited Stops: [5, 3, 6, 8, 1, 7, 2]\n",
      "Visited Stops: [5, 3, 6, 8, 1, 7, 2]\n",
      "Visited Stops: [5, 3, 6, 8, 1, 7, 2, 0]\n",
      "Visited Stops: [5, 3, 6, 8, 1, 7, 2, 0, 9]\n",
      "Visited Stops: [5, 3, 6, 8, 1, 7, 2, 0, 9]\n",
      "Visited Stops: [5, 3, 6, 8, 1, 7, 2, 0, 9]\n",
      "Visited Stops: [5, 3, 6, 8, 1, 7, 2, 0, 9]\n",
      "Visited Stops: [5, 3, 6, 8, 1, 7, 2, 0, 9]\n",
      "Visited Stops: [5, 3, 6, 8, 1, 7, 2, 0, 9, 4]\n",
      "Epsiode: 2 Score: -895.81 Episodes: 24\n",
      "Visited Stops: [0, 4]\n",
      "Visited Stops: [0, 4, 2]\n",
      "Visited Stops: [0, 4, 2, 5]\n",
      "Visited Stops: [0, 4, 2, 5]\n",
      "Visited Stops: [0, 4, 2, 5]\n",
      "Visited Stops: [0, 4, 2, 5, 8]\n",
      "Visited Stops: [0, 4, 2, 5, 8]\n",
      "Visited Stops: [0, 4, 2, 5, 8]\n",
      "Visited Stops: [0, 4, 2, 5, 8]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3, 1]\n",
      "Epsiode: 3 Score: -3539.76 Episodes: 101\n",
      "Visited Stops: [1]\n",
      "Visited Stops: [1, 9]\n",
      "Visited Stops: [1, 9, 6]\n",
      "Visited Stops: [1, 9, 6, 4]\n",
      "Visited Stops: [1, 9, 6, 4]\n",
      "Visited Stops: [1, 9, 6, 4, 2]\n",
      "Visited Stops: [1, 9, 6, 4, 2]\n",
      "Visited Stops: [1, 9, 6, 4, 2]\n",
      "Visited Stops: [1, 9, 6, 4, 2, 3]\n",
      "Visited Stops: [1, 9, 6, 4, 2, 3]\n",
      "Visited Stops: [1, 9, 6, 4, 2, 3]\n",
      "Visited Stops: [1, 9, 6, 4, 2, 3, 0]\n",
      "Visited Stops: [1, 9, 6, 4, 2, 3, 0]\n",
      "Visited Stops: [1, 9, 6, 4, 2, 3, 0]\n",
      "Visited Stops: [1, 9, 6, 4, 2, 3, 0, 5]\n",
      "Visited Stops: [1, 9, 6, 4, 2, 3, 0, 5]\n",
      "Visited Stops: [1, 9, 6, 4, 2, 3, 0, 5]\n",
      "Visited Stops: [1, 9, 6, 4, 2, 3, 0, 5]\n",
      "Visited Stops: [1, 9, 6, 4, 2, 3, 0, 5]\n",
      "Visited Stops: [1, 9, 6, 4, 2, 3, 0, 5]\n",
      "Visited Stops: [1, 9, 6, 4, 2, 3, 0, 5]\n",
      "Visited Stops: [1, 9, 6, 4, 2, 3, 0, 5]\n",
      "Visited Stops: [1, 9, 6, 4, 2, 3, 0, 5]\n",
      "Visited Stops: [1, 9, 6, 4, 2, 3, 0, 5, 8]\n",
      "Visited Stops: [1, 9, 6, 4, 2, 3, 0, 5, 8]\n",
      "Visited Stops: [1, 9, 6, 4, 2, 3, 0, 5, 8]\n",
      "Visited Stops: [1, 9, 6, 4, 2, 3, 0, 5, 8]\n",
      "Visited Stops: [1, 9, 6, 4, 2, 3, 0, 5, 8]\n",
      "Visited Stops: [1, 9, 6, 4, 2, 3, 0, 5, 8]\n",
      "Visited Stops: [1, 9, 6, 4, 2, 3, 0, 5, 8]\n",
      "Visited Stops: [1, 9, 6, 4, 2, 3, 0, 5, 8]\n",
      "Visited Stops: [1, 9, 6, 4, 2, 3, 0, 5, 8]\n",
      "Visited Stops: [1, 9, 6, 4, 2, 3, 0, 5, 8]\n",
      "Visited Stops: [1, 9, 6, 4, 2, 3, 0, 5, 8]\n",
      "Visited Stops: [1, 9, 6, 4, 2, 3, 0, 5, 8]\n",
      "Visited Stops: [1, 9, 6, 4, 2, 3, 0, 5, 8]\n",
      "Visited Stops: [1, 9, 6, 4, 2, 3, 0, 5, 8]\n",
      "Visited Stops: [1, 9, 6, 4, 2, 3, 0, 5, 8]\n",
      "Visited Stops: [1, 9, 6, 4, 2, 3, 0, 5, 8, 7]\n",
      "Epsiode: 4 Score: -1112.01 Episodes: 39\n",
      "Visited Stops: [8, 3]\n",
      "Visited Stops: [8, 3, 4]\n",
      "Visited Stops: [8, 3, 4, 7]\n",
      "Visited Stops: [8, 3, 4, 7, 6]\n",
      "Visited Stops: [8, 3, 4, 7, 6]\n",
      "Visited Stops: [8, 3, 4, 7, 6, 5]\n",
      "Visited Stops: [8, 3, 4, 7, 6, 5]\n",
      "Visited Stops: [8, 3, 4, 7, 6, 5]\n",
      "Visited Stops: [8, 3, 4, 7, 6, 5, 9]\n",
      "Visited Stops: [8, 3, 4, 7, 6, 5, 9]\n",
      "Visited Stops: [8, 3, 4, 7, 6, 5, 9, 0]\n",
      "Visited Stops: [8, 3, 4, 7, 6, 5, 9, 0]\n",
      "Visited Stops: [8, 3, 4, 7, 6, 5, 9, 0]\n",
      "Visited Stops: [8, 3, 4, 7, 6, 5, 9, 0]\n",
      "Visited Stops: [8, 3, 4, 7, 6, 5, 9, 0, 1]\n",
      "Visited Stops: [8, 3, 4, 7, 6, 5, 9, 0, 1]\n",
      "Visited Stops: [8, 3, 4, 7, 6, 5, 9, 0, 1]\n",
      "Visited Stops: [8, 3, 4, 7, 6, 5, 9, 0, 1]\n",
      "Visited Stops: [8, 3, 4, 7, 6, 5, 9, 0, 1]\n",
      "Visited Stops: [8, 3, 4, 7, 6, 5, 9, 0, 1]\n",
      "Visited Stops: [8, 3, 4, 7, 6, 5, 9, 0, 1]\n",
      "Visited Stops: [8, 3, 4, 7, 6, 5, 9, 0, 1]\n",
      "Visited Stops: [8, 3, 4, 7, 6, 5, 9, 0, 1]\n",
      "Visited Stops: [8, 3, 4, 7, 6, 5, 9, 0, 1]\n",
      "Visited Stops: [8, 3, 4, 7, 6, 5, 9, 0, 1]\n",
      "Visited Stops: [8, 3, 4, 7, 6, 5, 9, 0, 1]\n",
      "Visited Stops: [8, 3, 4, 7, 6, 5, 9, 0, 1]\n",
      "Visited Stops: [8, 3, 4, 7, 6, 5, 9, 0, 1]\n",
      "Visited Stops: [8, 3, 4, 7, 6, 5, 9, 0, 1]\n",
      "Visited Stops: [8, 3, 4, 7, 6, 5, 9, 0, 1]\n",
      "Visited Stops: [8, 3, 4, 7, 6, 5, 9, 0, 1]\n",
      "Visited Stops: [8, 3, 4, 7, 6, 5, 9, 0, 1]\n",
      "Visited Stops: [8, 3, 4, 7, 6, 5, 9, 0, 1, 2]\n",
      "Epsiode: 5 Score: -1345.02 Episodes: 33\n",
      "Visited Stops: [0, 9]\n",
      "Visited Stops: [0, 9, 6]\n",
      "Visited Stops: [0, 9, 6, 5]\n",
      "Visited Stops: [0, 9, 6, 5]\n",
      "Visited Stops: [0, 9, 6, 5]\n",
      "Visited Stops: [0, 9, 6, 5]\n",
      "Visited Stops: [0, 9, 6, 5, 7]\n",
      "Visited Stops: [0, 9, 6, 5, 7]\n",
      "Visited Stops: [0, 9, 6, 5, 7]\n",
      "Visited Stops: [0, 9, 6, 5, 7, 2]\n",
      "Visited Stops: [0, 9, 6, 5, 7, 2]\n",
      "Visited Stops: [0, 9, 6, 5, 7, 2, 3]\n",
      "Visited Stops: [0, 9, 6, 5, 7, 2, 3, 4]\n",
      "Visited Stops: [0, 9, 6, 5, 7, 2, 3, 4]\n",
      "Visited Stops: [0, 9, 6, 5, 7, 2, 3, 4, 1]\n",
      "Visited Stops: [0, 9, 6, 5, 7, 2, 3, 4, 1]\n",
      "Visited Stops: [0, 9, 6, 5, 7, 2, 3, 4, 1]\n",
      "Visited Stops: [0, 9, 6, 5, 7, 2, 3, 4, 1]\n",
      "Visited Stops: [0, 9, 6, 5, 7, 2, 3, 4, 1]\n",
      "Visited Stops: [0, 9, 6, 5, 7, 2, 3, 4, 1]\n",
      "Visited Stops: [0, 9, 6, 5, 7, 2, 3, 4, 1]\n",
      "Visited Stops: [0, 9, 6, 5, 7, 2, 3, 4, 1]\n",
      "Visited Stops: [0, 9, 6, 5, 7, 2, 3, 4, 1]\n",
      "Visited Stops: [0, 9, 6, 5, 7, 2, 3, 4, 1]\n",
      "Visited Stops: [0, 9, 6, 5, 7, 2, 3, 4, 1]\n",
      "Visited Stops: [0, 9, 6, 5, 7, 2, 3, 4, 1]\n",
      "Visited Stops: [0, 9, 6, 5, 7, 2, 3, 4, 1]\n",
      "Visited Stops: [0, 9, 6, 5, 7, 2, 3, 4, 1, 8]\n",
      "Epsiode: 6 Score: -856.66 Episodes: 28\n",
      "Visited Stops: [7, 6]\n",
      "Visited Stops: [7, 6, 4]\n",
      "Visited Stops: [7, 6, 4, 2]\n",
      "Visited Stops: [7, 6, 4, 2, 3]\n",
      "Visited Stops: [7, 6, 4, 2, 3, 8]\n",
      "Visited Stops: [7, 6, 4, 2, 3, 8, 5]\n",
      "Visited Stops: [7, 6, 4, 2, 3, 8, 5, 9]\n",
      "Visited Stops: [7, 6, 4, 2, 3, 8, 5, 9]\n",
      "Visited Stops: [7, 6, 4, 2, 3, 8, 5, 9]\n",
      "Visited Stops: [7, 6, 4, 2, 3, 8, 5, 9]\n",
      "Visited Stops: [7, 6, 4, 2, 3, 8, 5, 9]\n",
      "Visited Stops: [7, 6, 4, 2, 3, 8, 5, 9]\n",
      "Visited Stops: [7, 6, 4, 2, 3, 8, 5, 9, 1]\n",
      "Visited Stops: [7, 6, 4, 2, 3, 8, 5, 9, 1]\n",
      "Visited Stops: [7, 6, 4, 2, 3, 8, 5, 9, 1]\n",
      "Visited Stops: [7, 6, 4, 2, 3, 8, 5, 9, 1]\n",
      "Visited Stops: [7, 6, 4, 2, 3, 8, 5, 9, 1]\n",
      "Visited Stops: [7, 6, 4, 2, 3, 8, 5, 9, 1, 0]\n",
      "Epsiode: 7 Score: -535.84 Episodes: 18\n",
      "Visited Stops: [3, 5]\n",
      "Visited Stops: [3, 5, 0]\n",
      "Visited Stops: [3, 5, 0]\n",
      "Visited Stops: [3, 5, 0, 8]\n",
      "Visited Stops: [3, 5, 0, 8]\n",
      "Visited Stops: [3, 5, 0, 8, 9]\n",
      "Visited Stops: [3, 5, 0, 8, 9, 4]\n",
      "Visited Stops: [3, 5, 0, 8, 9, 4]\n",
      "Visited Stops: [3, 5, 0, 8, 9, 4, 1]\n",
      "Visited Stops: [3, 5, 0, 8, 9, 4, 1, 6]\n",
      "Visited Stops: [3, 5, 0, 8, 9, 4, 1, 6]\n",
      "Visited Stops: [3, 5, 0, 8, 9, 4, 1, 6]\n",
      "Visited Stops: [3, 5, 0, 8, 9, 4, 1, 6]\n",
      "Visited Stops: [3, 5, 0, 8, 9, 4, 1, 6]\n",
      "Visited Stops: [3, 5, 0, 8, 9, 4, 1, 6, 7]\n",
      "Visited Stops: [3, 5, 0, 8, 9, 4, 1, 6, 7]\n",
      "Visited Stops: [3, 5, 0, 8, 9, 4, 1, 6, 7]\n",
      "Visited Stops: [3, 5, 0, 8, 9, 4, 1, 6, 7]\n",
      "Visited Stops: [3, 5, 0, 8, 9, 4, 1, 6, 7]\n",
      "Visited Stops: [3, 5, 0, 8, 9, 4, 1, 6, 7]\n",
      "Visited Stops: [3, 5, 0, 8, 9, 4, 1, 6, 7]\n",
      "Visited Stops: [3, 5, 0, 8, 9, 4, 1, 6, 7]\n",
      "Visited Stops: [3, 5, 0, 8, 9, 4, 1, 6, 7]\n",
      "Visited Stops: [3, 5, 0, 8, 9, 4, 1, 6, 7]\n",
      "Visited Stops: [3, 5, 0, 8, 9, 4, 1, 6, 7]\n",
      "Visited Stops: [3, 5, 0, 8, 9, 4, 1, 6, 7, 2]\n",
      "Epsiode: 8 Score: -994.61 Episodes: 26\n",
      "Visited Stops: [0]\n",
      "Visited Stops: [0, 6]\n",
      "Visited Stops: [0, 6, 7]\n",
      "Visited Stops: [0, 6, 7, 9]\n",
      "Visited Stops: [0, 6, 7, 9]\n",
      "Visited Stops: [0, 6, 7, 9, 2]\n",
      "Visited Stops: [0, 6, 7, 9, 2, 1]\n",
      "Visited Stops: [0, 6, 7, 9, 2, 1]\n",
      "Visited Stops: [0, 6, 7, 9, 2, 1, 8]\n",
      "Visited Stops: [0, 6, 7, 9, 2, 1, 8, 4]\n",
      "Visited Stops: [0, 6, 7, 9, 2, 1, 8, 4]\n",
      "Visited Stops: [0, 6, 7, 9, 2, 1, 8, 4]\n",
      "Visited Stops: [0, 6, 7, 9, 2, 1, 8, 4, 5]\n",
      "Visited Stops: [0, 6, 7, 9, 2, 1, 8, 4, 5]\n",
      "Visited Stops: [0, 6, 7, 9, 2, 1, 8, 4, 5, 3]\n",
      "Epsiode: 9 Score: -428.07 Episodes: 15\n",
      "Visited Stops: [2, 5]\n",
      "Visited Stops: [2, 5, 8]\n",
      "Visited Stops: [2, 5, 8]\n",
      "Visited Stops: [2, 5, 8]\n",
      "Visited Stops: [2, 5, 8]\n",
      "Visited Stops: [2, 5, 8, 4]\n",
      "Visited Stops: [2, 5, 8, 4, 9]\n",
      "Visited Stops: [2, 5, 8, 4, 9]\n",
      "Visited Stops: [2, 5, 8, 4, 9]\n",
      "Visited Stops: [2, 5, 8, 4, 9, 3]\n",
      "Visited Stops: [2, 5, 8, 4, 9, 3, 7]\n",
      "Visited Stops: [2, 5, 8, 4, 9, 3, 7, 6]\n",
      "Visited Stops: [2, 5, 8, 4, 9, 3, 7, 6]\n",
      "Visited Stops: [2, 5, 8, 4, 9, 3, 7, 6, 1]\n",
      "Visited Stops: [2, 5, 8, 4, 9, 3, 7, 6, 1, 0]\n",
      "Epsiode: 10 Score: -410.91 Episodes: 15\n"
     ]
    }
   ],
   "source": [
    "episodes = 10 \n",
    "\n",
    "env = TSPEnvironment(episodes)\n",
    "\n",
    "\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    Loops = 0\n",
    "    \n",
    "\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score+=reward\n",
    "        Loops += 1\n",
    "    print('Epsiode: {} Score: {} Episodes: {}'.format(episode,score.round(2),Loops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSP-Environment initialized with 50 random stops\n",
      "Starting Point: [35]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'get_config'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [81], line 74\u001b[0m\n\u001b[0;32m     72\u001b[0m agent \u001b[39m=\u001b[39m build_agent(model, action_size)\n\u001b[0;32m     73\u001b[0m \u001b[39m# Train the agent\u001b[39;00m\n\u001b[1;32m---> 74\u001b[0m agent\u001b[39m.\u001b[39;49mcompile(Adam(lr\u001b[39m=\u001b[39;49m\u001b[39m1e-3\u001b[39;49m), metrics\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mmae\u001b[39;49m\u001b[39m'\u001b[39;49m])\n",
      "File \u001b[1;32mc:\\Users\\fabian.woellenweber\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\rl\\agents\\dqn.py:601\u001b[0m, in \u001b[0;36mNAFAgent.compile\u001b[1;34m(self, optimizer, metrics)\u001b[0m\n\u001b[0;32m    598\u001b[0m metrics \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [mean_q]  \u001b[39m# register default metrics\u001b[39;00m\n\u001b[0;32m    600\u001b[0m \u001b[39m# Create target V model. We don't need targets for mu or L.\u001b[39;00m\n\u001b[1;32m--> 601\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_V_model \u001b[39m=\u001b[39m clone_model(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mV_model, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcustom_model_objects)\n\u001b[0;32m    602\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_V_model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msgd\u001b[39m\u001b[39m'\u001b[39m, loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmse\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    604\u001b[0m \u001b[39m# Build combined model.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\fabian.woellenweber\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\rl\\util.py:12\u001b[0m, in \u001b[0;36mclone_model\u001b[1;34m(model, custom_objects)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclone_model\u001b[39m(model, custom_objects\u001b[39m=\u001b[39m{}):\n\u001b[0;32m      9\u001b[0m     \u001b[39m# Requires Keras 1.0.7 since get_config has breaking changes.\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     config \u001b[39m=\u001b[39m {\n\u001b[0;32m     11\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mclass_name\u001b[39m\u001b[39m'\u001b[39m: model\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m,\n\u001b[1;32m---> 12\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mconfig\u001b[39m\u001b[39m'\u001b[39m: model\u001b[39m.\u001b[39;49mget_config(),\n\u001b[0;32m     13\u001b[0m     }\n\u001b[0;32m     14\u001b[0m     clone \u001b[39m=\u001b[39m model_from_config(config, custom_objects\u001b[39m=\u001b[39mcustom_objects)\n\u001b[0;32m     15\u001b[0m     clone\u001b[39m.\u001b[39mset_weights(model\u001b[39m.\u001b[39mget_weights())\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'get_config'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from rl.agents import DQNAgent, DDPGAgent, ContinuousDQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from tensorflow.keras.layers import Lambda\n",
    "\n",
    "\n",
    "# Learning factors...\n",
    "EPISODES = 50\n",
    "discount_factor = 0.99\n",
    "learning_rate = 0.001\n",
    "epsilon_start = 1.0\n",
    "epsilon_decay_steps = 0.999\n",
    "epsilon_end = 0.01\n",
    "batch_size = 64\n",
    "train_start = 1000\n",
    "\n",
    "# create replay memory using deque\n",
    "#memory = deque(maxlen=2000)\n",
    "\n",
    "# Create gym environment\n",
    "env = TSPEnvironment(EPISODES)\n",
    "\n",
    "state_size = env.observation_space.n\n",
    "action_size = env.action_space.n\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "loss_fn = tf.keras.losses.mean_squared_error\n",
    "\n",
    "# Create a replay buffer to store transitions\n",
    "replay_buffer = []\n",
    "\n",
    "# Define the update frequency and batch size\n",
    "update_frequency = 10\n",
    "batch_size = 10\n",
    "\n",
    "# The following function creates a neural network which is used as an \n",
    "# approximate Q function\n",
    "# Input: state \n",
    "# Output: Q Value of each action\n",
    "def build_model(state_size, action_size):\n",
    "    model = Sequential()\n",
    "    model.get_config()\n",
    "    model.add(Dense(32, input_dim=state_size, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(action_size, activation='linear'))\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "# Create the TSP agent\n",
    "def build_agent(model, action_size):\n",
    "    # Use Epsilon-Greedy policy for exploration\n",
    "    policy = EpsGreedyQPolicy()\n",
    "    # Create memory for storing transitions\n",
    "    memory = SequentialMemory(limit=50000, window_length=1)\n",
    "    # Create the DQN agent\n",
    "    agent = ContinuousDQNAgent(nb_actions=action_size, memory=memory, nb_steps_warmup=10,\n",
    "                               V_model=None, L_model=None, mu_model=None)\n",
    "    \n",
    "    return agent\n",
    "\n",
    "\n",
    "# Create the TSP model\n",
    "model = build_model(state_size, action_size)\n",
    "# Create the TSP agent\n",
    "agent = build_agent(model, action_size)\n",
    "# Train the agent\n",
    "agent.compile(Adam(lr=1e-3), metrics=['mae'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fabian.woellenweber\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'get_config'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [80], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m agent\u001b[39m.\u001b[39;49mcompile(Adam(lr\u001b[39m=\u001b[39;49m\u001b[39m1e-3\u001b[39;49m), metrics\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mmae\u001b[39;49m\u001b[39m'\u001b[39;49m])\n",
      "File \u001b[1;32mc:\\Users\\fabian.woellenweber\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\rl\\agents\\dqn.py:601\u001b[0m, in \u001b[0;36mNAFAgent.compile\u001b[1;34m(self, optimizer, metrics)\u001b[0m\n\u001b[0;32m    598\u001b[0m metrics \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [mean_q]  \u001b[39m# register default metrics\u001b[39;00m\n\u001b[0;32m    600\u001b[0m \u001b[39m# Create target V model. We don't need targets for mu or L.\u001b[39;00m\n\u001b[1;32m--> 601\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_V_model \u001b[39m=\u001b[39m clone_model(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mV_model, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcustom_model_objects)\n\u001b[0;32m    602\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_V_model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msgd\u001b[39m\u001b[39m'\u001b[39m, loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmse\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    604\u001b[0m \u001b[39m# Build combined model.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\fabian.woellenweber\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\rl\\util.py:12\u001b[0m, in \u001b[0;36mclone_model\u001b[1;34m(model, custom_objects)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclone_model\u001b[39m(model, custom_objects\u001b[39m=\u001b[39m{}):\n\u001b[0;32m      9\u001b[0m     \u001b[39m# Requires Keras 1.0.7 since get_config has breaking changes.\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     config \u001b[39m=\u001b[39m {\n\u001b[0;32m     11\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mclass_name\u001b[39m\u001b[39m'\u001b[39m: model\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m,\n\u001b[1;32m---> 12\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mconfig\u001b[39m\u001b[39m'\u001b[39m: model\u001b[39m.\u001b[39;49mget_config(),\n\u001b[0;32m     13\u001b[0m     }\n\u001b[0;32m     14\u001b[0m     clone \u001b[39m=\u001b[39m model_from_config(config, custom_objects\u001b[39m=\u001b[39mcustom_objects)\n\u001b[0;32m     15\u001b[0m     clone\u001b[39m.\u001b[39mset_weights(model\u001b[39m.\u001b[39mget_weights())\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'get_config'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSP-Environment initialized with 99 random stops\n",
      "Starting Point: [50]\n",
      "Model: \"sequential_40\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_114 (Dense)           (None, 32)                3200      \n",
      "                                                                 \n",
      " dense_115 (Dense)           (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_116 (Dense)           (None, 99)                3267      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,523\n",
      "Trainable params: 7,523\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_41\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_117 (Dense)           (None, 32)                3200      \n",
      "                                                                 \n",
      " dense_118 (Dense)           (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_119 (Dense)           (None, 99)                3267      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,523\n",
      "Trainable params: 7,523\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Visited Stops: [62, 36]\n",
      "Visited Stops: [62, 36, 47]\n",
      "Visited Stops: [62, 36, 47, 78]\n",
      "Visited Stops: [62, 36, 47, 78, 29]\n",
      "Visited Stops: [62, 36, 47, 78, 29, 22]\n",
      "Visited Stops: [62, 36, 47, 78, 29, 22, 4]\n",
      "Visited Stops: [62, 36, 47, 78, 29, 22, 4, 49]\n",
      "Visited Stops: [62, 36, 47, 78, 29, 22, 4, 49, 3]\n",
      "Visited Stops: [62, 36, 47, 78, 29, 22, 4, 49, 3, 84]\n",
      "Visited Stops: [62, 36, 47, 78, 29, 22, 4, 49, 3, 84, 10]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'int' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [58], line 91\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[39m# If the replay buffer has reached a certain size, sample a batch of transitions and update the model\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(replay_buffer) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m batch_size:\n\u001b[0;32m     90\u001b[0m     \u001b[39m# Sample a batch of transitions\u001b[39;00m\n\u001b[1;32m---> 91\u001b[0m     transitions \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39msample(replay_buffer,\u001b[39mlen\u001b[39;49m(batch_size))\n\u001b[0;32m     92\u001b[0m     states, actions, rewards, next_states \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mtransitions)\n\u001b[0;32m     94\u001b[0m     \u001b[39m# Convert the states and next_states to tensors\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'int' has no len()"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "## Initialize important variables\n",
    "env_name = 'TSPEnv'\n",
    "bRender = False\n",
    "load_model = False\n",
    "\n",
    "# Learning factors...\n",
    "EPISODES = 99\n",
    "discount_factor = 0.99\n",
    "learning_rate = 0.001\n",
    "epsilon_start = 1.0\n",
    "epsilon_decay_steps = 0.999\n",
    "epsilon_end = 0.01\n",
    "batch_size = 64\n",
    "train_start = 1000\n",
    "\n",
    "# create replay memory using deque\n",
    "#memory = deque(maxlen=2000)\n",
    "\n",
    "# Create gym environment\n",
    "env = TSPEnvironment(EPISODES)\n",
    "\n",
    "state_size = env.observation_space.n\n",
    "action_size = env.action_space.n\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "loss_fn = tf.keras.losses.mean_squared_error\n",
    "\n",
    "# Create a replay buffer to store transitions\n",
    "replay_buffer = []\n",
    "\n",
    "# Define the update frequency and batch size\n",
    "update_frequency = 10\n",
    "batch_size = 10\n",
    "\n",
    "# The following function creates a neural network which is used as an \n",
    "# approximate Q function\n",
    "# Input: state \n",
    "# Output: Q Value of each action\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, input_dim=state_size, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(32, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(action_size, activation='linear', kernel_initializer='he_uniform'))\n",
    "    model.summary()\n",
    "    model.compile(loss='mse', optimizer=Adam(learning_rate=learning_rate))\n",
    "    return model\n",
    "\n",
    "# after some time interval update the target model to be same with model\n",
    "# This is done using the following function\n",
    "def update_target_model():\n",
    "    target_model.set_weights(model.get_weights())\n",
    "\n",
    "# create main model and target model\n",
    "model = build_model()\n",
    "target_model = build_model()\n",
    "\n",
    "# We call that function in the beginning to make sure model and target_model have the same weights initally\n",
    "update_target_model()\n",
    "\n",
    "# Run the TSP loop\n",
    "num_episodes = 50\n",
    "for episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    state = env.reset()\n",
    "\n",
    "    # Calculate the epsilon for this episode\n",
    "    epsilon = epsilon_end + (epsilon_start - epsilon_end) * np.exp(-episode / epsilon_decay_steps)\n",
    "\n",
    "    while True:\n",
    "        # Select an action using the DQN model and an epsilon-greedy policy\n",
    "        if np.random.rand() < epsilon:\n",
    "            # Explore: choose a random action\n",
    "            action = np.random.randint(env.action_space.n)\n",
    "        else:\n",
    "            # Exploit: choose the action with the highest Q-value\n",
    "            action = np.argmax(model(np.expand_dims(state, axis=0)).numpy())\n",
    "\n",
    "        # Take the action and observe the reward and next state\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # Store the transition in the replay buffer\n",
    "        replay_buffer.append((state, action, reward, next_state))\n",
    "\n",
    "        # If the replay buffer has reached a certain size, sample a batch of transitions and update the model\n",
    "        if len(replay_buffer) >= batch_size:\n",
    "            # Sample a batch of transitions\n",
    "            transitions = np.random.sample(replay_buffer,batch_size)\n",
    "            states, actions, rewards, next_states = zip(*transitions)\n",
    "\n",
    "            # Convert the states and next_states to tensors\n",
    "            states = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "            next_states = tf.convert_to_tensor(next_states, dtype=tf.float32)\n",
    "            \n",
    "            print(f'States: {states}')\n",
    "            print(f'States: {next_states}')\n",
    "\n",
    "            # Compute the Q-values for the current states and the next states\n",
    "            current_q_values = model(states)\n",
    "            next_q_values = target_model(next_states)\n",
    "\n",
    "            # Compute the expected Q-values\n",
    "            expected_q_values = rewards + np.max(next_q_values.numpy(), axis=1)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = loss_fn(expected_q_values, current_q_values)\n",
    "\n",
    "            # Update the model\n",
    "            optimizer.minimize(loss, model.trainable_variables)\n",
    "\n",
    "            # Clear the replay buffer\n",
    "            replay_buffer = []\n",
    "\n",
    "        # Update the target model every `update_frequency` steps\n",
    "        if episode % update_frequency == 0:\n",
    "            update_target_model()\n",
    "\n",
    "        # Update the state\n",
    "        state = next_state\n",
    "\n",
    "        # If the episode is done, break out of the inner loop\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5a312273ffa6240539e020f48a5de0b17350fba060a017bafd7c66c880b86767"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
