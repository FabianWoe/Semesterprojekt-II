{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #!/usr/bin/env python\n",
    "# # -*- coding: utf-8 -*- \n",
    "\n",
    "\n",
    "# \"\"\"--------------------------------------------------------------------\n",
    "# REINFORCEMENT LEARNING\n",
    "\n",
    "# Started on the 25/08/2017\n",
    "\n",
    "\n",
    "# theo.alves.da.costa@gmail.com\n",
    "# https://github.com/theolvs\n",
    "# ------------------------------------------------------------------------\n",
    "# \"\"\"\n",
    "\n",
    "# import os\n",
    "# import matplotlib.pyplot as plt\n",
    "# import pandas as pd\n",
    "# import numpy as np \n",
    "# import sys\n",
    "# import random\n",
    "# import time\n",
    "# import random\n",
    "# import numpy as np\n",
    "\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "# from keras.optimizers import Adam\n",
    "\n",
    "# from rl.memory import Memory\n",
    "\n",
    "# class Agent(object):\n",
    "#     def __init__(self):\n",
    "#         pass\n",
    "\n",
    "\n",
    "#     def expand_state_vector(self,state):\n",
    "#         if len(state.shape) == 1 or len(state.shape)==3:\n",
    "#             return np.expand_dims(state,axis = 0)\n",
    "#         else:\n",
    "#             return state\n",
    "\n",
    "\n",
    "\n",
    "#     def remember(self,*args):\n",
    "#         self.memory.save(args)\n",
    "\n",
    "# class QAgent(Agent):\n",
    "#     def __init__(self,states_size,actions_size,epsilon = 1.0,epsilon_min = 0.01,epsilon_decay = 0.999,gamma = 0.95,lr = 0.8):\n",
    "#         self.states_size = states_size\n",
    "#         self.actions_size = actions_size\n",
    "#         self.epsilon = epsilon\n",
    "#         self.epsilon_min = epsilon_min\n",
    "#         self.epsilon_decay = epsilon_decay\n",
    "#         self.gamma = gamma\n",
    "#         self.lr = lr\n",
    "#         self.Q = self.build_model(states_size,actions_size)\n",
    "\n",
    "\n",
    "#     def build_model(self,states_size,actions_size):\n",
    "#         Q = np.zeros([states_size,actions_size])\n",
    "#         return Q\n",
    "\n",
    "\n",
    "#     def train(self,s,a,r,s_next):\n",
    "#         self.Q[s,a] = self.Q[s,a] + self.lr * (r + self.gamma*np.max(self.Q[s_next,a]) - self.Q[s,a])\n",
    "\n",
    "#         if self.epsilon > self.epsilon_min:\n",
    "#             self.epsilon *= self.epsilon_decay\n",
    "\n",
    "\n",
    "#     def act(self,s):\n",
    "\n",
    "#         q = self.Q[s,:]\n",
    "\n",
    "#         if np.random.rand() > self.epsilon:\n",
    "#             a = np.argmax(q)\n",
    "#         else:\n",
    "#             a = np.random.randint(self.actions_size)\n",
    "\n",
    "#         return a\n",
    "\n",
    "# class DQNAgentTSP(Agent):\n",
    "#     def __init__(self,states_size,actions_size,epsilon = 1.0,epsilon_min = 0.01,epsilon_decay = 0.995,gamma = 0.95,lr = 0.001,low = 0,high = 1,max_memory = 2000,observation_type = \"discrete\"):\n",
    "#         assert observation_type in [\"discrete\",\"continuous\"]\n",
    "#         self.states_size = states_size\n",
    "#         self.actions_size = actions_size\n",
    "#         self.memory = Memory(max_memory = max_memory)\n",
    "#         self.epsilon = epsilon\n",
    "#         self.low = low\n",
    "#         self.high = high\n",
    "#         self.observation_type = observation_type\n",
    "#         self.epsilon_min = epsilon_min\n",
    "#         self.epsilon_decay = epsilon_decay\n",
    "#         self.gamma = gamma\n",
    "#         self.lr = lr\n",
    "#         self.model = self.build_model(states_size,actions_size)\n",
    "\n",
    "#     def build_model(self,states_size,actions_size):\n",
    "#         model = Sequential()\n",
    "#         model.add(Dense(24,input_dim = states_size,activation = \"relu\"))\n",
    "#         model.add(Dense(24,activation = \"relu\"))\n",
    "#         model.add(Dense(actions_size,activation = \"linear\"))\n",
    "#         model.compile(loss='mse',\n",
    "#                       optimizer=Adam(lr=self.lr))\n",
    "#         return model\n",
    "\n",
    "#     def train(self,batch_size = 32):\n",
    "#         if len(self.memory.cache) > batch_size:\n",
    "#             batch = random.sample(self.memory.cache, batch_size)\n",
    "#         else:\n",
    "#             batch = self.memory.cache\n",
    "\n",
    "#         for state,action,reward,next_state,done in batch:\n",
    "#             state = self.expand_state_vector(state)\n",
    "#             next_state = self.expand_state_vector(next_state)\n",
    "\n",
    "\n",
    "#             targets = self.model.predict(state)\n",
    "\n",
    "#             if not done:\n",
    "#                 target = reward + self.gamma * np.max(self.model.predict(next_state))\n",
    "#             else:\n",
    "#                 target = reward\n",
    "\n",
    "#             targets[0][action] = target\n",
    "\n",
    "#             self.model.fit(state,targets,epochs = 1,verbose = 0)\n",
    "\n",
    "\n",
    "#         if self.epsilon > self.epsilon_min:\n",
    "#             self.epsilon *= self.epsilon_decay\n",
    "\n",
    "#     def act(self,state):\n",
    "#         state = self.expand_state_vector(state)\n",
    "\n",
    "\n",
    "#         if np.random.rand() > self.epsilon:\n",
    "#             q = self.model.predict(state)\n",
    "\n",
    "#             if self.observation_type == \"discrete\":\n",
    "#                 a = np.argmax(q[0])\n",
    "#             elif self.observation_type == \"continuous\":\n",
    "#                 a = np.squeeze(np.clip(q,self.low,self.high))\n",
    "\n",
    "#         else:\n",
    "#             if self.observation_type == \"discrete\":\n",
    "#                 a = np.random.randint(self.actions_size)\n",
    "#             elif self.observation_type == \"continuous\":\n",
    "#                 a = np.random.uniform(self.low,self.high,self.actions_size)\n",
    "#         return a \n",
    "\n",
    "# class DeliveryQAgent(QAgent):\n",
    "\n",
    "#     def __init__(self,*args,**kwargs):\n",
    "#         super().__init__(*args,**kwargs)\n",
    "#         self.reset_memory()\n",
    "\n",
    "#     def act(self,s):\n",
    "\n",
    "#         # Get Q Vector\n",
    "#         q = np.copy(self.Q[s,:])\n",
    "\n",
    "#         # Avoid already visited states\n",
    "#         q[self.states_memory] = -np.inf\n",
    "\n",
    "#         if np.random.rand() > self.epsilon:\n",
    "#             a = np.argmax(q)\n",
    "#         else:\n",
    "#             a = np.random.choice([x for x in range(self.actions_size) if x not in self.states_memory])\n",
    "\n",
    "#         return a\n",
    "\n",
    "\n",
    "#     def remember_state(self,s):\n",
    "#         self.states_memory.append(s)\n",
    "\n",
    "#     def reset_memory(self):\n",
    "#         self.states_memory = []\n",
    "\n",
    "\n",
    "\n",
    "# def run_n_episodes(env,agent,name=\"training.gif\",n_episodes=1000,render_each=10,fps=10):\n",
    "\n",
    "#     # Store the rewards\n",
    "#     rewards = []\n",
    "#     imgs = []\n",
    "\n",
    "#     # Experience replay\n",
    "#     for i in tqdm_notebook(range(n_episodes)):\n",
    "\n",
    "#         # Run the episode\n",
    "#         env,agent,episode_reward = run_episode(env,agent,verbose = 0)\n",
    "#         rewards.append(episode_reward)\n",
    "        \n",
    "#         if i % render_each == 0:\n",
    "#             img = env.render(return_img = True)\n",
    "#             imgs.append(img)\n",
    "\n",
    "#     # Show rewards\n",
    "#     plt.figure(figsize = (15,3))\n",
    "#     plt.title(\"Rewards over training\")\n",
    "#     plt.plot(rewards)\n",
    "#     plt.show()\n",
    "\n",
    "#     # Save imgs as gif\n",
    "#     imageio.mimsave(name,imgs,fps = fps)\n",
    "\n",
    "#     return env,agent\n",
    "\n",
    "\n",
    "# class DeliveryQAgent(QAgent):\n",
    "\n",
    "#     def __init__(self,*args,**kwargs):\n",
    "#         super().__init__(*args,**kwargs)\n",
    "#         self.reset_memory()\n",
    "\n",
    "#     def act(self,s):\n",
    "\n",
    "#         # Get Q Vector\n",
    "#         q = np.copy(self.Q[s,:])\n",
    "\n",
    "#         # Avoid already visited states\n",
    "#         q[self.states_memory] = -np.inf\n",
    "\n",
    "#         if np.random.rand() > self.epsilon:\n",
    "#             a = np.argmax(q)\n",
    "#         else:\n",
    "#             a = np.random.choice([x for x in range(self.actions_size) if x not in self.states_memory])\n",
    "\n",
    "#         return a\n",
    "\n",
    "\n",
    "#     def remember_state(self,s):\n",
    "#         self.states_memory.append(s)\n",
    "\n",
    "#     def reset_memory(self):\n",
    "#         self.states_memory = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TSP Environment\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "\n",
    "\n",
    "# Define the environment\n",
    "class TSPEnv(gym.Env):\n",
    "  def __init__(self, n_cities=4, show_debug_data = False):\n",
    "    self.n_cities = n_cities\n",
    "    # self.xy = (np.random.rand(self.n_cities,2)*100).round(2)\n",
    "    self.xy = np.array([[20.01, 80.01], [10.01, 50.01],[50.01, 30.01],[80.01 , 80.01]])\n",
    "    self.x=self.xy[:,0]\n",
    "    self.y=self.xy[:,1]\n",
    "    self.step_counter = 0\n",
    "    self.show_debug_data = show_debug_data\n",
    "    self._array_visited = np.zeros(self.n_cities)\n",
    "    self._total_distance = 0\n",
    "\n",
    "    #print(f'genrated stops xy: {self.xy}')\n",
    "    self.distance_matrix = cdist(self.xy,self.xy,'euclidean').round(0)\n",
    "    # print(self.distance_matrix.shape)\n",
    "\n",
    "    self.current_city = np.random.randint(n_cities)\n",
    "    self.visited_cities = [self.current_city]\n",
    "    self._array_visited[self.current_city] = 1\n",
    "    self.remaining_cities = [i for i in range(n_cities)]\n",
    "    self.cities_list = [i for i in range(n_cities)]\n",
    "    self.remaining_cities.remove(self.current_city)\n",
    "    # Define the action space\n",
    "    self.action_space = gym.spaces.Discrete(n_cities)\n",
    "\n",
    "    high = np.array(\n",
    "      [\n",
    "        np.zeros(4),\n",
    "        np.zeros(4),\n",
    "      ],\n",
    "      dtype=np.int32,\n",
    "    )\n",
    "\n",
    "    # Define the observation space\n",
    "    self.observation_space = gym.spaces.Box(-high, high, dtype=np.int32)\n",
    "    #self.observation_space = gym.spaces.MultiBinary(n_cities)\n",
    "\n",
    "    if(self.show_debug_data):\n",
    "        print(f'Current city in init: {self.current_city}')\n",
    "        print(f'Remaining city in init: {self.remaining_cities}')\n",
    "        print(f'Visited city in init: {self.visited_cities}')\n",
    "        print(f'Stepcounter in init: {self.step_counter}')\n",
    "        print(f'Observation in init: {self._array_visited}')\n",
    "\n",
    "  def reset(self):\n",
    "    self.step_counter = 0\n",
    "    self._total_distance = 0\n",
    "    self._array_visited = np.zeros(self.n_cities)\n",
    "    self.current_city = np.random.randint(self.n_cities)\n",
    "    self._array_visited[self.current_city] = 1\n",
    "    self.visited_cities = [self.current_city]\n",
    "    self.remaining_cities = [i for i in range(self.n_cities)]\n",
    "    self.remaining_cities.remove(self.current_city)\n",
    "    \n",
    "    if(self.show_debug_data):\n",
    "      print(f'Observation in Reset: {self._array_visited}')\n",
    "    return self._get_observation()\n",
    "\n",
    "  def step(self, action):\n",
    "    done = False\n",
    "    self.step_counter += 1\n",
    "    reward = -999\n",
    "\n",
    "    if(self.show_debug_data):\n",
    "      print(f'Action in Step(top): {action}')\n",
    "      print(f'Remaining-City Step(top): {self.remaining_cities}')\n",
    "      print(f'Observation in step: {self._array_visited}')\n",
    "    \n",
    "    if (action in self.remaining_cities):\n",
    "      if(self.show_debug_data):\n",
    "        print('Action True')\n",
    "      \n",
    "      reward = -self.distance_matrix[self.current_city][action]\n",
    "      self._total_distance += reward\n",
    "\n",
    "      self.remaining_cities.remove(action)\n",
    "      self.visited_cities.append(action)\n",
    "      self._array_visited[action] = 1\n",
    "      self.current_city = action\n",
    "\n",
    "      if (len(self.remaining_cities) == 0):\n",
    "        startingpoint = self.visited_cities[0]\n",
    "        self.visited_cities.append(startingpoint)\n",
    "        reward += -self.distance_matrix[self.current_city][startingpoint]\n",
    "        done = True\n",
    "      \n",
    "      if(self.show_debug_data):\n",
    "        print(f'Action in step: {action}')\n",
    "        print(f'Reward in step: {reward}')\n",
    "        print(f'Current city in step: {self.current_city}')\n",
    "        print(f'Remaining city in step: {self.remaining_cities}')\n",
    "        print(f'Visited city in step: {self.visited_cities}')\n",
    "        print(f'Stepcounter in step: {self.step_counter}')\n",
    "        print(f'Observation in step: {self._array_visited}')\n",
    "      \n",
    "      return self._get_observation(), reward, done, {}\n",
    "    else:\n",
    "        return self._get_observation(), reward, False, {}\n",
    "\n",
    "\n",
    "\n",
    "  def _get_observation(self):\n",
    "    observationDistance = self.distance_matrix[self.current_city]\n",
    "    # observation[self.current_city] = 1\n",
    "    # return observation\n",
    "    observation = self._array_visited, observationDistance\n",
    "    \n",
    "    return np.ravel(observation)\n",
    "\n",
    "  \n",
    "  def _test_distance(self,CurrentCity, NextCity):\n",
    "    return -self.distance_matrix[CurrentCity][NextCity]\n",
    "    \n",
    "  def plotCities(self):\n",
    "    fig, ax = plt.subplots(1, figsize=(7,7))\n",
    "    fig.suptitle = \"Delivery Stops\"\n",
    "    plt.scatter(self.x,self.y)\n",
    "    xcoord = []\n",
    "    ycoord = []\n",
    "    for i in range(0,len(self.visited_cities)):\n",
    "      xcoord.append(self.x[self.visited_cities[i]])\n",
    "      ycoord.append(self.y[self.visited_cities[i]])\n",
    "      if(i == 0):\n",
    "        ax.annotate(\"Anfang\", xy=(xcoord[i], ycoord[i]), xytext=(xcoord[i]+0.5, ycoord[i]))\n",
    "      ax.annotate(str(i), xy=(xcoord[i], ycoord[i]), xytext=(xcoord[i]+0.5, ycoord[i]))\n",
    "\n",
    "    plt.plot(xcoord, ycoord)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TSPEnv(4,False)\n",
    "samples = []\n",
    "\n",
    "print('----------')\n",
    "for i in range(0,env.n_cities):\n",
    "    samples.append(env.step(i))\n",
    "print(env.visited_cities)    \n",
    "env.plotCities()\n",
    "\n",
    "print(env.step(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Buffer\n",
    "import torch\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class Buffer:\n",
    "    \n",
    "    def __init__(self, batch_size, buffer_size, seed):\n",
    "        \n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.rand_generator = np.random.RandomState(seed)\n",
    "        \n",
    "        self.buffer = []\n",
    "        \n",
    "        \n",
    "    def append(self, state, action, terminal, reward, next_state):\n",
    "        \"\"\"\n",
    "        Append the next experience.\n",
    "        \n",
    "        Args:\n",
    "            state: the state (torch tensor).\n",
    "            action: the action (integer).\n",
    "            terminal: 1 if the next state is the terminal, 0 otherwise.\n",
    "        \n",
    "        \"\"\"\n",
    "        # delete the first experience if the size is reaching the maximum\n",
    "        if len(self.buffer) == self.buffer_size:\n",
    "            del self.buffer[0]\n",
    "            \n",
    "        self.buffer.append((torch.tensor(state), torch.tensor(action), torch.tensor(terminal), torch.tensor(reward).float(), torch.tensor(next_state)))\n",
    "        \n",
    "    def sample(self):\n",
    "        \"\"\"\n",
    "        Sample from the buffer and return the virtual experience.\n",
    "        \n",
    "        Args:\n",
    "            None\n",
    "        Returns:\n",
    "            A list of transition tuples (state, action, terminal, reward, next_state), list length: batch_size\n",
    "        \"\"\"\n",
    "        \n",
    "        indexs = self.rand_generator.choice(len(self.buffer), size = self.batch_size)\n",
    "        \n",
    "        transitions = [self.buffer[idx] for idx in indexs]\n",
    "        \n",
    "        return transitions\n",
    "       \n",
    "    def get_buffer(self):\n",
    "        \"\"\"\n",
    "        Return the current buffer\n",
    "        \"\"\"\n",
    "        return self.buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_4 (Flatten)         (None, 8)                 0         \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 24)                216       \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 24)                600       \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 4)                 100       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 916\n",
      "Trainable params: 916\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Training for 50000 steps ...\n",
      "     8/50000: episode: 1, duration: 0.088s, episode steps:   8, steps per second:  91, episode reward: -5219.000, mean reward: -652.375 [-999.000, -58.000], mean action: 1.000 [0.000, 3.000],  loss: --, mae: --, accuracy: --, mean_q: --, mean_eps: --\n",
      "    11/50000: episode: 2, duration: 0.001s, episode steps:   3, steps per second: 2457, episode reward: -224.000, mean reward: -74.667 [-116.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: --, mae: --, accuracy: --, mean_q: --, mean_eps: --\n",
      "    20/50000: episode: 3, duration: 0.003s, episode steps:   9, steps per second: 3163, episode reward: -6233.000, mean reward: -692.556 [-999.000, -45.000], mean action: 1.556 [0.000, 3.000],  loss: --, mae: --, accuracy: --, mean_q: --, mean_eps: --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fabian.woellenweber\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "c:\\Users\\fabian.woellenweber\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\rl\\memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n",
      "c:\\Users\\fabian.woellenweber\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    27/50000: episode: 4, duration: 0.471s, episode steps:   7, steps per second:  15, episode reward: -4235.000, mean reward: -605.000 [-999.000, -60.000], mean action: 1.429 [0.000, 3.000],  loss: 307982.000000, mae: 166.166534, accuracy: 0.375000, mean_q: 12.423618, mean_eps: 0.997660\n",
      "    36/50000: episode: 5, duration: 0.041s, episode steps:   9, steps per second: 219, episode reward: -6218.000, mean reward: -690.889 [-999.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 280596.680556, mae: 153.572845, accuracy: 0.479167, mean_q: 8.018642, mean_eps: 0.997210\n",
      "    49/50000: episode: 6, duration: 0.051s, episode steps:  13, steps per second: 255, episode reward: -10214.000, mean reward: -785.692 [-999.000, -32.000], mean action: 1.154 [0.000, 3.000],  loss: 300744.250000, mae: 163.332781, accuracy: 0.288462, mean_q: 1.693867, mean_eps: 0.996220\n",
      "    57/50000: episode: 7, duration: 0.031s, episode steps:   8, steps per second: 258, episode reward: -5234.000, mean reward: -654.250 [-999.000, -45.000], mean action: 1.750 [0.000, 3.000],  loss: 313690.816406, mae: 173.759548, accuracy: 0.121094, mean_q: -0.079108, mean_eps: 0.995275\n",
      "    60/50000: episode: 8, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 324963.385417, mae: 182.180216, accuracy: 0.104167, mean_q: -0.442301, mean_eps: 0.994780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fabian.woellenweber\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\rl\\memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    83/50000: episode: 9, duration: 0.073s, episode steps:  23, steps per second: 313, episode reward: -20204.000, mean reward: -878.435 [-999.000, -58.000], mean action: 1.696 [0.000, 3.000],  loss: 300736.954484, mae: 177.083742, accuracy: 0.116848, mean_q: -2.389409, mean_eps: 0.993610\n",
      "    87/50000: episode: 10, duration: 0.015s, episode steps:   4, steps per second: 265, episode reward: -1238.000, mean reward: -309.500 [-999.000, -45.000], mean action: 1.250 [0.000, 3.000],  loss: 298332.921875, mae: 183.206657, accuracy: 0.031250, mean_q: -7.521979, mean_eps: 0.992395\n",
      "    93/50000: episode: 11, duration: 0.020s, episode steps:   6, steps per second: 297, episode reward: -3221.000, mean reward: -536.833 [-999.000, -58.000], mean action: 1.833 [0.000, 3.000],  loss: 295482.067708, mae: 187.988480, accuracy: 0.000000, mean_q: -11.663946, mean_eps: 0.991945\n",
      "    99/50000: episode: 12, duration: 0.022s, episode steps:   6, steps per second: 279, episode reward: -3192.000, mean reward: -532.000 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 288889.911458, mae: 192.295392, accuracy: 0.000000, mean_q: -16.477964, mean_eps: 0.991405\n",
      "   104/50000: episode: 13, duration: 0.022s, episode steps:   5, steps per second: 231, episode reward: -2237.000, mean reward: -447.400 [-999.000, -45.000], mean action: 1.600 [0.000, 3.000],  loss: 260382.156250, mae: 185.781308, accuracy: 0.000000, mean_q: -22.949715, mean_eps: 0.990910\n",
      "   112/50000: episode: 14, duration: 0.027s, episode steps:   8, steps per second: 292, episode reward: -5219.000, mean reward: -652.375 [-999.000, -58.000], mean action: 2.125 [0.000, 3.000],  loss: 270365.300781, mae: 200.671949, accuracy: 0.000000, mean_q: -30.771106, mean_eps: 0.990325\n",
      "   119/50000: episode: 15, duration: 0.024s, episode steps:   7, steps per second: 287, episode reward: -4220.000, mean reward: -602.857 [-999.000, -32.000], mean action: 1.143 [0.000, 3.000],  loss: 243647.794643, mae: 201.049532, accuracy: 0.000000, mean_q: -44.363019, mean_eps: 0.989650\n",
      "   126/50000: episode: 16, duration: 0.024s, episode steps:   7, steps per second: 290, episode reward: -4220.000, mean reward: -602.857 [-999.000, -32.000], mean action: 1.429 [0.000, 3.000],  loss: 272813.383929, mae: 231.596832, accuracy: 0.000000, mean_q: -57.610817, mean_eps: 0.989020\n",
      "   149/50000: episode: 17, duration: 0.076s, episode steps:  23, steps per second: 302, episode reward: -20175.000, mean reward: -877.174 [-999.000, -32.000], mean action: 0.957 [0.000, 3.000],  loss: 236790.767663, mae: 253.273995, accuracy: 0.000000, mean_q: -89.784135, mean_eps: 0.987670\n",
      "   157/50000: episode: 18, duration: 0.033s, episode steps:   8, steps per second: 244, episode reward: -5219.000, mean reward: -652.375 [-999.000, -58.000], mean action: 2.125 [0.000, 3.000],  loss: 218624.123047, mae: 293.498081, accuracy: 0.000000, mean_q: -130.866733, mean_eps: 0.986275\n",
      "   162/50000: episode: 19, duration: 0.022s, episode steps:   5, steps per second: 226, episode reward: -2222.000, mean reward: -444.400 [-999.000, -58.000], mean action: 1.200 [0.000, 3.000],  loss: 207149.306250, mae: 316.046216, accuracy: 0.000000, mean_q: -153.835410, mean_eps: 0.985690\n",
      "   166/50000: episode: 20, duration: 0.019s, episode steps:   4, steps per second: 212, episode reward: -1238.000, mean reward: -309.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 181232.257812, mae: 326.286491, accuracy: 0.000000, mean_q: -173.764206, mean_eps: 0.985285\n",
      "   173/50000: episode: 21, duration: 0.028s, episode steps:   7, steps per second: 248, episode reward: -4235.000, mean reward: -605.000 [-999.000, -60.000], mean action: 1.429 [0.000, 3.000],  loss: 162084.950893, mae: 343.828631, accuracy: 0.000000, mean_q: -199.405396, mean_eps: 0.984790\n",
      "   178/50000: episode: 22, duration: 0.019s, episode steps:   5, steps per second: 257, episode reward: -2237.000, mean reward: -447.400 [-999.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 158633.315625, mae: 373.671204, accuracy: 0.000000, mean_q: -229.525378, mean_eps: 0.984250\n",
      "   182/50000: episode: 23, duration: 0.015s, episode steps:   4, steps per second: 273, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 166709.722656, mae: 396.932373, accuracy: 0.000000, mean_q: -251.300533, mean_eps: 0.983845\n",
      "   188/50000: episode: 24, duration: 0.021s, episode steps:   6, steps per second: 282, episode reward: -3192.000, mean reward: -532.000 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 140123.399740, mae: 411.405858, accuracy: 0.000000, mean_q: -279.122350, mean_eps: 0.983395\n",
      "   195/50000: episode: 25, duration: 0.024s, episode steps:   7, steps per second: 290, episode reward: -4220.000, mean reward: -602.857 [-999.000, -58.000], mean action: 2.000 [0.000, 3.000],  loss: 133606.436384, mae: 443.479318, accuracy: 0.000000, mean_q: -314.589273, mean_eps: 0.982810\n",
      "   198/50000: episode: 26, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 123674.835938, mae: 458.309977, accuracy: 0.000000, mean_q: -341.931508, mean_eps: 0.982360\n",
      "   207/50000: episode: 27, duration: 0.030s, episode steps:   9, steps per second: 298, episode reward: -6233.000, mean reward: -692.556 [-999.000, -45.000], mean action: 1.889 [0.000, 3.000],  loss: 126785.955729, mae: 491.317434, accuracy: 0.000000, mean_q: -373.820177, mean_eps: 0.981820\n",
      "   212/50000: episode: 28, duration: 0.019s, episode steps:   5, steps per second: 265, episode reward: -2193.000, mean reward: -438.600 [-999.000, -45.000], mean action: 1.800 [0.000, 3.000],  loss: 121688.214063, mae: 515.240546, accuracy: 0.000000, mean_q: -410.783856, mean_eps: 0.981190\n",
      "   218/50000: episode: 29, duration: 0.022s, episode steps:   6, steps per second: 270, episode reward: -3192.000, mean reward: -532.000 [-999.000, -32.000], mean action: 0.833 [0.000, 3.000],  loss: 121774.493490, mae: 537.178111, accuracy: 0.000000, mean_q: -439.997340, mean_eps: 0.980695\n",
      "   238/50000: episode: 30, duration: 0.070s, episode steps:  20, steps per second: 287, episode reward: -17178.000, mean reward: -858.900 [-999.000, -45.000], mean action: 1.750 [0.000, 3.000],  loss: 116788.114844, mae: 588.076489, accuracy: 0.000000, mean_q: -504.276332, mean_eps: 0.979525\n",
      "   242/50000: episode: 31, duration: 0.016s, episode steps:   4, steps per second: 248, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 119362.281250, mae: 639.782806, accuracy: 0.000000, mean_q: -563.662415, mean_eps: 0.978445\n",
      "   250/50000: episode: 32, duration: 0.028s, episode steps:   8, steps per second: 287, episode reward: -5234.000, mean reward: -654.250 [-999.000, -45.000], mean action: 1.875 [0.000, 3.000],  loss: 115757.420898, mae: 660.998329, accuracy: 0.000000, mean_q: -599.354034, mean_eps: 0.977905\n",
      "   257/50000: episode: 33, duration: 0.027s, episode steps:   7, steps per second: 264, episode reward: -4191.000, mean reward: -598.714 [-999.000, -32.000], mean action: 1.286 [0.000, 3.000],  loss: 120013.478795, mae: 681.769523, accuracy: 0.000000, mean_q: -637.464103, mean_eps: 0.977230\n",
      "   264/50000: episode: 34, duration: 0.027s, episode steps:   7, steps per second: 259, episode reward: -4220.000, mean reward: -602.857 [-999.000, -58.000], mean action: 1.429 [0.000, 3.000],  loss: 118899.389509, mae: 700.899806, accuracy: 0.000000, mean_q: -666.546430, mean_eps: 0.976600\n",
      "   274/50000: episode: 35, duration: 0.041s, episode steps:  10, steps per second: 241, episode reward: -7188.000, mean reward: -718.800 [-999.000, -45.000], mean action: 2.100 [0.000, 3.000],  loss: 124691.382031, mae: 722.154565, accuracy: 0.000000, mean_q: -700.556262, mean_eps: 0.975835\n",
      "   277/50000: episode: 36, duration: 0.016s, episode steps:   3, steps per second: 189, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 143991.421875, mae: 736.329895, accuracy: 0.000000, mean_q: -716.703695, mean_eps: 0.975250\n",
      "   283/50000: episode: 37, duration: 0.031s, episode steps:   6, steps per second: 196, episode reward: -3192.000, mean reward: -532.000 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 124231.286458, mae: 739.317088, accuracy: 0.000000, mean_q: -740.148214, mean_eps: 0.974845\n",
      "   291/50000: episode: 38, duration: 0.038s, episode steps:   8, steps per second: 213, episode reward: -5190.000, mean reward: -648.750 [-999.000, -32.000], mean action: 1.125 [0.000, 3.000],  loss: 126699.828125, mae: 752.404541, accuracy: 0.000000, mean_q: -767.151321, mean_eps: 0.974215\n",
      "   304/50000: episode: 39, duration: 0.051s, episode steps:  13, steps per second: 257, episode reward: -10229.000, mean reward: -786.846 [-999.000, -60.000], mean action: 1.615 [0.000, 3.000],  loss: 132288.944111, mae: 796.522099, accuracy: 0.000000, mean_q: -830.174481, mean_eps: 0.973270\n",
      "   307/50000: episode: 40, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 141765.223958, mae: 831.300720, accuracy: 0.000000, mean_q: -879.598755, mean_eps: 0.972550\n",
      "   311/50000: episode: 41, duration: 0.016s, episode steps:   4, steps per second: 246, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 142007.046875, mae: 841.667130, accuracy: 0.000000, mean_q: -896.649353, mean_eps: 0.972235\n",
      "   315/50000: episode: 42, duration: 0.015s, episode steps:   4, steps per second: 262, episode reward: -1223.000, mean reward: -305.750 [-999.000, -58.000], mean action: 1.750 [0.000, 3.000],  loss: 129299.740234, mae: 850.022308, accuracy: 0.000000, mean_q: -916.367142, mean_eps: 0.971875\n",
      "   326/50000: episode: 43, duration: 0.039s, episode steps:  11, steps per second: 285, episode reward: -8187.000, mean reward: -744.273 [-999.000, -58.000], mean action: 1.818 [0.000, 3.000],  loss: 140731.651278, mae: 876.378989, accuracy: 0.000000, mean_q: -953.519642, mean_eps: 0.971200\n",
      "   339/50000: episode: 44, duration: 0.044s, episode steps:  13, steps per second: 298, episode reward: -10185.000, mean reward: -783.462 [-999.000, -58.000], mean action: 1.615 [0.000, 3.000],  loss: 147335.474159, mae: 907.540269, accuracy: 0.021635, mean_q: -996.365417, mean_eps: 0.970120\n",
      "   342/50000: episode: 45, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 150335.880208, mae: 931.595459, accuracy: 0.156250, mean_q: -1028.277751, mean_eps: 0.969400\n",
      "   350/50000: episode: 46, duration: 0.027s, episode steps:   8, steps per second: 298, episode reward: -5190.000, mean reward: -648.750 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 152914.219727, mae: 944.803520, accuracy: 0.140625, mean_q: -1041.217575, mean_eps: 0.968905\n",
      "   356/50000: episode: 47, duration: 0.020s, episode steps:   6, steps per second: 293, episode reward: -3236.000, mean reward: -539.333 [-999.000, -60.000], mean action: 1.833 [1.000, 3.000],  loss: 157687.455729, mae: 968.974294, accuracy: 0.244792, mean_q: -1064.972188, mean_eps: 0.968275\n",
      "   363/50000: episode: 48, duration: 0.024s, episode steps:   7, steps per second: 287, episode reward: -4235.000, mean reward: -605.000 [-999.000, -45.000], mean action: 1.429 [0.000, 3.000],  loss: 164887.823661, mae: 994.555612, accuracy: 0.191964, mean_q: -1084.429688, mean_eps: 0.967690\n",
      "   368/50000: episode: 49, duration: 0.018s, episode steps:   5, steps per second: 278, episode reward: -2222.000, mean reward: -444.400 [-999.000, -58.000], mean action: 1.400 [0.000, 3.000],  loss: 148467.550000, mae: 994.747156, accuracy: 0.237500, mean_q: -1086.234937, mean_eps: 0.967150\n",
      "   374/50000: episode: 50, duration: 0.022s, episode steps:   6, steps per second: 278, episode reward: -3236.000, mean reward: -539.333 [-999.000, -60.000], mean action: 1.500 [0.000, 3.000],  loss: 175542.627604, mae: 1017.264242, accuracy: 0.208333, mean_q: -1091.915365, mean_eps: 0.966655\n",
      "   383/50000: episode: 51, duration: 0.033s, episode steps:   9, steps per second: 272, episode reward: -6233.000, mean reward: -692.556 [-999.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 190920.723958, mae: 1041.056641, accuracy: 0.236111, mean_q: -1109.451063, mean_eps: 0.965980\n",
      "   386/50000: episode: 52, duration: 0.015s, episode steps:   3, steps per second: 199, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 195406.026042, mae: 1050.770020, accuracy: 0.187500, mean_q: -1122.959106, mean_eps: 0.965440\n",
      "   400/50000: episode: 53, duration: 0.059s, episode steps:  14, steps per second: 237, episode reward: -11184.000, mean reward: -798.857 [-999.000, -32.000], mean action: 1.357 [0.000, 3.000],  loss: 190102.870536, mae: 1079.360439, accuracy: 0.276786, mean_q: -1171.057513, mean_eps: 0.964675\n",
      "   415/50000: episode: 54, duration: 0.050s, episode steps:  15, steps per second: 298, episode reward: -12183.000, mean reward: -812.200 [-999.000, -32.000], mean action: 0.933 [0.000, 3.000],  loss: 188040.705208, mae: 1134.695378, accuracy: 0.245833, mean_q: -1259.105843, mean_eps: 0.963370\n",
      "   426/50000: episode: 55, duration: 0.036s, episode steps:  11, steps per second: 310, episode reward: -8187.000, mean reward: -744.273 [-999.000, -45.000], mean action: 2.000 [0.000, 3.000],  loss: 207677.578125, mae: 1183.172441, accuracy: 0.488636, mean_q: -1334.291371, mean_eps: 0.962200\n",
      "   442/50000: episode: 56, duration: 0.054s, episode steps:  16, steps per second: 298, episode reward: -13226.000, mean reward: -826.625 [-999.000, -45.000], mean action: 0.875 [0.000, 3.000],  loss: 207861.614258, mae: 1230.069679, accuracy: 0.757812, mean_q: -1403.996010, mean_eps: 0.960985\n",
      "   452/50000: episode: 57, duration: 0.034s, episode steps:  10, steps per second: 292, episode reward: -7188.000, mean reward: -718.800 [-999.000, -58.000], mean action: 2.000 [0.000, 3.000],  loss: 194738.986719, mae: 1273.052234, accuracy: 0.568750, mean_q: -1475.557300, mean_eps: 0.959815\n",
      "   460/50000: episode: 58, duration: 0.027s, episode steps:   8, steps per second: 296, episode reward: -5219.000, mean reward: -652.375 [-999.000, -58.000], mean action: 1.875 [0.000, 3.000],  loss: 238388.046875, mae: 1324.730087, accuracy: 0.472656, mean_q: -1528.536102, mean_eps: 0.959005\n",
      "   465/50000: episode: 59, duration: 0.017s, episode steps:   5, steps per second: 289, episode reward: -2193.000, mean reward: -438.600 [-999.000, -58.000], mean action: 1.800 [0.000, 3.000],  loss: 229857.562500, mae: 1329.206201, accuracy: 0.387500, mean_q: -1538.515503, mean_eps: 0.958420\n",
      "   474/50000: episode: 60, duration: 0.029s, episode steps:   9, steps per second: 308, episode reward: -6218.000, mean reward: -690.889 [-999.000, -32.000], mean action: 0.889 [0.000, 3.000],  loss: 236259.876736, mae: 1334.577243, accuracy: 0.274306, mean_q: -1543.320841, mean_eps: 0.957790\n",
      "   485/50000: episode: 61, duration: 0.036s, episode steps:  11, steps per second: 310, episode reward: -8231.000, mean reward: -748.273 [-999.000, -58.000], mean action: 1.636 [0.000, 3.000],  loss: 213219.186080, mae: 1371.602062, accuracy: 0.522727, mean_q: -1600.742753, mean_eps: 0.956890\n",
      "   493/50000: episode: 62, duration: 0.031s, episode steps:   8, steps per second: 254, episode reward: -5219.000, mean reward: -652.375 [-999.000, -32.000], mean action: 1.750 [0.000, 3.000],  loss: 248663.800781, mae: 1423.181747, accuracy: 0.703125, mean_q: -1658.161301, mean_eps: 0.956035\n",
      "   496/50000: episode: 63, duration: 0.015s, episode steps:   3, steps per second: 203, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 257083.984375, mae: 1455.415609, accuracy: 0.802083, mean_q: -1697.024333, mean_eps: 0.955540\n",
      "   506/50000: episode: 64, duration: 0.042s, episode steps:  10, steps per second: 236, episode reward: -7188.000, mean reward: -718.800 [-999.000, -32.000], mean action: 1.000 [0.000, 3.000],  loss: 256467.178906, mae: 1476.347168, accuracy: 0.725000, mean_q: -1717.738110, mean_eps: 0.954955\n",
      "   518/50000: episode: 65, duration: 0.044s, episode steps:  12, steps per second: 273, episode reward: -9230.000, mean reward: -769.167 [-999.000, -45.000], mean action: 1.167 [0.000, 3.000],  loss: 283879.351562, mae: 1488.963348, accuracy: 0.757812, mean_q: -1721.741597, mean_eps: 0.953965\n",
      "   532/50000: episode: 66, duration: 0.044s, episode steps:  14, steps per second: 321, episode reward: -11228.000, mean reward: -802.000 [-999.000, -60.000], mean action: 1.071 [0.000, 3.000],  loss: 300517.935268, mae: 1499.941336, accuracy: 0.774554, mean_q: -1721.161360, mean_eps: 0.952795\n",
      "   548/50000: episode: 67, duration: 0.049s, episode steps:  16, steps per second: 325, episode reward: -13211.000, mean reward: -825.688 [-999.000, -32.000], mean action: 1.312 [0.000, 3.000],  loss: 314973.022461, mae: 1526.328445, accuracy: 0.750000, mean_q: -1716.224983, mean_eps: 0.951445\n",
      "   552/50000: episode: 68, duration: 0.015s, episode steps:   4, steps per second: 275, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.750 [0.000, 3.000],  loss: 264905.867188, mae: 1549.208282, accuracy: 0.867188, mean_q: -1763.165039, mean_eps: 0.950545\n",
      "   555/50000: episode: 69, duration: 0.011s, episode steps:   3, steps per second: 264, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 273544.661458, mae: 1587.072347, accuracy: 0.739583, mean_q: -1797.217285, mean_eps: 0.950230\n",
      "   564/50000: episode: 70, duration: 0.029s, episode steps:   9, steps per second: 309, episode reward: -6189.000, mean reward: -687.667 [-999.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 316428.043403, mae: 1628.801609, accuracy: 0.788194, mean_q: -1846.946886, mean_eps: 0.949690\n",
      "   568/50000: episode: 71, duration: 0.015s, episode steps:   4, steps per second: 263, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 380439.031250, mae: 1660.180481, accuracy: 0.726562, mean_q: -1877.491730, mean_eps: 0.949105\n",
      "   572/50000: episode: 72, duration: 0.015s, episode steps:   4, steps per second: 261, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 316185.226562, mae: 1638.915497, accuracy: 0.726562, mean_q: -1877.274536, mean_eps: 0.948745\n",
      "   581/50000: episode: 73, duration: 0.034s, episode steps:   9, steps per second: 267, episode reward: -6233.000, mean reward: -692.556 [-999.000, -45.000], mean action: 1.333 [0.000, 3.000],  loss: 357449.652778, mae: 1653.400024, accuracy: 0.739583, mean_q: -1888.175985, mean_eps: 0.948160\n",
      "   587/50000: episode: 74, duration: 0.021s, episode steps:   6, steps per second: 289, episode reward: -3192.000, mean reward: -532.000 [-999.000, -32.000], mean action: 1.000 [0.000, 3.000],  loss: 315932.307292, mae: 1651.644633, accuracy: 0.781250, mean_q: -1900.520549, mean_eps: 0.947485\n",
      "   590/50000: episode: 75, duration: 0.014s, episode steps:   3, steps per second: 222, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 297003.442708, mae: 1658.032145, accuracy: 0.822917, mean_q: -1930.613973, mean_eps: 0.947080\n",
      "   597/50000: episode: 76, duration: 0.025s, episode steps:   7, steps per second: 278, episode reward: -4191.000, mean reward: -598.714 [-999.000, -58.000], mean action: 1.714 [0.000, 3.000],  loss: 342285.071429, mae: 1686.017927, accuracy: 0.794643, mean_q: -1972.108189, mean_eps: 0.946630\n",
      "   607/50000: episode: 77, duration: 0.036s, episode steps:  10, steps per second: 276, episode reward: -7188.000, mean reward: -718.800 [-999.000, -32.000], mean action: 0.800 [0.000, 3.000],  loss: 385346.378125, mae: 1727.564514, accuracy: 0.612500, mean_q: -2022.071130, mean_eps: 0.945865\n",
      "   621/50000: episode: 78, duration: 0.049s, episode steps:  14, steps per second: 286, episode reward: -11228.000, mean reward: -802.000 [-999.000, -58.000], mean action: 1.571 [0.000, 3.000],  loss: 363292.410714, mae: 1733.148551, accuracy: 0.178571, mean_q: -2028.013759, mean_eps: 0.944785\n",
      "   636/50000: episode: 79, duration: 0.055s, episode steps:  15, steps per second: 271, episode reward: -12227.000, mean reward: -815.133 [-999.000, -60.000], mean action: 1.667 [0.000, 3.000],  loss: 385651.633333, mae: 1770.877148, accuracy: 0.425000, mean_q: -2073.402303, mean_eps: 0.943480\n",
      "   643/50000: episode: 80, duration: 0.030s, episode steps:   7, steps per second: 232, episode reward: -4191.000, mean reward: -598.714 [-999.000, -32.000], mean action: 1.000 [0.000, 3.000],  loss: 315970.883929, mae: 1786.972674, accuracy: 0.741071, mean_q: -2094.000558, mean_eps: 0.942490\n",
      "   648/50000: episode: 81, duration: 0.023s, episode steps:   5, steps per second: 216, episode reward: -2222.000, mean reward: -444.400 [-999.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 357179.612500, mae: 1821.051270, accuracy: 0.706250, mean_q: -2124.907373, mean_eps: 0.941950\n",
      "   657/50000: episode: 82, duration: 0.037s, episode steps:   9, steps per second: 242, episode reward: -6189.000, mean reward: -687.667 [-999.000, -45.000], mean action: 2.000 [0.000, 3.000],  loss: 388585.607639, mae: 1855.224854, accuracy: 0.767361, mean_q: -2186.703071, mean_eps: 0.941320\n",
      "   661/50000: episode: 83, duration: 0.018s, episode steps:   4, steps per second: 219, episode reward: -1238.000, mean reward: -309.500 [-999.000, -60.000], mean action: 1.500 [0.000, 3.000],  loss: 393235.820312, mae: 1870.509247, accuracy: 0.773438, mean_q: -2214.581726, mean_eps: 0.940735\n",
      "   668/50000: episode: 84, duration: 0.025s, episode steps:   7, steps per second: 280, episode reward: -4220.000, mean reward: -602.857 [-999.000, -58.000], mean action: 1.571 [0.000, 3.000],  loss: 418618.959821, mae: 1899.998465, accuracy: 0.517857, mean_q: -2240.881801, mean_eps: 0.940240\n",
      "   673/50000: episode: 85, duration: 0.020s, episode steps:   5, steps per second: 247, episode reward: -2237.000, mean reward: -447.400 [-999.000, -58.000], mean action: 1.400 [0.000, 3.000],  loss: 442479.918750, mae: 1911.715527, accuracy: 0.162500, mean_q: -2254.118652, mean_eps: 0.939700\n",
      "   677/50000: episode: 86, duration: 0.018s, episode steps:   4, steps per second: 221, episode reward: -1223.000, mean reward: -305.750 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 421412.156250, mae: 1909.739319, accuracy: 0.187500, mean_q: -2261.524231, mean_eps: 0.939295\n",
      "   688/50000: episode: 87, duration: 0.040s, episode steps:  11, steps per second: 275, episode reward: -8216.000, mean reward: -746.909 [-999.000, -32.000], mean action: 1.091 [0.000, 3.000],  loss: 498471.582386, mae: 1916.669345, accuracy: 0.176136, mean_q: -2248.920987, mean_eps: 0.938620\n",
      "   694/50000: episode: 88, duration: 0.022s, episode steps:   6, steps per second: 279, episode reward: -3192.000, mean reward: -532.000 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 446629.619792, mae: 1904.718913, accuracy: 0.625000, mean_q: -2229.636353, mean_eps: 0.937855\n",
      "   700/50000: episode: 89, duration: 0.024s, episode steps:   6, steps per second: 249, episode reward: -3236.000, mean reward: -539.333 [-999.000, -58.000], mean action: 1.833 [0.000, 3.000],  loss: 475541.812500, mae: 1909.332906, accuracy: 0.578125, mean_q: -2222.259440, mean_eps: 0.937315\n",
      "   709/50000: episode: 90, duration: 0.033s, episode steps:   9, steps per second: 276, episode reward: -6189.000, mean reward: -687.667 [-999.000, -32.000], mean action: 0.889 [0.000, 3.000],  loss: 477458.406250, mae: 1923.868137, accuracy: 0.611111, mean_q: -2249.520399, mean_eps: 0.936640\n",
      "   715/50000: episode: 91, duration: 0.021s, episode steps:   6, steps per second: 285, episode reward: -3221.000, mean reward: -536.833 [-999.000, -58.000], mean action: 2.000 [0.000, 3.000],  loss: 359395.898438, mae: 1922.982625, accuracy: 0.598958, mean_q: -2272.930664, mean_eps: 0.935965\n",
      "   720/50000: episode: 92, duration: 0.018s, episode steps:   5, steps per second: 277, episode reward: -2193.000, mean reward: -438.600 [-999.000, -58.000], mean action: 1.200 [0.000, 3.000],  loss: 445159.778125, mae: 1982.543677, accuracy: 0.662500, mean_q: -2333.378857, mean_eps: 0.935470\n",
      "   729/50000: episode: 93, duration: 0.035s, episode steps:   9, steps per second: 254, episode reward: -6233.000, mean reward: -692.556 [-999.000, -60.000], mean action: 2.000 [0.000, 3.000],  loss: 494233.684028, mae: 2014.359918, accuracy: 0.631944, mean_q: -2358.808295, mean_eps: 0.934840\n",
      "   745/50000: episode: 94, duration: 0.069s, episode steps:  16, steps per second: 233, episode reward: -13211.000, mean reward: -825.688 [-999.000, -58.000], mean action: 1.438 [0.000, 3.000],  loss: 451069.919922, mae: 2026.182236, accuracy: 0.746094, mean_q: -2376.970459, mean_eps: 0.933715\n",
      "   748/50000: episode: 95, duration: 0.016s, episode steps:   3, steps per second: 184, episode reward: -224.000, mean reward: -74.667 [-116.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 539823.385417, mae: 2075.026286, accuracy: 0.760417, mean_q: -2434.113525, mean_eps: 0.932860\n",
      "   754/50000: episode: 96, duration: 0.027s, episode steps:   6, steps per second: 219, episode reward: -3236.000, mean reward: -539.333 [-999.000, -60.000], mean action: 2.500 [1.000, 3.000],  loss: 490606.598958, mae: 2071.257426, accuracy: 0.671875, mean_q: -2463.916219, mean_eps: 0.932455\n",
      "   765/50000: episode: 97, duration: 0.037s, episode steps:  11, steps per second: 297, episode reward: -8187.000, mean reward: -744.273 [-999.000, -32.000], mean action: 1.364 [0.000, 3.000],  loss: 514773.812500, mae: 2098.341187, accuracy: 0.261364, mean_q: -2515.906605, mean_eps: 0.931690\n",
      "   773/50000: episode: 98, duration: 0.028s, episode steps:   8, steps per second: 284, episode reward: -5190.000, mean reward: -648.750 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 470372.869141, mae: 2068.354034, accuracy: 0.171875, mean_q: -2480.676605, mean_eps: 0.930835\n",
      "   776/50000: episode: 99, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 694229.666667, mae: 2132.461751, accuracy: 0.135417, mean_q: -2516.873861, mean_eps: 0.930340\n",
      "   782/50000: episode: 100, duration: 0.022s, episode steps:   6, steps per second: 275, episode reward: -3192.000, mean reward: -532.000 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 469183.354167, mae: 2082.735067, accuracy: 0.197917, mean_q: -2504.546265, mean_eps: 0.929935\n",
      "   786/50000: episode: 101, duration: 0.016s, episode steps:   4, steps per second: 255, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 553101.695312, mae: 2127.926880, accuracy: 0.203125, mean_q: -2527.641602, mean_eps: 0.929485\n",
      "   790/50000: episode: 102, duration: 0.016s, episode steps:   4, steps per second: 246, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.000 [0.000, 3.000],  loss: 609376.914062, mae: 2159.912964, accuracy: 0.242188, mean_q: -2555.282349, mean_eps: 0.929125\n",
      "   805/50000: episode: 103, duration: 0.054s, episode steps:  15, steps per second: 278, episode reward: -12227.000, mean reward: -815.133 [-999.000, -58.000], mean action: 1.533 [0.000, 3.000],  loss: 502712.040625, mae: 2173.370361, accuracy: 0.062500, mean_q: -2591.485205, mean_eps: 0.928270\n",
      "   813/50000: episode: 104, duration: 0.037s, episode steps:   8, steps per second: 218, episode reward: -5190.000, mean reward: -648.750 [-999.000, -45.000], mean action: 1.875 [0.000, 3.000],  loss: 550961.750000, mae: 2225.669525, accuracy: 0.000000, mean_q: -2641.949524, mean_eps: 0.927235\n",
      "   820/50000: episode: 105, duration: 0.042s, episode steps:   7, steps per second: 167, episode reward: -4235.000, mean reward: -605.000 [-999.000, -58.000], mean action: 1.571 [0.000, 3.000],  loss: 557423.357143, mae: 2268.087158, accuracy: 0.000000, mean_q: -2702.475272, mean_eps: 0.926560\n",
      "   823/50000: episode: 106, duration: 0.018s, episode steps:   3, steps per second: 163, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 636256.145833, mae: 2270.821370, accuracy: 0.000000, mean_q: -2707.225016, mean_eps: 0.926110\n",
      "   836/50000: episode: 107, duration: 0.054s, episode steps:  13, steps per second: 240, episode reward: -10214.000, mean reward: -785.692 [-999.000, -32.000], mean action: 1.462 [0.000, 3.000],  loss: 550561.377404, mae: 2255.548227, accuracy: 0.132212, mean_q: -2700.185866, mean_eps: 0.925390\n",
      "   842/50000: episode: 108, duration: 0.026s, episode steps:   6, steps per second: 227, episode reward: -3221.000, mean reward: -536.833 [-999.000, -58.000], mean action: 1.833 [0.000, 3.000],  loss: 542126.916667, mae: 2257.183350, accuracy: 0.234375, mean_q: -2686.091105, mean_eps: 0.924535\n",
      "   848/50000: episode: 109, duration: 0.024s, episode steps:   6, steps per second: 249, episode reward: -3192.000, mean reward: -532.000 [-999.000, -32.000], mean action: 0.833 [0.000, 3.000],  loss: 656412.697917, mae: 2286.615153, accuracy: 0.239583, mean_q: -2694.211019, mean_eps: 0.923995\n",
      "   855/50000: episode: 110, duration: 0.028s, episode steps:   7, steps per second: 254, episode reward: -4235.000, mean reward: -605.000 [-999.000, -45.000], mean action: 1.286 [0.000, 3.000],  loss: 497168.162946, mae: 2275.526297, accuracy: 0.276786, mean_q: -2716.325893, mean_eps: 0.923410\n",
      "   863/50000: episode: 111, duration: 0.034s, episode steps:   8, steps per second: 236, episode reward: -5219.000, mean reward: -652.375 [-999.000, -32.000], mean action: 1.000 [0.000, 3.000],  loss: 629374.421875, mae: 2345.724030, accuracy: 0.214844, mean_q: -2774.532806, mean_eps: 0.922735\n",
      "   873/50000: episode: 112, duration: 0.037s, episode steps:  10, steps per second: 269, episode reward: -7232.000, mean reward: -723.200 [-999.000, -60.000], mean action: 1.200 [0.000, 3.000],  loss: 640638.825000, mae: 2363.336353, accuracy: 0.231250, mean_q: -2793.822510, mean_eps: 0.921925\n",
      "   884/50000: episode: 113, duration: 0.048s, episode steps:  11, steps per second: 231, episode reward: -8216.000, mean reward: -746.909 [-999.000, -32.000], mean action: 1.455 [0.000, 3.000],  loss: 648584.366477, mae: 2361.173584, accuracy: 0.286932, mean_q: -2761.714067, mean_eps: 0.920980\n",
      "   890/50000: episode: 114, duration: 0.033s, episode steps:   6, steps per second: 185, episode reward: -3221.000, mean reward: -536.833 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 599003.567708, mae: 2335.650024, accuracy: 0.276042, mean_q: -2741.714925, mean_eps: 0.920215\n",
      "   898/50000: episode: 115, duration: 0.037s, episode steps:   8, steps per second: 219, episode reward: -5190.000, mean reward: -648.750 [-999.000, -45.000], mean action: 1.625 [0.000, 3.000],  loss: 666169.785156, mae: 2342.242249, accuracy: 0.265625, mean_q: -2744.369812, mean_eps: 0.919585\n",
      "   903/50000: episode: 116, duration: 0.026s, episode steps:   5, steps per second: 191, episode reward: -2237.000, mean reward: -447.400 [-999.000, -45.000], mean action: 1.400 [0.000, 3.000],  loss: 656094.812500, mae: 2344.016211, accuracy: 0.293750, mean_q: -2769.800391, mean_eps: 0.919000\n",
      "   910/50000: episode: 117, duration: 0.033s, episode steps:   7, steps per second: 214, episode reward: -4191.000, mean reward: -598.714 [-999.000, -32.000], mean action: 1.857 [1.000, 3.000],  loss: 734610.955357, mae: 2371.269113, accuracy: 0.196429, mean_q: -2802.453544, mean_eps: 0.918460\n",
      "   917/50000: episode: 118, duration: 0.029s, episode steps:   7, steps per second: 240, episode reward: -4235.000, mean reward: -605.000 [-999.000, -45.000], mean action: 1.571 [0.000, 3.000],  loss: 641465.964286, mae: 2365.724505, accuracy: 0.267857, mean_q: -2817.588658, mean_eps: 0.917830\n",
      "   922/50000: episode: 119, duration: 0.018s, episode steps:   5, steps per second: 271, episode reward: -2222.000, mean reward: -444.400 [-999.000, -58.000], mean action: 1.200 [0.000, 3.000],  loss: 701649.900000, mae: 2395.062305, accuracy: 0.431250, mean_q: -2842.575732, mean_eps: 0.917290\n",
      "   926/50000: episode: 120, duration: 0.017s, episode steps:   4, steps per second: 238, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 600240.359375, mae: 2394.227051, accuracy: 0.367188, mean_q: -2862.115784, mean_eps: 0.916885\n",
      "   930/50000: episode: 121, duration: 0.016s, episode steps:   4, steps per second: 250, episode reward: -1238.000, mean reward: -309.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 738471.054688, mae: 2424.758606, accuracy: 0.351562, mean_q: -2888.099243, mean_eps: 0.916525\n",
      "   942/50000: episode: 122, duration: 0.042s, episode steps:  12, steps per second: 288, episode reward: -9230.000, mean reward: -769.167 [-999.000, -60.000], mean action: 1.250 [0.000, 3.000],  loss: 775253.312500, mae: 2423.574219, accuracy: 0.390625, mean_q: -2874.581726, mean_eps: 0.915805\n",
      "   949/50000: episode: 123, duration: 0.025s, episode steps:   7, steps per second: 278, episode reward: -4235.000, mean reward: -605.000 [-999.000, -45.000], mean action: 1.857 [0.000, 3.000],  loss: 686322.790179, mae: 2398.872733, accuracy: 0.272321, mean_q: -2851.110805, mean_eps: 0.914950\n",
      "   962/50000: episode: 124, duration: 0.044s, episode steps:  13, steps per second: 299, episode reward: -10214.000, mean reward: -785.692 [-999.000, -32.000], mean action: 1.154 [0.000, 3.000],  loss: 641344.437500, mae: 2405.123178, accuracy: 0.492788, mean_q: -2867.543532, mean_eps: 0.914050\n",
      "   966/50000: episode: 125, duration: 0.018s, episode steps:   4, steps per second: 226, episode reward: -1223.000, mean reward: -305.750 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 646330.750000, mae: 2451.548218, accuracy: 0.375000, mean_q: -2922.178223, mean_eps: 0.913285\n",
      "   974/50000: episode: 126, duration: 0.027s, episode steps:   8, steps per second: 293, episode reward: -5219.000, mean reward: -652.375 [-999.000, -58.000], mean action: 2.125 [0.000, 3.000],  loss: 664936.515625, mae: 2474.967438, accuracy: 0.328125, mean_q: -2955.303558, mean_eps: 0.912745\n",
      "   980/50000: episode: 127, duration: 0.023s, episode steps:   6, steps per second: 258, episode reward: -3192.000, mean reward: -532.000 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 934856.020833, mae: 2538.557414, accuracy: 0.239583, mean_q: -2982.409424, mean_eps: 0.912115\n",
      "   983/50000: episode: 128, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 2.000 [1.000, 3.000],  loss: 902522.979167, mae: 2523.065837, accuracy: 0.354167, mean_q: -2959.971354, mean_eps: 0.911710\n",
      "   987/50000: episode: 129, duration: 0.015s, episode steps:   4, steps per second: 266, episode reward: -1238.000, mean reward: -309.500 [-999.000, -60.000], mean action: 2.000 [0.000, 3.000],  loss: 666109.242188, mae: 2469.182129, accuracy: 0.390625, mean_q: -2947.231445, mean_eps: 0.911395\n",
      "   994/50000: episode: 130, duration: 0.026s, episode steps:   7, steps per second: 267, episode reward: -4220.000, mean reward: -602.857 [-999.000, -58.000], mean action: 1.429 [0.000, 3.000],  loss: 610328.941964, mae: 2468.300781, accuracy: 0.294643, mean_q: -2972.093785, mean_eps: 0.910900\n",
      "  1005/50000: episode: 131, duration: 0.037s, episode steps:  11, steps per second: 297, episode reward: -8187.000, mean reward: -744.273 [-999.000, -32.000], mean action: 0.818 [0.000, 3.000],  loss: 642099.298295, mae: 2520.556419, accuracy: 0.264205, mean_q: -3028.916526, mean_eps: 0.910090\n",
      "  1012/50000: episode: 132, duration: 0.026s, episode steps:   7, steps per second: 267, episode reward: -4220.000, mean reward: -602.857 [-999.000, -32.000], mean action: 1.429 [0.000, 3.000],  loss: 788477.723214, mae: 2589.761858, accuracy: 0.254464, mean_q: -3085.961496, mean_eps: 0.909280\n",
      "  1017/50000: episode: 133, duration: 0.019s, episode steps:   5, steps per second: 259, episode reward: -2193.000, mean reward: -438.600 [-999.000, -58.000], mean action: 2.000 [0.000, 3.000],  loss: 892684.150000, mae: 2622.917871, accuracy: 0.281250, mean_q: -3114.211719, mean_eps: 0.908740\n",
      "  1029/50000: episode: 134, duration: 0.041s, episode steps:  12, steps per second: 292, episode reward: -9186.000, mean reward: -765.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 794702.020833, mae: 2565.243612, accuracy: 0.273438, mean_q: -3039.605530, mean_eps: 0.907975\n",
      "  1036/50000: episode: 135, duration: 0.027s, episode steps:   7, steps per second: 261, episode reward: -4191.000, mean reward: -598.714 [-999.000, -45.000], mean action: 1.857 [0.000, 3.000],  loss: 800372.098214, mae: 2555.244908, accuracy: 0.299107, mean_q: -3024.793283, mean_eps: 0.907120\n",
      "  1047/50000: episode: 136, duration: 0.037s, episode steps:  11, steps per second: 295, episode reward: -8231.000, mean reward: -748.273 [-999.000, -45.000], mean action: 1.182 [0.000, 3.000],  loss: 755944.335227, mae: 2573.198087, accuracy: 0.349432, mean_q: -3065.786821, mean_eps: 0.906310\n",
      "  1055/50000: episode: 137, duration: 0.028s, episode steps:   8, steps per second: 284, episode reward: -5190.000, mean reward: -648.750 [-999.000, -32.000], mean action: 1.125 [0.000, 3.000],  loss: 767879.230469, mae: 2599.377930, accuracy: 0.457031, mean_q: -3115.362122, mean_eps: 0.905455\n",
      "  1061/50000: episode: 138, duration: 0.022s, episode steps:   6, steps per second: 279, episode reward: -3192.000, mean reward: -532.000 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 923265.927083, mae: 2621.371053, accuracy: 0.494792, mean_q: -3120.981242, mean_eps: 0.904825\n",
      "  1073/50000: episode: 139, duration: 0.041s, episode steps:  12, steps per second: 295, episode reward: -9230.000, mean reward: -769.167 [-999.000, -45.000], mean action: 1.833 [0.000, 3.000],  loss: 802989.765625, mae: 2592.693258, accuracy: 0.356771, mean_q: -3094.776794, mean_eps: 0.904015\n",
      "  1076/50000: episode: 140, duration: 0.014s, episode steps:   3, steps per second: 217, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 727615.312500, mae: 2582.021159, accuracy: 0.395833, mean_q: -3095.016195, mean_eps: 0.903340\n",
      "  1081/50000: episode: 141, duration: 0.019s, episode steps:   5, steps per second: 270, episode reward: -2193.000, mean reward: -438.600 [-999.000, -32.000], mean action: 1.200 [0.000, 3.000],  loss: 572929.096875, mae: 2573.361914, accuracy: 0.325000, mean_q: -3119.469629, mean_eps: 0.902980\n",
      "  1098/50000: episode: 142, duration: 0.055s, episode steps:  17, steps per second: 307, episode reward: -14210.000, mean reward: -835.882 [-999.000, -32.000], mean action: 0.765 [0.000, 3.000],  loss: 760360.000000, mae: 2647.326028, accuracy: 0.406250, mean_q: -3191.505946, mean_eps: 0.901990\n",
      "  1105/50000: episode: 143, duration: 0.027s, episode steps:   7, steps per second: 255, episode reward: -4220.000, mean reward: -602.857 [-999.000, -58.000], mean action: 2.000 [0.000, 3.000],  loss: 739198.540179, mae: 2675.366106, accuracy: 0.723214, mean_q: -3190.553885, mean_eps: 0.900910\n",
      "  1109/50000: episode: 144, duration: 0.017s, episode steps:   4, steps per second: 238, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 768213.890625, mae: 2694.803162, accuracy: 0.734375, mean_q: -3200.135986, mean_eps: 0.900415\n",
      "  1112/50000: episode: 145, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -224.000, mean reward: -74.667 [-116.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 832125.104167, mae: 2716.925049, accuracy: 0.687500, mean_q: -3201.061279, mean_eps: 0.900100\n",
      "  1127/50000: episode: 146, duration: 0.051s, episode steps:  15, steps per second: 296, episode reward: -12212.000, mean reward: -814.133 [-999.000, -58.000], mean action: 1.600 [0.000, 3.000],  loss: 781471.108333, mae: 2707.578516, accuracy: 0.733333, mean_q: -3207.645622, mean_eps: 0.899290\n",
      "  1135/50000: episode: 147, duration: 0.030s, episode steps:   8, steps per second: 267, episode reward: -5190.000, mean reward: -648.750 [-999.000, -32.000], mean action: 1.625 [0.000, 3.000],  loss: 651982.320312, mae: 2722.769501, accuracy: 0.554688, mean_q: -3296.674835, mean_eps: 0.898255\n",
      "  1139/50000: episode: 148, duration: 0.017s, episode steps:   4, steps per second: 242, episode reward: -1238.000, mean reward: -309.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 1179874.968750, mae: 2822.782166, accuracy: 0.398438, mean_q: -3351.181580, mean_eps: 0.897715\n",
      "  1149/50000: episode: 149, duration: 0.037s, episode steps:  10, steps per second: 268, episode reward: -7232.000, mean reward: -723.200 [-999.000, -58.000], mean action: 1.400 [0.000, 3.000],  loss: 790589.296875, mae: 2755.987109, accuracy: 0.390625, mean_q: -3317.874194, mean_eps: 0.897085\n",
      "  1159/50000: episode: 150, duration: 0.035s, episode steps:  10, steps per second: 290, episode reward: -7217.000, mean reward: -721.700 [-999.000, -32.000], mean action: 1.100 [0.000, 3.000],  loss: 753405.325000, mae: 2750.513647, accuracy: 0.375000, mean_q: -3316.249438, mean_eps: 0.896185\n",
      "  1164/50000: episode: 151, duration: 0.019s, episode steps:   5, steps per second: 257, episode reward: -2222.000, mean reward: -444.400 [-999.000, -58.000], mean action: 1.600 [0.000, 3.000],  loss: 1238549.725000, mae: 2827.672217, accuracy: 0.343750, mean_q: -3348.749365, mean_eps: 0.895510\n",
      "  1171/50000: episode: 152, duration: 0.026s, episode steps:   7, steps per second: 264, episode reward: -4191.000, mean reward: -598.714 [-999.000, -32.000], mean action: 1.571 [0.000, 3.000],  loss: 914854.000000, mae: 2749.477713, accuracy: 0.401786, mean_q: -3300.063895, mean_eps: 0.894970\n",
      "  1180/50000: episode: 153, duration: 0.032s, episode steps:   9, steps per second: 281, episode reward: -6189.000, mean reward: -687.667 [-999.000, -32.000], mean action: 0.778 [0.000, 2.000],  loss: 783074.760417, mae: 2746.189290, accuracy: 0.454861, mean_q: -3312.588569, mean_eps: 0.894250\n",
      "  1190/50000: episode: 154, duration: 0.035s, episode steps:  10, steps per second: 286, episode reward: -7217.000, mean reward: -721.700 [-999.000, -58.000], mean action: 1.400 [0.000, 3.000],  loss: 839237.456250, mae: 2785.008984, accuracy: 0.553125, mean_q: -3355.823901, mean_eps: 0.893395\n",
      "  1202/50000: episode: 155, duration: 0.041s, episode steps:  12, steps per second: 290, episode reward: -9186.000, mean reward: -765.500 [-999.000, -58.000], mean action: 1.833 [0.000, 3.000],  loss: 894032.229167, mae: 2815.042460, accuracy: 0.752604, mean_q: -3376.030558, mean_eps: 0.892405\n",
      "  1208/50000: episode: 156, duration: 0.023s, episode steps:   6, steps per second: 265, episode reward: -3236.000, mean reward: -539.333 [-999.000, -58.000], mean action: 1.000 [0.000, 3.000],  loss: 1034378.562500, mae: 2844.995524, accuracy: 0.583333, mean_q: -3388.093221, mean_eps: 0.891595\n",
      "  1214/50000: episode: 157, duration: 0.022s, episode steps:   6, steps per second: 268, episode reward: -3236.000, mean reward: -539.333 [-999.000, -60.000], mean action: 1.500 [0.000, 3.000],  loss: 841416.343750, mae: 2808.538289, accuracy: 0.557292, mean_q: -3369.584269, mean_eps: 0.891055\n",
      "  1221/50000: episode: 158, duration: 0.027s, episode steps:   7, steps per second: 257, episode reward: -4220.000, mean reward: -602.857 [-999.000, -32.000], mean action: 1.000 [0.000, 3.000],  loss: 946155.241071, mae: 2800.930420, accuracy: 0.473214, mean_q: -3328.324672, mean_eps: 0.890470\n",
      "  1224/50000: episode: 159, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 736166.947917, mae: 2763.509766, accuracy: 0.510417, mean_q: -3309.208577, mean_eps: 0.890020\n",
      "  1231/50000: episode: 160, duration: 0.027s, episode steps:   7, steps per second: 263, episode reward: -4220.000, mean reward: -602.857 [-999.000, -32.000], mean action: 1.571 [0.000, 3.000],  loss: 1009025.107143, mae: 2817.109201, accuracy: 0.580357, mean_q: -3337.615199, mean_eps: 0.889570\n",
      "  1236/50000: episode: 161, duration: 0.019s, episode steps:   5, steps per second: 268, episode reward: -2237.000, mean reward: -447.400 [-999.000, -45.000], mean action: 1.600 [0.000, 3.000],  loss: 985441.225000, mae: 2821.769141, accuracy: 0.556250, mean_q: -3349.333398, mean_eps: 0.889030\n",
      "  1241/50000: episode: 162, duration: 0.019s, episode steps:   5, steps per second: 261, episode reward: -2237.000, mean reward: -447.400 [-999.000, -58.000], mean action: 1.200 [0.000, 3.000],  loss: 1117691.425000, mae: 2816.003271, accuracy: 0.731250, mean_q: -3318.847998, mean_eps: 0.888580\n",
      "  1246/50000: episode: 163, duration: 0.023s, episode steps:   5, steps per second: 219, episode reward: -2222.000, mean reward: -444.400 [-999.000, -32.000], mean action: 1.400 [0.000, 3.000],  loss: 862345.012500, mae: 2801.912500, accuracy: 0.731250, mean_q: -3336.287500, mean_eps: 0.888130\n",
      "  1251/50000: episode: 164, duration: 0.021s, episode steps:   5, steps per second: 241, episode reward: -2237.000, mean reward: -447.400 [-999.000, -58.000], mean action: 1.200 [0.000, 3.000],  loss: 943961.737500, mae: 2809.694873, accuracy: 0.718750, mean_q: -3352.275732, mean_eps: 0.887680\n",
      "  1259/50000: episode: 165, duration: 0.032s, episode steps:   8, steps per second: 253, episode reward: -5219.000, mean reward: -652.375 [-999.000, -32.000], mean action: 0.875 [0.000, 3.000],  loss: 923668.025391, mae: 2819.926300, accuracy: 0.765625, mean_q: -3345.569427, mean_eps: 0.887095\n",
      "  1264/50000: episode: 166, duration: 0.021s, episode steps:   5, steps per second: 244, episode reward: -2222.000, mean reward: -444.400 [-999.000, -58.000], mean action: 1.600 [0.000, 3.000],  loss: 610365.300000, mae: 2823.343848, accuracy: 0.731250, mean_q: -3408.800928, mean_eps: 0.886510\n",
      "  1284/50000: episode: 167, duration: 0.071s, episode steps:  20, steps per second: 281, episode reward: -17222.000, mean reward: -861.100 [-999.000, -58.000], mean action: 1.450 [0.000, 3.000],  loss: 903191.367188, mae: 2879.463562, accuracy: 0.690625, mean_q: -3447.249512, mean_eps: 0.885385\n",
      "  1305/50000: episode: 168, duration: 0.069s, episode steps:  21, steps per second: 306, episode reward: -18221.000, mean reward: -867.667 [-999.000, -60.000], mean action: 1.571 [0.000, 3.000],  loss: 992027.549851, mae: 2897.924514, accuracy: 0.270833, mean_q: -3473.991176, mean_eps: 0.883540\n",
      "  1312/50000: episode: 169, duration: 0.027s, episode steps:   7, steps per second: 262, episode reward: -4191.000, mean reward: -598.714 [-999.000, -58.000], mean action: 2.000 [0.000, 3.000],  loss: 1010024.285714, mae: 2891.732108, accuracy: 0.174107, mean_q: -3459.670689, mean_eps: 0.882280\n",
      "  1317/50000: episode: 170, duration: 0.018s, episode steps:   5, steps per second: 281, episode reward: -2237.000, mean reward: -447.400 [-999.000, -60.000], mean action: 1.800 [0.000, 3.000],  loss: 1038020.162500, mae: 2866.422314, accuracy: 0.150000, mean_q: -3430.767725, mean_eps: 0.881740\n",
      "  1324/50000: episode: 171, duration: 0.028s, episode steps:   7, steps per second: 252, episode reward: -4220.000, mean reward: -602.857 [-999.000, -58.000], mean action: 2.000 [0.000, 3.000],  loss: 830935.602679, mae: 2843.334577, accuracy: 0.191964, mean_q: -3434.167097, mean_eps: 0.881200\n",
      "  1329/50000: episode: 172, duration: 0.020s, episode steps:   5, steps per second: 255, episode reward: -2222.000, mean reward: -444.400 [-999.000, -58.000], mean action: 1.400 [0.000, 3.000],  loss: 860525.000000, mae: 2883.588916, accuracy: 0.137500, mean_q: -3481.144922, mean_eps: 0.880660\n",
      "  1335/50000: episode: 173, duration: 0.025s, episode steps:   6, steps per second: 244, episode reward: -3221.000, mean reward: -536.833 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 777528.942708, mae: 2903.758708, accuracy: 0.098958, mean_q: -3529.115275, mean_eps: 0.880165\n",
      "  1342/50000: episode: 174, duration: 0.024s, episode steps:   7, steps per second: 289, episode reward: -4220.000, mean reward: -602.857 [-999.000, -32.000], mean action: 1.571 [0.000, 3.000],  loss: 1136425.660714, mae: 2997.703857, accuracy: 0.120536, mean_q: -3604.945731, mean_eps: 0.879580\n",
      "  1347/50000: episode: 175, duration: 0.019s, episode steps:   5, steps per second: 260, episode reward: -2222.000, mean reward: -444.400 [-999.000, -32.000], mean action: 1.600 [0.000, 3.000],  loss: 961032.675000, mae: 2963.699121, accuracy: 0.231250, mean_q: -3587.951807, mean_eps: 0.879040\n",
      "  1353/50000: episode: 176, duration: 0.022s, episode steps:   6, steps per second: 267, episode reward: -3192.000, mean reward: -532.000 [-999.000, -58.000], mean action: 2.000 [0.000, 3.000],  loss: 1072973.791667, mae: 2954.023966, accuracy: 0.479167, mean_q: -3545.673258, mean_eps: 0.878545\n",
      "  1363/50000: episode: 177, duration: 0.036s, episode steps:  10, steps per second: 279, episode reward: -7232.000, mean reward: -723.200 [-999.000, -60.000], mean action: 1.700 [0.000, 3.000],  loss: 844408.612500, mae: 2933.563354, accuracy: 0.290625, mean_q: -3556.166333, mean_eps: 0.877825\n",
      "  1374/50000: episode: 178, duration: 0.037s, episode steps:  11, steps per second: 297, episode reward: -8216.000, mean reward: -746.909 [-999.000, -58.000], mean action: 1.455 [0.000, 3.000],  loss: 954352.650568, mae: 2968.230868, accuracy: 0.551136, mean_q: -3596.997093, mean_eps: 0.876880\n",
      "  1377/50000: episode: 179, duration: 0.013s, episode steps:   3, steps per second: 225, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 1.667 [0.000, 3.000],  loss: 646520.125000, mae: 2952.157145, accuracy: 0.260417, mean_q: -3617.158447, mean_eps: 0.876250\n",
      "  1382/50000: episode: 180, duration: 0.019s, episode steps:   5, steps per second: 267, episode reward: -2193.000, mean reward: -438.600 [-999.000, -32.000], mean action: 1.200 [0.000, 3.000],  loss: 1072074.025000, mae: 3011.510303, accuracy: 0.237500, mean_q: -3633.058887, mean_eps: 0.875890\n",
      "  1391/50000: episode: 181, duration: 0.034s, episode steps:   9, steps per second: 263, episode reward: -6218.000, mean reward: -690.889 [-999.000, -58.000], mean action: 1.889 [0.000, 3.000],  loss: 1106145.500000, mae: 3041.503309, accuracy: 0.156250, mean_q: -3648.558539, mean_eps: 0.875260\n",
      "  1398/50000: episode: 182, duration: 0.025s, episode steps:   7, steps per second: 278, episode reward: -4235.000, mean reward: -605.000 [-999.000, -58.000], mean action: 1.857 [0.000, 3.000],  loss: 853551.982143, mae: 2961.347900, accuracy: 0.071429, mean_q: -3579.311279, mean_eps: 0.874540\n",
      "  1407/50000: episode: 183, duration: 0.033s, episode steps:   9, steps per second: 273, episode reward: -6233.000, mean reward: -692.556 [-999.000, -58.000], mean action: 1.556 [0.000, 3.000],  loss: 883397.111111, mae: 2989.552680, accuracy: 0.208333, mean_q: -3599.553982, mean_eps: 0.873820\n",
      "  1413/50000: episode: 184, duration: 0.022s, episode steps:   6, steps per second: 270, episode reward: -3221.000, mean reward: -536.833 [-999.000, -58.000], mean action: 2.000 [0.000, 3.000],  loss: 1050206.114583, mae: 3026.044352, accuracy: 0.250000, mean_q: -3633.910482, mean_eps: 0.873145\n",
      "  1418/50000: episode: 185, duration: 0.020s, episode steps:   5, steps per second: 247, episode reward: -2222.000, mean reward: -444.400 [-999.000, -32.000], mean action: 1.800 [0.000, 3.000],  loss: 1069474.300000, mae: 3052.251270, accuracy: 0.225000, mean_q: -3666.827930, mean_eps: 0.872650\n",
      "  1427/50000: episode: 186, duration: 0.033s, episode steps:   9, steps per second: 274, episode reward: -6218.000, mean reward: -690.889 [-999.000, -32.000], mean action: 1.444 [0.000, 3.000],  loss: 1092481.291667, mae: 3018.353868, accuracy: 0.458333, mean_q: -3617.479004, mean_eps: 0.872020\n",
      "  1435/50000: episode: 187, duration: 0.029s, episode steps:   8, steps per second: 276, episode reward: -5190.000, mean reward: -648.750 [-999.000, -32.000], mean action: 1.000 [0.000, 3.000],  loss: 921253.710938, mae: 2967.503510, accuracy: 0.613281, mean_q: -3564.272675, mean_eps: 0.871255\n",
      "  1441/50000: episode: 188, duration: 0.022s, episode steps:   6, steps per second: 269, episode reward: -3236.000, mean reward: -539.333 [-999.000, -58.000], mean action: 1.000 [0.000, 3.000],  loss: 1098474.291667, mae: 3001.593953, accuracy: 0.770833, mean_q: -3550.281209, mean_eps: 0.870625\n",
      "  1445/50000: episode: 189, duration: 0.018s, episode steps:   4, steps per second: 228, episode reward: -1223.000, mean reward: -305.750 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 982959.781250, mae: 2986.102112, accuracy: 0.593750, mean_q: -3545.664490, mean_eps: 0.870175\n",
      "  1450/50000: episode: 190, duration: 0.021s, episode steps:   5, steps per second: 239, episode reward: -2193.000, mean reward: -438.600 [-999.000, -58.000], mean action: 1.800 [0.000, 3.000],  loss: 896701.625000, mae: 2974.395557, accuracy: 0.562500, mean_q: -3541.059277, mean_eps: 0.869770\n",
      "  1458/50000: episode: 191, duration: 0.029s, episode steps:   8, steps per second: 276, episode reward: -5190.000, mean reward: -648.750 [-999.000, -32.000], mean action: 0.875 [0.000, 3.000],  loss: 955523.562500, mae: 2996.451050, accuracy: 0.664062, mean_q: -3562.612366, mean_eps: 0.869185\n",
      "  1464/50000: episode: 192, duration: 0.022s, episode steps:   6, steps per second: 278, episode reward: -3236.000, mean reward: -539.333 [-999.000, -58.000], mean action: 1.000 [0.000, 3.000],  loss: 1216747.843750, mae: 3042.256470, accuracy: 0.760417, mean_q: -3574.249308, mean_eps: 0.868555\n",
      "  1473/50000: episode: 193, duration: 0.032s, episode steps:   9, steps per second: 277, episode reward: -6189.000, mean reward: -687.667 [-999.000, -58.000], mean action: 1.111 [0.000, 3.000],  loss: 882536.765625, mae: 3020.944661, accuracy: 0.743056, mean_q: -3603.008816, mean_eps: 0.867880\n",
      "  1478/50000: episode: 194, duration: 0.020s, episode steps:   5, steps per second: 253, episode reward: -2193.000, mean reward: -438.600 [-999.000, -32.000], mean action: 1.400 [0.000, 3.000],  loss: 855651.175000, mae: 2998.347998, accuracy: 0.712500, mean_q: -3613.236963, mean_eps: 0.867250\n",
      "  1483/50000: episode: 195, duration: 0.019s, episode steps:   5, steps per second: 262, episode reward: -2222.000, mean reward: -444.400 [-999.000, -58.000], mean action: 1.200 [0.000, 3.000],  loss: 819592.387500, mae: 3054.730225, accuracy: 0.587500, mean_q: -3702.227832, mean_eps: 0.866800\n",
      "  1488/50000: episode: 196, duration: 0.018s, episode steps:   5, steps per second: 279, episode reward: -2222.000, mean reward: -444.400 [-999.000, -32.000], mean action: 1.200 [0.000, 3.000],  loss: 985670.787500, mae: 3076.755615, accuracy: 0.518750, mean_q: -3712.582324, mean_eps: 0.866350\n",
      "  1496/50000: episode: 197, duration: 0.031s, episode steps:   8, steps per second: 258, episode reward: -5190.000, mean reward: -648.750 [-999.000, -32.000], mean action: 1.375 [0.000, 3.000],  loss: 1315086.843750, mae: 3087.657715, accuracy: 0.718750, mean_q: -3661.295563, mean_eps: 0.865765\n",
      "  1499/50000: episode: 198, duration: 0.013s, episode steps:   3, steps per second: 230, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 1176148.062500, mae: 3042.649984, accuracy: 0.718750, mean_q: -3613.450765, mean_eps: 0.865270\n",
      "  1503/50000: episode: 199, duration: 0.018s, episode steps:   4, steps per second: 218, episode reward: -1223.000, mean reward: -305.750 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 1035000.937500, mae: 2994.262817, accuracy: 0.570312, mean_q: -3561.807678, mean_eps: 0.864955\n",
      "  1509/50000: episode: 200, duration: 0.024s, episode steps:   6, steps per second: 248, episode reward: -3221.000, mean reward: -536.833 [-999.000, -32.000], mean action: 1.000 [0.000, 3.000],  loss: 875768.406250, mae: 2964.745199, accuracy: 0.395833, mean_q: -3553.836955, mean_eps: 0.864505\n",
      "  1520/50000: episode: 201, duration: 0.041s, episode steps:  11, steps per second: 266, episode reward: -8187.000, mean reward: -744.273 [-999.000, -32.000], mean action: 1.182 [0.000, 3.000],  loss: 1026278.681818, mae: 3026.253596, accuracy: 0.187500, mean_q: -3632.224165, mean_eps: 0.863740\n",
      "  1535/50000: episode: 202, duration: 0.057s, episode steps:  15, steps per second: 261, episode reward: -12183.000, mean reward: -812.200 [-999.000, -32.000], mean action: 1.067 [0.000, 3.000],  loss: 871100.429167, mae: 3062.594287, accuracy: 0.162500, mean_q: -3688.877783, mean_eps: 0.862570\n",
      "  1540/50000: episode: 203, duration: 0.024s, episode steps:   5, steps per second: 210, episode reward: -2193.000, mean reward: -438.600 [-999.000, -58.000], mean action: 1.200 [0.000, 3.000],  loss: 733851.650000, mae: 3111.844629, accuracy: 0.181250, mean_q: -3785.311377, mean_eps: 0.861670\n",
      "  1546/50000: episode: 204, duration: 0.024s, episode steps:   6, steps per second: 248, episode reward: -3221.000, mean reward: -536.833 [-999.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 1174534.583333, mae: 3151.261963, accuracy: 0.208333, mean_q: -3767.481079, mean_eps: 0.861175\n",
      "  1551/50000: episode: 205, duration: 0.024s, episode steps:   5, steps per second: 208, episode reward: -2237.000, mean reward: -447.400 [-999.000, -45.000], mean action: 1.200 [0.000, 3.000],  loss: 1320036.212500, mae: 3166.508643, accuracy: 0.200000, mean_q: -3762.650342, mean_eps: 0.860680\n",
      "  1558/50000: episode: 206, duration: 0.029s, episode steps:   7, steps per second: 239, episode reward: -4220.000, mean reward: -602.857 [-999.000, -32.000], mean action: 1.429 [0.000, 3.000],  loss: 890787.026786, mae: 3093.096401, accuracy: 0.254464, mean_q: -3697.002267, mean_eps: 0.860140\n",
      "  1562/50000: episode: 207, duration: 0.017s, episode steps:   4, steps per second: 238, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 1034171.562500, mae: 3137.940369, accuracy: 0.265625, mean_q: -3729.554504, mean_eps: 0.859645\n",
      "  1569/50000: episode: 208, duration: 0.029s, episode steps:   7, steps per second: 240, episode reward: -4191.000, mean reward: -598.714 [-999.000, -45.000], mean action: 1.286 [0.000, 2.000],  loss: 1264762.339286, mae: 3173.843680, accuracy: 0.263393, mean_q: -3766.889335, mean_eps: 0.859150\n",
      "  1573/50000: episode: 209, duration: 0.017s, episode steps:   4, steps per second: 232, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.750 [0.000, 3.000],  loss: 1291258.718750, mae: 3166.072693, accuracy: 0.164062, mean_q: -3776.433655, mean_eps: 0.858655\n",
      "  1577/50000: episode: 210, duration: 0.019s, episode steps:   4, steps per second: 209, episode reward: -1223.000, mean reward: -305.750 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 1543801.718750, mae: 3147.210449, accuracy: 0.203125, mean_q: -3711.595947, mean_eps: 0.858295\n",
      "  1581/50000: episode: 211, duration: 0.020s, episode steps:   4, steps per second: 202, episode reward: -1238.000, mean reward: -309.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 1289152.875000, mae: 3076.060486, accuracy: 0.195312, mean_q: -3662.262390, mean_eps: 0.857935\n",
      "  1589/50000: episode: 212, duration: 0.031s, episode steps:   8, steps per second: 260, episode reward: -5234.000, mean reward: -654.250 [-999.000, -60.000], mean action: 1.375 [0.000, 3.000],  loss: 1052423.710938, mae: 3020.366943, accuracy: 0.082031, mean_q: -3628.658234, mean_eps: 0.857395\n",
      "  1598/50000: episode: 213, duration: 0.036s, episode steps:   9, steps per second: 252, episode reward: -6233.000, mean reward: -692.556 [-999.000, -45.000], mean action: 1.778 [0.000, 3.000],  loss: 1080950.784722, mae: 3044.474202, accuracy: 0.090278, mean_q: -3646.003798, mean_eps: 0.856630\n",
      "  1604/50000: episode: 214, duration: 0.025s, episode steps:   6, steps per second: 239, episode reward: -3192.000, mean reward: -532.000 [-999.000, -32.000], mean action: 1.833 [1.000, 3.000],  loss: 1000809.822917, mae: 3053.535482, accuracy: 0.046875, mean_q: -3668.823853, mean_eps: 0.855955\n",
      "  1607/50000: episode: 215, duration: 0.016s, episode steps:   3, steps per second: 193, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 861366.083333, mae: 3053.305094, accuracy: 0.062500, mean_q: -3696.179850, mean_eps: 0.855550\n",
      "  1613/50000: episode: 216, duration: 0.027s, episode steps:   6, steps per second: 225, episode reward: -3221.000, mean reward: -536.833 [-999.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 1342494.020833, mae: 3122.511434, accuracy: 0.067708, mean_q: -3709.993286, mean_eps: 0.855145\n",
      "  1616/50000: episode: 217, duration: 0.013s, episode steps:   3, steps per second: 230, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 866514.979167, mae: 3080.274740, accuracy: 0.104167, mean_q: -3715.302734, mean_eps: 0.854740\n",
      "  1629/50000: episode: 218, duration: 0.044s, episode steps:  13, steps per second: 292, episode reward: -10214.000, mean reward: -785.692 [-999.000, -58.000], mean action: 1.846 [0.000, 3.000],  loss: 1248721.536058, mae: 3102.980056, accuracy: 0.072115, mean_q: -3692.429631, mean_eps: 0.854020\n",
      "  1632/50000: episode: 219, duration: 0.014s, episode steps:   3, steps per second: 221, episode reward: -224.000, mean reward: -74.667 [-116.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 725410.583333, mae: 3025.049642, accuracy: 0.020833, mean_q: -3658.902425, mean_eps: 0.853300\n",
      "  1641/50000: episode: 220, duration: 0.031s, episode steps:   9, steps per second: 289, episode reward: -6233.000, mean reward: -692.556 [-999.000, -45.000], mean action: 1.333 [0.000, 3.000],  loss: 992805.201389, mae: 3066.114041, accuracy: 0.065972, mean_q: -3671.202148, mean_eps: 0.852760\n",
      "  1655/50000: episode: 221, duration: 0.049s, episode steps:  14, steps per second: 288, episode reward: -11184.000, mean reward: -798.857 [-999.000, -32.000], mean action: 1.714 [0.000, 3.000],  loss: 840039.611607, mae: 3101.492728, accuracy: 0.169643, mean_q: -3745.632551, mean_eps: 0.851725\n",
      "  1658/50000: episode: 222, duration: 0.013s, episode steps:   3, steps per second: 223, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 1117815.041667, mae: 3166.653158, accuracy: 0.208333, mean_q: -3793.194010, mean_eps: 0.850960\n",
      "  1665/50000: episode: 223, duration: 0.026s, episode steps:   7, steps per second: 270, episode reward: -4191.000, mean reward: -598.714 [-999.000, -45.000], mean action: 1.714 [0.000, 3.000],  loss: 1072819.000000, mae: 3179.561175, accuracy: 0.066964, mean_q: -3813.792899, mean_eps: 0.850510\n",
      "  1686/50000: episode: 224, duration: 0.071s, episode steps:  21, steps per second: 296, episode reward: -18221.000, mean reward: -867.667 [-999.000, -60.000], mean action: 1.571 [0.000, 3.000],  loss: 1124417.702381, mae: 3148.405390, accuracy: 0.123512, mean_q: -3766.145578, mean_eps: 0.849250\n",
      "  1689/50000: episode: 225, duration: 0.012s, episode steps:   3, steps per second: 253, episode reward: -224.000, mean reward: -74.667 [-116.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 1469471.916667, mae: 3145.567627, accuracy: 0.114583, mean_q: -3739.771077, mean_eps: 0.848170\n",
      "  1695/50000: episode: 226, duration: 0.022s, episode steps:   6, steps per second: 276, episode reward: -3192.000, mean reward: -532.000 [-999.000, -58.000], mean action: 1.833 [0.000, 3.000],  loss: 1002070.989583, mae: 3091.699544, accuracy: 0.062500, mean_q: -3726.690430, mean_eps: 0.847765\n",
      "  1704/50000: episode: 227, duration: 0.032s, episode steps:   9, steps per second: 282, episode reward: -6189.000, mean reward: -687.667 [-999.000, -58.000], mean action: 1.556 [0.000, 3.000],  loss: 1167552.923611, mae: 3140.982395, accuracy: 0.114583, mean_q: -3744.658556, mean_eps: 0.847090\n",
      "  1715/50000: episode: 228, duration: 0.037s, episode steps:  11, steps per second: 295, episode reward: -8187.000, mean reward: -744.273 [-999.000, -45.000], mean action: 1.545 [0.000, 3.000],  loss: 1245958.715909, mae: 3146.779341, accuracy: 0.088068, mean_q: -3750.023682, mean_eps: 0.846190\n",
      "  1720/50000: episode: 229, duration: 0.020s, episode steps:   5, steps per second: 245, episode reward: -2222.000, mean reward: -444.400 [-999.000, -58.000], mean action: 1.800 [0.000, 3.000],  loss: 1258326.900000, mae: 3148.986865, accuracy: 0.100000, mean_q: -3764.189111, mean_eps: 0.845470\n",
      "  1727/50000: episode: 230, duration: 0.025s, episode steps:   7, steps per second: 275, episode reward: -4220.000, mean reward: -602.857 [-999.000, -32.000], mean action: 1.000 [0.000, 3.000],  loss: 1371765.294643, mae: 3153.537388, accuracy: 0.125000, mean_q: -3739.792132, mean_eps: 0.844930\n",
      "  1735/50000: episode: 231, duration: 0.029s, episode steps:   8, steps per second: 277, episode reward: -5219.000, mean reward: -652.375 [-999.000, -58.000], mean action: 1.750 [0.000, 3.000],  loss: 975238.804688, mae: 3095.129578, accuracy: 0.132812, mean_q: -3715.921875, mean_eps: 0.844255\n",
      "  1742/50000: episode: 232, duration: 0.024s, episode steps:   7, steps per second: 295, episode reward: -4235.000, mean reward: -605.000 [-999.000, -58.000], mean action: 1.429 [0.000, 3.000],  loss: 970184.723214, mae: 3133.706159, accuracy: 0.138393, mean_q: -3766.353481, mean_eps: 0.843580\n",
      "  1747/50000: episode: 233, duration: 0.020s, episode steps:   5, steps per second: 254, episode reward: -2237.000, mean reward: -447.400 [-999.000, -58.000], mean action: 1.400 [0.000, 3.000],  loss: 1084380.525000, mae: 3196.250146, accuracy: 0.168750, mean_q: -3837.093701, mean_eps: 0.843040\n",
      "  1750/50000: episode: 234, duration: 0.014s, episode steps:   3, steps per second: 214, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 1119766.750000, mae: 3201.286621, accuracy: 0.208333, mean_q: -3847.805339, mean_eps: 0.842680\n",
      "  1761/50000: episode: 235, duration: 0.039s, episode steps:  11, steps per second: 285, episode reward: -8231.000, mean reward: -748.273 [-999.000, -45.000], mean action: 1.364 [0.000, 3.000],  loss: 1173572.573864, mae: 3225.722434, accuracy: 0.207386, mean_q: -3864.180686, mean_eps: 0.842050\n",
      "  1767/50000: episode: 236, duration: 0.023s, episode steps:   6, steps per second: 261, episode reward: -3192.000, mean reward: -532.000 [-999.000, -32.000], mean action: 0.500 [0.000, 2.000],  loss: 1424333.854167, mae: 3216.397786, accuracy: 0.442708, mean_q: -3820.896484, mean_eps: 0.841285\n",
      "  1777/50000: episode: 237, duration: 0.037s, episode steps:  10, steps per second: 273, episode reward: -7188.000, mean reward: -718.800 [-999.000, -32.000], mean action: 0.900 [0.000, 3.000],  loss: 1232031.200000, mae: 3170.997388, accuracy: 0.725000, mean_q: -3768.010425, mean_eps: 0.840565\n",
      "  1780/50000: episode: 238, duration: 0.014s, episode steps:   3, steps per second: 214, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 1586645.958333, mae: 3194.745361, accuracy: 0.666667, mean_q: -3747.418864, mean_eps: 0.839980\n",
      "  1783/50000: episode: 239, duration: 0.014s, episode steps:   3, steps per second: 221, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 1339800.083333, mae: 3139.860596, accuracy: 0.479167, mean_q: -3729.535482, mean_eps: 0.839710\n",
      "  1790/50000: episode: 240, duration: 0.026s, episode steps:   7, steps per second: 274, episode reward: -4220.000, mean reward: -602.857 [-999.000, -58.000], mean action: 1.286 [0.000, 2.000],  loss: 1181844.785714, mae: 3119.622454, accuracy: 0.406250, mean_q: -3712.497001, mean_eps: 0.839260\n",
      "  1796/50000: episode: 241, duration: 0.022s, episode steps:   6, steps per second: 270, episode reward: -3236.000, mean reward: -539.333 [-999.000, -45.000], mean action: 1.333 [0.000, 3.000],  loss: 1000710.510417, mae: 3096.893799, accuracy: 0.364583, mean_q: -3716.955933, mean_eps: 0.838675\n",
      "  1802/50000: episode: 242, duration: 0.023s, episode steps:   6, steps per second: 258, episode reward: -3221.000, mean reward: -536.833 [-999.000, -32.000], mean action: 1.167 [0.000, 3.000],  loss: 1199154.072917, mae: 3140.630859, accuracy: 0.354167, mean_q: -3742.442790, mean_eps: 0.838135\n",
      "  1805/50000: episode: 243, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 1043520.708333, mae: 3127.829427, accuracy: 0.229167, mean_q: -3761.142415, mean_eps: 0.837730\n",
      "  1809/50000: episode: 244, duration: 0.016s, episode steps:   4, steps per second: 255, episode reward: -1238.000, mean reward: -309.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 1049605.250000, mae: 3147.188965, accuracy: 0.375000, mean_q: -3798.873596, mean_eps: 0.837415\n",
      "  1819/50000: episode: 245, duration: 0.035s, episode steps:  10, steps per second: 285, episode reward: -7217.000, mean reward: -721.700 [-999.000, -58.000], mean action: 1.100 [0.000, 3.000],  loss: 1177096.012500, mae: 3170.210474, accuracy: 0.406250, mean_q: -3808.219019, mean_eps: 0.836785\n",
      "  1828/50000: episode: 246, duration: 0.032s, episode steps:   9, steps per second: 277, episode reward: -6189.000, mean reward: -687.667 [-999.000, -45.000], mean action: 2.222 [0.000, 3.000],  loss: 1083777.156250, mae: 3170.118001, accuracy: 0.600694, mean_q: -3813.565972, mean_eps: 0.835930\n",
      "  1833/50000: episode: 247, duration: 0.018s, episode steps:   5, steps per second: 277, episode reward: -2193.000, mean reward: -438.600 [-999.000, -45.000], mean action: 1.800 [0.000, 3.000],  loss: 985194.350000, mae: 3157.040039, accuracy: 0.437500, mean_q: -3806.042969, mean_eps: 0.835300\n",
      "  1844/50000: episode: 248, duration: 0.040s, episode steps:  11, steps per second: 274, episode reward: -8187.000, mean reward: -744.273 [-999.000, -45.000], mean action: 2.091 [0.000, 3.000],  loss: 1194803.664773, mae: 3178.676847, accuracy: 0.343750, mean_q: -3823.141824, mean_eps: 0.834580\n",
      "  1849/50000: episode: 249, duration: 0.021s, episode steps:   5, steps per second: 237, episode reward: -2193.000, mean reward: -438.600 [-999.000, -32.000], mean action: 1.800 [1.000, 3.000],  loss: 898578.325000, mae: 3145.939648, accuracy: 0.293750, mean_q: -3819.264697, mean_eps: 0.833860\n",
      "  1857/50000: episode: 250, duration: 0.030s, episode steps:   8, steps per second: 267, episode reward: -5219.000, mean reward: -652.375 [-999.000, -58.000], mean action: 1.875 [0.000, 3.000],  loss: 823232.488281, mae: 3150.301941, accuracy: 0.214844, mean_q: -3850.372131, mean_eps: 0.833275\n",
      "  1861/50000: episode: 251, duration: 0.016s, episode steps:   4, steps per second: 244, episode reward: -1238.000, mean reward: -309.500 [-999.000, -58.000], mean action: 1.000 [0.000, 3.000],  loss: 1053884.242188, mae: 3218.319946, accuracy: 0.148438, mean_q: -3902.079590, mean_eps: 0.832735\n",
      "  1864/50000: episode: 252, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 1021009.375000, mae: 3215.232503, accuracy: 0.104167, mean_q: -3899.532389, mean_eps: 0.832420\n",
      "  1870/50000: episode: 253, duration: 0.022s, episode steps:   6, steps per second: 270, episode reward: -3236.000, mean reward: -539.333 [-999.000, -58.000], mean action: 2.000 [0.000, 3.000],  loss: 1220048.325521, mae: 3264.717285, accuracy: 0.130208, mean_q: -3927.005615, mean_eps: 0.832015\n",
      "  1873/50000: episode: 254, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 1106503.041667, mae: 3227.214518, accuracy: 0.218750, mean_q: -3915.883789, mean_eps: 0.831610\n",
      "  1880/50000: episode: 255, duration: 0.029s, episode steps:   7, steps per second: 246, episode reward: -4235.000, mean reward: -605.000 [-999.000, -58.000], mean action: 0.857 [0.000, 3.000],  loss: 1358218.803571, mae: 3234.509975, accuracy: 0.348214, mean_q: -3855.868722, mean_eps: 0.831160\n",
      "  1883/50000: episode: 256, duration: 0.014s, episode steps:   3, steps per second: 217, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.667 [0.000, 3.000],  loss: 1279926.166667, mae: 3186.928711, accuracy: 0.291667, mean_q: -3816.128174, mean_eps: 0.830710\n",
      "  1889/50000: episode: 257, duration: 0.025s, episode steps:   6, steps per second: 239, episode reward: -3221.000, mean reward: -536.833 [-999.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 1461455.937500, mae: 3200.909180, accuracy: 0.375000, mean_q: -3804.150187, mean_eps: 0.830305\n",
      "  1893/50000: episode: 258, duration: 0.018s, episode steps:   4, steps per second: 228, episode reward: -1238.000, mean reward: -309.500 [-999.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 1237018.718750, mae: 3118.144592, accuracy: 0.250000, mean_q: -3705.925110, mean_eps: 0.829855\n",
      "  1896/50000: episode: 259, duration: 0.015s, episode steps:   3, steps per second: 200, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 1323976.666667, mae: 3157.671224, accuracy: 0.135417, mean_q: -3717.807210, mean_eps: 0.829540\n",
      "  1899/50000: episode: 260, duration: 0.015s, episode steps:   3, steps per second: 202, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 1451544.750000, mae: 3183.378418, accuracy: 0.104167, mean_q: -3753.227702, mean_eps: 0.829270\n",
      "  1904/50000: episode: 261, duration: 0.023s, episode steps:   5, steps per second: 221, episode reward: -2222.000, mean reward: -444.400 [-999.000, -32.000], mean action: 1.400 [0.000, 3.000],  loss: 1304831.937500, mae: 3149.396289, accuracy: 0.050000, mean_q: -3723.163721, mean_eps: 0.828910\n",
      "  1907/50000: episode: 262, duration: 0.015s, episode steps:   3, steps per second: 205, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 1340356.125000, mae: 3131.214518, accuracy: 0.125000, mean_q: -3713.171712, mean_eps: 0.828550\n",
      "  1911/50000: episode: 263, duration: 0.017s, episode steps:   4, steps per second: 233, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 2.000 [0.000, 3.000],  loss: 869007.890625, mae: 3080.330627, accuracy: 0.148438, mean_q: -3710.472595, mean_eps: 0.828235\n",
      "  1920/50000: episode: 264, duration: 0.037s, episode steps:   9, steps per second: 245, episode reward: -6233.000, mean reward: -692.556 [-999.000, -45.000], mean action: 1.222 [0.000, 3.000],  loss: 1308700.812500, mae: 3153.602241, accuracy: 0.368056, mean_q: -3756.161241, mean_eps: 0.827650\n",
      "  1926/50000: episode: 265, duration: 0.022s, episode steps:   6, steps per second: 268, episode reward: -3221.000, mean reward: -536.833 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 1037291.208333, mae: 3141.696208, accuracy: 0.354167, mean_q: -3774.703573, mean_eps: 0.826975\n",
      "  1929/50000: episode: 266, duration: 0.013s, episode steps:   3, steps per second: 223, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 1260911.843750, mae: 3165.392822, accuracy: 0.281250, mean_q: -3769.145345, mean_eps: 0.826570\n",
      "  1943/50000: episode: 267, duration: 0.048s, episode steps:  14, steps per second: 293, episode reward: -11184.000, mean reward: -798.857 [-999.000, -32.000], mean action: 1.286 [0.000, 3.000],  loss: 1039567.328125, mae: 3180.882202, accuracy: 0.133929, mean_q: -3822.316284, mean_eps: 0.825805\n",
      "  1948/50000: episode: 268, duration: 0.019s, episode steps:   5, steps per second: 257, episode reward: -2237.000, mean reward: -447.400 [-999.000, -58.000], mean action: 1.600 [0.000, 3.000],  loss: 989110.575000, mae: 3184.940430, accuracy: 0.125000, mean_q: -3821.319336, mean_eps: 0.824950\n",
      "  1955/50000: episode: 269, duration: 0.027s, episode steps:   7, steps per second: 258, episode reward: -4191.000, mean reward: -598.714 [-999.000, -58.000], mean action: 2.143 [0.000, 3.000],  loss: 1347409.214286, mae: 3220.505197, accuracy: 0.062500, mean_q: -3817.357143, mean_eps: 0.824410\n",
      "  1958/50000: episode: 270, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 880436.791667, mae: 3127.682861, accuracy: 0.083333, mean_q: -3770.185140, mean_eps: 0.823960\n",
      "  1966/50000: episode: 271, duration: 0.028s, episode steps:   8, steps per second: 286, episode reward: -5234.000, mean reward: -654.250 [-999.000, -45.000], mean action: 1.625 [0.000, 3.000],  loss: 979654.898438, mae: 3153.697144, accuracy: 0.089844, mean_q: -3782.504883, mean_eps: 0.823465\n",
      "  1980/50000: episode: 272, duration: 0.048s, episode steps:  14, steps per second: 289, episode reward: -11228.000, mean reward: -802.000 [-999.000, -60.000], mean action: 1.286 [0.000, 3.000],  loss: 1093772.162946, mae: 3199.922346, accuracy: 0.064732, mean_q: -3846.837507, mean_eps: 0.822475\n",
      "  1991/50000: episode: 273, duration: 0.039s, episode steps:  11, steps per second: 281, episode reward: -8231.000, mean reward: -748.273 [-999.000, -58.000], mean action: 2.182 [0.000, 3.000],  loss: 1256054.147727, mae: 3242.577792, accuracy: 0.196023, mean_q: -3895.193248, mean_eps: 0.821350\n",
      "  1996/50000: episode: 274, duration: 0.018s, episode steps:   5, steps per second: 276, episode reward: -2222.000, mean reward: -444.400 [-999.000, -58.000], mean action: 1.800 [0.000, 3.000],  loss: 934802.212500, mae: 3160.859814, accuracy: 0.381250, mean_q: -3832.520264, mean_eps: 0.820630\n",
      "  2005/50000: episode: 275, duration: 0.032s, episode steps:   9, steps per second: 281, episode reward: -6189.000, mean reward: -687.667 [-999.000, -58.000], mean action: 1.556 [0.000, 3.000],  loss: 1087774.291667, mae: 3204.088623, accuracy: 0.381944, mean_q: -3847.524468, mean_eps: 0.820000\n",
      "  2012/50000: episode: 276, duration: 0.030s, episode steps:   7, steps per second: 231, episode reward: -4235.000, mean reward: -605.000 [-999.000, -45.000], mean action: 1.286 [0.000, 3.000],  loss: 1179170.428571, mae: 3217.598458, accuracy: 0.357143, mean_q: -3848.019113, mean_eps: 0.819280\n",
      "  2015/50000: episode: 277, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 744709.333333, mae: 3150.311930, accuracy: 0.416667, mean_q: -3831.582926, mean_eps: 0.818830\n",
      "  2025/50000: episode: 278, duration: 0.037s, episode steps:  10, steps per second: 269, episode reward: -7232.000, mean reward: -723.200 [-999.000, -45.000], mean action: 1.600 [0.000, 3.000],  loss: 1145967.118750, mae: 3204.368579, accuracy: 0.368750, mean_q: -3849.818481, mean_eps: 0.818245\n",
      "  2033/50000: episode: 279, duration: 0.029s, episode steps:   8, steps per second: 277, episode reward: -5234.000, mean reward: -654.250 [-999.000, -45.000], mean action: 1.375 [0.000, 3.000],  loss: 1127716.753906, mae: 3206.971313, accuracy: 0.371094, mean_q: -3876.187073, mean_eps: 0.817435\n",
      "  2045/50000: episode: 280, duration: 0.043s, episode steps:  12, steps per second: 278, episode reward: -9186.000, mean reward: -765.500 [-999.000, -58.000], mean action: 2.167 [0.000, 3.000],  loss: 979244.325521, mae: 3191.194702, accuracy: 0.252604, mean_q: -3860.215861, mean_eps: 0.816535\n",
      "  2049/50000: episode: 281, duration: 0.015s, episode steps:   4, steps per second: 261, episode reward: -1238.000, mean reward: -309.500 [-999.000, -60.000], mean action: 1.750 [1.000, 3.000],  loss: 1103983.953125, mae: 3236.482361, accuracy: 0.164062, mean_q: -3916.124207, mean_eps: 0.815815\n",
      "  2060/50000: episode: 282, duration: 0.039s, episode steps:  11, steps per second: 284, episode reward: -8231.000, mean reward: -748.273 [-999.000, -45.000], mean action: 1.818 [0.000, 3.000],  loss: 1096569.392045, mae: 3223.013184, accuracy: 0.142045, mean_q: -3892.945956, mean_eps: 0.815140\n",
      "  2076/50000: episode: 283, duration: 0.057s, episode steps:  16, steps per second: 281, episode reward: -13182.000, mean reward: -823.875 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 1105104.189453, mae: 3223.073212, accuracy: 0.333984, mean_q: -3890.773285, mean_eps: 0.813925\n",
      "  2086/50000: episode: 284, duration: 0.037s, episode steps:  10, steps per second: 269, episode reward: -7188.000, mean reward: -718.800 [-999.000, -58.000], mean action: 1.200 [0.000, 3.000],  loss: 1238417.437500, mae: 3232.053345, accuracy: 0.462500, mean_q: -3877.930469, mean_eps: 0.812755\n",
      "  2095/50000: episode: 285, duration: 0.033s, episode steps:   9, steps per second: 271, episode reward: -6233.000, mean reward: -692.556 [-999.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 1168340.826389, mae: 3215.207845, accuracy: 0.253472, mean_q: -3852.052246, mean_eps: 0.811900\n",
      "  2099/50000: episode: 286, duration: 0.017s, episode steps:   4, steps per second: 237, episode reward: -1223.000, mean reward: -305.750 [-999.000, -58.000], mean action: 1.250 [0.000, 3.000],  loss: 1260532.953125, mae: 3203.832825, accuracy: 0.031250, mean_q: -3844.056335, mean_eps: 0.811315\n",
      "  2103/50000: episode: 287, duration: 0.015s, episode steps:   4, steps per second: 266, episode reward: -1223.000, mean reward: -305.750 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 1332533.500000, mae: 3231.418152, accuracy: 0.054688, mean_q: -3845.640198, mean_eps: 0.810955\n",
      "  2109/50000: episode: 288, duration: 0.022s, episode steps:   6, steps per second: 278, episode reward: -3192.000, mean reward: -532.000 [-999.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 1265785.729167, mae: 3192.018188, accuracy: 0.067708, mean_q: -3801.156169, mean_eps: 0.810505\n",
      "  2115/50000: episode: 289, duration: 0.023s, episode steps:   6, steps per second: 258, episode reward: -3236.000, mean reward: -539.333 [-999.000, -60.000], mean action: 2.000 [0.000, 3.000],  loss: 1450980.177083, mae: 3189.483480, accuracy: 0.281250, mean_q: -3768.717448, mean_eps: 0.809965\n",
      "  2126/50000: episode: 290, duration: 0.040s, episode steps:  11, steps per second: 275, episode reward: -8187.000, mean reward: -744.273 [-999.000, -32.000], mean action: 0.636 [0.000, 3.000],  loss: 1161068.994318, mae: 3155.123424, accuracy: 0.571023, mean_q: -3738.428134, mean_eps: 0.809200\n",
      "  2132/50000: episode: 291, duration: 0.022s, episode steps:   6, steps per second: 274, episode reward: -3192.000, mean reward: -532.000 [-999.000, -32.000], mean action: 2.000 [0.000, 3.000],  loss: 1001908.708333, mae: 3153.396159, accuracy: 0.552083, mean_q: -3731.511515, mean_eps: 0.808435\n",
      "  2143/50000: episode: 292, duration: 0.039s, episode steps:  11, steps per second: 279, episode reward: -8187.000, mean reward: -744.273 [-999.000, -58.000], mean action: 1.364 [0.000, 3.000],  loss: 1122015.471591, mae: 3217.091064, accuracy: 0.517045, mean_q: -3791.874778, mean_eps: 0.807670\n",
      "  2157/50000: episode: 293, duration: 0.047s, episode steps:  14, steps per second: 296, episode reward: -11228.000, mean reward: -802.000 [-999.000, -60.000], mean action: 1.500 [0.000, 3.000],  loss: 1118229.174107, mae: 3245.189000, accuracy: 0.493304, mean_q: -3869.383475, mean_eps: 0.806545\n",
      "  2161/50000: episode: 294, duration: 0.015s, episode steps:   4, steps per second: 259, episode reward: -1238.000, mean reward: -309.500 [-999.000, -60.000], mean action: 1.500 [0.000, 3.000],  loss: 1021872.531250, mae: 3247.357544, accuracy: 0.476562, mean_q: -3890.218445, mean_eps: 0.805735\n",
      "  2169/50000: episode: 295, duration: 0.029s, episode steps:   8, steps per second: 277, episode reward: -5234.000, mean reward: -654.250 [-999.000, -60.000], mean action: 1.250 [0.000, 3.000],  loss: 1215519.281250, mae: 3253.518646, accuracy: 0.343750, mean_q: -3884.664307, mean_eps: 0.805195\n",
      "  2176/50000: episode: 296, duration: 0.026s, episode steps:   7, steps per second: 265, episode reward: -4220.000, mean reward: -602.857 [-999.000, -32.000], mean action: 1.571 [0.000, 3.000],  loss: 1029372.866071, mae: 3207.734515, accuracy: 0.334821, mean_q: -3839.419399, mean_eps: 0.804520\n",
      "  2197/50000: episode: 297, duration: 0.070s, episode steps:  21, steps per second: 299, episode reward: -18177.000, mean reward: -865.571 [-999.000, -32.000], mean action: 1.048 [0.000, 3.000],  loss: 1172109.175595, mae: 3246.349458, accuracy: 0.181548, mean_q: -3865.329346, mean_eps: 0.803260\n",
      "  2201/50000: episode: 298, duration: 0.016s, episode steps:   4, steps per second: 255, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 1121608.390625, mae: 3242.429016, accuracy: 0.156250, mean_q: -3869.420105, mean_eps: 0.802135\n",
      "  2209/50000: episode: 299, duration: 0.031s, episode steps:   8, steps per second: 261, episode reward: -5219.000, mean reward: -652.375 [-999.000, -58.000], mean action: 2.125 [0.000, 3.000],  loss: 1143730.320312, mae: 3232.941467, accuracy: 0.191406, mean_q: -3867.742584, mean_eps: 0.801595\n",
      "  2213/50000: episode: 300, duration: 0.017s, episode steps:   4, steps per second: 230, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 1440514.375000, mae: 3267.276489, accuracy: 0.195312, mean_q: -3852.588867, mean_eps: 0.801055\n",
      "  2223/50000: episode: 301, duration: 0.035s, episode steps:  10, steps per second: 289, episode reward: -7188.000, mean reward: -718.800 [-999.000, -32.000], mean action: 1.300 [0.000, 3.000],  loss: 1027272.750000, mae: 3201.170264, accuracy: 0.181250, mean_q: -3832.082471, mean_eps: 0.800425\n",
      "  2241/50000: episode: 302, duration: 0.063s, episode steps:  18, steps per second: 284, episode reward: -15224.000, mean reward: -845.778 [-999.000, -45.000], mean action: 1.444 [0.000, 3.000],  loss: 1131851.607639, mae: 3226.192885, accuracy: 0.185764, mean_q: -3864.438178, mean_eps: 0.799165\n",
      "  2253/50000: episode: 303, duration: 0.043s, episode steps:  12, steps per second: 278, episode reward: -9215.000, mean reward: -767.917 [-999.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 981875.843750, mae: 3246.415019, accuracy: 0.127604, mean_q: -3918.778381, mean_eps: 0.797815\n",
      "  2262/50000: episode: 304, duration: 0.033s, episode steps:   9, steps per second: 271, episode reward: -6233.000, mean reward: -692.556 [-999.000, -58.000], mean action: 0.889 [0.000, 3.000],  loss: 1320261.062500, mae: 3293.692329, accuracy: 0.079861, mean_q: -3920.897298, mean_eps: 0.796870\n",
      "  2267/50000: episode: 305, duration: 0.019s, episode steps:   5, steps per second: 264, episode reward: -2237.000, mean reward: -447.400 [-999.000, -60.000], mean action: 1.200 [0.000, 3.000],  loss: 1189303.625000, mae: 3250.010742, accuracy: 0.056250, mean_q: -3878.265869, mean_eps: 0.796240\n",
      "  2274/50000: episode: 306, duration: 0.025s, episode steps:   7, steps per second: 278, episode reward: -4220.000, mean reward: -602.857 [-999.000, -58.000], mean action: 1.143 [0.000, 3.000],  loss: 1057169.312500, mae: 3231.346157, accuracy: 0.049107, mean_q: -3882.196498, mean_eps: 0.795700\n",
      "  2278/50000: episode: 307, duration: 0.015s, episode steps:   4, steps per second: 269, episode reward: -1223.000, mean reward: -305.750 [-999.000, -58.000], mean action: 2.000 [0.000, 3.000],  loss: 1684574.375000, mae: 3305.649780, accuracy: 0.046875, mean_q: -3874.761841, mean_eps: 0.795205\n",
      "  2283/50000: episode: 308, duration: 0.019s, episode steps:   5, steps per second: 259, episode reward: -2237.000, mean reward: -447.400 [-999.000, -45.000], mean action: 1.800 [0.000, 3.000],  loss: 1072866.112500, mae: 3228.780078, accuracy: 0.043750, mean_q: -3864.064648, mean_eps: 0.794800\n",
      "  2293/50000: episode: 309, duration: 0.035s, episode steps:  10, steps per second: 286, episode reward: -7217.000, mean reward: -721.700 [-999.000, -32.000], mean action: 1.700 [0.000, 3.000],  loss: 1045424.468750, mae: 3220.465869, accuracy: 0.075000, mean_q: -3848.851636, mean_eps: 0.794125\n",
      "  2302/50000: episode: 310, duration: 0.033s, episode steps:   9, steps per second: 275, episode reward: -6233.000, mean reward: -692.556 [-999.000, -45.000], mean action: 1.667 [1.000, 3.000],  loss: 896981.368056, mae: 3219.519287, accuracy: 0.083333, mean_q: -3890.525336, mean_eps: 0.793270\n",
      "  2307/50000: episode: 311, duration: 0.019s, episode steps:   5, steps per second: 259, episode reward: -2222.000, mean reward: -444.400 [-999.000, -58.000], mean action: 1.400 [0.000, 3.000],  loss: 1508405.125000, mae: 3335.290332, accuracy: 0.056250, mean_q: -3955.676416, mean_eps: 0.792640\n",
      "  2312/50000: episode: 312, duration: 0.020s, episode steps:   5, steps per second: 254, episode reward: -2222.000, mean reward: -444.400 [-999.000, -58.000], mean action: 1.800 [0.000, 3.000],  loss: 1297584.025000, mae: 3285.688916, accuracy: 0.056250, mean_q: -3907.210986, mean_eps: 0.792190\n",
      "  2324/50000: episode: 313, duration: 0.040s, episode steps:  12, steps per second: 302, episode reward: -9186.000, mean reward: -765.500 [-999.000, -32.000], mean action: 1.583 [0.000, 3.000],  loss: 1167062.338542, mae: 3255.033712, accuracy: 0.059896, mean_q: -3869.992472, mean_eps: 0.791425\n",
      "  2331/50000: episode: 314, duration: 0.028s, episode steps:   7, steps per second: 253, episode reward: -4220.000, mean reward: -602.857 [-999.000, -32.000], mean action: 2.143 [0.000, 3.000],  loss: 871068.437500, mae: 3228.240374, accuracy: 0.098214, mean_q: -3886.862898, mean_eps: 0.790570\n",
      "  2336/50000: episode: 315, duration: 0.019s, episode steps:   5, steps per second: 269, episode reward: -2193.000, mean reward: -438.600 [-999.000, -32.000], mean action: 1.400 [0.000, 3.000],  loss: 1234297.675000, mae: 3293.403076, accuracy: 0.081250, mean_q: -3924.271680, mean_eps: 0.790030\n",
      "  2349/50000: episode: 316, duration: 0.045s, episode steps:  13, steps per second: 291, episode reward: -10185.000, mean reward: -783.462 [-999.000, -58.000], mean action: 1.308 [0.000, 3.000],  loss: 1056515.951923, mae: 3265.622352, accuracy: 0.062500, mean_q: -3919.869310, mean_eps: 0.789220\n",
      "  2352/50000: episode: 317, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 1385993.041667, mae: 3333.028158, accuracy: 0.052083, mean_q: -3927.729085, mean_eps: 0.788500\n",
      "  2357/50000: episode: 318, duration: 0.020s, episode steps:   5, steps per second: 246, episode reward: -2193.000, mean reward: -438.600 [-999.000, -58.000], mean action: 2.000 [0.000, 3.000],  loss: 1080714.862500, mae: 3253.830859, accuracy: 0.075000, mean_q: -3882.702393, mean_eps: 0.788140\n",
      "  2361/50000: episode: 319, duration: 0.016s, episode steps:   4, steps per second: 249, episode reward: -1223.000, mean reward: -305.750 [-999.000, -58.000], mean action: 2.250 [1.000, 3.000],  loss: 1006421.843750, mae: 3228.302917, accuracy: 0.109375, mean_q: -3852.579529, mean_eps: 0.787735\n",
      "  2371/50000: episode: 320, duration: 0.036s, episode steps:  10, steps per second: 278, episode reward: -7188.000, mean reward: -718.800 [-999.000, -32.000], mean action: 1.200 [0.000, 3.000],  loss: 1296148.012500, mae: 3264.680469, accuracy: 0.087500, mean_q: -3865.802417, mean_eps: 0.787105\n",
      "  2374/50000: episode: 321, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 1579911.125000, mae: 3291.201823, accuracy: 0.052083, mean_q: -3847.392090, mean_eps: 0.786520\n",
      "  2378/50000: episode: 322, duration: 0.017s, episode steps:   4, steps per second: 238, episode reward: -1223.000, mean reward: -305.750 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 1214979.312500, mae: 3229.203430, accuracy: 0.093750, mean_q: -3816.823669, mean_eps: 0.786205\n",
      "  2396/50000: episode: 323, duration: 0.061s, episode steps:  18, steps per second: 294, episode reward: -15180.000, mean reward: -843.333 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 1361215.309028, mae: 3210.248928, accuracy: 0.078125, mean_q: -3748.868476, mean_eps: 0.785215\n",
      "  2403/50000: episode: 324, duration: 0.026s, episode steps:   7, steps per second: 274, episode reward: -4235.000, mean reward: -605.000 [-999.000, -45.000], mean action: 1.714 [0.000, 3.000],  loss: 1241791.812500, mae: 3197.159842, accuracy: 0.053571, mean_q: -3764.299456, mean_eps: 0.784090\n",
      "  2409/50000: episode: 325, duration: 0.023s, episode steps:   6, steps per second: 264, episode reward: -3236.000, mean reward: -539.333 [-999.000, -45.000], mean action: 1.167 [0.000, 3.000],  loss: 1336050.583333, mae: 3217.210042, accuracy: 0.088542, mean_q: -3803.437134, mean_eps: 0.783505\n",
      "  2413/50000: episode: 326, duration: 0.017s, episode steps:   4, steps per second: 234, episode reward: -1238.000, mean reward: -309.500 [-999.000, -60.000], mean action: 1.500 [0.000, 3.000],  loss: 708532.820312, mae: 3164.109253, accuracy: 0.234375, mean_q: -3850.108032, mean_eps: 0.783055\n",
      "  2421/50000: episode: 327, duration: 0.029s, episode steps:   8, steps per second: 273, episode reward: -5190.000, mean reward: -648.750 [-999.000, -32.000], mean action: 1.000 [0.000, 3.000],  loss: 1112312.113281, mae: 3205.059143, accuracy: 0.332031, mean_q: -3854.714050, mean_eps: 0.782515\n",
      "  2425/50000: episode: 328, duration: 0.016s, episode steps:   4, steps per second: 255, episode reward: -1223.000, mean reward: -305.750 [-999.000, -58.000], mean action: 1.750 [0.000, 3.000],  loss: 1301983.062500, mae: 3226.662537, accuracy: 0.359375, mean_q: -3857.508362, mean_eps: 0.781975\n",
      "  2430/50000: episode: 329, duration: 0.020s, episode steps:   5, steps per second: 249, episode reward: -2193.000, mean reward: -438.600 [-999.000, -58.000], mean action: 1.200 [0.000, 3.000],  loss: 1107240.462500, mae: 3204.641113, accuracy: 0.187500, mean_q: -3856.203369, mean_eps: 0.781570\n",
      "  2437/50000: episode: 330, duration: 0.027s, episode steps:   7, steps per second: 259, episode reward: -4235.000, mean reward: -605.000 [-999.000, -45.000], mean action: 1.857 [1.000, 3.000],  loss: 1044351.875000, mae: 3223.977713, accuracy: 0.125000, mean_q: -3875.323486, mean_eps: 0.781030\n",
      "  2454/50000: episode: 331, duration: 0.060s, episode steps:  17, steps per second: 283, episode reward: -14210.000, mean reward: -835.882 [-999.000, -32.000], mean action: 1.235 [0.000, 3.000],  loss: 1022557.952206, mae: 3253.694020, accuracy: 0.165441, mean_q: -3930.081270, mean_eps: 0.779950\n",
      "  2458/50000: episode: 332, duration: 0.017s, episode steps:   4, steps per second: 239, episode reward: -1223.000, mean reward: -305.750 [-999.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 1578783.421875, mae: 3314.026917, accuracy: 0.078125, mean_q: -3930.778076, mean_eps: 0.779005\n",
      "  2462/50000: episode: 333, duration: 0.017s, episode steps:   4, steps per second: 238, episode reward: -1223.000, mean reward: -305.750 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 1079448.875000, mae: 3245.772766, accuracy: 0.070312, mean_q: -3915.967163, mean_eps: 0.778645\n",
      "  2466/50000: episode: 334, duration: 0.016s, episode steps:   4, steps per second: 249, episode reward: -1238.000, mean reward: -309.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 973561.812500, mae: 3245.475403, accuracy: 0.054688, mean_q: -3921.385498, mean_eps: 0.778285\n",
      "  2469/50000: episode: 335, duration: 0.012s, episode steps:   3, steps per second: 255, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 1475202.666667, mae: 3278.533773, accuracy: 0.104167, mean_q: -3889.629395, mean_eps: 0.777970\n",
      "  2484/50000: episode: 336, duration: 0.053s, episode steps:  15, steps per second: 285, episode reward: -12212.000, mean reward: -814.133 [-999.000, -32.000], mean action: 1.800 [0.000, 3.000],  loss: 1164941.870833, mae: 3234.595264, accuracy: 0.212500, mean_q: -3868.820622, mean_eps: 0.777160\n",
      "  2504/50000: episode: 337, duration: 0.068s, episode steps:  20, steps per second: 294, episode reward: -17222.000, mean reward: -861.100 [-999.000, -60.000], mean action: 1.500 [0.000, 3.000],  loss: 989099.093750, mae: 3219.738086, accuracy: 0.404687, mean_q: -3864.249792, mean_eps: 0.775585\n",
      "  2508/50000: episode: 338, duration: 0.016s, episode steps:   4, steps per second: 251, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 1164069.640625, mae: 3256.420776, accuracy: 0.390625, mean_q: -3884.582092, mean_eps: 0.774505\n",
      "  2517/50000: episode: 339, duration: 0.032s, episode steps:   9, steps per second: 277, episode reward: -6189.000, mean reward: -687.667 [-999.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 1067619.895833, mae: 3255.578776, accuracy: 0.531250, mean_q: -3877.997287, mean_eps: 0.773920\n",
      "  2522/50000: episode: 340, duration: 0.020s, episode steps:   5, steps per second: 253, episode reward: -2222.000, mean reward: -444.400 [-999.000, -32.000], mean action: 1.600 [0.000, 3.000],  loss: 1196427.037500, mae: 3262.213281, accuracy: 0.512500, mean_q: -3870.750293, mean_eps: 0.773290\n",
      "  2535/50000: episode: 341, duration: 0.043s, episode steps:  13, steps per second: 301, episode reward: -10214.000, mean reward: -785.692 [-999.000, -58.000], mean action: 1.000 [0.000, 3.000],  loss: 1220240.225962, mae: 3283.892165, accuracy: 0.492788, mean_q: -3900.615234, mean_eps: 0.772480\n",
      "  2540/50000: episode: 342, duration: 0.020s, episode steps:   5, steps per second: 250, episode reward: -2237.000, mean reward: -447.400 [-999.000, -58.000], mean action: 1.200 [0.000, 3.000],  loss: 1119321.137500, mae: 3250.689600, accuracy: 0.412500, mean_q: -3899.853271, mean_eps: 0.771670\n",
      "  2550/50000: episode: 343, duration: 0.038s, episode steps:  10, steps per second: 267, episode reward: -7232.000, mean reward: -723.200 [-999.000, -45.000], mean action: 1.700 [0.000, 3.000],  loss: 1276671.518750, mae: 3273.891211, accuracy: 0.318750, mean_q: -3906.715479, mean_eps: 0.770995\n",
      "  2554/50000: episode: 344, duration: 0.017s, episode steps:   4, steps per second: 233, episode reward: -1223.000, mean reward: -305.750 [-999.000, -32.000], mean action: 0.750 [0.000, 2.000],  loss: 1402641.468750, mae: 3258.148132, accuracy: 0.125000, mean_q: -3879.965027, mean_eps: 0.770365\n",
      "  2557/50000: episode: 345, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 1536873.791667, mae: 3268.486165, accuracy: 0.041667, mean_q: -3867.397380, mean_eps: 0.770050\n",
      "  2563/50000: episode: 346, duration: 0.022s, episode steps:   6, steps per second: 271, episode reward: -3192.000, mean reward: -532.000 [-999.000, -58.000], mean action: 1.833 [0.000, 3.000],  loss: 1138634.989583, mae: 3204.548218, accuracy: 0.078125, mean_q: -3822.903605, mean_eps: 0.769645\n",
      "  2566/50000: episode: 347, duration: 0.014s, episode steps:   3, steps per second: 219, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 1264231.687500, mae: 3209.913493, accuracy: 0.093750, mean_q: -3802.469808, mean_eps: 0.769240\n",
      "  2572/50000: episode: 348, duration: 0.022s, episode steps:   6, steps per second: 273, episode reward: -3192.000, mean reward: -532.000 [-999.000, -58.000], mean action: 1.833 [0.000, 3.000],  loss: 1154268.906250, mae: 3199.246948, accuracy: 0.067708, mean_q: -3809.173584, mean_eps: 0.768835\n",
      "  2595/50000: episode: 349, duration: 0.078s, episode steps:  23, steps per second: 295, episode reward: -20175.000, mean reward: -877.174 [-999.000, -58.000], mean action: 1.739 [0.000, 3.000],  loss: 1349274.967391, mae: 3231.691077, accuracy: 0.047554, mean_q: -3829.452297, mean_eps: 0.767530\n",
      "  2601/50000: episode: 350, duration: 0.021s, episode steps:   6, steps per second: 280, episode reward: -3192.000, mean reward: -532.000 [-999.000, -58.000], mean action: 2.167 [0.000, 3.000],  loss: 1175984.270833, mae: 3183.127563, accuracy: 0.130208, mean_q: -3755.306722, mean_eps: 0.766225\n",
      "  2606/50000: episode: 351, duration: 0.023s, episode steps:   5, steps per second: 219, episode reward: -2193.000, mean reward: -438.600 [-999.000, -45.000], mean action: 1.400 [0.000, 3.000],  loss: 1205030.737500, mae: 3195.915088, accuracy: 0.087500, mean_q: -3784.323975, mean_eps: 0.765730\n",
      "  2612/50000: episode: 352, duration: 0.024s, episode steps:   6, steps per second: 247, episode reward: -3236.000, mean reward: -539.333 [-999.000, -45.000], mean action: 1.833 [0.000, 3.000],  loss: 1439683.479167, mae: 3253.144165, accuracy: 0.062500, mean_q: -3832.452311, mean_eps: 0.765235\n",
      "  2617/50000: episode: 353, duration: 0.021s, episode steps:   5, steps per second: 242, episode reward: -2237.000, mean reward: -447.400 [-999.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 1145907.400000, mae: 3203.356250, accuracy: 0.112500, mean_q: -3820.434473, mean_eps: 0.764740\n",
      "  2620/50000: episode: 354, duration: 0.014s, episode steps:   3, steps per second: 213, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 1578018.166667, mae: 3255.341797, accuracy: 0.072917, mean_q: -3827.473226, mean_eps: 0.764380\n",
      "  2626/50000: episode: 355, duration: 0.025s, episode steps:   6, steps per second: 236, episode reward: -3192.000, mean reward: -532.000 [-999.000, -45.000], mean action: 1.333 [0.000, 3.000],  loss: 1324506.302083, mae: 3229.397746, accuracy: 0.072917, mean_q: -3817.830241, mean_eps: 0.763975\n",
      "  2635/50000: episode: 356, duration: 0.036s, episode steps:   9, steps per second: 252, episode reward: -6233.000, mean reward: -692.556 [-999.000, -58.000], mean action: 1.889 [0.000, 3.000],  loss: 1127512.878472, mae: 3189.832058, accuracy: 0.107639, mean_q: -3803.514920, mean_eps: 0.763300\n",
      "  2643/50000: episode: 357, duration: 0.030s, episode steps:   8, steps per second: 264, episode reward: -5234.000, mean reward: -654.250 [-999.000, -45.000], mean action: 1.750 [0.000, 3.000],  loss: 1446857.773438, mae: 3228.933502, accuracy: 0.078125, mean_q: -3806.416870, mean_eps: 0.762535\n",
      "  2665/50000: episode: 358, duration: 0.076s, episode steps:  22, steps per second: 289, episode reward: -19205.000, mean reward: -872.955 [-999.000, -32.000], mean action: 1.136 [0.000, 3.000],  loss: 1176983.502841, mae: 3187.881758, accuracy: 0.171875, mean_q: -3794.927468, mean_eps: 0.761185\n",
      "  2678/50000: episode: 359, duration: 0.046s, episode steps:  13, steps per second: 281, episode reward: -10229.000, mean reward: -786.846 [-999.000, -45.000], mean action: 1.538 [0.000, 3.000],  loss: 1213895.230769, mae: 3198.401386, accuracy: 0.247596, mean_q: -3804.408485, mean_eps: 0.759610\n",
      "  2683/50000: episode: 360, duration: 0.020s, episode steps:   5, steps per second: 250, episode reward: -2237.000, mean reward: -447.400 [-999.000, -45.000], mean action: 1.600 [0.000, 3.000],  loss: 1070980.200000, mae: 3180.708105, accuracy: 0.287500, mean_q: -3790.810596, mean_eps: 0.758800\n",
      "  2693/50000: episode: 361, duration: 0.037s, episode steps:  10, steps per second: 268, episode reward: -7232.000, mean reward: -723.200 [-999.000, -45.000], mean action: 1.700 [0.000, 3.000],  loss: 1250225.703125, mae: 3205.739722, accuracy: 0.187500, mean_q: -3805.293848, mean_eps: 0.758125\n",
      "  2706/50000: episode: 362, duration: 0.043s, episode steps:  13, steps per second: 304, episode reward: -10229.000, mean reward: -786.846 [-999.000, -45.000], mean action: 1.615 [0.000, 3.000],  loss: 1299681.975962, mae: 3192.408560, accuracy: 0.112981, mean_q: -3763.958778, mean_eps: 0.757090\n",
      "  2716/50000: episode: 363, duration: 0.037s, episode steps:  10, steps per second: 269, episode reward: -7188.000, mean reward: -718.800 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 936529.000000, mae: 3147.520215, accuracy: 0.146875, mean_q: -3743.401123, mean_eps: 0.756055\n",
      "  2730/50000: episode: 364, duration: 0.052s, episode steps:  14, steps per second: 271, episode reward: -11228.000, mean reward: -802.000 [-999.000, -45.000], mean action: 1.429 [0.000, 3.000],  loss: 1040949.747768, mae: 3184.143188, accuracy: 0.140625, mean_q: -3772.838902, mean_eps: 0.754975\n",
      "  2735/50000: episode: 365, duration: 0.019s, episode steps:   5, steps per second: 256, episode reward: -2193.000, mean reward: -438.600 [-999.000, -45.000], mean action: 1.400 [0.000, 2.000],  loss: 1207658.975000, mae: 3219.969385, accuracy: 0.143750, mean_q: -3795.898779, mean_eps: 0.754120\n",
      "  2740/50000: episode: 366, duration: 0.020s, episode steps:   5, steps per second: 248, episode reward: -2237.000, mean reward: -447.400 [-999.000, -60.000], mean action: 1.400 [0.000, 3.000],  loss: 1507909.375000, mae: 3258.654492, accuracy: 0.175000, mean_q: -3797.439941, mean_eps: 0.753670\n",
      "  2746/50000: episode: 367, duration: 0.025s, episode steps:   6, steps per second: 243, episode reward: -3236.000, mean reward: -539.333 [-999.000, -45.000], mean action: 1.167 [0.000, 2.000],  loss: 885470.739583, mae: 3144.984131, accuracy: 0.187500, mean_q: -3741.812215, mean_eps: 0.753175\n",
      "  2753/50000: episode: 368, duration: 0.026s, episode steps:   7, steps per second: 274, episode reward: -4235.000, mean reward: -605.000 [-999.000, -45.000], mean action: 1.714 [0.000, 3.000],  loss: 1025389.578125, mae: 3144.759277, accuracy: 0.165179, mean_q: -3724.832380, mean_eps: 0.752590\n",
      "  2758/50000: episode: 369, duration: 0.018s, episode steps:   5, steps per second: 277, episode reward: -2237.000, mean reward: -447.400 [-999.000, -45.000], mean action: 1.800 [1.000, 3.000],  loss: 1182409.937500, mae: 3187.749121, accuracy: 0.137500, mean_q: -3759.683594, mean_eps: 0.752050\n",
      "  2768/50000: episode: 370, duration: 0.036s, episode steps:  10, steps per second: 282, episode reward: -7232.000, mean reward: -723.200 [-999.000, -45.000], mean action: 1.400 [0.000, 3.000],  loss: 1153063.950000, mae: 3196.628076, accuracy: 0.128125, mean_q: -3797.462476, mean_eps: 0.751375\n",
      "  2781/50000: episode: 371, duration: 0.047s, episode steps:  13, steps per second: 277, episode reward: -10214.000, mean reward: -785.692 [-999.000, -58.000], mean action: 2.000 [0.000, 3.000],  loss: 1359812.504808, mae: 3224.332106, accuracy: 0.064904, mean_q: -3800.636324, mean_eps: 0.750340\n",
      "  2786/50000: episode: 372, duration: 0.018s, episode steps:   5, steps per second: 276, episode reward: -2193.000, mean reward: -438.600 [-999.000, -32.000], mean action: 1.800 [0.000, 3.000],  loss: 970361.293750, mae: 3151.826172, accuracy: 0.075000, mean_q: -3754.483887, mean_eps: 0.749530\n",
      "  2800/50000: episode: 373, duration: 0.051s, episode steps:  14, steps per second: 275, episode reward: -11213.000, mean reward: -800.929 [-999.000, -58.000], mean action: 1.357 [0.000, 3.000],  loss: 1281799.495536, mae: 3174.066616, accuracy: 0.064732, mean_q: -3714.217268, mean_eps: 0.748675\n",
      "  2806/50000: episode: 374, duration: 0.023s, episode steps:   6, steps per second: 258, episode reward: -3236.000, mean reward: -539.333 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 1061517.817708, mae: 3149.769979, accuracy: 0.062500, mean_q: -3707.365112, mean_eps: 0.747775\n",
      "  2809/50000: episode: 375, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -224.000, mean reward: -74.667 [-116.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 977937.666667, mae: 3156.808919, accuracy: 0.052083, mean_q: -3705.401042, mean_eps: 0.747370\n",
      "  2818/50000: episode: 376, duration: 0.032s, episode steps:   9, steps per second: 277, episode reward: -6218.000, mean reward: -690.889 [-999.000, -58.000], mean action: 1.667 [0.000, 3.000],  loss: 1002281.888889, mae: 3174.667345, accuracy: 0.097222, mean_q: -3741.682970, mean_eps: 0.746830\n",
      "  2825/50000: episode: 377, duration: 0.025s, episode steps:   7, steps per second: 281, episode reward: -4191.000, mean reward: -598.714 [-999.000, -45.000], mean action: 1.857 [0.000, 3.000],  loss: 986003.392857, mae: 3189.146136, accuracy: 0.111607, mean_q: -3778.988421, mean_eps: 0.746110\n",
      "  2837/50000: episode: 378, duration: 0.041s, episode steps:  12, steps per second: 291, episode reward: -9230.000, mean reward: -769.167 [-999.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 1137755.151042, mae: 3226.730245, accuracy: 0.161458, mean_q: -3816.736003, mean_eps: 0.745255\n",
      "  2848/50000: episode: 379, duration: 0.039s, episode steps:  11, steps per second: 282, episode reward: -8216.000, mean reward: -746.909 [-999.000, -58.000], mean action: 1.909 [0.000, 3.000],  loss: 936089.451705, mae: 3207.984064, accuracy: 0.096591, mean_q: -3808.629994, mean_eps: 0.744220\n",
      "  2855/50000: episode: 380, duration: 0.028s, episode steps:   7, steps per second: 248, episode reward: -4235.000, mean reward: -605.000 [-999.000, -60.000], mean action: 2.000 [0.000, 3.000],  loss: 1110814.000000, mae: 3254.425991, accuracy: 0.133929, mean_q: -3860.009173, mean_eps: 0.743410\n",
      "  2863/50000: episode: 381, duration: 0.035s, episode steps:   8, steps per second: 228, episode reward: -5190.000, mean reward: -648.750 [-999.000, -58.000], mean action: 2.250 [0.000, 3.000],  loss: 1212102.164062, mae: 3270.889069, accuracy: 0.113281, mean_q: -3880.399567, mean_eps: 0.742735\n",
      "  2870/50000: episode: 382, duration: 0.030s, episode steps:   7, steps per second: 231, episode reward: -4235.000, mean reward: -605.000 [-999.000, -45.000], mean action: 1.286 [0.000, 3.000],  loss: 1162814.245536, mae: 3237.723807, accuracy: 0.142857, mean_q: -3842.462088, mean_eps: 0.742060\n",
      "  2874/50000: episode: 383, duration: 0.019s, episode steps:   4, steps per second: 211, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 1677801.281250, mae: 3284.229126, accuracy: 0.179688, mean_q: -3820.806580, mean_eps: 0.741565\n",
      "  2879/50000: episode: 384, duration: 0.021s, episode steps:   5, steps per second: 233, episode reward: -2237.000, mean reward: -447.400 [-999.000, -60.000], mean action: 1.200 [0.000, 3.000],  loss: 1025243.162500, mae: 3169.970557, accuracy: 0.125000, mean_q: -3765.497070, mean_eps: 0.741160\n",
      "  2888/50000: episode: 385, duration: 0.035s, episode steps:   9, steps per second: 261, episode reward: -6189.000, mean reward: -687.667 [-999.000, -32.000], mean action: 1.000 [0.000, 3.000],  loss: 1165243.798611, mae: 3175.946343, accuracy: 0.142361, mean_q: -3744.339545, mean_eps: 0.740530\n",
      "  2892/50000: episode: 386, duration: 0.016s, episode steps:   4, steps per second: 247, episode reward: -1238.000, mean reward: -309.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 1098612.500000, mae: 3169.167236, accuracy: 0.117188, mean_q: -3731.864563, mean_eps: 0.739945\n",
      "  2899/50000: episode: 387, duration: 0.029s, episode steps:   7, steps per second: 239, episode reward: -4191.000, mean reward: -598.714 [-999.000, -32.000], mean action: 1.000 [0.000, 3.000],  loss: 1247611.419643, mae: 3185.130964, accuracy: 0.151786, mean_q: -3755.648542, mean_eps: 0.739450\n",
      "  2908/50000: episode: 388, duration: 0.037s, episode steps:   9, steps per second: 242, episode reward: -6189.000, mean reward: -687.667 [-999.000, -45.000], mean action: 1.556 [0.000, 3.000],  loss: 1304361.944444, mae: 3190.564453, accuracy: 0.111111, mean_q: -3738.327230, mean_eps: 0.738730\n",
      "  2915/50000: episode: 389, duration: 0.030s, episode steps:   7, steps per second: 232, episode reward: -4191.000, mean reward: -598.714 [-999.000, -45.000], mean action: 1.857 [0.000, 3.000],  loss: 940093.821429, mae: 3119.074289, accuracy: 0.160714, mean_q: -3698.046352, mean_eps: 0.738010\n",
      "  2924/50000: episode: 390, duration: 0.036s, episode steps:   9, steps per second: 252, episode reward: -6218.000, mean reward: -690.889 [-999.000, -32.000], mean action: 1.111 [0.000, 3.000],  loss: 1013514.111111, mae: 3133.437717, accuracy: 0.121528, mean_q: -3715.003933, mean_eps: 0.737290\n",
      "  2941/50000: episode: 391, duration: 0.066s, episode steps:  17, steps per second: 257, episode reward: -14225.000, mean reward: -836.765 [-999.000, -45.000], mean action: 1.706 [0.000, 3.000],  loss: 1147810.334559, mae: 3185.116986, accuracy: 0.123162, mean_q: -3753.371841, mean_eps: 0.736120\n",
      "  2947/50000: episode: 392, duration: 0.022s, episode steps:   6, steps per second: 268, episode reward: -3236.000, mean reward: -539.333 [-999.000, -60.000], mean action: 1.333 [0.000, 3.000],  loss: 1056445.822917, mae: 3182.485636, accuracy: 0.109375, mean_q: -3787.320272, mean_eps: 0.735085\n",
      "  2955/50000: episode: 393, duration: 0.027s, episode steps:   8, steps per second: 297, episode reward: -5234.000, mean reward: -654.250 [-999.000, -45.000], mean action: 1.750 [0.000, 3.000],  loss: 1360783.500000, mae: 3216.352081, accuracy: 0.160156, mean_q: -3778.058502, mean_eps: 0.734455\n",
      "  2961/50000: episode: 394, duration: 0.024s, episode steps:   6, steps per second: 255, episode reward: -3236.000, mean reward: -539.333 [-999.000, -45.000], mean action: 1.167 [0.000, 3.000],  loss: 1015289.552083, mae: 3163.702637, accuracy: 0.104167, mean_q: -3747.307454, mean_eps: 0.733825\n",
      "  2964/50000: episode: 395, duration: 0.012s, episode steps:   3, steps per second: 256, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 817640.125000, mae: 3147.120199, accuracy: 0.114583, mean_q: -3744.302327, mean_eps: 0.733420\n",
      "  2985/50000: episode: 396, duration: 0.071s, episode steps:  21, steps per second: 296, episode reward: -18177.000, mean reward: -865.571 [-999.000, -32.000], mean action: 1.143 [0.000, 3.000],  loss: 1214806.428571, mae: 3201.031145, accuracy: 0.136905, mean_q: -3778.442499, mean_eps: 0.732340\n",
      "  3005/50000: episode: 397, duration: 0.066s, episode steps:  20, steps per second: 304, episode reward: -17207.000, mean reward: -860.350 [-999.000, -32.000], mean action: 1.000 [0.000, 3.000],  loss: 1038973.560937, mae: 3147.964685, accuracy: 0.151562, mean_q: -3708.146826, mean_eps: 0.730495\n",
      "  3015/50000: episode: 398, duration: 0.034s, episode steps:  10, steps per second: 297, episode reward: -7188.000, mean reward: -718.800 [-999.000, -45.000], mean action: 1.300 [0.000, 3.000],  loss: 1245680.981250, mae: 3190.439136, accuracy: 0.125000, mean_q: -3738.372168, mean_eps: 0.729145\n",
      "  3021/50000: episode: 399, duration: 0.023s, episode steps:   6, steps per second: 265, episode reward: -3236.000, mean reward: -539.333 [-999.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 1197407.395833, mae: 3173.974772, accuracy: 0.135417, mean_q: -3740.126058, mean_eps: 0.728425\n",
      "  3024/50000: episode: 400, duration: 0.012s, episode steps:   3, steps per second: 254, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 2.000 [1.000, 3.000],  loss: 939989.395833, mae: 3164.363200, accuracy: 0.135417, mean_q: -3741.570719, mean_eps: 0.728020\n",
      "  3027/50000: episode: 401, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -224.000, mean reward: -74.667 [-116.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 946322.333333, mae: 3158.461100, accuracy: 0.145833, mean_q: -3760.852214, mean_eps: 0.727750\n",
      "  3043/50000: episode: 402, duration: 0.054s, episode steps:  16, steps per second: 298, episode reward: -13226.000, mean reward: -826.625 [-999.000, -45.000], mean action: 1.312 [0.000, 3.000],  loss: 1262107.761719, mae: 3184.799133, accuracy: 0.138672, mean_q: -3751.416199, mean_eps: 0.726895\n",
      "  3047/50000: episode: 403, duration: 0.017s, episode steps:   4, steps per second: 231, episode reward: -1223.000, mean reward: -305.750 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 1041558.328125, mae: 3132.602112, accuracy: 0.179688, mean_q: -3704.404907, mean_eps: 0.725995\n",
      "  3052/50000: episode: 404, duration: 0.019s, episode steps:   5, steps per second: 264, episode reward: -2237.000, mean reward: -447.400 [-999.000, -45.000], mean action: 1.600 [0.000, 3.000],  loss: 770166.962500, mae: 3097.090381, accuracy: 0.112500, mean_q: -3696.349414, mean_eps: 0.725590\n",
      "  3055/50000: episode: 405, duration: 0.014s, episode steps:   3, steps per second: 217, episode reward: -224.000, mean reward: -74.667 [-116.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 1077146.166667, mae: 3147.807129, accuracy: 0.052083, mean_q: -3732.014404, mean_eps: 0.725230\n",
      "  3058/50000: episode: 406, duration: 0.013s, episode steps:   3, steps per second: 225, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 1263777.666667, mae: 3173.011393, accuracy: 0.052083, mean_q: -3716.657715, mean_eps: 0.724960\n",
      "  3068/50000: episode: 407, duration: 0.036s, episode steps:  10, steps per second: 280, episode reward: -7217.000, mean reward: -721.700 [-999.000, -58.000], mean action: 1.900 [0.000, 3.000],  loss: 1224175.875000, mae: 3161.058984, accuracy: 0.096875, mean_q: -3699.086035, mean_eps: 0.724375\n",
      "  3073/50000: episode: 408, duration: 0.020s, episode steps:   5, steps per second: 254, episode reward: -2237.000, mean reward: -447.400 [-999.000, -58.000], mean action: 1.200 [0.000, 3.000],  loss: 1146181.575000, mae: 3141.635254, accuracy: 0.037500, mean_q: -3688.786914, mean_eps: 0.723700\n",
      "  3090/50000: episode: 409, duration: 0.055s, episode steps:  17, steps per second: 307, episode reward: -14210.000, mean reward: -835.882 [-999.000, -32.000], mean action: 1.353 [0.000, 3.000],  loss: 1247370.301471, mae: 3146.669261, accuracy: 0.148897, mean_q: -3652.261274, mean_eps: 0.722710\n",
      "  3094/50000: episode: 410, duration: 0.015s, episode steps:   4, steps per second: 270, episode reward: -1223.000, mean reward: -305.750 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 862844.234375, mae: 3085.661377, accuracy: 0.210938, mean_q: -3639.014648, mean_eps: 0.721765\n",
      "  3099/50000: episode: 411, duration: 0.021s, episode steps:   5, steps per second: 243, episode reward: -2237.000, mean reward: -447.400 [-999.000, -60.000], mean action: 1.800 [0.000, 3.000],  loss: 942994.000000, mae: 3106.297363, accuracy: 0.168750, mean_q: -3693.230029, mean_eps: 0.721360\n",
      "  3103/50000: episode: 412, duration: 0.019s, episode steps:   4, steps per second: 211, episode reward: -1238.000, mean reward: -309.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 1090272.375000, mae: 3129.152283, accuracy: 0.210938, mean_q: -3700.535767, mean_eps: 0.720955\n",
      "  3109/50000: episode: 413, duration: 0.024s, episode steps:   6, steps per second: 249, episode reward: -3221.000, mean reward: -536.833 [-999.000, -32.000], mean action: 2.000 [0.000, 3.000],  loss: 1261724.822917, mae: 3152.113322, accuracy: 0.203125, mean_q: -3706.037109, mean_eps: 0.720505\n",
      "  3120/50000: episode: 414, duration: 0.043s, episode steps:  11, steps per second: 254, episode reward: -8216.000, mean reward: -746.909 [-999.000, -58.000], mean action: 1.909 [0.000, 3.000],  loss: 984426.465909, mae: 3130.906894, accuracy: 0.130682, mean_q: -3700.327903, mean_eps: 0.719740\n",
      "  3126/50000: episode: 415, duration: 0.023s, episode steps:   6, steps per second: 264, episode reward: -3192.000, mean reward: -532.000 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 990925.802083, mae: 3144.913167, accuracy: 0.119792, mean_q: -3715.208903, mean_eps: 0.718975\n",
      "  3130/50000: episode: 416, duration: 0.016s, episode steps:   4, steps per second: 246, episode reward: -1223.000, mean reward: -305.750 [-999.000, -58.000], mean action: 1.750 [0.000, 3.000],  loss: 1007439.382812, mae: 3156.842041, accuracy: 0.125000, mean_q: -3728.815613, mean_eps: 0.718525\n",
      "  3141/50000: episode: 417, duration: 0.039s, episode steps:  11, steps per second: 283, episode reward: -8187.000, mean reward: -744.273 [-999.000, -58.000], mean action: 2.273 [0.000, 3.000],  loss: 1179403.738636, mae: 3178.834672, accuracy: 0.215909, mean_q: -3743.526567, mean_eps: 0.717850\n",
      "  3151/50000: episode: 418, duration: 0.035s, episode steps:  10, steps per second: 288, episode reward: -7232.000, mean reward: -723.200 [-999.000, -58.000], mean action: 1.600 [0.000, 3.000],  loss: 1115733.706250, mae: 3151.834766, accuracy: 0.278125, mean_q: -3708.977710, mean_eps: 0.716905\n",
      "  3155/50000: episode: 419, duration: 0.017s, episode steps:   4, steps per second: 240, episode reward: -1238.000, mean reward: -309.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 1034837.687500, mae: 3127.057739, accuracy: 0.382812, mean_q: -3708.963806, mean_eps: 0.716275\n",
      "  3159/50000: episode: 420, duration: 0.016s, episode steps:   4, steps per second: 247, episode reward: -1238.000, mean reward: -309.500 [-999.000, -60.000], mean action: 1.250 [0.000, 3.000],  loss: 1182132.531250, mae: 3154.751343, accuracy: 0.367188, mean_q: -3727.174377, mean_eps: 0.715915\n",
      "  3163/50000: episode: 421, duration: 0.017s, episode steps:   4, steps per second: 235, episode reward: -1238.000, mean reward: -309.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 1110216.234375, mae: 3140.087830, accuracy: 0.382812, mean_q: -3722.084045, mean_eps: 0.715555\n",
      "  3168/50000: episode: 422, duration: 0.019s, episode steps:   5, steps per second: 262, episode reward: -2237.000, mean reward: -447.400 [-999.000, -60.000], mean action: 1.800 [0.000, 3.000],  loss: 1218600.050000, mae: 3158.334180, accuracy: 0.356250, mean_q: -3712.417090, mean_eps: 0.715150\n",
      "  3186/50000: episode: 423, duration: 0.064s, episode steps:  18, steps per second: 281, episode reward: -15224.000, mean reward: -845.778 [-999.000, -58.000], mean action: 1.389 [0.000, 3.000],  loss: 1123435.555556, mae: 3139.885200, accuracy: 0.368056, mean_q: -3707.097968, mean_eps: 0.714115\n",
      "  3189/50000: episode: 424, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 989800.729167, mae: 3111.971436, accuracy: 0.364583, mean_q: -3687.203776, mean_eps: 0.713170\n",
      "  3198/50000: episode: 425, duration: 0.031s, episode steps:   9, steps per second: 289, episode reward: -6233.000, mean reward: -692.556 [-999.000, -60.000], mean action: 1.889 [0.000, 3.000],  loss: 1033629.444444, mae: 3139.787543, accuracy: 0.385417, mean_q: -3697.553602, mean_eps: 0.712630\n",
      "  3202/50000: episode: 426, duration: 0.016s, episode steps:   4, steps per second: 250, episode reward: -1238.000, mean reward: -309.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 1270150.437500, mae: 3171.589783, accuracy: 0.398438, mean_q: -3725.522156, mean_eps: 0.712045\n",
      "  3207/50000: episode: 427, duration: 0.021s, episode steps:   5, steps per second: 240, episode reward: -2237.000, mean reward: -447.400 [-999.000, -45.000], mean action: 1.600 [0.000, 3.000],  loss: 1190299.050000, mae: 3173.749902, accuracy: 0.337500, mean_q: -3732.657080, mean_eps: 0.711640\n",
      "  3212/50000: episode: 428, duration: 0.022s, episode steps:   5, steps per second: 229, episode reward: -2237.000, mean reward: -447.400 [-999.000, -45.000], mean action: 1.400 [0.000, 3.000],  loss: 1262704.437500, mae: 3170.672803, accuracy: 0.318750, mean_q: -3725.952979, mean_eps: 0.711190\n",
      "  3221/50000: episode: 429, duration: 0.036s, episode steps:   9, steps per second: 247, episode reward: -6189.000, mean reward: -687.667 [-999.000, -32.000], mean action: 1.000 [0.000, 3.000],  loss: 1148431.402778, mae: 3132.358860, accuracy: 0.218750, mean_q: -3710.074816, mean_eps: 0.710560\n",
      "  3227/50000: episode: 430, duration: 0.028s, episode steps:   6, steps per second: 212, episode reward: -3236.000, mean reward: -539.333 [-999.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 1239679.812500, mae: 3117.347249, accuracy: 0.171875, mean_q: -3673.089518, mean_eps: 0.709885\n",
      "  3231/50000: episode: 431, duration: 0.016s, episode steps:   4, steps per second: 246, episode reward: -1238.000, mean reward: -309.500 [-999.000, -45.000], mean action: 1.250 [0.000, 3.000],  loss: 1109154.828125, mae: 3108.703857, accuracy: 0.195312, mean_q: -3648.156555, mean_eps: 0.709435\n",
      "  3238/50000: episode: 432, duration: 0.028s, episode steps:   7, steps per second: 251, episode reward: -4191.000, mean reward: -598.714 [-999.000, -45.000], mean action: 1.286 [0.000, 2.000],  loss: 802006.616071, mae: 3074.471087, accuracy: 0.151786, mean_q: -3661.596715, mean_eps: 0.708940\n",
      "  3254/50000: episode: 433, duration: 0.060s, episode steps:  16, steps per second: 268, episode reward: -13226.000, mean reward: -826.625 [-999.000, -45.000], mean action: 1.375 [0.000, 3.000],  loss: 1108840.902344, mae: 3167.182129, accuracy: 0.199219, mean_q: -3748.672089, mean_eps: 0.707905\n",
      "  3258/50000: episode: 434, duration: 0.016s, episode steps:   4, steps per second: 245, episode reward: -1238.000, mean reward: -309.500 [-999.000, -45.000], mean action: 1.250 [0.000, 3.000],  loss: 1449820.937500, mae: 3197.810181, accuracy: 0.179688, mean_q: -3751.445618, mean_eps: 0.707005\n",
      "  3271/50000: episode: 435, duration: 0.044s, episode steps:  13, steps per second: 296, episode reward: -10229.000, mean reward: -786.846 [-999.000, -60.000], mean action: 1.615 [0.000, 3.000],  loss: 1078934.706731, mae: 3137.792086, accuracy: 0.300481, mean_q: -3707.740798, mean_eps: 0.706240\n",
      "  3274/50000: episode: 436, duration: 0.015s, episode steps:   3, steps per second: 204, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 838437.531250, mae: 3091.256999, accuracy: 0.218750, mean_q: -3684.373698, mean_eps: 0.705520\n",
      "  3283/50000: episode: 437, duration: 0.043s, episode steps:   9, steps per second: 210, episode reward: -6233.000, mean reward: -692.556 [-999.000, -45.000], mean action: 1.889 [0.000, 3.000],  loss: 958617.416667, mae: 3108.012560, accuracy: 0.156250, mean_q: -3682.480035, mean_eps: 0.704980\n",
      "  3291/50000: episode: 438, duration: 0.029s, episode steps:   8, steps per second: 279, episode reward: -5234.000, mean reward: -654.250 [-999.000, -60.000], mean action: 2.125 [0.000, 3.000],  loss: 1080872.804688, mae: 3149.929077, accuracy: 0.113281, mean_q: -3715.650482, mean_eps: 0.704215\n",
      "  3295/50000: episode: 439, duration: 0.016s, episode steps:   4, steps per second: 251, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 891804.937500, mae: 3119.927429, accuracy: 0.070312, mean_q: -3712.255676, mean_eps: 0.703675\n",
      "  3309/50000: episode: 440, duration: 0.048s, episode steps:  14, steps per second: 294, episode reward: -11228.000, mean reward: -802.000 [-999.000, -60.000], mean action: 1.643 [0.000, 3.000],  loss: 1221555.799107, mae: 3138.325439, accuracy: 0.125000, mean_q: -3666.941092, mean_eps: 0.702865\n",
      "  3328/50000: episode: 441, duration: 0.064s, episode steps:  19, steps per second: 298, episode reward: -16223.000, mean reward: -853.842 [-999.000, -45.000], mean action: 1.789 [0.000, 3.000],  loss: 1023158.049342, mae: 3090.849134, accuracy: 0.143092, mean_q: -3621.130114, mean_eps: 0.701380\n",
      "  3334/50000: episode: 442, duration: 0.022s, episode steps:   6, steps per second: 276, episode reward: -3192.000, mean reward: -532.000 [-999.000, -58.000], mean action: 1.000 [0.000, 3.000],  loss: 928884.953125, mae: 3106.453654, accuracy: 0.119792, mean_q: -3656.985718, mean_eps: 0.700255\n",
      "  3341/50000: episode: 443, duration: 0.026s, episode steps:   7, steps per second: 270, episode reward: -4235.000, mean reward: -605.000 [-999.000, -45.000], mean action: 1.571 [0.000, 3.000],  loss: 1132394.491071, mae: 3150.529750, accuracy: 0.138393, mean_q: -3709.106445, mean_eps: 0.699670\n",
      "  3351/50000: episode: 444, duration: 0.033s, episode steps:  10, steps per second: 307, episode reward: -7217.000, mean reward: -721.700 [-999.000, -58.000], mean action: 1.900 [0.000, 3.000],  loss: 940663.231250, mae: 3126.647949, accuracy: 0.284375, mean_q: -3723.911646, mean_eps: 0.698905\n",
      "  3356/50000: episode: 445, duration: 0.019s, episode steps:   5, steps per second: 267, episode reward: -2193.000, mean reward: -438.600 [-999.000, -32.000], mean action: 1.800 [0.000, 3.000],  loss: 1258139.437500, mae: 3177.338965, accuracy: 0.231250, mean_q: -3724.950293, mean_eps: 0.698230\n",
      "  3365/50000: episode: 446, duration: 0.031s, episode steps:   9, steps per second: 286, episode reward: -6233.000, mean reward: -692.556 [-999.000, -60.000], mean action: 2.111 [0.000, 3.000],  loss: 1266004.513889, mae: 3150.536458, accuracy: 0.197917, mean_q: -3706.509684, mean_eps: 0.697600\n",
      "  3372/50000: episode: 447, duration: 0.024s, episode steps:   7, steps per second: 286, episode reward: -4235.000, mean reward: -605.000 [-999.000, -45.000], mean action: 1.714 [0.000, 3.000],  loss: 1100217.625000, mae: 3105.597203, accuracy: 0.138393, mean_q: -3654.252860, mean_eps: 0.696880\n",
      "  3376/50000: episode: 448, duration: 0.017s, episode steps:   4, steps per second: 236, episode reward: -1238.000, mean reward: -309.500 [-999.000, -60.000], mean action: 1.500 [0.000, 3.000],  loss: 891292.390625, mae: 3072.424500, accuracy: 0.140625, mean_q: -3633.606934, mean_eps: 0.696385\n",
      "  3381/50000: episode: 449, duration: 0.019s, episode steps:   5, steps per second: 269, episode reward: -2237.000, mean reward: -447.400 [-999.000, -60.000], mean action: 1.600 [0.000, 3.000],  loss: 922722.212500, mae: 3085.313428, accuracy: 0.068750, mean_q: -3643.313867, mean_eps: 0.695980\n",
      "  3392/50000: episode: 450, duration: 0.039s, episode steps:  11, steps per second: 279, episode reward: -8187.000, mean reward: -744.273 [-999.000, -32.000], mean action: 1.364 [0.000, 3.000],  loss: 968375.738636, mae: 3108.931752, accuracy: 0.085227, mean_q: -3689.524836, mean_eps: 0.695260\n",
      "  3397/50000: episode: 451, duration: 0.019s, episode steps:   5, steps per second: 260, episode reward: -2237.000, mean reward: -447.400 [-999.000, -60.000], mean action: 1.200 [0.000, 3.000],  loss: 1066745.125000, mae: 3152.557568, accuracy: 0.062500, mean_q: -3727.792334, mean_eps: 0.694540\n",
      "  3405/50000: episode: 452, duration: 0.028s, episode steps:   8, steps per second: 285, episode reward: -5234.000, mean reward: -654.250 [-999.000, -45.000], mean action: 1.250 [0.000, 3.000],  loss: 1268013.226562, mae: 3173.547821, accuracy: 0.089844, mean_q: -3724.231140, mean_eps: 0.693955\n",
      "  3408/50000: episode: 453, duration: 0.011s, episode steps:   3, steps per second: 264, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 752152.625000, mae: 3084.108561, accuracy: 0.093750, mean_q: -3696.316406, mean_eps: 0.693460\n",
      "  3422/50000: episode: 454, duration: 0.049s, episode steps:  14, steps per second: 287, episode reward: -11184.000, mean reward: -798.857 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 981682.883929, mae: 3099.053066, accuracy: 0.198661, mean_q: -3662.957694, mean_eps: 0.692695\n",
      "  3433/50000: episode: 455, duration: 0.039s, episode steps:  11, steps per second: 284, episode reward: -8187.000, mean reward: -744.273 [-999.000, -58.000], mean action: 2.091 [0.000, 3.000],  loss: 937019.136364, mae: 3086.014116, accuracy: 0.221591, mean_q: -3658.743031, mean_eps: 0.691570\n",
      "  3439/50000: episode: 456, duration: 0.022s, episode steps:   6, steps per second: 277, episode reward: -3192.000, mean reward: -532.000 [-999.000, -58.000], mean action: 1.000 [0.000, 3.000],  loss: 1048975.729167, mae: 3120.794108, accuracy: 0.255208, mean_q: -3680.935181, mean_eps: 0.690805\n",
      "  3467/50000: episode: 457, duration: 0.096s, episode steps:  28, steps per second: 291, episode reward: -25170.000, mean reward: -898.929 [-999.000, -32.000], mean action: 1.714 [0.000, 3.000],  loss: 1144816.475446, mae: 3137.747114, accuracy: 0.362723, mean_q: -3678.524275, mean_eps: 0.689275\n",
      "  3471/50000: episode: 458, duration: 0.017s, episode steps:   4, steps per second: 234, episode reward: -1238.000, mean reward: -309.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 1512322.312500, mae: 3155.635803, accuracy: 0.414062, mean_q: -3594.248535, mean_eps: 0.687835\n",
      "  3478/50000: episode: 459, duration: 0.024s, episode steps:   7, steps per second: 295, episode reward: -4235.000, mean reward: -605.000 [-999.000, -45.000], mean action: 1.571 [0.000, 3.000],  loss: 1139049.875000, mae: 3103.115165, accuracy: 0.334821, mean_q: -3575.742292, mean_eps: 0.687340\n",
      "  3489/50000: episode: 460, duration: 0.041s, episode steps:  11, steps per second: 267, episode reward: -8187.000, mean reward: -744.273 [-999.000, -58.000], mean action: 1.727 [0.000, 3.000],  loss: 1168393.528409, mae: 3085.841642, accuracy: 0.241477, mean_q: -3576.004306, mean_eps: 0.686530\n",
      "  3493/50000: episode: 461, duration: 0.016s, episode steps:   4, steps per second: 255, episode reward: -1238.000, mean reward: -309.500 [-999.000, -60.000], mean action: 1.500 [0.000, 3.000],  loss: 734543.796875, mae: 3031.891357, accuracy: 0.289062, mean_q: -3573.114136, mean_eps: 0.685855\n",
      "  3532/50000: episode: 462, duration: 0.130s, episode steps:  39, steps per second: 300, episode reward: -36188.000, mean reward: -927.897 [-999.000, -58.000], mean action: 0.974 [0.000, 3.000],  loss: 1040203.176282, mae: 3083.994904, accuracy: 0.375000, mean_q: -3609.594101, mean_eps: 0.683920\n",
      "  3539/50000: episode: 463, duration: 0.026s, episode steps:   7, steps per second: 271, episode reward: -4235.000, mean reward: -605.000 [-999.000, -58.000], mean action: 1.000 [0.000, 3.000],  loss: 820710.964286, mae: 3069.848110, accuracy: 0.352679, mean_q: -3614.126535, mean_eps: 0.681850\n",
      "  3547/50000: episode: 464, duration: 0.031s, episode steps:   8, steps per second: 260, episode reward: -5234.000, mean reward: -654.250 [-999.000, -58.000], mean action: 1.125 [0.000, 3.000],  loss: 891128.695312, mae: 3105.427887, accuracy: 0.359375, mean_q: -3650.103241, mean_eps: 0.681175\n",
      "  3555/50000: episode: 465, duration: 0.030s, episode steps:   8, steps per second: 271, episode reward: -5190.000, mean reward: -648.750 [-999.000, -58.000], mean action: 2.375 [0.000, 3.000],  loss: 1291701.058594, mae: 3166.442505, accuracy: 0.246094, mean_q: -3673.222565, mean_eps: 0.680455\n",
      "  3558/50000: episode: 466, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.667 [0.000, 3.000],  loss: 1256159.729167, mae: 3141.357829, accuracy: 0.166667, mean_q: -3652.894613, mean_eps: 0.679960\n",
      "  3581/50000: episode: 467, duration: 0.079s, episode steps:  23, steps per second: 292, episode reward: -20219.000, mean reward: -879.087 [-999.000, -60.000], mean action: 2.130 [0.000, 3.000],  loss: 1022611.282609, mae: 3079.805791, accuracy: 0.230978, mean_q: -3603.967200, mean_eps: 0.678790\n",
      "  3595/50000: episode: 468, duration: 0.051s, episode steps:  14, steps per second: 275, episode reward: -11184.000, mean reward: -798.857 [-999.000, -58.000], mean action: 1.357 [0.000, 3.000],  loss: 923790.229911, mae: 3062.163871, accuracy: 0.294643, mean_q: -3590.378749, mean_eps: 0.677125\n",
      "  3602/50000: episode: 469, duration: 0.027s, episode steps:   7, steps per second: 255, episode reward: -4191.000, mean reward: -598.714 [-999.000, -58.000], mean action: 1.286 [0.000, 3.000],  loss: 1196399.875000, mae: 3098.963902, accuracy: 0.397321, mean_q: -3614.615827, mean_eps: 0.676180\n",
      "  3606/50000: episode: 470, duration: 0.017s, episode steps:   4, steps per second: 237, episode reward: -1223.000, mean reward: -305.750 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 995637.218750, mae: 3080.125610, accuracy: 0.382812, mean_q: -3590.726013, mean_eps: 0.675685\n",
      "  3613/50000: episode: 471, duration: 0.026s, episode steps:   7, steps per second: 270, episode reward: -4191.000, mean reward: -598.714 [-999.000, -58.000], mean action: 1.571 [0.000, 3.000],  loss: 1067489.330357, mae: 3094.596401, accuracy: 0.370536, mean_q: -3623.490792, mean_eps: 0.675190\n",
      "  3619/50000: episode: 472, duration: 0.022s, episode steps:   6, steps per second: 273, episode reward: -3221.000, mean reward: -536.833 [-999.000, -58.000], mean action: 1.000 [0.000, 3.000],  loss: 987182.708333, mae: 3090.770508, accuracy: 0.375000, mean_q: -3618.238525, mean_eps: 0.674605\n",
      "  3625/50000: episode: 473, duration: 0.023s, episode steps:   6, steps per second: 263, episode reward: -3236.000, mean reward: -539.333 [-999.000, -58.000], mean action: 2.000 [0.000, 3.000],  loss: 961696.458333, mae: 3081.592244, accuracy: 0.411458, mean_q: -3644.673421, mean_eps: 0.674065\n",
      "  3632/50000: episode: 474, duration: 0.027s, episode steps:   7, steps per second: 260, episode reward: -4235.000, mean reward: -605.000 [-999.000, -45.000], mean action: 1.429 [0.000, 3.000],  loss: 1011252.473214, mae: 3103.307199, accuracy: 0.218750, mean_q: -3651.449044, mean_eps: 0.673480\n",
      "  3649/50000: episode: 475, duration: 0.056s, episode steps:  17, steps per second: 306, episode reward: -14225.000, mean reward: -836.765 [-999.000, -58.000], mean action: 1.471 [0.000, 3.000],  loss: 1190139.617647, mae: 3096.471981, accuracy: 0.354779, mean_q: -3608.823314, mean_eps: 0.672400\n",
      "  3653/50000: episode: 476, duration: 0.016s, episode steps:   4, steps per second: 244, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 761137.773438, mae: 3021.630737, accuracy: 0.382812, mean_q: -3564.393799, mean_eps: 0.671455\n",
      "  3659/50000: episode: 477, duration: 0.023s, episode steps:   6, steps per second: 265, episode reward: -3221.000, mean reward: -536.833 [-999.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 1162547.010417, mae: 3081.503621, accuracy: 0.328125, mean_q: -3575.476400, mean_eps: 0.671005\n",
      "  3671/50000: episode: 478, duration: 0.041s, episode steps:  12, steps per second: 293, episode reward: -9186.000, mean reward: -765.500 [-999.000, -58.000], mean action: 0.750 [0.000, 3.000],  loss: 875108.598958, mae: 3037.163371, accuracy: 0.398438, mean_q: -3586.131856, mean_eps: 0.670195\n",
      "  3677/50000: episode: 479, duration: 0.022s, episode steps:   6, steps per second: 271, episode reward: -3236.000, mean reward: -539.333 [-999.000, -60.000], mean action: 1.500 [0.000, 3.000],  loss: 1003533.791667, mae: 3087.566406, accuracy: 0.307292, mean_q: -3625.678752, mean_eps: 0.669385\n",
      "  3680/50000: episode: 480, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 790646.666667, mae: 3060.763672, accuracy: 0.291667, mean_q: -3639.788818, mean_eps: 0.668980\n",
      "  3690/50000: episode: 481, duration: 0.034s, episode steps:  10, steps per second: 290, episode reward: -7188.000, mean reward: -718.800 [-999.000, -32.000], mean action: 0.800 [0.000, 3.000],  loss: 829004.240625, mae: 3089.267505, accuracy: 0.350000, mean_q: -3663.585474, mean_eps: 0.668395\n",
      "  3694/50000: episode: 482, duration: 0.017s, episode steps:   4, steps per second: 231, episode reward: -1238.000, mean reward: -309.500 [-999.000, -60.000], mean action: 2.250 [1.000, 3.000],  loss: 1055077.500000, mae: 3136.605652, accuracy: 0.187500, mean_q: -3692.257629, mean_eps: 0.667765\n",
      "  3700/50000: episode: 483, duration: 0.024s, episode steps:   6, steps per second: 249, episode reward: -3236.000, mean reward: -539.333 [-999.000, -60.000], mean action: 1.333 [0.000, 3.000],  loss: 1214537.333333, mae: 3160.290771, accuracy: 0.145833, mean_q: -3691.041219, mean_eps: 0.667315\n",
      "  3705/50000: episode: 484, duration: 0.019s, episode steps:   5, steps per second: 259, episode reward: -2222.000, mean reward: -444.400 [-999.000, -32.000], mean action: 1.400 [0.000, 3.000],  loss: 1235668.587500, mae: 3140.574072, accuracy: 0.175000, mean_q: -3641.509424, mean_eps: 0.666820\n",
      "  3715/50000: episode: 485, duration: 0.035s, episode steps:  10, steps per second: 287, episode reward: -7217.000, mean reward: -721.700 [-999.000, -32.000], mean action: 1.700 [0.000, 3.000],  loss: 906614.406250, mae: 3073.713135, accuracy: 0.203125, mean_q: -3600.192383, mean_eps: 0.666145\n",
      "  3734/50000: episode: 486, duration: 0.068s, episode steps:  19, steps per second: 281, episode reward: -16208.000, mean reward: -853.053 [-999.000, -32.000], mean action: 1.474 [0.000, 3.000],  loss: 1051889.496711, mae: 3082.714523, accuracy: 0.156250, mean_q: -3586.304996, mean_eps: 0.664840\n",
      "  3738/50000: episode: 487, duration: 0.015s, episode steps:   4, steps per second: 272, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.000 [0.000, 3.000],  loss: 864258.984375, mae: 3075.902771, accuracy: 0.164062, mean_q: -3571.921753, mean_eps: 0.663805\n",
      "  3744/50000: episode: 488, duration: 0.022s, episode steps:   6, steps per second: 275, episode reward: -3236.000, mean reward: -539.333 [-999.000, -45.000], mean action: 2.000 [0.000, 3.000],  loss: 1207479.562500, mae: 3105.582967, accuracy: 0.145833, mean_q: -3575.632446, mean_eps: 0.663355\n",
      "  3749/50000: episode: 489, duration: 0.021s, episode steps:   5, steps per second: 237, episode reward: -2193.000, mean reward: -438.600 [-999.000, -58.000], mean action: 1.800 [0.000, 3.000],  loss: 862144.578125, mae: 3053.533740, accuracy: 0.156250, mean_q: -3560.987744, mean_eps: 0.662860\n",
      "  3753/50000: episode: 490, duration: 0.016s, episode steps:   4, steps per second: 244, episode reward: -1238.000, mean reward: -309.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 1014621.078125, mae: 3090.176453, accuracy: 0.156250, mean_q: -3573.113953, mean_eps: 0.662455\n",
      "  3768/50000: episode: 491, duration: 0.050s, episode steps:  15, steps per second: 299, episode reward: -12183.000, mean reward: -812.200 [-999.000, -32.000], mean action: 1.133 [0.000, 3.000],  loss: 1078207.633333, mae: 3104.345752, accuracy: 0.187500, mean_q: -3597.301237, mean_eps: 0.661600\n",
      "  3776/50000: episode: 492, duration: 0.030s, episode steps:   8, steps per second: 268, episode reward: -5219.000, mean reward: -652.375 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 1028051.312500, mae: 3102.330475, accuracy: 0.148438, mean_q: -3613.055756, mean_eps: 0.660565\n",
      "  3802/50000: episode: 493, duration: 0.088s, episode steps:  26, steps per second: 297, episode reward: -23201.000, mean reward: -892.346 [-999.000, -32.000], mean action: 1.423 [0.000, 3.000],  loss: 894259.661058, mae: 3088.540067, accuracy: 0.162260, mean_q: -3639.578970, mean_eps: 0.659035\n",
      "  3809/50000: episode: 494, duration: 0.025s, episode steps:   7, steps per second: 285, episode reward: -4220.000, mean reward: -602.857 [-999.000, -32.000], mean action: 2.286 [1.000, 3.000],  loss: 897383.830357, mae: 3132.553432, accuracy: 0.120536, mean_q: -3720.497768, mean_eps: 0.657550\n",
      "  3816/50000: episode: 495, duration: 0.026s, episode steps:   7, steps per second: 266, episode reward: -4191.000, mean reward: -598.714 [-999.000, -32.000], mean action: 1.286 [0.000, 3.000],  loss: 806838.133929, mae: 3131.880964, accuracy: 0.160714, mean_q: -3745.630197, mean_eps: 0.656920\n",
      "  3822/50000: episode: 496, duration: 0.021s, episode steps:   6, steps per second: 292, episode reward: -3236.000, mean reward: -539.333 [-999.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 1019031.695312, mae: 3176.635254, accuracy: 0.265625, mean_q: -3744.881104, mean_eps: 0.656335\n",
      "  3827/50000: episode: 497, duration: 0.020s, episode steps:   5, steps per second: 249, episode reward: -2237.000, mean reward: -447.400 [-999.000, -60.000], mean action: 1.400 [0.000, 3.000],  loss: 1167969.000000, mae: 3178.090186, accuracy: 0.162500, mean_q: -3743.479541, mean_eps: 0.655840\n",
      "  3836/50000: episode: 498, duration: 0.031s, episode steps:   9, steps per second: 289, episode reward: -6233.000, mean reward: -692.556 [-999.000, -60.000], mean action: 2.333 [0.000, 3.000],  loss: 1052956.256944, mae: 3151.912218, accuracy: 0.173611, mean_q: -3706.126546, mean_eps: 0.655210\n",
      "  3848/50000: episode: 499, duration: 0.046s, episode steps:  12, steps per second: 263, episode reward: -9215.000, mean reward: -767.917 [-999.000, -58.000], mean action: 2.250 [0.000, 3.000],  loss: 1068250.177083, mae: 3114.454346, accuracy: 0.125000, mean_q: -3660.127869, mean_eps: 0.654265\n",
      "  3867/50000: episode: 500, duration: 0.064s, episode steps:  19, steps per second: 299, episode reward: -16208.000, mean reward: -853.053 [-999.000, -58.000], mean action: 1.895 [0.000, 3.000],  loss: 1010973.612664, mae: 3080.820647, accuracy: 0.143092, mean_q: -3613.242457, mean_eps: 0.652870\n",
      "  3871/50000: episode: 501, duration: 0.017s, episode steps:   4, steps per second: 235, episode reward: -1223.000, mean reward: -305.750 [-999.000, -58.000], mean action: 2.250 [1.000, 3.000],  loss: 920581.437500, mae: 3097.461365, accuracy: 0.250000, mean_q: -3639.937378, mean_eps: 0.651835\n",
      "  3883/50000: episode: 502, duration: 0.042s, episode steps:  12, steps per second: 284, episode reward: -9186.000, mean reward: -765.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 944527.789062, mae: 3116.662496, accuracy: 0.229167, mean_q: -3678.897196, mean_eps: 0.651115\n",
      "  3887/50000: episode: 503, duration: 0.015s, episode steps:   4, steps per second: 261, episode reward: -1238.000, mean reward: -309.500 [-999.000, -60.000], mean action: 2.250 [1.000, 3.000],  loss: 1120733.781250, mae: 3151.433044, accuracy: 0.257812, mean_q: -3690.864075, mean_eps: 0.650395\n",
      "  3891/50000: episode: 504, duration: 0.014s, episode steps:   4, steps per second: 279, episode reward: -1223.000, mean reward: -305.750 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 1225065.578125, mae: 3166.457886, accuracy: 0.210938, mean_q: -3691.000977, mean_eps: 0.650035\n",
      "  3909/50000: episode: 505, duration: 0.064s, episode steps:  18, steps per second: 280, episode reward: -15180.000, mean reward: -843.333 [-999.000, -58.000], mean action: 2.056 [0.000, 3.000],  loss: 1139490.871528, mae: 3141.218560, accuracy: 0.140625, mean_q: -3661.646606, mean_eps: 0.649045\n",
      "  3918/50000: episode: 506, duration: 0.034s, episode steps:   9, steps per second: 265, episode reward: -6189.000, mean reward: -687.667 [-999.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 1144406.409722, mae: 3132.955702, accuracy: 0.125000, mean_q: -3636.245090, mean_eps: 0.647830\n",
      "  3922/50000: episode: 507, duration: 0.017s, episode steps:   4, steps per second: 239, episode reward: -1238.000, mean reward: -309.500 [-999.000, -58.000], mean action: 1.250 [0.000, 2.000],  loss: 946944.109375, mae: 3114.045044, accuracy: 0.109375, mean_q: -3628.902222, mean_eps: 0.647245\n",
      "  3927/50000: episode: 508, duration: 0.021s, episode steps:   5, steps per second: 237, episode reward: -2193.000, mean reward: -438.600 [-999.000, -45.000], mean action: 1.600 [0.000, 3.000],  loss: 1061706.150000, mae: 3128.362500, accuracy: 0.106250, mean_q: -3630.899023, mean_eps: 0.646840\n",
      "  3937/50000: episode: 509, duration: 0.035s, episode steps:  10, steps per second: 282, episode reward: -7232.000, mean reward: -723.200 [-999.000, -60.000], mean action: 1.600 [0.000, 3.000],  loss: 1040822.331250, mae: 3118.656030, accuracy: 0.093750, mean_q: -3628.034033, mean_eps: 0.646165\n",
      "  3943/50000: episode: 510, duration: 0.022s, episode steps:   6, steps per second: 267, episode reward: -3236.000, mean reward: -539.333 [-999.000, -60.000], mean action: 2.167 [1.000, 3.000],  loss: 1071434.010417, mae: 3126.132609, accuracy: 0.119792, mean_q: -3632.382528, mean_eps: 0.645445\n",
      "  3948/50000: episode: 511, duration: 0.018s, episode steps:   5, steps per second: 278, episode reward: -2222.000, mean reward: -444.400 [-999.000, -58.000], mean action: 1.200 [0.000, 3.000],  loss: 787635.612500, mae: 3102.982178, accuracy: 0.181250, mean_q: -3652.845459, mean_eps: 0.644950\n",
      "  3962/50000: episode: 512, duration: 0.049s, episode steps:  14, steps per second: 287, episode reward: -11228.000, mean reward: -802.000 [-999.000, -45.000], mean action: 2.143 [0.000, 3.000],  loss: 1086144.064732, mae: 3150.601493, accuracy: 0.167411, mean_q: -3692.733102, mean_eps: 0.644095\n",
      "  3972/50000: episode: 513, duration: 0.042s, episode steps:  10, steps per second: 239, episode reward: -7232.000, mean reward: -723.200 [-999.000, -60.000], mean action: 1.100 [0.000, 3.000],  loss: 1034468.743750, mae: 3143.744458, accuracy: 0.221875, mean_q: -3698.023926, mean_eps: 0.643015\n",
      "  3979/50000: episode: 514, duration: 0.029s, episode steps:   7, steps per second: 246, episode reward: -4191.000, mean reward: -598.714 [-999.000, -32.000], mean action: 1.000 [0.000, 3.000],  loss: 887690.598214, mae: 3118.735003, accuracy: 0.250000, mean_q: -3693.227469, mean_eps: 0.642250\n",
      "  3985/50000: episode: 515, duration: 0.024s, episode steps:   6, steps per second: 248, episode reward: -3236.000, mean reward: -539.333 [-999.000, -60.000], mean action: 2.000 [0.000, 3.000],  loss: 1245702.468750, mae: 3162.400391, accuracy: 0.192708, mean_q: -3685.882772, mean_eps: 0.641665\n",
      "  3988/50000: episode: 516, duration: 0.013s, episode steps:   3, steps per second: 223, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 1298957.208333, mae: 3139.731364, accuracy: 0.166667, mean_q: -3651.652832, mean_eps: 0.641260\n",
      "  3992/50000: episode: 517, duration: 0.017s, episode steps:   4, steps per second: 234, episode reward: -1238.000, mean reward: -309.500 [-999.000, -60.000], mean action: 2.000 [0.000, 3.000],  loss: 710142.937500, mae: 3063.350525, accuracy: 0.132812, mean_q: -3626.647278, mean_eps: 0.640945\n",
      "  3995/50000: episode: 518, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 1.667 [0.000, 3.000],  loss: 854388.520833, mae: 3103.773275, accuracy: 0.177083, mean_q: -3637.343262, mean_eps: 0.640630\n",
      "  3998/50000: episode: 519, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 1.667 [0.000, 3.000],  loss: 1153843.500000, mae: 3141.498128, accuracy: 0.156250, mean_q: -3643.358561, mean_eps: 0.640360\n",
      "  4012/50000: episode: 520, duration: 0.048s, episode steps:  14, steps per second: 289, episode reward: -11228.000, mean reward: -802.000 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 1013100.357143, mae: 3127.679670, accuracy: 0.191964, mean_q: -3656.284180, mean_eps: 0.639595\n",
      "  4019/50000: episode: 521, duration: 0.028s, episode steps:   7, steps per second: 252, episode reward: -4235.000, mean reward: -605.000 [-999.000, -45.000], mean action: 1.429 [0.000, 3.000],  loss: 1011546.383929, mae: 3128.654890, accuracy: 0.276786, mean_q: -3657.322545, mean_eps: 0.638650\n",
      "  4023/50000: episode: 522, duration: 0.015s, episode steps:   4, steps per second: 260, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 820224.742188, mae: 3110.402466, accuracy: 0.281250, mean_q: -3665.115234, mean_eps: 0.638155\n",
      "  4031/50000: episode: 523, duration: 0.031s, episode steps:   8, steps per second: 254, episode reward: -5219.000, mean reward: -652.375 [-999.000, -32.000], mean action: 1.125 [0.000, 3.000],  loss: 899667.535156, mae: 3124.072510, accuracy: 0.273438, mean_q: -3689.672699, mean_eps: 0.637615\n",
      "  4035/50000: episode: 524, duration: 0.017s, episode steps:   4, steps per second: 238, episode reward: -1238.000, mean reward: -309.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 1086238.000000, mae: 3160.329712, accuracy: 0.312500, mean_q: -3695.842773, mean_eps: 0.637075\n",
      "  4043/50000: episode: 525, duration: 0.027s, episode steps:   8, steps per second: 299, episode reward: -5190.000, mean reward: -648.750 [-999.000, -58.000], mean action: 1.625 [0.000, 3.000],  loss: 1067385.273438, mae: 3147.291260, accuracy: 0.234375, mean_q: -3699.010529, mean_eps: 0.636535\n",
      "  4047/50000: episode: 526, duration: 0.015s, episode steps:   4, steps per second: 258, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.750 [0.000, 3.000],  loss: 1197838.031250, mae: 3148.688049, accuracy: 0.250000, mean_q: -3684.028870, mean_eps: 0.635995\n",
      "  4050/50000: episode: 527, duration: 0.014s, episode steps:   3, steps per second: 218, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 1147001.145833, mae: 3134.920166, accuracy: 0.250000, mean_q: -3670.091715, mean_eps: 0.635680\n",
      "  4054/50000: episode: 528, duration: 0.016s, episode steps:   4, steps per second: 248, episode reward: -1223.000, mean reward: -305.750 [-999.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 882964.328125, mae: 3089.611023, accuracy: 0.234375, mean_q: -3664.434509, mean_eps: 0.635365\n",
      "  4059/50000: episode: 529, duration: 0.020s, episode steps:   5, steps per second: 253, episode reward: -2237.000, mean reward: -447.400 [-999.000, -60.000], mean action: 1.800 [0.000, 3.000],  loss: 1038245.962500, mae: 3134.466504, accuracy: 0.231250, mean_q: -3647.895508, mean_eps: 0.634960\n",
      "  4064/50000: episode: 530, duration: 0.020s, episode steps:   5, steps per second: 248, episode reward: -2193.000, mean reward: -438.600 [-999.000, -45.000], mean action: 1.800 [0.000, 3.000],  loss: 944196.637500, mae: 3128.032520, accuracy: 0.250000, mean_q: -3655.220605, mean_eps: 0.634510\n",
      "  4070/50000: episode: 531, duration: 0.022s, episode steps:   6, steps per second: 274, episode reward: -3192.000, mean reward: -532.000 [-999.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 995221.218750, mae: 3127.091838, accuracy: 0.322917, mean_q: -3690.892497, mean_eps: 0.634015\n",
      "  4073/50000: episode: 532, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 2.000 [1.000, 3.000],  loss: 1136404.666667, mae: 3159.786051, accuracy: 0.322917, mean_q: -3692.696126, mean_eps: 0.633610\n",
      "  4076/50000: episode: 533, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 1273153.208333, mae: 3185.199544, accuracy: 0.281250, mean_q: -3704.510986, mean_eps: 0.633340\n",
      "  4083/50000: episode: 534, duration: 0.024s, episode steps:   7, steps per second: 296, episode reward: -4235.000, mean reward: -605.000 [-999.000, -60.000], mean action: 1.857 [0.000, 3.000],  loss: 1235332.553571, mae: 3169.444510, accuracy: 0.312500, mean_q: -3685.639369, mean_eps: 0.632890\n",
      "  4096/50000: episode: 535, duration: 0.044s, episode steps:  13, steps per second: 297, episode reward: -10185.000, mean reward: -783.462 [-999.000, -32.000], mean action: 1.154 [0.000, 3.000],  loss: 1008470.060096, mae: 3106.158034, accuracy: 0.278846, mean_q: -3641.302302, mean_eps: 0.631990\n",
      "  4110/50000: episode: 536, duration: 0.047s, episode steps:  14, steps per second: 297, episode reward: -11213.000, mean reward: -800.929 [-999.000, -32.000], mean action: 1.000 [0.000, 3.000],  loss: 1131292.267857, mae: 3118.575875, accuracy: 0.218750, mean_q: -3641.839495, mean_eps: 0.630775\n",
      "  4115/50000: episode: 537, duration: 0.020s, episode steps:   5, steps per second: 248, episode reward: -2237.000, mean reward: -447.400 [-999.000, -45.000], mean action: 1.200 [0.000, 2.000],  loss: 968364.687500, mae: 3102.726221, accuracy: 0.181250, mean_q: -3647.935645, mean_eps: 0.629920\n",
      "  4138/50000: episode: 538, duration: 0.072s, episode steps:  23, steps per second: 318, episode reward: -20204.000, mean reward: -878.435 [-999.000, -32.000], mean action: 1.826 [0.000, 3.000],  loss: 974705.016304, mae: 3107.207573, accuracy: 0.175272, mean_q: -3644.457604, mean_eps: 0.628660\n",
      "  4142/50000: episode: 539, duration: 0.016s, episode steps:   4, steps per second: 249, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 2.000 [0.000, 3.000],  loss: 848577.203125, mae: 3119.971924, accuracy: 0.210938, mean_q: -3674.374817, mean_eps: 0.627445\n",
      "  4149/50000: episode: 540, duration: 0.026s, episode steps:   7, steps per second: 273, episode reward: -4235.000, mean reward: -605.000 [-999.000, -45.000], mean action: 1.429 [0.000, 3.000],  loss: 1032365.098214, mae: 3137.366804, accuracy: 0.241071, mean_q: -3680.820836, mean_eps: 0.626950\n",
      "  4153/50000: episode: 541, duration: 0.014s, episode steps:   4, steps per second: 282, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.750 [0.000, 3.000],  loss: 1037086.515625, mae: 3155.540771, accuracy: 0.234375, mean_q: -3695.686890, mean_eps: 0.626455\n",
      "  4156/50000: episode: 542, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 1193342.729167, mae: 3181.449056, accuracy: 0.197917, mean_q: -3717.056071, mean_eps: 0.626140\n",
      "  4180/50000: episode: 543, duration: 0.083s, episode steps:  24, steps per second: 289, episode reward: -21203.000, mean reward: -883.458 [-999.000, -58.000], mean action: 2.167 [0.000, 3.000],  loss: 937708.033203, mae: 3146.751343, accuracy: 0.242188, mean_q: -3710.322540, mean_eps: 0.624925\n",
      "  4197/50000: episode: 544, duration: 0.063s, episode steps:  17, steps per second: 271, episode reward: -14210.000, mean reward: -835.882 [-999.000, -58.000], mean action: 2.176 [0.000, 3.000],  loss: 1068096.411765, mae: 3162.182703, accuracy: 0.222426, mean_q: -3708.954547, mean_eps: 0.623080\n",
      "  4202/50000: episode: 545, duration: 0.021s, episode steps:   5, steps per second: 238, episode reward: -2237.000, mean reward: -447.400 [-999.000, -45.000], mean action: 1.400 [0.000, 3.000],  loss: 925710.775000, mae: 3115.347998, accuracy: 0.181250, mean_q: -3664.680566, mean_eps: 0.622090\n",
      "  4218/50000: episode: 546, duration: 0.065s, episode steps:  16, steps per second: 247, episode reward: -13211.000, mean reward: -825.688 [-999.000, -58.000], mean action: 1.938 [0.000, 3.000],  loss: 1207266.589844, mae: 3142.059937, accuracy: 0.164062, mean_q: -3631.249084, mean_eps: 0.621145\n",
      "  4230/50000: episode: 547, duration: 0.051s, episode steps:  12, steps per second: 235, episode reward: -9230.000, mean reward: -769.167 [-999.000, -60.000], mean action: 1.333 [0.000, 3.000],  loss: 1096396.903646, mae: 3103.324585, accuracy: 0.226562, mean_q: -3583.626099, mean_eps: 0.619885\n",
      "  4234/50000: episode: 548, duration: 0.018s, episode steps:   4, steps per second: 220, episode reward: -1223.000, mean reward: -305.750 [-999.000, -58.000], mean action: 2.000 [0.000, 3.000],  loss: 1101736.312500, mae: 3099.726624, accuracy: 0.125000, mean_q: -3588.176941, mean_eps: 0.619165\n",
      "  4237/50000: episode: 549, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 1018743.947917, mae: 3084.945882, accuracy: 0.281250, mean_q: -3576.126953, mean_eps: 0.618850\n",
      "  4243/50000: episode: 550, duration: 0.027s, episode steps:   6, steps per second: 226, episode reward: -3192.000, mean reward: -532.000 [-999.000, -58.000], mean action: 2.000 [0.000, 3.000],  loss: 1103506.562500, mae: 3114.376058, accuracy: 0.171875, mean_q: -3621.272705, mean_eps: 0.618445\n",
      "  4248/50000: episode: 551, duration: 0.021s, episode steps:   5, steps per second: 243, episode reward: -2237.000, mean reward: -447.400 [-999.000, -45.000], mean action: 1.400 [0.000, 3.000],  loss: 998297.390625, mae: 3094.373486, accuracy: 0.212500, mean_q: -3615.393066, mean_eps: 0.617950\n",
      "  4254/50000: episode: 552, duration: 0.027s, episode steps:   6, steps per second: 218, episode reward: -3192.000, mean reward: -532.000 [-999.000, -32.000], mean action: 2.000 [0.000, 3.000],  loss: 1105619.593750, mae: 3121.407023, accuracy: 0.213542, mean_q: -3611.964274, mean_eps: 0.617455\n",
      "  4258/50000: episode: 553, duration: 0.019s, episode steps:   4, steps per second: 208, episode reward: -1238.000, mean reward: -309.500 [-999.000, -45.000], mean action: 2.000 [1.000, 3.000],  loss: 961364.820312, mae: 3082.320068, accuracy: 0.312500, mean_q: -3609.297058, mean_eps: 0.617005\n",
      "  4268/50000: episode: 554, duration: 0.039s, episode steps:  10, steps per second: 254, episode reward: -7232.000, mean reward: -723.200 [-999.000, -60.000], mean action: 2.100 [0.000, 3.000],  loss: 1004183.668750, mae: 3101.793579, accuracy: 0.309375, mean_q: -3621.682690, mean_eps: 0.616375\n",
      "  4273/50000: episode: 555, duration: 0.022s, episode steps:   5, steps per second: 223, episode reward: -2237.000, mean reward: -447.400 [-999.000, -45.000], mean action: 1.600 [0.000, 3.000],  loss: 939289.062500, mae: 3080.307520, accuracy: 0.356250, mean_q: -3614.594922, mean_eps: 0.615700\n",
      "  4278/50000: episode: 556, duration: 0.021s, episode steps:   5, steps per second: 233, episode reward: -2222.000, mean reward: -444.400 [-999.000, -58.000], mean action: 1.200 [0.000, 3.000],  loss: 1272477.662500, mae: 3127.889160, accuracy: 0.368750, mean_q: -3621.285156, mean_eps: 0.615250\n",
      "  4282/50000: episode: 557, duration: 0.017s, episode steps:   4, steps per second: 238, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 2.250 [1.000, 3.000],  loss: 796461.992188, mae: 3046.040649, accuracy: 0.359375, mean_q: -3615.439697, mean_eps: 0.614845\n",
      "  4287/50000: episode: 558, duration: 0.018s, episode steps:   5, steps per second: 278, episode reward: -2193.000, mean reward: -438.600 [-999.000, -32.000], mean action: 1.600 [0.000, 3.000],  loss: 818567.343750, mae: 3054.056104, accuracy: 0.262500, mean_q: -3632.711475, mean_eps: 0.614440\n",
      "  4295/50000: episode: 559, duration: 0.029s, episode steps:   8, steps per second: 275, episode reward: -5234.000, mean reward: -654.250 [-999.000, -45.000], mean action: 1.750 [0.000, 3.000],  loss: 1156604.765625, mae: 3115.083038, accuracy: 0.296875, mean_q: -3622.663757, mean_eps: 0.613855\n",
      "  4300/50000: episode: 560, duration: 0.019s, episode steps:   5, steps per second: 261, episode reward: -2237.000, mean reward: -447.400 [-999.000, -58.000], mean action: 1.600 [0.000, 3.000],  loss: 1178671.968750, mae: 3107.375977, accuracy: 0.312500, mean_q: -3628.746777, mean_eps: 0.613270\n",
      "  4306/50000: episode: 561, duration: 0.023s, episode steps:   6, steps per second: 262, episode reward: -3192.000, mean reward: -532.000 [-999.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 962360.635417, mae: 3066.775594, accuracy: 0.265625, mean_q: -3596.555908, mean_eps: 0.612775\n",
      "  4316/50000: episode: 562, duration: 0.034s, episode steps:  10, steps per second: 292, episode reward: -7188.000, mean reward: -718.800 [-999.000, -58.000], mean action: 1.800 [0.000, 3.000],  loss: 1079915.462500, mae: 3071.245776, accuracy: 0.250000, mean_q: -3557.149878, mean_eps: 0.612055\n",
      "  4323/50000: episode: 563, duration: 0.026s, episode steps:   7, steps per second: 264, episode reward: -4191.000, mean reward: -598.714 [-999.000, -58.000], mean action: 1.143 [0.000, 3.000],  loss: 1072124.000000, mae: 3068.402169, accuracy: 0.191964, mean_q: -3537.388602, mean_eps: 0.611290\n",
      "  4331/50000: episode: 564, duration: 0.033s, episode steps:   8, steps per second: 242, episode reward: -5234.000, mean reward: -654.250 [-999.000, -45.000], mean action: 1.625 [0.000, 3.000],  loss: 1284544.664062, mae: 3089.441681, accuracy: 0.187500, mean_q: -3527.263611, mean_eps: 0.610615\n",
      "  4338/50000: episode: 565, duration: 0.028s, episode steps:   7, steps per second: 254, episode reward: -4191.000, mean reward: -598.714 [-999.000, -45.000], mean action: 1.571 [0.000, 3.000],  loss: 981651.741071, mae: 3042.798131, accuracy: 0.232143, mean_q: -3485.675398, mean_eps: 0.609940\n",
      "  4355/50000: episode: 566, duration: 0.057s, episode steps:  17, steps per second: 300, episode reward: -14210.000, mean reward: -835.882 [-999.000, -32.000], mean action: 1.765 [0.000, 3.000],  loss: 1031952.707721, mae: 3055.605770, accuracy: 0.358456, mean_q: -3515.833051, mean_eps: 0.608860\n",
      "  4370/50000: episode: 567, duration: 0.051s, episode steps:  15, steps per second: 293, episode reward: -12183.000, mean reward: -812.200 [-999.000, -32.000], mean action: 0.800 [0.000, 3.000],  loss: 901914.397917, mae: 3050.875830, accuracy: 0.385417, mean_q: -3549.381771, mean_eps: 0.607420\n",
      "  4395/50000: episode: 568, duration: 0.084s, episode steps:  25, steps per second: 299, episode reward: -22217.000, mean reward: -888.680 [-999.000, -45.000], mean action: 1.320 [0.000, 3.000],  loss: 1107566.572500, mae: 3078.608623, accuracy: 0.411250, mean_q: -3547.828115, mean_eps: 0.605620\n",
      "  4400/50000: episode: 569, duration: 0.021s, episode steps:   5, steps per second: 239, episode reward: -2237.000, mean reward: -447.400 [-999.000, -45.000], mean action: 1.200 [0.000, 3.000],  loss: 1252664.875000, mae: 3076.570605, accuracy: 0.387500, mean_q: -3514.938770, mean_eps: 0.604270\n",
      "  4413/50000: episode: 570, duration: 0.046s, episode steps:  13, steps per second: 282, episode reward: -10229.000, mean reward: -786.846 [-999.000, -45.000], mean action: 1.692 [0.000, 3.000],  loss: 1106703.432692, mae: 3025.974440, accuracy: 0.355769, mean_q: -3488.548847, mean_eps: 0.603460\n",
      "  4417/50000: episode: 571, duration: 0.017s, episode steps:   4, steps per second: 234, episode reward: -1238.000, mean reward: -309.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 1054707.593750, mae: 3027.646362, accuracy: 0.367188, mean_q: -3472.810913, mean_eps: 0.602695\n",
      "  4424/50000: episode: 572, duration: 0.026s, episode steps:   7, steps per second: 268, episode reward: -4191.000, mean reward: -598.714 [-999.000, -32.000], mean action: 1.714 [0.000, 3.000],  loss: 1264969.321429, mae: 3053.607666, accuracy: 0.299107, mean_q: -3490.362130, mean_eps: 0.602200\n",
      "  4428/50000: episode: 573, duration: 0.016s, episode steps:   4, steps per second: 247, episode reward: -1238.000, mean reward: -309.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 766676.726562, mae: 2982.573853, accuracy: 0.242188, mean_q: -3474.018616, mean_eps: 0.601705\n",
      "  4433/50000: episode: 574, duration: 0.018s, episode steps:   5, steps per second: 279, episode reward: -2237.000, mean reward: -447.400 [-999.000, -45.000], mean action: 1.600 [0.000, 3.000],  loss: 1096633.025000, mae: 3012.338867, accuracy: 0.250000, mean_q: -3515.602539, mean_eps: 0.601300\n",
      "  4440/50000: episode: 575, duration: 0.027s, episode steps:   7, steps per second: 262, episode reward: -4220.000, mean reward: -602.857 [-999.000, -32.000], mean action: 0.857 [0.000, 3.000],  loss: 925866.468750, mae: 3003.931466, accuracy: 0.290179, mean_q: -3497.262591, mean_eps: 0.600760\n",
      "  4445/50000: episode: 576, duration: 0.018s, episode steps:   5, steps per second: 272, episode reward: -2193.000, mean reward: -438.600 [-999.000, -32.000], mean action: 1.400 [0.000, 3.000],  loss: 936944.725000, mae: 3011.168164, accuracy: 0.275000, mean_q: -3501.872998, mean_eps: 0.600220\n",
      "  4449/50000: episode: 577, duration: 0.018s, episode steps:   4, steps per second: 225, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 1182238.000000, mae: 3055.431519, accuracy: 0.250000, mean_q: -3511.989258, mean_eps: 0.599815\n",
      "  4452/50000: episode: 578, duration: 0.014s, episode steps:   3, steps per second: 214, episode reward: -224.000, mean reward: -74.667 [-116.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 599529.906250, mae: 2960.754639, accuracy: 0.239583, mean_q: -3517.713460, mean_eps: 0.599500\n",
      "  4462/50000: episode: 579, duration: 0.035s, episode steps:  10, steps per second: 289, episode reward: -7188.000, mean reward: -718.800 [-999.000, -32.000], mean action: 1.800 [0.000, 3.000],  loss: 862525.625000, mae: 3035.522607, accuracy: 0.246875, mean_q: -3547.808789, mean_eps: 0.598915\n",
      "  4465/50000: episode: 580, duration: 0.014s, episode steps:   3, steps per second: 219, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.333 [0.000, 3.000],  loss: 1058799.166667, mae: 3073.218913, accuracy: 0.260417, mean_q: -3589.485270, mean_eps: 0.598330\n",
      "  4471/50000: episode: 581, duration: 0.022s, episode steps:   6, steps per second: 271, episode reward: -3236.000, mean reward: -539.333 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 1063883.135417, mae: 3074.102417, accuracy: 0.270833, mean_q: -3586.858480, mean_eps: 0.597925\n",
      "  4475/50000: episode: 582, duration: 0.017s, episode steps:   4, steps per second: 235, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 1335935.062500, mae: 3112.202820, accuracy: 0.242188, mean_q: -3571.425171, mean_eps: 0.597475\n",
      "  4482/50000: episode: 583, duration: 0.026s, episode steps:   7, steps per second: 271, episode reward: -4235.000, mean reward: -605.000 [-999.000, -60.000], mean action: 1.429 [0.000, 3.000],  loss: 1117068.616071, mae: 3046.885951, accuracy: 0.232143, mean_q: -3537.474854, mean_eps: 0.596980\n",
      "  4485/50000: episode: 584, duration: 0.012s, episode steps:   3, steps per second: 256, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 1258700.291667, mae: 3045.271403, accuracy: 0.208333, mean_q: -3511.943929, mean_eps: 0.596530\n",
      "  4500/50000: episode: 585, duration: 0.050s, episode steps:  15, steps per second: 300, episode reward: -12227.000, mean reward: -815.133 [-999.000, -45.000], mean action: 1.400 [0.000, 3.000],  loss: 875925.087500, mae: 3009.227083, accuracy: 0.216667, mean_q: -3506.199854, mean_eps: 0.595720\n",
      "  4504/50000: episode: 586, duration: 0.016s, episode steps:   4, steps per second: 251, episode reward: -1238.000, mean reward: -309.500 [-999.000, -60.000], mean action: 1.500 [0.000, 3.000],  loss: 1239321.187500, mae: 3063.064209, accuracy: 0.203125, mean_q: -3520.885010, mean_eps: 0.594865\n",
      "  4507/50000: episode: 587, duration: 0.014s, episode steps:   3, steps per second: 222, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 978941.937500, mae: 3027.914062, accuracy: 0.114583, mean_q: -3501.953044, mean_eps: 0.594550\n",
      "  4511/50000: episode: 588, duration: 0.015s, episode steps:   4, steps per second: 260, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 946115.625000, mae: 3023.404968, accuracy: 0.164062, mean_q: -3509.528320, mean_eps: 0.594235\n",
      "  4528/50000: episode: 589, duration: 0.057s, episode steps:  17, steps per second: 297, episode reward: -14225.000, mean reward: -836.765 [-999.000, -45.000], mean action: 1.353 [0.000, 3.000],  loss: 986807.099265, mae: 3031.538876, accuracy: 0.233456, mean_q: -3516.222010, mean_eps: 0.593290\n",
      "  4535/50000: episode: 590, duration: 0.026s, episode steps:   7, steps per second: 268, episode reward: -4191.000, mean reward: -598.714 [-999.000, -32.000], mean action: 1.714 [1.000, 3.000],  loss: 842086.017857, mae: 3027.643938, accuracy: 0.223214, mean_q: -3540.424909, mean_eps: 0.592210\n",
      "  4541/50000: episode: 591, duration: 0.024s, episode steps:   6, steps per second: 253, episode reward: -3192.000, mean reward: -532.000 [-999.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 897521.437500, mae: 3043.076538, accuracy: 0.270833, mean_q: -3553.851969, mean_eps: 0.591625\n",
      "  4545/50000: episode: 592, duration: 0.019s, episode steps:   4, steps per second: 215, episode reward: -1238.000, mean reward: -309.500 [-999.000, -58.000], mean action: 0.750 [0.000, 2.000],  loss: 631828.691406, mae: 3021.465515, accuracy: 0.234375, mean_q: -3577.241333, mean_eps: 0.591175\n",
      "  4559/50000: episode: 593, duration: 0.053s, episode steps:  14, steps per second: 266, episode reward: -11213.000, mean reward: -800.929 [-999.000, -58.000], mean action: 1.857 [0.000, 3.000],  loss: 881554.875000, mae: 3075.010829, accuracy: 0.265625, mean_q: -3617.366228, mean_eps: 0.590365\n",
      "  4564/50000: episode: 594, duration: 0.021s, episode steps:   5, steps per second: 240, episode reward: -2237.000, mean reward: -447.400 [-999.000, -58.000], mean action: 1.200 [0.000, 3.000],  loss: 857353.312500, mae: 3081.489648, accuracy: 0.256250, mean_q: -3625.105273, mean_eps: 0.589510\n",
      "  4580/50000: episode: 595, duration: 0.064s, episode steps:  16, steps per second: 251, episode reward: -13182.000, mean reward: -823.875 [-999.000, -58.000], mean action: 1.688 [0.000, 3.000],  loss: 962966.880859, mae: 3065.828384, accuracy: 0.224609, mean_q: -3573.401352, mean_eps: 0.588565\n",
      "  4586/50000: episode: 596, duration: 0.028s, episode steps:   6, steps per second: 211, episode reward: -3236.000, mean reward: -539.333 [-999.000, -58.000], mean action: 2.000 [0.000, 3.000],  loss: 1112656.729167, mae: 3063.471883, accuracy: 0.208333, mean_q: -3531.055542, mean_eps: 0.587575\n",
      "  4598/50000: episode: 597, duration: 0.041s, episode steps:  12, steps per second: 291, episode reward: -9215.000, mean reward: -767.917 [-999.000, -58.000], mean action: 1.583 [0.000, 3.000],  loss: 950412.135417, mae: 3037.852173, accuracy: 0.195312, mean_q: -3504.754395, mean_eps: 0.586765\n",
      "  4605/50000: episode: 598, duration: 0.027s, episode steps:   7, steps per second: 257, episode reward: -4191.000, mean reward: -598.714 [-999.000, -58.000], mean action: 1.857 [0.000, 3.000],  loss: 1064525.946429, mae: 3051.346261, accuracy: 0.160714, mean_q: -3500.112200, mean_eps: 0.585910\n",
      "  4609/50000: episode: 599, duration: 0.018s, episode steps:   4, steps per second: 225, episode reward: -1238.000, mean reward: -309.500 [-999.000, -60.000], mean action: 1.500 [0.000, 3.000],  loss: 1024759.578125, mae: 3047.131897, accuracy: 0.234375, mean_q: -3508.879150, mean_eps: 0.585415\n",
      "  4613/50000: episode: 600, duration: 0.017s, episode steps:   4, steps per second: 240, episode reward: -1223.000, mean reward: -305.750 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 1067068.906250, mae: 3025.783386, accuracy: 0.273438, mean_q: -3515.652222, mean_eps: 0.585055\n",
      "  4616/50000: episode: 601, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 1209637.979167, mae: 3051.314616, accuracy: 0.197917, mean_q: -3508.916341, mean_eps: 0.584740\n",
      "  4621/50000: episode: 602, duration: 0.020s, episode steps:   5, steps per second: 246, episode reward: -2193.000, mean reward: -438.600 [-999.000, -58.000], mean action: 1.200 [0.000, 3.000],  loss: 1109655.862500, mae: 3037.628955, accuracy: 0.300000, mean_q: -3497.178760, mean_eps: 0.584380\n",
      "  4627/50000: episode: 603, duration: 0.028s, episode steps:   6, steps per second: 212, episode reward: -3236.000, mean reward: -539.333 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 869551.322917, mae: 2993.229818, accuracy: 0.203125, mean_q: -3500.261719, mean_eps: 0.583885\n",
      "  4633/50000: episode: 604, duration: 0.023s, episode steps:   6, steps per second: 264, episode reward: -3236.000, mean reward: -539.333 [-999.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 982836.562500, mae: 3015.156372, accuracy: 0.270833, mean_q: -3511.138916, mean_eps: 0.583345\n",
      "  4640/50000: episode: 605, duration: 0.030s, episode steps:   7, steps per second: 232, episode reward: -4220.000, mean reward: -602.857 [-999.000, -58.000], mean action: 1.714 [0.000, 3.000],  loss: 999039.321429, mae: 3028.995675, accuracy: 0.267857, mean_q: -3515.565883, mean_eps: 0.582760\n",
      "  4656/50000: episode: 606, duration: 0.067s, episode steps:  16, steps per second: 239, episode reward: -13182.000, mean reward: -823.875 [-999.000, -45.000], mean action: 1.812 [0.000, 3.000],  loss: 957633.822266, mae: 3028.480881, accuracy: 0.224609, mean_q: -3536.552048, mean_eps: 0.581725\n",
      "  4662/50000: episode: 607, duration: 0.030s, episode steps:   6, steps per second: 202, episode reward: -3236.000, mean reward: -539.333 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 909213.937500, mae: 3032.815104, accuracy: 0.250000, mean_q: -3538.852865, mean_eps: 0.580735\n",
      "  4667/50000: episode: 608, duration: 0.022s, episode steps:   5, steps per second: 225, episode reward: -2237.000, mean reward: -447.400 [-999.000, -45.000], mean action: 1.600 [0.000, 3.000],  loss: 1073992.587500, mae: 3053.288672, accuracy: 0.143750, mean_q: -3533.761621, mean_eps: 0.580240\n",
      "  4671/50000: episode: 609, duration: 0.018s, episode steps:   4, steps per second: 220, episode reward: -1238.000, mean reward: -309.500 [-999.000, -58.000], mean action: 1.000 [0.000, 3.000],  loss: 1169309.359375, mae: 3066.458130, accuracy: 0.148438, mean_q: -3521.314636, mean_eps: 0.579835\n",
      "  4675/50000: episode: 610, duration: 0.024s, episode steps:   4, steps per second: 169, episode reward: -1238.000, mean reward: -309.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 1155511.296875, mae: 3045.941711, accuracy: 0.187500, mean_q: -3507.261047, mean_eps: 0.579475\n",
      "  4681/50000: episode: 611, duration: 0.024s, episode steps:   6, steps per second: 249, episode reward: -3236.000, mean reward: -539.333 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 1261222.364583, mae: 3044.585612, accuracy: 0.192708, mean_q: -3456.449178, mean_eps: 0.579025\n",
      "  4686/50000: episode: 612, duration: 0.021s, episode steps:   5, steps per second: 239, episode reward: -2237.000, mean reward: -447.400 [-999.000, -58.000], mean action: 1.200 [0.000, 3.000],  loss: 847456.825000, mae: 2944.663477, accuracy: 0.168750, mean_q: -3419.049512, mean_eps: 0.578530\n",
      "  4691/50000: episode: 613, duration: 0.019s, episode steps:   5, steps per second: 257, episode reward: -2222.000, mean reward: -444.400 [-999.000, -32.000], mean action: 1.400 [0.000, 3.000],  loss: 1016621.275000, mae: 2972.427979, accuracy: 0.137500, mean_q: -3402.730566, mean_eps: 0.578080\n",
      "  4696/50000: episode: 614, duration: 0.021s, episode steps:   5, steps per second: 242, episode reward: -2222.000, mean reward: -444.400 [-999.000, -58.000], mean action: 1.400 [0.000, 3.000],  loss: 848649.525000, mae: 2949.169092, accuracy: 0.150000, mean_q: -3395.027148, mean_eps: 0.577630\n",
      "  4701/50000: episode: 615, duration: 0.021s, episode steps:   5, steps per second: 243, episode reward: -2237.000, mean reward: -447.400 [-999.000, -45.000], mean action: 1.600 [0.000, 3.000],  loss: 1170914.912500, mae: 3009.635059, accuracy: 0.143750, mean_q: -3417.788330, mean_eps: 0.577180\n",
      "  4709/50000: episode: 616, duration: 0.032s, episode steps:   8, steps per second: 253, episode reward: -5234.000, mean reward: -654.250 [-999.000, -60.000], mean action: 1.125 [0.000, 3.000],  loss: 992923.937500, mae: 2971.245209, accuracy: 0.148438, mean_q: -3419.495331, mean_eps: 0.576595\n",
      "  4715/50000: episode: 617, duration: 0.021s, episode steps:   6, steps per second: 284, episode reward: -3236.000, mean reward: -539.333 [-999.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 893826.135417, mae: 2976.434285, accuracy: 0.171875, mean_q: -3417.586507, mean_eps: 0.575965\n",
      "  4720/50000: episode: 618, duration: 0.022s, episode steps:   5, steps per second: 230, episode reward: -2237.000, mean reward: -447.400 [-999.000, -60.000], mean action: 2.200 [0.000, 3.000],  loss: 1097733.937500, mae: 3013.436865, accuracy: 0.175000, mean_q: -3415.288281, mean_eps: 0.575470\n",
      "  4730/50000: episode: 619, duration: 0.036s, episode steps:  10, steps per second: 279, episode reward: -7232.000, mean reward: -723.200 [-999.000, -45.000], mean action: 2.000 [0.000, 3.000],  loss: 1010202.825000, mae: 2994.107764, accuracy: 0.156250, mean_q: -3432.690894, mean_eps: 0.574795\n",
      "  4733/50000: episode: 620, duration: 0.014s, episode steps:   3, steps per second: 215, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 1032312.375000, mae: 3003.467367, accuracy: 0.156250, mean_q: -3429.869873, mean_eps: 0.574210\n",
      "  4744/50000: episode: 621, duration: 0.041s, episode steps:  11, steps per second: 266, episode reward: -8231.000, mean reward: -748.273 [-999.000, -45.000], mean action: 1.182 [0.000, 3.000],  loss: 966694.647727, mae: 2995.326904, accuracy: 0.190341, mean_q: -3456.198375, mean_eps: 0.573580\n",
      "  4751/50000: episode: 622, duration: 0.027s, episode steps:   7, steps per second: 263, episode reward: -4191.000, mean reward: -598.714 [-999.000, -45.000], mean action: 1.286 [0.000, 3.000],  loss: 888489.375000, mae: 2987.464948, accuracy: 0.183036, mean_q: -3454.873710, mean_eps: 0.572770\n",
      "  4757/50000: episode: 623, duration: 0.023s, episode steps:   6, steps per second: 257, episode reward: -3221.000, mean reward: -536.833 [-999.000, -32.000], mean action: 1.167 [0.000, 3.000],  loss: 612920.927083, mae: 2972.752075, accuracy: 0.130208, mean_q: -3467.617106, mean_eps: 0.572185\n",
      "  4762/50000: episode: 624, duration: 0.021s, episode steps:   5, steps per second: 242, episode reward: -2222.000, mean reward: -444.400 [-999.000, -32.000], mean action: 1.400 [0.000, 3.000],  loss: 795734.806250, mae: 2996.724365, accuracy: 0.243750, mean_q: -3495.829980, mean_eps: 0.571690\n",
      "  4765/50000: episode: 625, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 695567.750000, mae: 3001.935872, accuracy: 0.166667, mean_q: -3530.628743, mean_eps: 0.571330\n",
      "  4768/50000: episode: 626, duration: 0.014s, episode steps:   3, steps per second: 221, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 671353.656250, mae: 3003.230062, accuracy: 0.208333, mean_q: -3539.616292, mean_eps: 0.571060\n",
      "  4773/50000: episode: 627, duration: 0.018s, episode steps:   5, steps per second: 276, episode reward: -2222.000, mean reward: -444.400 [-999.000, -32.000], mean action: 1.600 [0.000, 3.000],  loss: 721983.512500, mae: 3023.043018, accuracy: 0.187500, mean_q: -3542.415820, mean_eps: 0.570700\n",
      "  4776/50000: episode: 628, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 1069029.375000, mae: 3057.006917, accuracy: 0.197917, mean_q: -3545.062012, mean_eps: 0.570340\n",
      "  4784/50000: episode: 629, duration: 0.032s, episode steps:   8, steps per second: 251, episode reward: -5190.000, mean reward: -648.750 [-999.000, -32.000], mean action: 1.000 [0.000, 3.000],  loss: 930473.179688, mae: 3039.167603, accuracy: 0.199219, mean_q: -3545.092255, mean_eps: 0.569845\n",
      "  4794/50000: episode: 630, duration: 0.036s, episode steps:  10, steps per second: 282, episode reward: -7217.000, mean reward: -721.700 [-999.000, -58.000], mean action: 1.900 [0.000, 3.000],  loss: 821859.628125, mae: 3033.805005, accuracy: 0.243750, mean_q: -3522.400928, mean_eps: 0.569035\n",
      "  4805/50000: episode: 631, duration: 0.039s, episode steps:  11, steps per second: 282, episode reward: -8187.000, mean reward: -744.273 [-999.000, -45.000], mean action: 2.182 [0.000, 3.000],  loss: 1171670.704545, mae: 3069.230047, accuracy: 0.221591, mean_q: -3520.439498, mean_eps: 0.568090\n",
      "  4810/50000: episode: 632, duration: 0.020s, episode steps:   5, steps per second: 247, episode reward: -2237.000, mean reward: -447.400 [-999.000, -60.000], mean action: 1.800 [0.000, 3.000],  loss: 788380.662500, mae: 2990.837842, accuracy: 0.293750, mean_q: -3458.195508, mean_eps: 0.567370\n",
      "  4816/50000: episode: 633, duration: 0.021s, episode steps:   6, steps per second: 286, episode reward: -3192.000, mean reward: -532.000 [-999.000, -58.000], mean action: 1.000 [0.000, 3.000],  loss: 1028604.843750, mae: 3017.668864, accuracy: 0.244792, mean_q: -3465.640544, mean_eps: 0.566875\n",
      "  4824/50000: episode: 634, duration: 0.030s, episode steps:   8, steps per second: 269, episode reward: -5234.000, mean reward: -654.250 [-999.000, -60.000], mean action: 1.875 [0.000, 3.000],  loss: 863472.492188, mae: 2979.034821, accuracy: 0.214844, mean_q: -3456.583282, mean_eps: 0.566245\n",
      "  4833/50000: episode: 635, duration: 0.032s, episode steps:   9, steps per second: 283, episode reward: -6189.000, mean reward: -687.667 [-999.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 756651.684028, mae: 2979.903727, accuracy: 0.260417, mean_q: -3458.030273, mean_eps: 0.565480\n",
      "  4844/50000: episode: 636, duration: 0.039s, episode steps:  11, steps per second: 280, episode reward: -8187.000, mean reward: -744.273 [-999.000, -58.000], mean action: 2.182 [0.000, 3.000],  loss: 867054.196023, mae: 3022.733354, accuracy: 0.258523, mean_q: -3491.479403, mean_eps: 0.564580\n",
      "  4849/50000: episode: 637, duration: 0.019s, episode steps:   5, steps per second: 262, episode reward: -2193.000, mean reward: -438.600 [-999.000, -58.000], mean action: 1.200 [0.000, 3.000],  loss: 1002538.737500, mae: 3046.647021, accuracy: 0.218750, mean_q: -3513.260645, mean_eps: 0.563860\n",
      "  4853/50000: episode: 638, duration: 0.016s, episode steps:   4, steps per second: 244, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 968677.343750, mae: 3040.116943, accuracy: 0.257812, mean_q: -3502.323303, mean_eps: 0.563455\n",
      "  4862/50000: episode: 639, duration: 0.032s, episode steps:   9, steps per second: 282, episode reward: -6189.000, mean reward: -687.667 [-999.000, -32.000], mean action: 2.000 [0.000, 3.000],  loss: 948627.798611, mae: 3022.923041, accuracy: 0.229167, mean_q: -3481.155328, mean_eps: 0.562870\n",
      "  4867/50000: episode: 640, duration: 0.020s, episode steps:   5, steps per second: 256, episode reward: -2222.000, mean reward: -444.400 [-999.000, -32.000], mean action: 0.800 [0.000, 3.000],  loss: 1052862.350000, mae: 3009.640088, accuracy: 0.262500, mean_q: -3448.178564, mean_eps: 0.562240\n",
      "  4871/50000: episode: 641, duration: 0.015s, episode steps:   4, steps per second: 259, episode reward: -1223.000, mean reward: -305.750 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 831733.187500, mae: 2971.073547, accuracy: 0.179688, mean_q: -3437.088562, mean_eps: 0.561835\n",
      "  4878/50000: episode: 642, duration: 0.028s, episode steps:   7, steps per second: 253, episode reward: -4235.000, mean reward: -605.000 [-999.000, -60.000], mean action: 1.714 [0.000, 3.000],  loss: 947937.750000, mae: 2979.987584, accuracy: 0.200893, mean_q: -3421.088309, mean_eps: 0.561340\n",
      "  4888/50000: episode: 643, duration: 0.034s, episode steps:  10, steps per second: 293, episode reward: -7188.000, mean reward: -718.800 [-999.000, -45.000], mean action: 2.300 [0.000, 3.000],  loss: 948308.781250, mae: 2969.273804, accuracy: 0.190625, mean_q: -3418.161719, mean_eps: 0.560575\n",
      "  4893/50000: episode: 644, duration: 0.020s, episode steps:   5, steps per second: 254, episode reward: -2237.000, mean reward: -447.400 [-999.000, -45.000], mean action: 1.400 [0.000, 3.000],  loss: 1012615.737500, mae: 2980.465332, accuracy: 0.206250, mean_q: -3432.382227, mean_eps: 0.559900\n",
      "  4897/50000: episode: 645, duration: 0.015s, episode steps:   4, steps per second: 259, episode reward: -1238.000, mean reward: -309.500 [-999.000, -45.000], mean action: 1.250 [0.000, 3.000],  loss: 1001232.828125, mae: 2997.770447, accuracy: 0.234375, mean_q: -3433.411072, mean_eps: 0.559495\n",
      "  4901/50000: episode: 646, duration: 0.016s, episode steps:   4, steps per second: 243, episode reward: -1238.000, mean reward: -309.500 [-999.000, -60.000], mean action: 2.250 [1.000, 3.000],  loss: 1002433.109375, mae: 2985.596802, accuracy: 0.226562, mean_q: -3445.277161, mean_eps: 0.559135\n",
      "  4906/50000: episode: 647, duration: 0.020s, episode steps:   5, steps per second: 252, episode reward: -2222.000, mean reward: -444.400 [-999.000, -32.000], mean action: 1.200 [0.000, 3.000],  loss: 924260.450000, mae: 2985.150635, accuracy: 0.243750, mean_q: -3438.828418, mean_eps: 0.558730\n",
      "  4911/50000: episode: 648, duration: 0.020s, episode steps:   5, steps per second: 246, episode reward: -2193.000, mean reward: -438.600 [-999.000, -32.000], mean action: 1.400 [0.000, 3.000],  loss: 875319.987500, mae: 2980.454736, accuracy: 0.225000, mean_q: -3467.275781, mean_eps: 0.558280\n",
      "  4917/50000: episode: 649, duration: 0.022s, episode steps:   6, steps per second: 269, episode reward: -3192.000, mean reward: -532.000 [-999.000, -32.000], mean action: 1.000 [0.000, 3.000],  loss: 1279202.395833, mae: 3031.743978, accuracy: 0.296875, mean_q: -3457.996053, mean_eps: 0.557785\n",
      "  4921/50000: episode: 650, duration: 0.015s, episode steps:   4, steps per second: 269, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 1104989.687500, mae: 2999.741882, accuracy: 0.218750, mean_q: -3457.218323, mean_eps: 0.557335\n",
      "  4925/50000: episode: 651, duration: 0.016s, episode steps:   4, steps per second: 244, episode reward: -1238.000, mean reward: -309.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 1079246.468750, mae: 2987.177246, accuracy: 0.234375, mean_q: -3440.124146, mean_eps: 0.556975\n",
      "  4930/50000: episode: 652, duration: 0.020s, episode steps:   5, steps per second: 255, episode reward: -2193.000, mean reward: -438.600 [-999.000, -32.000], mean action: 1.400 [0.000, 3.000],  loss: 871792.787500, mae: 2963.434131, accuracy: 0.212500, mean_q: -3430.629004, mean_eps: 0.556570\n",
      "  4938/50000: episode: 653, duration: 0.029s, episode steps:   8, steps per second: 278, episode reward: -5234.000, mean reward: -654.250 [-999.000, -45.000], mean action: 1.875 [1.000, 3.000],  loss: 1164391.789062, mae: 3012.019745, accuracy: 0.218750, mean_q: -3440.430115, mean_eps: 0.555985\n",
      "  4946/50000: episode: 654, duration: 0.028s, episode steps:   8, steps per second: 286, episode reward: -5190.000, mean reward: -648.750 [-999.000, -45.000], mean action: 1.250 [0.000, 3.000],  loss: 922932.601562, mae: 2966.132782, accuracy: 0.261719, mean_q: -3427.886536, mean_eps: 0.555265\n",
      "  4959/50000: episode: 655, duration: 0.044s, episode steps:  13, steps per second: 296, episode reward: -10185.000, mean reward: -783.462 [-999.000, -32.000], mean action: 1.692 [0.000, 3.000],  loss: 962579.985577, mae: 2970.281381, accuracy: 0.259615, mean_q: -3419.661415, mean_eps: 0.554320\n",
      "  4964/50000: episode: 656, duration: 0.019s, episode steps:   5, steps per second: 262, episode reward: -2237.000, mean reward: -447.400 [-999.000, -45.000], mean action: 1.600 [0.000, 3.000],  loss: 864485.437500, mae: 2956.782080, accuracy: 0.287500, mean_q: -3414.416016, mean_eps: 0.553510\n",
      "  4968/50000: episode: 657, duration: 0.017s, episode steps:   4, steps per second: 231, episode reward: -1223.000, mean reward: -305.750 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 1216507.546875, mae: 3012.398132, accuracy: 0.226562, mean_q: -3438.695801, mean_eps: 0.553105\n",
      "  4974/50000: episode: 658, duration: 0.022s, episode steps:   6, steps per second: 268, episode reward: -3236.000, mean reward: -539.333 [-999.000, -60.000], mean action: 2.000 [0.000, 3.000],  loss: 1263985.083333, mae: 3011.085164, accuracy: 0.265625, mean_q: -3441.469604, mean_eps: 0.552655\n",
      "  4990/50000: episode: 659, duration: 0.067s, episode steps:  16, steps per second: 240, episode reward: -13226.000, mean reward: -826.625 [-999.000, -45.000], mean action: 1.625 [0.000, 3.000],  loss: 918718.585938, mae: 2956.342377, accuracy: 0.238281, mean_q: -3423.763641, mean_eps: 0.551665\n",
      "  5006/50000: episode: 660, duration: 0.070s, episode steps:  16, steps per second: 230, episode reward: -13182.000, mean reward: -823.875 [-999.000, -58.000], mean action: 1.938 [0.000, 3.000],  loss: 906883.164062, mae: 2964.943054, accuracy: 0.261719, mean_q: -3437.958176, mean_eps: 0.550225\n",
      "  5009/50000: episode: 661, duration: 0.016s, episode steps:   3, steps per second: 183, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 1246362.041667, mae: 3027.430990, accuracy: 0.260417, mean_q: -3462.846598, mean_eps: 0.549370\n",
      "  5015/50000: episode: 662, duration: 0.031s, episode steps:   6, steps per second: 193, episode reward: -3192.000, mean reward: -532.000 [-999.000, -58.000], mean action: 1.000 [0.000, 3.000],  loss: 941609.583333, mae: 2972.252848, accuracy: 0.286458, mean_q: -3453.561076, mean_eps: 0.548965\n",
      "  5024/50000: episode: 663, duration: 0.038s, episode steps:   9, steps per second: 235, episode reward: -6189.000, mean reward: -687.667 [-999.000, -32.000], mean action: 2.000 [0.000, 3.000],  loss: 1148125.447917, mae: 2999.929959, accuracy: 0.250000, mean_q: -3445.839898, mean_eps: 0.548290\n",
      "  5027/50000: episode: 664, duration: 0.015s, episode steps:   3, steps per second: 199, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.333 [0.000, 3.000],  loss: 860805.041667, mae: 2926.889811, accuracy: 0.229167, mean_q: -3403.339925, mean_eps: 0.547750\n",
      "  5030/50000: episode: 665, duration: 0.015s, episode steps:   3, steps per second: 197, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.333 [0.000, 3.000],  loss: 1154485.250000, mae: 2973.546794, accuracy: 0.229167, mean_q: -3401.186198, mean_eps: 0.547480\n",
      "  5033/50000: episode: 666, duration: 0.017s, episode steps:   3, steps per second: 181, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 1.667 [0.000, 3.000],  loss: 1014577.000000, mae: 2943.204671, accuracy: 0.239583, mean_q: -3391.713298, mean_eps: 0.547210\n",
      "  5038/50000: episode: 667, duration: 0.024s, episode steps:   5, steps per second: 210, episode reward: -2237.000, mean reward: -447.400 [-999.000, -45.000], mean action: 1.400 [0.000, 3.000],  loss: 1134459.400000, mae: 2965.250293, accuracy: 0.243750, mean_q: -3393.541016, mean_eps: 0.546850\n",
      "  5044/50000: episode: 668, duration: 0.026s, episode steps:   6, steps per second: 233, episode reward: -3236.000, mean reward: -539.333 [-999.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 687612.750000, mae: 2906.601969, accuracy: 0.265625, mean_q: -3366.391317, mean_eps: 0.546355\n",
      "  5049/50000: episode: 669, duration: 0.021s, episode steps:   5, steps per second: 233, episode reward: -2193.000, mean reward: -438.600 [-999.000, -32.000], mean action: 1.400 [0.000, 3.000],  loss: 1186939.800000, mae: 2979.154687, accuracy: 0.225000, mean_q: -3394.349268, mean_eps: 0.545860\n",
      "  5060/50000: episode: 670, duration: 0.054s, episode steps:  11, steps per second: 205, episode reward: -8231.000, mean reward: -748.273 [-999.000, -58.000], mean action: 2.000 [0.000, 3.000],  loss: 937622.153409, mae: 2954.305642, accuracy: 0.235795, mean_q: -3398.260165, mean_eps: 0.545140\n",
      "  5069/50000: episode: 671, duration: 0.041s, episode steps:   9, steps per second: 220, episode reward: -6189.000, mean reward: -687.667 [-999.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 1048700.791667, mae: 2957.328261, accuracy: 0.277778, mean_q: -3396.954698, mean_eps: 0.544240\n",
      "  5079/50000: episode: 672, duration: 0.046s, episode steps:  10, steps per second: 215, episode reward: -7232.000, mean reward: -723.200 [-999.000, -60.000], mean action: 2.100 [0.000, 3.000],  loss: 840654.568750, mae: 2939.331470, accuracy: 0.206250, mean_q: -3395.749121, mean_eps: 0.543385\n",
      "  5086/50000: episode: 673, duration: 0.030s, episode steps:   7, steps per second: 233, episode reward: -4191.000, mean reward: -598.714 [-999.000, -32.000], mean action: 1.143 [0.000, 3.000],  loss: 1059504.428571, mae: 2962.904297, accuracy: 0.218750, mean_q: -3398.041295, mean_eps: 0.542620\n",
      "  5091/50000: episode: 674, duration: 0.020s, episode steps:   5, steps per second: 254, episode reward: -2237.000, mean reward: -447.400 [-999.000, -45.000], mean action: 1.800 [0.000, 3.000],  loss: 840526.775000, mae: 2925.855518, accuracy: 0.231250, mean_q: -3389.764697, mean_eps: 0.542080\n",
      "  5104/50000: episode: 675, duration: 0.041s, episode steps:  13, steps per second: 316, episode reward: -10229.000, mean reward: -786.846 [-999.000, -60.000], mean action: 2.308 [0.000, 3.000],  loss: 739355.069712, mae: 2934.658579, accuracy: 0.254808, mean_q: -3413.933537, mean_eps: 0.541270\n",
      "  5117/50000: episode: 676, duration: 0.043s, episode steps:  13, steps per second: 303, episode reward: -10185.000, mean reward: -783.462 [-999.000, -32.000], mean action: 2.308 [0.000, 3.000],  loss: 1002538.223558, mae: 2994.313909, accuracy: 0.228365, mean_q: -3450.483060, mean_eps: 0.540100\n",
      "  5123/50000: episode: 677, duration: 0.026s, episode steps:   6, steps per second: 230, episode reward: -3236.000, mean reward: -539.333 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 1010325.604167, mae: 2994.382609, accuracy: 0.239583, mean_q: -3423.083740, mean_eps: 0.539245\n",
      "  5128/50000: episode: 678, duration: 0.018s, episode steps:   5, steps per second: 275, episode reward: -2193.000, mean reward: -438.600 [-999.000, -58.000], mean action: 1.600 [0.000, 3.000],  loss: 1148591.200000, mae: 2982.043262, accuracy: 0.237500, mean_q: -3401.100879, mean_eps: 0.538750\n",
      "  5132/50000: episode: 679, duration: 0.015s, episode steps:   4, steps per second: 270, episode reward: -1238.000, mean reward: -309.500 [-999.000, -45.000], mean action: 1.250 [0.000, 3.000],  loss: 839674.718750, mae: 2908.796570, accuracy: 0.296875, mean_q: -3362.282593, mean_eps: 0.538345\n",
      "  5142/50000: episode: 680, duration: 0.033s, episode steps:  10, steps per second: 299, episode reward: -7188.000, mean reward: -718.800 [-999.000, -58.000], mean action: 2.400 [0.000, 3.000],  loss: 1058917.425000, mae: 2944.489966, accuracy: 0.181250, mean_q: -3361.787451, mean_eps: 0.537715\n",
      "  5146/50000: episode: 681, duration: 0.015s, episode steps:   4, steps per second: 268, episode reward: -1223.000, mean reward: -305.750 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 779696.343750, mae: 2886.549866, accuracy: 0.226562, mean_q: -3340.075439, mean_eps: 0.537085\n",
      "  5152/50000: episode: 682, duration: 0.021s, episode steps:   6, steps per second: 290, episode reward: -3192.000, mean reward: -532.000 [-999.000, -58.000], mean action: 2.000 [0.000, 3.000],  loss: 865711.666667, mae: 2909.750773, accuracy: 0.203125, mean_q: -3325.097941, mean_eps: 0.536635\n",
      "  5163/50000: episode: 683, duration: 0.035s, episode steps:  11, steps per second: 312, episode reward: -8216.000, mean reward: -746.909 [-999.000, -58.000], mean action: 1.636 [0.000, 3.000],  loss: 877055.386364, mae: 2909.457031, accuracy: 0.250000, mean_q: -3370.925914, mean_eps: 0.535870\n",
      "  5183/50000: episode: 684, duration: 0.064s, episode steps:  20, steps per second: 314, episode reward: -17222.000, mean reward: -861.100 [-999.000, -60.000], mean action: 2.250 [0.000, 3.000],  loss: 1003827.965625, mae: 2944.268823, accuracy: 0.251563, mean_q: -3391.416492, mean_eps: 0.534475\n",
      "  5191/50000: episode: 685, duration: 0.037s, episode steps:   8, steps per second: 219, episode reward: -5234.000, mean reward: -654.250 [-999.000, -45.000], mean action: 2.000 [0.000, 3.000],  loss: 932095.890625, mae: 2917.926910, accuracy: 0.246094, mean_q: -3347.553253, mean_eps: 0.533215\n",
      "  5201/50000: episode: 686, duration: 0.044s, episode steps:  10, steps per second: 229, episode reward: -7217.000, mean reward: -721.700 [-999.000, -58.000], mean action: 2.000 [0.000, 3.000],  loss: 959468.106250, mae: 2918.518311, accuracy: 0.243750, mean_q: -3326.583740, mean_eps: 0.532405\n",
      "  5215/50000: episode: 687, duration: 0.058s, episode steps:  14, steps per second: 242, episode reward: -11228.000, mean reward: -802.000 [-999.000, -60.000], mean action: 2.214 [0.000, 3.000],  loss: 860459.379464, mae: 2905.554147, accuracy: 0.227679, mean_q: -3317.074463, mean_eps: 0.531325\n",
      "  5225/50000: episode: 688, duration: 0.042s, episode steps:  10, steps per second: 237, episode reward: -7188.000, mean reward: -718.800 [-999.000, -58.000], mean action: 2.100 [0.000, 3.000],  loss: 935821.018750, mae: 2944.362134, accuracy: 0.221875, mean_q: -3364.717334, mean_eps: 0.530245\n",
      "  5232/50000: episode: 689, duration: 0.026s, episode steps:   7, steps per second: 272, episode reward: -4235.000, mean reward: -605.000 [-999.000, -60.000], mean action: 2.000 [0.000, 3.000],  loss: 900743.669643, mae: 2939.962507, accuracy: 0.250000, mean_q: -3371.756871, mean_eps: 0.529480\n",
      "  5240/50000: episode: 690, duration: 0.028s, episode steps:   8, steps per second: 285, episode reward: -5190.000, mean reward: -648.750 [-999.000, -32.000], mean action: 1.125 [0.000, 3.000],  loss: 873226.929688, mae: 2927.601166, accuracy: 0.261719, mean_q: -3383.560516, mean_eps: 0.528805\n",
      "  5248/50000: episode: 691, duration: 0.027s, episode steps:   8, steps per second: 291, episode reward: -5190.000, mean reward: -648.750 [-999.000, -32.000], mean action: 0.625 [0.000, 3.000],  loss: 876712.621094, mae: 2943.275024, accuracy: 0.273438, mean_q: -3400.120026, mean_eps: 0.528085\n",
      "  5258/50000: episode: 692, duration: 0.034s, episode steps:  10, steps per second: 298, episode reward: -7232.000, mean reward: -723.200 [-999.000, -60.000], mean action: 1.800 [0.000, 3.000],  loss: 973261.856250, mae: 2972.778931, accuracy: 0.212500, mean_q: -3389.277856, mean_eps: 0.527275\n",
      "  5263/50000: episode: 693, duration: 0.017s, episode steps:   5, steps per second: 286, episode reward: -2237.000, mean reward: -447.400 [-999.000, -58.000], mean action: 1.800 [0.000, 3.000],  loss: 996445.250000, mae: 2962.396094, accuracy: 0.187500, mean_q: -3390.632812, mean_eps: 0.526600\n",
      "  5268/50000: episode: 694, duration: 0.018s, episode steps:   5, steps per second: 279, episode reward: -2237.000, mean reward: -447.400 [-999.000, -45.000], mean action: 1.400 [0.000, 3.000],  loss: 977746.456250, mae: 2951.412549, accuracy: 0.175000, mean_q: -3372.430811, mean_eps: 0.526150\n",
      "  5273/50000: episode: 695, duration: 0.018s, episode steps:   5, steps per second: 283, episode reward: -2237.000, mean reward: -447.400 [-999.000, -58.000], mean action: 1.800 [0.000, 3.000],  loss: 808708.256250, mae: 2910.600391, accuracy: 0.212500, mean_q: -3375.310791, mean_eps: 0.525700\n",
      "  5278/50000: episode: 696, duration: 0.018s, episode steps:   5, steps per second: 280, episode reward: -2193.000, mean reward: -438.600 [-999.000, -32.000], mean action: 1.200 [0.000, 3.000],  loss: 1201463.525000, mae: 2972.826514, accuracy: 0.237500, mean_q: -3377.257031, mean_eps: 0.525250\n",
      "  5281/50000: episode: 697, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 821128.270833, mae: 2909.538167, accuracy: 0.208333, mean_q: -3358.015706, mean_eps: 0.524890\n",
      "  5287/50000: episode: 698, duration: 0.021s, episode steps:   6, steps per second: 285, episode reward: -3192.000, mean reward: -532.000 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 1026847.755208, mae: 2940.789103, accuracy: 0.218750, mean_q: -3352.108968, mean_eps: 0.524485\n",
      "  5290/50000: episode: 699, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 1062633.062500, mae: 2924.360026, accuracy: 0.333333, mean_q: -3341.583171, mean_eps: 0.524080\n",
      "  5300/50000: episode: 700, duration: 0.036s, episode steps:  10, steps per second: 277, episode reward: -7232.000, mean reward: -723.200 [-999.000, -45.000], mean action: 1.400 [0.000, 3.000],  loss: 1001570.037500, mae: 2911.386060, accuracy: 0.253125, mean_q: -3341.357642, mean_eps: 0.523495\n",
      "  5311/50000: episode: 701, duration: 0.039s, episode steps:  11, steps per second: 281, episode reward: -8231.000, mean reward: -748.273 [-999.000, -60.000], mean action: 1.727 [0.000, 3.000],  loss: 964395.397727, mae: 2919.103138, accuracy: 0.244318, mean_q: -3333.870983, mean_eps: 0.522550\n",
      "  5324/50000: episode: 702, duration: 0.044s, episode steps:  13, steps per second: 294, episode reward: -10229.000, mean reward: -786.846 [-999.000, -60.000], mean action: 2.154 [0.000, 3.000],  loss: 957480.451923, mae: 2895.521447, accuracy: 0.225962, mean_q: -3330.475379, mean_eps: 0.521470\n",
      "  5328/50000: episode: 703, duration: 0.015s, episode steps:   4, steps per second: 267, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 1116199.171875, mae: 2905.897217, accuracy: 0.273438, mean_q: -3305.930115, mean_eps: 0.520705\n",
      "  5348/50000: episode: 704, duration: 0.062s, episode steps:  20, steps per second: 321, episode reward: -17178.000, mean reward: -858.900 [-999.000, -32.000], mean action: 1.850 [0.000, 3.000],  loss: 976873.634375, mae: 2877.010730, accuracy: 0.259375, mean_q: -3285.837256, mean_eps: 0.519625\n",
      "  5351/50000: episode: 705, duration: 0.012s, episode steps:   3, steps per second: 254, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 978130.895833, mae: 2888.186198, accuracy: 0.250000, mean_q: -3298.221029, mean_eps: 0.518590\n",
      "  5368/50000: episode: 706, duration: 0.055s, episode steps:  17, steps per second: 312, episode reward: -14181.000, mean reward: -834.176 [-999.000, -58.000], mean action: 0.882 [0.000, 3.000],  loss: 888577.691176, mae: 2878.167954, accuracy: 0.251838, mean_q: -3313.889017, mean_eps: 0.517690\n",
      "  5376/50000: episode: 707, duration: 0.029s, episode steps:   8, steps per second: 280, episode reward: -5190.000, mean reward: -648.750 [-999.000, -58.000], mean action: 1.125 [0.000, 3.000],  loss: 959971.015625, mae: 2902.828735, accuracy: 0.289062, mean_q: -3341.514709, mean_eps: 0.516565\n",
      "  5380/50000: episode: 708, duration: 0.016s, episode steps:   4, steps per second: 251, episode reward: -1238.000, mean reward: -309.500 [-999.000, -58.000], mean action: 0.750 [0.000, 2.000],  loss: 773347.789062, mae: 2870.151001, accuracy: 0.335938, mean_q: -3329.704834, mean_eps: 0.516025\n",
      "  5385/50000: episode: 709, duration: 0.018s, episode steps:   5, steps per second: 276, episode reward: -2237.000, mean reward: -447.400 [-999.000, -45.000], mean action: 1.600 [0.000, 3.000],  loss: 868958.600000, mae: 2898.688086, accuracy: 0.206250, mean_q: -3355.994922, mean_eps: 0.515620\n",
      "  5392/50000: episode: 710, duration: 0.023s, episode steps:   7, steps per second: 298, episode reward: -4191.000, mean reward: -598.714 [-999.000, -58.000], mean action: 1.714 [0.000, 3.000],  loss: 832849.437500, mae: 2905.886475, accuracy: 0.209821, mean_q: -3377.532401, mean_eps: 0.515080\n",
      "  5396/50000: episode: 711, duration: 0.016s, episode steps:   4, steps per second: 257, episode reward: -1238.000, mean reward: -309.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 979656.421875, mae: 2925.252014, accuracy: 0.304688, mean_q: -3360.613037, mean_eps: 0.514585\n",
      "  5400/50000: episode: 712, duration: 0.015s, episode steps:   4, steps per second: 263, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.750 [1.000, 3.000],  loss: 920400.625000, mae: 2914.788330, accuracy: 0.265625, mean_q: -3376.455444, mean_eps: 0.514225\n",
      "  5405/50000: episode: 713, duration: 0.019s, episode steps:   5, steps per second: 268, episode reward: -2237.000, mean reward: -447.400 [-999.000, -45.000], mean action: 1.600 [0.000, 3.000],  loss: 1001766.050000, mae: 2924.186621, accuracy: 0.218750, mean_q: -3361.285986, mean_eps: 0.513820\n",
      "  5411/50000: episode: 714, duration: 0.021s, episode steps:   6, steps per second: 290, episode reward: -3236.000, mean reward: -539.333 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 1094209.947917, mae: 2915.016439, accuracy: 0.260417, mean_q: -3332.356812, mean_eps: 0.513325\n",
      "  5419/50000: episode: 715, duration: 0.027s, episode steps:   8, steps per second: 292, episode reward: -5190.000, mean reward: -648.750 [-999.000, -45.000], mean action: 1.375 [0.000, 3.000],  loss: 1183767.820312, mae: 2906.593811, accuracy: 0.250000, mean_q: -3302.328156, mean_eps: 0.512695\n",
      "  5434/50000: episode: 716, duration: 0.049s, episode steps:  15, steps per second: 306, episode reward: -12227.000, mean reward: -815.133 [-999.000, -45.000], mean action: 1.400 [0.000, 3.000],  loss: 941993.502083, mae: 2847.124723, accuracy: 0.220833, mean_q: -3247.068913, mean_eps: 0.511660\n",
      "  5446/50000: episode: 717, duration: 0.044s, episode steps:  12, steps per second: 271, episode reward: -9186.000, mean reward: -765.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 708641.854167, mae: 2830.922445, accuracy: 0.270833, mean_q: -3260.198324, mean_eps: 0.510445\n",
      "  5453/50000: episode: 718, duration: 0.024s, episode steps:   7, steps per second: 287, episode reward: -4191.000, mean reward: -598.714 [-999.000, -32.000], mean action: 1.571 [0.000, 3.000],  loss: 908368.446429, mae: 2904.264683, accuracy: 0.227679, mean_q: -3340.271554, mean_eps: 0.509590\n",
      "  5456/50000: episode: 719, duration: 0.012s, episode steps:   3, steps per second: 253, episode reward: -224.000, mean reward: -74.667 [-116.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 889005.437500, mae: 2918.979818, accuracy: 0.239583, mean_q: -3352.522217, mean_eps: 0.509140\n",
      "  5465/50000: episode: 720, duration: 0.031s, episode steps:   9, steps per second: 287, episode reward: -6189.000, mean reward: -687.667 [-999.000, -45.000], mean action: 2.000 [0.000, 3.000],  loss: 844663.145833, mae: 2915.759467, accuracy: 0.236111, mean_q: -3370.880480, mean_eps: 0.508600\n",
      "  5468/50000: episode: 721, duration: 0.012s, episode steps:   3, steps per second: 253, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 1.667 [0.000, 3.000],  loss: 838972.520833, mae: 2917.141357, accuracy: 0.312500, mean_q: -3387.984212, mean_eps: 0.508060\n",
      "  5471/50000: episode: 722, duration: 0.012s, episode steps:   3, steps per second: 256, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 420778.979167, mae: 2863.200521, accuracy: 0.270833, mean_q: -3389.157715, mean_eps: 0.507790\n",
      "  5475/50000: episode: 723, duration: 0.016s, episode steps:   4, steps per second: 244, episode reward: -1223.000, mean reward: -305.750 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 1390388.062500, mae: 3010.487671, accuracy: 0.195312, mean_q: -3413.183960, mean_eps: 0.507475\n",
      "  5487/50000: episode: 724, duration: 0.043s, episode steps:  12, steps per second: 277, episode reward: -9230.000, mean reward: -769.167 [-999.000, -58.000], mean action: 1.250 [0.000, 3.000],  loss: 822731.466146, mae: 2913.382487, accuracy: 0.197917, mean_q: -3383.883423, mean_eps: 0.506755\n",
      "  5490/50000: episode: 725, duration: 0.013s, episode steps:   3, steps per second: 230, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 935485.927083, mae: 2923.809163, accuracy: 0.291667, mean_q: -3349.993815, mean_eps: 0.506080\n",
      "  5497/50000: episode: 726, duration: 0.026s, episode steps:   7, steps per second: 268, episode reward: -4235.000, mean reward: -605.000 [-999.000, -45.000], mean action: 1.571 [0.000, 3.000],  loss: 1072691.937500, mae: 2918.981968, accuracy: 0.285714, mean_q: -3349.209612, mean_eps: 0.505630\n",
      "  5510/50000: episode: 727, duration: 0.050s, episode steps:  13, steps per second: 259, episode reward: -10185.000, mean reward: -783.462 [-999.000, -45.000], mean action: 1.538 [0.000, 3.000],  loss: 832872.417067, mae: 2892.244873, accuracy: 0.213942, mean_q: -3341.677434, mean_eps: 0.504730\n",
      "  5515/50000: episode: 728, duration: 0.020s, episode steps:   5, steps per second: 249, episode reward: -2193.000, mean reward: -438.600 [-999.000, -45.000], mean action: 1.400 [0.000, 3.000],  loss: 895029.050000, mae: 2904.008350, accuracy: 0.243750, mean_q: -3352.076514, mean_eps: 0.503920\n",
      "  5518/50000: episode: 729, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 664663.291667, mae: 2871.377197, accuracy: 0.291667, mean_q: -3338.906331, mean_eps: 0.503560\n",
      "  5522/50000: episode: 730, duration: 0.017s, episode steps:   4, steps per second: 241, episode reward: -1238.000, mean reward: -309.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 1020989.328125, mae: 2920.711487, accuracy: 0.273438, mean_q: -3364.201965, mean_eps: 0.503245\n",
      "  5526/50000: episode: 731, duration: 0.019s, episode steps:   4, steps per second: 216, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 781605.078125, mae: 2893.159363, accuracy: 0.218750, mean_q: -3363.436951, mean_eps: 0.502885\n",
      "  5529/50000: episode: 732, duration: 0.016s, episode steps:   3, steps per second: 184, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 1036842.500000, mae: 2925.084473, accuracy: 0.270833, mean_q: -3354.484945, mean_eps: 0.502570\n",
      "  5540/50000: episode: 733, duration: 0.045s, episode steps:  11, steps per second: 242, episode reward: -8231.000, mean reward: -748.273 [-999.000, -45.000], mean action: 1.636 [0.000, 3.000],  loss: 1044750.176136, mae: 2928.761563, accuracy: 0.232955, mean_q: -3341.451949, mean_eps: 0.501940\n",
      "  5545/50000: episode: 734, duration: 0.020s, episode steps:   5, steps per second: 256, episode reward: -2193.000, mean reward: -438.600 [-999.000, -32.000], mean action: 1.200 [0.000, 3.000],  loss: 692229.400000, mae: 2872.520264, accuracy: 0.243750, mean_q: -3341.429102, mean_eps: 0.501220\n",
      "  5551/50000: episode: 735, duration: 0.024s, episode steps:   6, steps per second: 252, episode reward: -3236.000, mean reward: -539.333 [-999.000, -60.000], mean action: 2.167 [1.000, 3.000],  loss: 1008399.812500, mae: 2896.299642, accuracy: 0.270833, mean_q: -3331.249878, mean_eps: 0.500725\n",
      "  5561/50000: episode: 736, duration: 0.037s, episode steps:  10, steps per second: 267, episode reward: -7232.000, mean reward: -723.200 [-999.000, -45.000], mean action: 1.100 [0.000, 3.000],  loss: 908681.393750, mae: 2909.464893, accuracy: 0.225000, mean_q: -3343.727368, mean_eps: 0.500005\n",
      "  5567/50000: episode: 737, duration: 0.023s, episode steps:   6, steps per second: 262, episode reward: -3192.000, mean reward: -532.000 [-999.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 871658.822917, mae: 2907.302165, accuracy: 0.244792, mean_q: -3371.649821, mean_eps: 0.499285\n",
      "  5573/50000: episode: 738, duration: 0.022s, episode steps:   6, steps per second: 278, episode reward: -3236.000, mean reward: -539.333 [-999.000, -60.000], mean action: 1.000 [0.000, 3.000],  loss: 942552.322917, mae: 2937.412679, accuracy: 0.250000, mean_q: -3363.066650, mean_eps: 0.498745\n",
      "  5576/50000: episode: 739, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 1178107.416667, mae: 2945.296305, accuracy: 0.322917, mean_q: -3340.890951, mean_eps: 0.498340\n",
      "  5583/50000: episode: 740, duration: 0.024s, episode steps:   7, steps per second: 296, episode reward: -4191.000, mean reward: -598.714 [-999.000, -45.000], mean action: 1.429 [0.000, 3.000],  loss: 1189940.017857, mae: 2921.312326, accuracy: 0.258929, mean_q: -3318.117920, mean_eps: 0.497890\n",
      "  5588/50000: episode: 741, duration: 0.018s, episode steps:   5, steps per second: 280, episode reward: -2193.000, mean reward: -438.600 [-999.000, -45.000], mean action: 1.400 [0.000, 2.000],  loss: 816139.325000, mae: 2848.097607, accuracy: 0.193750, mean_q: -3304.276904, mean_eps: 0.497350\n",
      "  5595/50000: episode: 742, duration: 0.024s, episode steps:   7, steps per second: 295, episode reward: -4235.000, mean reward: -605.000 [-999.000, -58.000], mean action: 1.714 [0.000, 3.000],  loss: 776066.906250, mae: 2845.233573, accuracy: 0.236607, mean_q: -3276.600725, mean_eps: 0.496810\n",
      "  5605/50000: episode: 743, duration: 0.033s, episode steps:  10, steps per second: 307, episode reward: -7188.000, mean reward: -718.800 [-999.000, -45.000], mean action: 1.800 [0.000, 3.000],  loss: 960359.196875, mae: 2899.058618, accuracy: 0.250000, mean_q: -3305.196167, mean_eps: 0.496045\n",
      "  5610/50000: episode: 744, duration: 0.018s, episode steps:   5, steps per second: 280, episode reward: -2193.000, mean reward: -438.600 [-999.000, -32.000], mean action: 1.800 [1.000, 3.000],  loss: 820931.612500, mae: 2871.708545, accuracy: 0.312500, mean_q: -3307.459717, mean_eps: 0.495370\n",
      "  5617/50000: episode: 745, duration: 0.025s, episode steps:   7, steps per second: 284, episode reward: -4235.000, mean reward: -605.000 [-999.000, -45.000], mean action: 2.000 [0.000, 3.000],  loss: 1036033.232143, mae: 2915.097831, accuracy: 0.241071, mean_q: -3335.143520, mean_eps: 0.494830\n",
      "  5629/50000: episode: 746, duration: 0.042s, episode steps:  12, steps per second: 286, episode reward: -9215.000, mean reward: -767.917 [-999.000, -32.000], mean action: 0.917 [0.000, 3.000],  loss: 986147.401042, mae: 2868.376302, accuracy: 0.291667, mean_q: -3274.187581, mean_eps: 0.493975\n",
      "  5632/50000: episode: 747, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 1030489.416667, mae: 2876.332194, accuracy: 0.270833, mean_q: -3223.255697, mean_eps: 0.493300\n",
      "  5637/50000: episode: 748, duration: 0.018s, episode steps:   5, steps per second: 278, episode reward: -2237.000, mean reward: -447.400 [-999.000, -45.000], mean action: 1.200 [0.000, 3.000],  loss: 936137.662500, mae: 2823.745166, accuracy: 0.312500, mean_q: -3228.610693, mean_eps: 0.492940\n",
      "  5642/50000: episode: 749, duration: 0.018s, episode steps:   5, steps per second: 285, episode reward: -2237.000, mean reward: -447.400 [-999.000, -60.000], mean action: 1.200 [0.000, 3.000],  loss: 723092.975000, mae: 2799.415137, accuracy: 0.268750, mean_q: -3247.362988, mean_eps: 0.492490\n",
      "  5649/50000: episode: 750, duration: 0.024s, episode steps:   7, steps per second: 293, episode reward: -4235.000, mean reward: -605.000 [-999.000, -58.000], mean action: 1.286 [0.000, 3.000],  loss: 971455.098214, mae: 2850.799142, accuracy: 0.245536, mean_q: -3282.672398, mean_eps: 0.491950\n",
      "  5653/50000: episode: 751, duration: 0.015s, episode steps:   4, steps per second: 266, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 1008006.421875, mae: 2864.330505, accuracy: 0.281250, mean_q: -3273.945557, mean_eps: 0.491455\n",
      "  5659/50000: episode: 752, duration: 0.021s, episode steps:   6, steps per second: 291, episode reward: -3192.000, mean reward: -532.000 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 775013.817708, mae: 2840.419759, accuracy: 0.229167, mean_q: -3281.443888, mean_eps: 0.491005\n",
      "  5670/50000: episode: 753, duration: 0.036s, episode steps:  11, steps per second: 305, episode reward: -8187.000, mean reward: -744.273 [-999.000, -58.000], mean action: 1.091 [0.000, 3.000],  loss: 1085327.727273, mae: 2893.295033, accuracy: 0.193182, mean_q: -3297.648082, mean_eps: 0.490240\n",
      "  5675/50000: episode: 754, duration: 0.018s, episode steps:   5, steps per second: 277, episode reward: -2237.000, mean reward: -447.400 [-999.000, -45.000], mean action: 1.400 [0.000, 3.000],  loss: 733182.762500, mae: 2819.772656, accuracy: 0.231250, mean_q: -3266.363184, mean_eps: 0.489520\n",
      "  5695/50000: episode: 755, duration: 0.069s, episode steps:  20, steps per second: 290, episode reward: -17207.000, mean reward: -860.350 [-999.000, -32.000], mean action: 2.100 [0.000, 3.000],  loss: 897458.898438, mae: 2852.010181, accuracy: 0.223438, mean_q: -3261.438049, mean_eps: 0.488395\n",
      "  5700/50000: episode: 756, duration: 0.018s, episode steps:   5, steps per second: 281, episode reward: -2193.000, mean reward: -438.600 [-999.000, -58.000], mean action: 1.800 [0.000, 3.000],  loss: 962431.350000, mae: 2876.981738, accuracy: 0.200000, mean_q: -3289.849609, mean_eps: 0.487270\n",
      "  5704/50000: episode: 757, duration: 0.015s, episode steps:   4, steps per second: 268, episode reward: -1238.000, mean reward: -309.500 [-999.000, -45.000], mean action: 1.250 [0.000, 3.000],  loss: 941139.359375, mae: 2864.121643, accuracy: 0.296875, mean_q: -3258.521667, mean_eps: 0.486865\n",
      "  5709/50000: episode: 758, duration: 0.017s, episode steps:   5, steps per second: 287, episode reward: -2193.000, mean reward: -438.600 [-999.000, -32.000], mean action: 1.200 [0.000, 3.000],  loss: 1124488.325000, mae: 2901.441309, accuracy: 0.187500, mean_q: -3290.964404, mean_eps: 0.486460\n",
      "  5714/50000: episode: 759, duration: 0.018s, episode steps:   5, steps per second: 281, episode reward: -2237.000, mean reward: -447.400 [-999.000, -58.000], mean action: 1.800 [0.000, 3.000],  loss: 1030586.825000, mae: 2873.280762, accuracy: 0.243750, mean_q: -3280.288330, mean_eps: 0.486010\n",
      "  5721/50000: episode: 760, duration: 0.023s, episode steps:   7, steps per second: 301, episode reward: -4191.000, mean reward: -598.714 [-999.000, -58.000], mean action: 2.286 [0.000, 3.000],  loss: 734335.205357, mae: 2809.401158, accuracy: 0.241071, mean_q: -3260.897914, mean_eps: 0.485470\n",
      "  5726/50000: episode: 761, duration: 0.017s, episode steps:   5, steps per second: 291, episode reward: -2193.000, mean reward: -438.600 [-999.000, -45.000], mean action: 1.400 [0.000, 3.000],  loss: 934112.200000, mae: 2844.519873, accuracy: 0.256250, mean_q: -3254.910205, mean_eps: 0.484930\n",
      "  5733/50000: episode: 762, duration: 0.025s, episode steps:   7, steps per second: 277, episode reward: -4191.000, mean reward: -598.714 [-999.000, -32.000], mean action: 1.143 [0.000, 3.000],  loss: 941909.370536, mae: 2820.940465, accuracy: 0.236607, mean_q: -3250.968262, mean_eps: 0.484390\n",
      "  5747/50000: episode: 763, duration: 0.048s, episode steps:  14, steps per second: 292, episode reward: -11184.000, mean reward: -798.857 [-999.000, -45.000], mean action: 1.643 [0.000, 3.000],  loss: 994946.707589, mae: 2830.707293, accuracy: 0.178571, mean_q: -3228.325928, mean_eps: 0.483445\n",
      "  5752/50000: episode: 764, duration: 0.018s, episode steps:   5, steps per second: 280, episode reward: -2193.000, mean reward: -438.600 [-999.000, -32.000], mean action: 1.800 [0.000, 3.000],  loss: 911991.937500, mae: 2807.586768, accuracy: 0.212500, mean_q: -3207.981104, mean_eps: 0.482590\n",
      "  5757/50000: episode: 765, duration: 0.018s, episode steps:   5, steps per second: 281, episode reward: -2237.000, mean reward: -447.400 [-999.000, -45.000], mean action: 1.600 [0.000, 3.000],  loss: 757535.412500, mae: 2790.621240, accuracy: 0.218750, mean_q: -3200.671143, mean_eps: 0.482140\n",
      "  5761/50000: episode: 766, duration: 0.015s, episode steps:   4, steps per second: 272, episode reward: -1238.000, mean reward: -309.500 [-999.000, -58.000], mean action: 1.250 [0.000, 2.000],  loss: 879779.140625, mae: 2803.750549, accuracy: 0.320312, mean_q: -3184.744141, mean_eps: 0.481735\n",
      "  5768/50000: episode: 767, duration: 0.024s, episode steps:   7, steps per second: 296, episode reward: -4191.000, mean reward: -598.714 [-999.000, -45.000], mean action: 1.429 [0.000, 3.000],  loss: 860215.776786, mae: 2818.451067, accuracy: 0.272321, mean_q: -3227.159040, mean_eps: 0.481240\n",
      "  5778/50000: episode: 768, duration: 0.033s, episode steps:  10, steps per second: 300, episode reward: -7217.000, mean reward: -721.700 [-999.000, -32.000], mean action: 1.200 [0.000, 3.000],  loss: 835992.718750, mae: 2805.608789, accuracy: 0.275000, mean_q: -3212.383203, mean_eps: 0.480475\n",
      "  5785/50000: episode: 769, duration: 0.031s, episode steps:   7, steps per second: 226, episode reward: -4235.000, mean reward: -605.000 [-999.000, -45.000], mean action: 1.714 [0.000, 3.000],  loss: 825157.375000, mae: 2812.314767, accuracy: 0.250000, mean_q: -3222.118373, mean_eps: 0.479710\n",
      "  5789/50000: episode: 770, duration: 0.020s, episode steps:   4, steps per second: 196, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 812300.562500, mae: 2819.353516, accuracy: 0.265625, mean_q: -3224.242920, mean_eps: 0.479215\n",
      "  5797/50000: episode: 771, duration: 0.038s, episode steps:   8, steps per second: 213, episode reward: -5234.000, mean reward: -654.250 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 809971.152344, mae: 2815.969269, accuracy: 0.281250, mean_q: -3251.382568, mean_eps: 0.478675\n",
      "  5801/50000: episode: 772, duration: 0.021s, episode steps:   4, steps per second: 189, episode reward: -1238.000, mean reward: -309.500 [-999.000, -58.000], mean action: 0.750 [0.000, 2.000],  loss: 887321.171875, mae: 2841.914734, accuracy: 0.242188, mean_q: -3266.201111, mean_eps: 0.478135\n",
      "  5810/50000: episode: 773, duration: 0.039s, episode steps:   9, steps per second: 233, episode reward: -6189.000, mean reward: -687.667 [-999.000, -45.000], mean action: 1.333 [0.000, 3.000],  loss: 785664.253472, mae: 2832.130778, accuracy: 0.215278, mean_q: -3256.928467, mean_eps: 0.477550\n",
      "  5824/50000: episode: 774, duration: 0.062s, episode steps:  14, steps per second: 225, episode reward: -11184.000, mean reward: -798.857 [-999.000, -32.000], mean action: 1.429 [0.000, 3.000],  loss: 983273.464286, mae: 2877.816929, accuracy: 0.241071, mean_q: -3275.014875, mean_eps: 0.476515\n",
      "  5827/50000: episode: 775, duration: 0.017s, episode steps:   3, steps per second: 180, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 751436.250000, mae: 2844.480062, accuracy: 0.239583, mean_q: -3263.083659, mean_eps: 0.475750\n",
      "  5830/50000: episode: 776, duration: 0.019s, episode steps:   3, steps per second: 159, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 730093.520833, mae: 2820.892822, accuracy: 0.177083, mean_q: -3277.022786, mean_eps: 0.475480\n",
      "  5836/50000: episode: 777, duration: 0.033s, episode steps:   6, steps per second: 184, episode reward: -3236.000, mean reward: -539.333 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 830287.447917, mae: 2827.130086, accuracy: 0.208333, mean_q: -3260.137858, mean_eps: 0.475075\n",
      "  5843/50000: episode: 778, duration: 0.037s, episode steps:   7, steps per second: 191, episode reward: -4191.000, mean reward: -598.714 [-999.000, -45.000], mean action: 1.429 [0.000, 3.000],  loss: 836088.919643, mae: 2824.807617, accuracy: 0.276786, mean_q: -3254.363281, mean_eps: 0.474490\n",
      "  5847/50000: episode: 779, duration: 0.024s, episode steps:   4, steps per second: 165, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 788139.015625, mae: 2830.140076, accuracy: 0.250000, mean_q: -3245.917969, mean_eps: 0.473995\n",
      "  5850/50000: episode: 780, duration: 0.020s, episode steps:   3, steps per second: 151, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 619396.229167, mae: 2822.595866, accuracy: 0.187500, mean_q: -3252.633870, mean_eps: 0.473680\n",
      "  5854/50000: episode: 781, duration: 0.021s, episode steps:   4, steps per second: 192, episode reward: -1238.000, mean reward: -309.500 [-999.000, -60.000], mean action: 1.500 [0.000, 3.000],  loss: 704784.093750, mae: 2832.113281, accuracy: 0.187500, mean_q: -3288.570984, mean_eps: 0.473365\n",
      "  5857/50000: episode: 782, duration: 0.018s, episode steps:   3, steps per second: 170, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 1099390.562500, mae: 2882.506429, accuracy: 0.218750, mean_q: -3278.551595, mean_eps: 0.473050\n",
      "  5863/50000: episode: 783, duration: 0.032s, episode steps:   6, steps per second: 186, episode reward: -3236.000, mean reward: -539.333 [-999.000, -58.000], mean action: 2.000 [0.000, 3.000],  loss: 1100628.822917, mae: 2884.937988, accuracy: 0.171875, mean_q: -3287.636434, mean_eps: 0.472645\n",
      "  5870/50000: episode: 784, duration: 0.032s, episode steps:   7, steps per second: 218, episode reward: -4235.000, mean reward: -605.000 [-999.000, -58.000], mean action: 1.000 [0.000, 3.000],  loss: 918850.830357, mae: 2832.700160, accuracy: 0.227679, mean_q: -3250.335589, mean_eps: 0.472060\n",
      "  5875/50000: episode: 785, duration: 0.025s, episode steps:   5, steps per second: 202, episode reward: -2193.000, mean reward: -438.600 [-999.000, -58.000], mean action: 1.200 [0.000, 3.000],  loss: 831210.762500, mae: 2838.723877, accuracy: 0.218750, mean_q: -3190.971826, mean_eps: 0.471520\n",
      "  5879/50000: episode: 786, duration: 0.020s, episode steps:   4, steps per second: 198, episode reward: -1238.000, mean reward: -309.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 925536.562500, mae: 2846.298462, accuracy: 0.187500, mean_q: -3238.583435, mean_eps: 0.471115\n",
      "  5884/50000: episode: 787, duration: 0.023s, episode steps:   5, steps per second: 219, episode reward: -2193.000, mean reward: -438.600 [-999.000, -32.000], mean action: 1.800 [0.000, 3.000],  loss: 701448.412500, mae: 2814.515576, accuracy: 0.231250, mean_q: -3248.801758, mean_eps: 0.470710\n",
      "  5887/50000: episode: 788, duration: 0.015s, episode steps:   3, steps per second: 198, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 944398.270833, mae: 2875.487712, accuracy: 0.156250, mean_q: -3295.017578, mean_eps: 0.470350\n",
      "  5895/50000: episode: 789, duration: 0.037s, episode steps:   8, steps per second: 214, episode reward: -5190.000, mean reward: -648.750 [-999.000, -32.000], mean action: 1.375 [0.000, 3.000],  loss: 936791.843750, mae: 2849.091492, accuracy: 0.234375, mean_q: -3279.677155, mean_eps: 0.469855\n",
      "  5901/50000: episode: 790, duration: 0.026s, episode steps:   6, steps per second: 231, episode reward: -3192.000, mean reward: -532.000 [-999.000, -45.000], mean action: 1.500 [0.000, 2.000],  loss: 911626.031250, mae: 2840.235596, accuracy: 0.192708, mean_q: -3250.457723, mean_eps: 0.469225\n",
      "  5909/50000: episode: 791, duration: 0.029s, episode steps:   8, steps per second: 271, episode reward: -5234.000, mean reward: -654.250 [-999.000, -45.000], mean action: 1.750 [0.000, 3.000],  loss: 713998.589844, mae: 2804.104248, accuracy: 0.273438, mean_q: -3232.607758, mean_eps: 0.468595\n",
      "  5917/50000: episode: 792, duration: 0.033s, episode steps:   8, steps per second: 245, episode reward: -5234.000, mean reward: -654.250 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 830610.695312, mae: 2825.498260, accuracy: 0.230469, mean_q: -3268.884155, mean_eps: 0.467875\n",
      "  5921/50000: episode: 793, duration: 0.019s, episode steps:   4, steps per second: 212, episode reward: -1223.000, mean reward: -305.750 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 999081.312500, mae: 2874.741333, accuracy: 0.171875, mean_q: -3283.867554, mean_eps: 0.467335\n",
      "  5942/50000: episode: 794, duration: 0.070s, episode steps:  21, steps per second: 301, episode reward: -18206.000, mean reward: -866.952 [-999.000, -58.000], mean action: 1.952 [0.000, 3.000],  loss: 950358.250000, mae: 2839.649809, accuracy: 0.190476, mean_q: -3236.674468, mean_eps: 0.466210\n",
      "  5946/50000: episode: 795, duration: 0.017s, episode steps:   4, steps per second: 239, episode reward: -1238.000, mean reward: -309.500 [-999.000, -45.000], mean action: 1.250 [0.000, 3.000],  loss: 787594.453125, mae: 2808.532776, accuracy: 0.257812, mean_q: -3221.689880, mean_eps: 0.465085\n",
      "  5962/50000: episode: 796, duration: 0.067s, episode steps:  16, steps per second: 240, episode reward: -13226.000, mean reward: -826.625 [-999.000, -58.000], mean action: 2.250 [0.000, 3.000],  loss: 927568.031250, mae: 2821.540741, accuracy: 0.255859, mean_q: -3200.990082, mean_eps: 0.464185\n",
      "  5979/50000: episode: 797, duration: 0.076s, episode steps:  17, steps per second: 224, episode reward: -14181.000, mean reward: -834.176 [-999.000, -32.000], mean action: 0.588 [0.000, 3.000],  loss: 874196.286765, mae: 2822.498837, accuracy: 0.240809, mean_q: -3224.809944, mean_eps: 0.462700\n",
      "  5983/50000: episode: 798, duration: 0.021s, episode steps:   4, steps per second: 188, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 766732.171875, mae: 2805.311646, accuracy: 0.257812, mean_q: -3237.329773, mean_eps: 0.461755\n",
      "  5988/50000: episode: 799, duration: 0.025s, episode steps:   5, steps per second: 198, episode reward: -2193.000, mean reward: -438.600 [-999.000, -58.000], mean action: 1.800 [0.000, 3.000],  loss: 637927.787500, mae: 2788.205127, accuracy: 0.231250, mean_q: -3245.210205, mean_eps: 0.461350\n",
      "  5991/50000: episode: 800, duration: 0.016s, episode steps:   3, steps per second: 184, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 894890.791667, mae: 2861.106771, accuracy: 0.187500, mean_q: -3257.000163, mean_eps: 0.460990\n",
      "  5996/50000: episode: 801, duration: 0.022s, episode steps:   5, steps per second: 224, episode reward: -2237.000, mean reward: -447.400 [-999.000, -45.000], mean action: 1.600 [0.000, 3.000],  loss: 781707.812500, mae: 2826.713818, accuracy: 0.250000, mean_q: -3272.158691, mean_eps: 0.460630\n",
      "  6000/50000: episode: 802, duration: 0.018s, episode steps:   4, steps per second: 228, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 683415.984375, mae: 2814.093567, accuracy: 0.234375, mean_q: -3276.770569, mean_eps: 0.460225\n",
      "  6007/50000: episode: 803, duration: 0.027s, episode steps:   7, steps per second: 259, episode reward: -4235.000, mean reward: -605.000 [-999.000, -60.000], mean action: 0.857 [0.000, 3.000],  loss: 766259.062500, mae: 2832.489676, accuracy: 0.236607, mean_q: -3262.328020, mean_eps: 0.459730\n",
      "  6012/50000: episode: 804, duration: 0.024s, episode steps:   5, steps per second: 207, episode reward: -2193.000, mean reward: -438.600 [-999.000, -32.000], mean action: 1.200 [0.000, 3.000],  loss: 1050911.662500, mae: 2879.318896, accuracy: 0.206250, mean_q: -3266.957275, mean_eps: 0.459190\n",
      "  6018/50000: episode: 805, duration: 0.030s, episode steps:   6, steps per second: 198, episode reward: -3236.000, mean reward: -539.333 [-999.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 1064078.593750, mae: 2865.327148, accuracy: 0.234375, mean_q: -3239.392537, mean_eps: 0.458695\n",
      "  6021/50000: episode: 806, duration: 0.016s, episode steps:   3, steps per second: 185, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 1.667 [0.000, 3.000],  loss: 694929.427083, mae: 2787.146973, accuracy: 0.187500, mean_q: -3206.403320, mean_eps: 0.458290\n",
      "  6026/50000: episode: 807, duration: 0.024s, episode steps:   5, steps per second: 208, episode reward: -2237.000, mean reward: -447.400 [-999.000, -45.000], mean action: 1.600 [0.000, 3.000],  loss: 909950.156250, mae: 2821.339209, accuracy: 0.281250, mean_q: -3179.137451, mean_eps: 0.457930\n",
      "  6031/50000: episode: 808, duration: 0.025s, episode steps:   5, steps per second: 203, episode reward: -2237.000, mean reward: -447.400 [-999.000, -45.000], mean action: 2.000 [1.000, 3.000],  loss: 835866.925000, mae: 2816.296338, accuracy: 0.200000, mean_q: -3191.307666, mean_eps: 0.457480\n",
      "  6036/50000: episode: 809, duration: 0.023s, episode steps:   5, steps per second: 215, episode reward: -2222.000, mean reward: -444.400 [-999.000, -32.000], mean action: 1.200 [0.000, 3.000],  loss: 986752.587500, mae: 2832.896680, accuracy: 0.281250, mean_q: -3177.679492, mean_eps: 0.457030\n",
      "  6042/50000: episode: 810, duration: 0.029s, episode steps:   6, steps per second: 205, episode reward: -3236.000, mean reward: -539.333 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 959604.052083, mae: 2847.272461, accuracy: 0.145833, mean_q: -3212.071574, mean_eps: 0.456535\n",
      "  6051/50000: episode: 811, duration: 0.044s, episode steps:   9, steps per second: 205, episode reward: -6218.000, mean reward: -690.889 [-999.000, -32.000], mean action: 0.889 [0.000, 3.000],  loss: 940520.173611, mae: 2808.615126, accuracy: 0.236111, mean_q: -3188.249485, mean_eps: 0.455860\n",
      "  6060/50000: episode: 812, duration: 0.040s, episode steps:   9, steps per second: 226, episode reward: -6233.000, mean reward: -692.556 [-999.000, -45.000], mean action: 1.556 [0.000, 3.000],  loss: 1023879.048611, mae: 2802.451850, accuracy: 0.187500, mean_q: -3161.868083, mean_eps: 0.455050\n",
      "  6072/50000: episode: 813, duration: 0.050s, episode steps:  12, steps per second: 240, episode reward: -9186.000, mean reward: -765.500 [-999.000, -45.000], mean action: 1.833 [0.000, 3.000],  loss: 773692.479167, mae: 2755.830444, accuracy: 0.216146, mean_q: -3151.470744, mean_eps: 0.454105\n",
      "  6076/50000: episode: 814, duration: 0.020s, episode steps:   4, steps per second: 201, episode reward: -1238.000, mean reward: -309.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 770640.250000, mae: 2767.175110, accuracy: 0.234375, mean_q: -3182.620361, mean_eps: 0.453385\n",
      "  6081/50000: episode: 815, duration: 0.022s, episode steps:   5, steps per second: 232, episode reward: -2193.000, mean reward: -438.600 [-999.000, -58.000], mean action: 1.800 [0.000, 3.000],  loss: 912608.862500, mae: 2817.494385, accuracy: 0.162500, mean_q: -3221.935449, mean_eps: 0.452980\n",
      "  6087/50000: episode: 816, duration: 0.024s, episode steps:   6, steps per second: 251, episode reward: -3192.000, mean reward: -532.000 [-999.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 901515.770833, mae: 2811.109131, accuracy: 0.234375, mean_q: -3222.422160, mean_eps: 0.452485\n",
      "  6093/50000: episode: 817, duration: 0.023s, episode steps:   6, steps per second: 263, episode reward: -3236.000, mean reward: -539.333 [-999.000, -60.000], mean action: 2.000 [0.000, 3.000],  loss: 908907.411458, mae: 2807.361857, accuracy: 0.229167, mean_q: -3214.494914, mean_eps: 0.451945\n",
      "  6096/50000: episode: 818, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 1318747.041667, mae: 2883.911621, accuracy: 0.187500, mean_q: -3216.178304, mean_eps: 0.451540\n",
      "  6104/50000: episode: 819, duration: 0.028s, episode steps:   8, steps per second: 289, episode reward: -5190.000, mean reward: -648.750 [-999.000, -58.000], mean action: 2.375 [0.000, 3.000],  loss: 811185.250000, mae: 2794.023895, accuracy: 0.222656, mean_q: -3191.375916, mean_eps: 0.451045\n",
      "  6113/50000: episode: 820, duration: 0.034s, episode steps:   9, steps per second: 266, episode reward: -6189.000, mean reward: -687.667 [-999.000, -32.000], mean action: 1.556 [0.000, 3.000],  loss: 964561.145833, mae: 2805.036404, accuracy: 0.211806, mean_q: -3181.828016, mean_eps: 0.450280\n",
      "  6117/50000: episode: 821, duration: 0.017s, episode steps:   4, steps per second: 239, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 2.250 [1.000, 3.000],  loss: 866681.218750, mae: 2785.206909, accuracy: 0.210938, mean_q: -3164.239746, mean_eps: 0.449695\n",
      "  6130/50000: episode: 822, duration: 0.045s, episode steps:  13, steps per second: 291, episode reward: -10185.000, mean reward: -783.462 [-999.000, -45.000], mean action: 1.385 [0.000, 3.000],  loss: 920299.788462, mae: 2768.801589, accuracy: 0.254808, mean_q: -3141.010892, mean_eps: 0.448930\n",
      "  6152/50000: episode: 823, duration: 0.081s, episode steps:  22, steps per second: 270, episode reward: -19220.000, mean reward: -873.636 [-999.000, -45.000], mean action: 1.864 [0.000, 3.000],  loss: 807759.468750, mae: 2776.115279, accuracy: 0.213068, mean_q: -3152.014260, mean_eps: 0.447355\n",
      "  6158/50000: episode: 824, duration: 0.023s, episode steps:   6, steps per second: 261, episode reward: -3236.000, mean reward: -539.333 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 765256.406250, mae: 2818.572754, accuracy: 0.229167, mean_q: -3212.426107, mean_eps: 0.446095\n",
      "  6163/50000: episode: 825, duration: 0.021s, episode steps:   5, steps per second: 238, episode reward: -2237.000, mean reward: -447.400 [-999.000, -58.000], mean action: 0.600 [0.000, 2.000],  loss: 932472.675000, mae: 2833.430811, accuracy: 0.218750, mean_q: -3251.051172, mean_eps: 0.445600\n",
      "  6169/50000: episode: 826, duration: 0.022s, episode steps:   6, steps per second: 268, episode reward: -3236.000, mean reward: -539.333 [-999.000, -60.000], mean action: 1.000 [0.000, 3.000],  loss: 988490.072917, mae: 2835.213542, accuracy: 0.161458, mean_q: -3240.657308, mean_eps: 0.445105\n",
      "  6172/50000: episode: 827, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 907280.729167, mae: 2828.720947, accuracy: 0.208333, mean_q: -3206.887370, mean_eps: 0.444700\n",
      "  6180/50000: episode: 828, duration: 0.028s, episode steps:   8, steps per second: 282, episode reward: -5190.000, mean reward: -648.750 [-999.000, -32.000], mean action: 1.375 [1.000, 3.000],  loss: 833401.007812, mae: 2779.860168, accuracy: 0.214844, mean_q: -3182.131958, mean_eps: 0.444205\n",
      "  6184/50000: episode: 829, duration: 0.015s, episode steps:   4, steps per second: 274, episode reward: -1238.000, mean reward: -309.500 [-999.000, -60.000], mean action: 2.250 [1.000, 3.000],  loss: 811997.468750, mae: 2781.123596, accuracy: 0.210938, mean_q: -3176.024841, mean_eps: 0.443665\n",
      "  6190/50000: episode: 830, duration: 0.023s, episode steps:   6, steps per second: 264, episode reward: -3192.000, mean reward: -532.000 [-999.000, -58.000], mean action: 2.333 [1.000, 3.000],  loss: 834490.088542, mae: 2774.282511, accuracy: 0.192708, mean_q: -3173.067993, mean_eps: 0.443215\n",
      "  6193/50000: episode: 831, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 672947.979167, mae: 2764.046468, accuracy: 0.281250, mean_q: -3169.600586, mean_eps: 0.442810\n",
      "  6197/50000: episode: 832, duration: 0.017s, episode steps:   4, steps per second: 233, episode reward: -1238.000, mean reward: -309.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 747116.382812, mae: 2776.239868, accuracy: 0.187500, mean_q: -3190.694458, mean_eps: 0.442495\n",
      "  6203/50000: episode: 833, duration: 0.024s, episode steps:   6, steps per second: 253, episode reward: -3236.000, mean reward: -539.333 [-999.000, -60.000], mean action: 1.500 [0.000, 3.000],  loss: 855442.104167, mae: 2809.073364, accuracy: 0.166667, mean_q: -3211.104248, mean_eps: 0.442045\n",
      "  6224/50000: episode: 834, duration: 0.071s, episode steps:  21, steps per second: 294, episode reward: -18177.000, mean reward: -865.571 [-999.000, -45.000], mean action: 2.000 [0.000, 3.000],  loss: 845767.250000, mae: 2822.435977, accuracy: 0.209821, mean_q: -3237.557571, mean_eps: 0.440830\n",
      "  6234/50000: episode: 835, duration: 0.036s, episode steps:  10, steps per second: 281, episode reward: -7232.000, mean reward: -723.200 [-999.000, -45.000], mean action: 1.800 [0.000, 3.000],  loss: 822286.909375, mae: 2804.290649, accuracy: 0.196875, mean_q: -3216.221313, mean_eps: 0.439435\n",
      "  6242/50000: episode: 836, duration: 0.028s, episode steps:   8, steps per second: 290, episode reward: -5234.000, mean reward: -654.250 [-999.000, -60.000], mean action: 2.250 [0.000, 3.000],  loss: 760592.671875, mae: 2783.746094, accuracy: 0.214844, mean_q: -3195.135834, mean_eps: 0.438625\n",
      "  6252/50000: episode: 837, duration: 0.036s, episode steps:  10, steps per second: 282, episode reward: -7188.000, mean reward: -718.800 [-999.000, -58.000], mean action: 0.900 [0.000, 3.000],  loss: 914163.921875, mae: 2812.388135, accuracy: 0.150000, mean_q: -3199.020410, mean_eps: 0.437815\n",
      "  6257/50000: episode: 838, duration: 0.022s, episode steps:   5, steps per second: 231, episode reward: -2237.000, mean reward: -447.400 [-999.000, -45.000], mean action: 1.800 [0.000, 3.000],  loss: 945117.350000, mae: 2831.633252, accuracy: 0.187500, mean_q: -3202.157422, mean_eps: 0.437140\n",
      "  6262/50000: episode: 839, duration: 0.018s, episode steps:   5, steps per second: 274, episode reward: -2237.000, mean reward: -447.400 [-999.000, -45.000], mean action: 1.400 [0.000, 3.000],  loss: 989115.112500, mae: 2810.875732, accuracy: 0.168750, mean_q: -3203.516504, mean_eps: 0.436690\n",
      "  6276/50000: episode: 840, duration: 0.052s, episode steps:  14, steps per second: 270, episode reward: -11213.000, mean reward: -800.929 [-999.000, -32.000], mean action: 0.714 [0.000, 3.000],  loss: 839591.678571, mae: 2801.718541, accuracy: 0.194196, mean_q: -3191.508597, mean_eps: 0.435835\n",
      "  6282/50000: episode: 841, duration: 0.028s, episode steps:   6, steps per second: 216, episode reward: -3236.000, mean reward: -539.333 [-999.000, -60.000], mean action: 2.000 [0.000, 3.000],  loss: 936919.786458, mae: 2816.447469, accuracy: 0.192708, mean_q: -3187.776123, mean_eps: 0.434935\n",
      "  6292/50000: episode: 842, duration: 0.045s, episode steps:  10, steps per second: 223, episode reward: -7188.000, mean reward: -718.800 [-999.000, -32.000], mean action: 1.800 [0.000, 3.000],  loss: 837538.412500, mae: 2783.688550, accuracy: 0.193750, mean_q: -3187.237573, mean_eps: 0.434215\n",
      "  6297/50000: episode: 843, duration: 0.026s, episode steps:   5, steps per second: 192, episode reward: -2193.000, mean reward: -438.600 [-999.000, -58.000], mean action: 2.200 [1.000, 3.000],  loss: 710019.112500, mae: 2778.351172, accuracy: 0.212500, mean_q: -3183.181445, mean_eps: 0.433540\n",
      "  6308/50000: episode: 844, duration: 0.043s, episode steps:  11, steps per second: 254, episode reward: -8187.000, mean reward: -744.273 [-999.000, -58.000], mean action: 0.545 [0.000, 3.000],  loss: 967065.426136, mae: 2837.440629, accuracy: 0.204545, mean_q: -3222.166792, mean_eps: 0.432820\n",
      "  6315/50000: episode: 845, duration: 0.029s, episode steps:   7, steps per second: 243, episode reward: -4235.000, mean reward: -605.000 [-999.000, -45.000], mean action: 1.429 [0.000, 3.000],  loss: 1037398.937500, mae: 2841.648961, accuracy: 0.245536, mean_q: -3216.969482, mean_eps: 0.432010\n",
      "  6322/50000: episode: 846, duration: 0.027s, episode steps:   7, steps per second: 258, episode reward: -4191.000, mean reward: -598.714 [-999.000, -32.000], mean action: 1.571 [0.000, 3.000],  loss: 877450.156250, mae: 2797.392369, accuracy: 0.174107, mean_q: -3194.801444, mean_eps: 0.431380\n",
      "  6325/50000: episode: 847, duration: 0.013s, episode steps:   3, steps per second: 230, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 784251.864583, mae: 2781.242676, accuracy: 0.218750, mean_q: -3183.902913, mean_eps: 0.430930\n",
      "  6333/50000: episode: 848, duration: 0.028s, episode steps:   8, steps per second: 282, episode reward: -5234.000, mean reward: -654.250 [-999.000, -45.000], mean action: 1.875 [0.000, 3.000],  loss: 780556.078125, mae: 2770.470703, accuracy: 0.234375, mean_q: -3159.080078, mean_eps: 0.430435\n",
      "  6336/50000: episode: 849, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.333 [0.000, 3.000],  loss: 1075279.968750, mae: 2791.928711, accuracy: 0.260417, mean_q: -3163.385335, mean_eps: 0.429940\n",
      "  6341/50000: episode: 850, duration: 0.020s, episode steps:   5, steps per second: 250, episode reward: -2222.000, mean reward: -444.400 [-999.000, -32.000], mean action: 1.400 [0.000, 3.000],  loss: 634062.071875, mae: 2753.186914, accuracy: 0.293750, mean_q: -3154.632959, mean_eps: 0.429580\n",
      "  6346/50000: episode: 851, duration: 0.022s, episode steps:   5, steps per second: 231, episode reward: -2193.000, mean reward: -438.600 [-999.000, -32.000], mean action: 1.400 [0.000, 3.000],  loss: 589067.687500, mae: 2743.095459, accuracy: 0.275000, mean_q: -3163.068262, mean_eps: 0.429130\n",
      "  6350/50000: episode: 852, duration: 0.017s, episode steps:   4, steps per second: 234, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 1103368.765625, mae: 2832.582153, accuracy: 0.210938, mean_q: -3184.315369, mean_eps: 0.428725\n",
      "  6354/50000: episode: 853, duration: 0.016s, episode steps:   4, steps per second: 245, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 734065.281250, mae: 2779.231323, accuracy: 0.226562, mean_q: -3174.760437, mean_eps: 0.428365\n",
      "  6357/50000: episode: 854, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.333 [0.000, 3.000],  loss: 853665.375000, mae: 2791.054281, accuracy: 0.218750, mean_q: -3172.217611, mean_eps: 0.428050\n",
      "  6361/50000: episode: 855, duration: 0.015s, episode steps:   4, steps per second: 273, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.750 [0.000, 3.000],  loss: 870046.125000, mae: 2798.572510, accuracy: 0.203125, mean_q: -3177.421143, mean_eps: 0.427735\n",
      "  6368/50000: episode: 856, duration: 0.027s, episode steps:   7, steps per second: 264, episode reward: -4235.000, mean reward: -605.000 [-999.000, -60.000], mean action: 2.143 [0.000, 3.000],  loss: 761175.125000, mae: 2786.442208, accuracy: 0.191964, mean_q: -3169.356445, mean_eps: 0.427240\n",
      "  6373/50000: episode: 857, duration: 0.018s, episode steps:   5, steps per second: 279, episode reward: -2237.000, mean reward: -447.400 [-999.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 770751.518750, mae: 2781.281348, accuracy: 0.162500, mean_q: -3191.481494, mean_eps: 0.426700\n",
      "  6381/50000: episode: 858, duration: 0.030s, episode steps:   8, steps per second: 269, episode reward: -5190.000, mean reward: -648.750 [-999.000, -32.000], mean action: 1.875 [0.000, 3.000],  loss: 812915.312500, mae: 2810.117462, accuracy: 0.250000, mean_q: -3188.867889, mean_eps: 0.426115\n",
      "  6389/50000: episode: 859, duration: 0.028s, episode steps:   8, steps per second: 286, episode reward: -5190.000, mean reward: -648.750 [-999.000, -45.000], mean action: 1.250 [0.000, 3.000],  loss: 927194.054688, mae: 2813.604279, accuracy: 0.214844, mean_q: -3169.503174, mean_eps: 0.425395\n",
      "  6398/50000: episode: 860, duration: 0.033s, episode steps:   9, steps per second: 276, episode reward: -6189.000, mean reward: -687.667 [-999.000, -32.000], mean action: 1.444 [1.000, 3.000],  loss: 911258.020833, mae: 2807.749023, accuracy: 0.263889, mean_q: -3165.711697, mean_eps: 0.424630\n",
      "  6410/50000: episode: 861, duration: 0.042s, episode steps:  12, steps per second: 288, episode reward: -9230.000, mean reward: -769.167 [-999.000, -45.000], mean action: 1.750 [0.000, 3.000],  loss: 903131.812500, mae: 2791.276164, accuracy: 0.242188, mean_q: -3149.116109, mean_eps: 0.423685\n",
      "  6414/50000: episode: 862, duration: 0.015s, episode steps:   4, steps per second: 263, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 605081.335938, mae: 2750.186890, accuracy: 0.257812, mean_q: -3174.486694, mean_eps: 0.422965\n",
      "  6428/50000: episode: 863, duration: 0.047s, episode steps:  14, steps per second: 296, episode reward: -11184.000, mean reward: -798.857 [-999.000, -45.000], mean action: 1.643 [0.000, 3.000],  loss: 982009.107143, mae: 2798.454119, accuracy: 0.232143, mean_q: -3153.157837, mean_eps: 0.422155\n",
      "  6431/50000: episode: 864, duration: 0.014s, episode steps:   3, steps per second: 222, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 737965.312500, mae: 2753.928385, accuracy: 0.177083, mean_q: -3135.319661, mean_eps: 0.421390\n",
      "  6434/50000: episode: 865, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 883491.062500, mae: 2781.934001, accuracy: 0.229167, mean_q: -3165.738363, mean_eps: 0.421120\n",
      "  6441/50000: episode: 866, duration: 0.027s, episode steps:   7, steps per second: 261, episode reward: -4220.000, mean reward: -602.857 [-999.000, -32.000], mean action: 0.857 [0.000, 3.000],  loss: 911813.883929, mae: 2783.669852, accuracy: 0.258929, mean_q: -3154.525286, mean_eps: 0.420670\n",
      "  6454/50000: episode: 867, duration: 0.046s, episode steps:  13, steps per second: 284, episode reward: -10185.000, mean reward: -783.462 [-999.000, -32.000], mean action: 1.154 [0.000, 3.000],  loss: 833566.014423, mae: 2785.876540, accuracy: 0.230769, mean_q: -3177.148869, mean_eps: 0.419770\n",
      "  6458/50000: episode: 868, duration: 0.018s, episode steps:   4, steps per second: 228, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.750 [0.000, 3.000],  loss: 983040.296875, mae: 2793.572266, accuracy: 0.195312, mean_q: -3184.934631, mean_eps: 0.419005\n",
      "  6464/50000: episode: 869, duration: 0.023s, episode steps:   6, steps per second: 264, episode reward: -3192.000, mean reward: -532.000 [-999.000, -32.000], mean action: 1.000 [0.000, 3.000],  loss: 809096.093750, mae: 2768.568807, accuracy: 0.234375, mean_q: -3154.357422, mean_eps: 0.418555\n",
      "  6475/50000: episode: 870, duration: 0.041s, episode steps:  11, steps per second: 271, episode reward: -8231.000, mean reward: -748.273 [-999.000, -60.000], mean action: 1.091 [0.000, 3.000],  loss: 738983.235795, mae: 2762.686745, accuracy: 0.204545, mean_q: -3181.194269, mean_eps: 0.417790\n",
      "  6478/50000: episode: 871, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 578261.437500, mae: 2750.119222, accuracy: 0.114583, mean_q: -3217.789469, mean_eps: 0.417160\n",
      "  6481/50000: episode: 872, duration: 0.012s, episode steps:   3, steps per second: 255, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 708201.333333, mae: 2790.366862, accuracy: 0.291667, mean_q: -3180.030111, mean_eps: 0.416890\n",
      "  6490/50000: episode: 873, duration: 0.034s, episode steps:   9, steps per second: 261, episode reward: -6233.000, mean reward: -692.556 [-999.000, -45.000], mean action: 1.778 [0.000, 3.000],  loss: 759120.937500, mae: 2805.041070, accuracy: 0.211806, mean_q: -3228.975966, mean_eps: 0.416350\n",
      "  6493/50000: episode: 874, duration: 0.014s, episode steps:   3, steps per second: 211, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 926442.489583, mae: 2845.308594, accuracy: 0.156250, mean_q: -3240.174561, mean_eps: 0.415810\n",
      "  6503/50000: episode: 875, duration: 0.037s, episode steps:  10, steps per second: 270, episode reward: -7232.000, mean reward: -723.200 [-999.000, -58.000], mean action: 2.100 [0.000, 3.000],  loss: 890591.825000, mae: 2818.711475, accuracy: 0.237500, mean_q: -3209.965283, mean_eps: 0.415225\n",
      "  6511/50000: episode: 876, duration: 0.028s, episode steps:   8, steps per second: 283, episode reward: -5234.000, mean reward: -654.250 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 925303.660156, mae: 2817.167877, accuracy: 0.230469, mean_q: -3186.023041, mean_eps: 0.414415\n",
      "  6522/50000: episode: 877, duration: 0.043s, episode steps:  11, steps per second: 258, episode reward: -8187.000, mean reward: -744.273 [-999.000, -32.000], mean action: 0.818 [0.000, 3.000],  loss: 888425.397727, mae: 2791.363991, accuracy: 0.221591, mean_q: -3172.701150, mean_eps: 0.413560\n",
      "  6526/50000: episode: 878, duration: 0.017s, episode steps:   4, steps per second: 230, episode reward: -1238.000, mean reward: -309.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 862297.625000, mae: 2798.925598, accuracy: 0.179688, mean_q: -3165.002563, mean_eps: 0.412885\n",
      "  6535/50000: episode: 879, duration: 0.038s, episode steps:   9, steps per second: 239, episode reward: -6233.000, mean reward: -692.556 [-999.000, -60.000], mean action: 2.111 [0.000, 3.000],  loss: 888789.388889, mae: 2794.048557, accuracy: 0.149306, mean_q: -3164.525228, mean_eps: 0.412300\n",
      "  6548/50000: episode: 880, duration: 0.054s, episode steps:  13, steps per second: 240, episode reward: -10214.000, mean reward: -785.692 [-999.000, -32.000], mean action: 1.308 [0.000, 3.000],  loss: 978817.673077, mae: 2795.552340, accuracy: 0.139423, mean_q: -3152.780536, mean_eps: 0.411310\n",
      "  6552/50000: episode: 881, duration: 0.023s, episode steps:   4, steps per second: 172, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.750 [0.000, 3.000],  loss: 909315.015625, mae: 2764.695435, accuracy: 0.234375, mean_q: -3122.291016, mean_eps: 0.410545\n",
      "  6561/50000: episode: 882, duration: 0.037s, episode steps:   9, steps per second: 246, episode reward: -6189.000, mean reward: -687.667 [-999.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 835774.506944, mae: 2752.067139, accuracy: 0.211806, mean_q: -3122.370063, mean_eps: 0.409960\n",
      "  6568/50000: episode: 883, duration: 0.032s, episode steps:   7, steps per second: 216, episode reward: -4235.000, mean reward: -605.000 [-999.000, -45.000], mean action: 1.714 [0.000, 3.000],  loss: 813568.821429, mae: 2744.122977, accuracy: 0.183036, mean_q: -3122.523263, mean_eps: 0.409240\n",
      "  6571/50000: episode: 884, duration: 0.016s, episode steps:   3, steps per second: 185, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.333 [0.000, 3.000],  loss: 894012.052083, mae: 2761.675944, accuracy: 0.177083, mean_q: -3127.944173, mean_eps: 0.408790\n",
      "  6576/50000: episode: 885, duration: 0.023s, episode steps:   5, steps per second: 215, episode reward: -2237.000, mean reward: -447.400 [-999.000, -45.000], mean action: 2.000 [1.000, 3.000],  loss: 830738.268750, mae: 2766.415674, accuracy: 0.206250, mean_q: -3128.054932, mean_eps: 0.408430\n",
      "  6579/50000: episode: 886, duration: 0.016s, episode steps:   3, steps per second: 190, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 879150.979167, mae: 2761.857747, accuracy: 0.135417, mean_q: -3118.714762, mean_eps: 0.408070\n",
      "  6614/50000: episode: 887, duration: 0.150s, episode steps:  35, steps per second: 233, episode reward: -32207.000, mean reward: -920.200 [-999.000, -45.000], mean action: 1.629 [0.000, 3.000],  loss: 810840.435714, mae: 2779.007959, accuracy: 0.206250, mean_q: -3150.270480, mean_eps: 0.406360\n",
      "  6625/50000: episode: 888, duration: 0.054s, episode steps:  11, steps per second: 205, episode reward: -8187.000, mean reward: -744.273 [-999.000, -32.000], mean action: 1.818 [0.000, 3.000],  loss: 869201.144886, mae: 2779.736528, accuracy: 0.224432, mean_q: -3148.609708, mean_eps: 0.404290\n",
      "  6649/50000: episode: 889, duration: 0.097s, episode steps:  24, steps per second: 248, episode reward: -21174.000, mean reward: -882.250 [-999.000, -45.000], mean action: 1.875 [0.000, 3.000],  loss: 814900.516927, mae: 2785.703166, accuracy: 0.214844, mean_q: -3176.758209, mean_eps: 0.402715\n",
      "  6652/50000: episode: 890, duration: 0.015s, episode steps:   3, steps per second: 202, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 943990.208333, mae: 2795.262695, accuracy: 0.270833, mean_q: -3170.324056, mean_eps: 0.401500\n",
      "  6656/50000: episode: 891, duration: 0.018s, episode steps:   4, steps per second: 222, episode reward: -1238.000, mean reward: -309.500 [-999.000, -58.000], mean action: 1.250 [0.000, 2.000],  loss: 1281426.453125, mae: 2824.177002, accuracy: 0.273438, mean_q: -3149.533020, mean_eps: 0.401185\n",
      "  6661/50000: episode: 892, duration: 0.022s, episode steps:   5, steps per second: 229, episode reward: -2237.000, mean reward: -447.400 [-999.000, -60.000], mean action: 1.800 [0.000, 3.000],  loss: 962004.675000, mae: 2757.995020, accuracy: 0.193750, mean_q: -3119.923438, mean_eps: 0.400780\n",
      "  6665/50000: episode: 893, duration: 0.016s, episode steps:   4, steps per second: 253, episode reward: -1238.000, mean reward: -309.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 782230.140625, mae: 2723.088867, accuracy: 0.234375, mean_q: -3085.474487, mean_eps: 0.400375\n",
      "  6674/50000: episode: 894, duration: 0.034s, episode steps:   9, steps per second: 263, episode reward: -6218.000, mean reward: -690.889 [-999.000, -32.000], mean action: 1.222 [0.000, 3.000],  loss: 754939.732639, mae: 2705.152235, accuracy: 0.218750, mean_q: -3088.500461, mean_eps: 0.399790\n",
      "  6693/50000: episode: 895, duration: 0.078s, episode steps:  19, steps per second: 242, episode reward: -16223.000, mean reward: -853.842 [-999.000, -45.000], mean action: 1.158 [0.000, 3.000],  loss: 836312.597039, mae: 2757.162829, accuracy: 0.197368, mean_q: -3128.330361, mean_eps: 0.398530\n",
      "  6703/50000: episode: 896, duration: 0.051s, episode steps:  10, steps per second: 197, episode reward: -7188.000, mean reward: -718.800 [-999.000, -32.000], mean action: 1.700 [0.000, 3.000],  loss: 661919.215625, mae: 2739.986719, accuracy: 0.221875, mean_q: -3151.245605, mean_eps: 0.397225\n",
      "  6708/50000: episode: 897, duration: 0.025s, episode steps:   5, steps per second: 203, episode reward: -2237.000, mean reward: -447.400 [-999.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 650710.137500, mae: 2772.408350, accuracy: 0.175000, mean_q: -3188.763086, mean_eps: 0.396550\n",
      "  6720/50000: episode: 898, duration: 0.050s, episode steps:  12, steps per second: 242, episode reward: -9230.000, mean reward: -769.167 [-999.000, -58.000], mean action: 0.500 [0.000, 3.000],  loss: 765141.250000, mae: 2779.902893, accuracy: 0.231771, mean_q: -3205.883972, mean_eps: 0.395785\n",
      "  6724/50000: episode: 899, duration: 0.019s, episode steps:   4, steps per second: 216, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 479464.062500, mae: 2738.616333, accuracy: 0.218750, mean_q: -3201.717468, mean_eps: 0.395065\n",
      "  6736/50000: episode: 900, duration: 0.048s, episode steps:  12, steps per second: 251, episode reward: -9186.000, mean reward: -765.500 [-999.000, -32.000], mean action: 0.583 [0.000, 3.000],  loss: 802910.997396, mae: 2806.180420, accuracy: 0.236979, mean_q: -3214.979594, mean_eps: 0.394345\n",
      "  6748/50000: episode: 901, duration: 0.050s, episode steps:  12, steps per second: 240, episode reward: -9230.000, mean reward: -769.167 [-999.000, -60.000], mean action: 2.250 [0.000, 3.000],  loss: 785057.290365, mae: 2791.252421, accuracy: 0.187500, mean_q: -3200.799886, mean_eps: 0.393265\n",
      "  6751/50000: episode: 902, duration: 0.018s, episode steps:   3, steps per second: 170, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 991269.062500, mae: 2806.684001, accuracy: 0.291667, mean_q: -3195.018880, mean_eps: 0.392590\n",
      "  6760/50000: episode: 903, duration: 0.044s, episode steps:   9, steps per second: 204, episode reward: -6189.000, mean reward: -687.667 [-999.000, -58.000], mean action: 2.444 [0.000, 3.000],  loss: 904770.572917, mae: 2800.706190, accuracy: 0.177083, mean_q: -3195.678684, mean_eps: 0.392050\n",
      "  6763/50000: episode: 904, duration: 0.016s, episode steps:   3, steps per second: 185, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 662476.625000, mae: 2784.805501, accuracy: 0.197917, mean_q: -3180.719238, mean_eps: 0.391510\n",
      "  6766/50000: episode: 905, duration: 0.033s, episode steps:   3, steps per second:  91, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 629816.718750, mae: 2738.083252, accuracy: 0.281250, mean_q: -3179.003581, mean_eps: 0.391240\n",
      "  6769/50000: episode: 906, duration: 0.024s, episode steps:   3, steps per second: 127, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 776244.729167, mae: 2766.292480, accuracy: 0.218750, mean_q: -3176.525146, mean_eps: 0.390970\n",
      "  6773/50000: episode: 907, duration: 0.023s, episode steps:   4, steps per second: 176, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.750 [0.000, 3.000],  loss: 868715.734375, mae: 2818.022766, accuracy: 0.171875, mean_q: -3204.783997, mean_eps: 0.390655\n",
      "  6776/50000: episode: 908, duration: 0.016s, episode steps:   3, steps per second: 192, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 698672.625000, mae: 2787.472982, accuracy: 0.177083, mean_q: -3212.541748, mean_eps: 0.390340\n",
      "  6785/50000: episode: 909, duration: 0.036s, episode steps:   9, steps per second: 252, episode reward: -6233.000, mean reward: -692.556 [-999.000, -45.000], mean action: 1.222 [0.000, 3.000],  loss: 1049493.729167, mae: 2830.135905, accuracy: 0.194444, mean_q: -3204.572456, mean_eps: 0.389800\n",
      "  6792/50000: episode: 910, duration: 0.032s, episode steps:   7, steps per second: 222, episode reward: -4191.000, mean reward: -598.714 [-999.000, -45.000], mean action: 1.429 [0.000, 3.000],  loss: 641062.397321, mae: 2771.467215, accuracy: 0.174107, mean_q: -3181.655448, mean_eps: 0.389080\n",
      "  6797/50000: episode: 911, duration: 0.026s, episode steps:   5, steps per second: 194, episode reward: -2237.000, mean reward: -447.400 [-999.000, -58.000], mean action: 1.800 [0.000, 3.000],  loss: 844856.775000, mae: 2789.117090, accuracy: 0.200000, mean_q: -3191.942432, mean_eps: 0.388540\n",
      "  6803/50000: episode: 912, duration: 0.033s, episode steps:   6, steps per second: 182, episode reward: -3236.000, mean reward: -539.333 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 685764.927083, mae: 2782.594889, accuracy: 0.208333, mean_q: -3193.219767, mean_eps: 0.388045\n",
      "  6827/50000: episode: 913, duration: 0.106s, episode steps:  24, steps per second: 227, episode reward: -21218.000, mean reward: -884.083 [-999.000, -45.000], mean action: 1.792 [0.000, 3.000],  loss: 809391.194010, mae: 2801.039602, accuracy: 0.220052, mean_q: -3214.892822, mean_eps: 0.386695\n",
      "  6854/50000: episode: 914, duration: 0.088s, episode steps:  27, steps per second: 307, episode reward: -24215.000, mean reward: -896.852 [-999.000, -58.000], mean action: 2.444 [0.000, 3.000],  loss: 804057.627315, mae: 2801.119774, accuracy: 0.164352, mean_q: -3205.510507, mean_eps: 0.384400\n",
      "  6859/50000: episode: 915, duration: 0.018s, episode steps:   5, steps per second: 285, episode reward: -2237.000, mean reward: -447.400 [-999.000, -58.000], mean action: 1.200 [0.000, 3.000],  loss: 949384.212500, mae: 2828.613721, accuracy: 0.181250, mean_q: -3199.573486, mean_eps: 0.382960\n",
      "  6862/50000: episode: 916, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 807866.052083, mae: 2810.834798, accuracy: 0.166667, mean_q: -3219.152669, mean_eps: 0.382600\n",
      "  6866/50000: episode: 917, duration: 0.016s, episode steps:   4, steps per second: 252, episode reward: -1223.000, mean reward: -305.750 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 846239.460938, mae: 2803.933960, accuracy: 0.203125, mean_q: -3218.718323, mean_eps: 0.382285\n",
      "  6874/50000: episode: 918, duration: 0.029s, episode steps:   8, steps per second: 276, episode reward: -5190.000, mean reward: -648.750 [-999.000, -32.000], mean action: 1.875 [0.000, 3.000],  loss: 957319.757812, mae: 2826.170715, accuracy: 0.187500, mean_q: -3219.987305, mean_eps: 0.381745\n",
      "  6877/50000: episode: 919, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 1106093.916667, mae: 2826.744222, accuracy: 0.187500, mean_q: -3194.126628, mean_eps: 0.381250\n",
      "  6882/50000: episode: 920, duration: 0.018s, episode steps:   5, steps per second: 271, episode reward: -2237.000, mean reward: -447.400 [-999.000, -60.000], mean action: 1.200 [0.000, 3.000],  loss: 918245.137500, mae: 2800.728662, accuracy: 0.200000, mean_q: -3163.071582, mean_eps: 0.380890\n",
      "  6887/50000: episode: 921, duration: 0.019s, episode steps:   5, steps per second: 266, episode reward: -2237.000, mean reward: -447.400 [-999.000, -45.000], mean action: 1.600 [0.000, 3.000],  loss: 887352.212500, mae: 2770.867529, accuracy: 0.225000, mean_q: -3142.095898, mean_eps: 0.380440\n",
      "  6910/50000: episode: 922, duration: 0.085s, episode steps:  23, steps per second: 269, episode reward: -20219.000, mean reward: -879.087 [-999.000, -60.000], mean action: 2.478 [0.000, 3.000],  loss: 897689.407609, mae: 2772.041143, accuracy: 0.197011, mean_q: -3138.991391, mean_eps: 0.379180\n",
      "  6914/50000: episode: 923, duration: 0.017s, episode steps:   4, steps per second: 234, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 1091255.296875, mae: 2765.859009, accuracy: 0.289062, mean_q: -3106.034790, mean_eps: 0.377965\n",
      "  6919/50000: episode: 924, duration: 0.021s, episode steps:   5, steps per second: 238, episode reward: -2193.000, mean reward: -438.600 [-999.000, -32.000], mean action: 1.800 [0.000, 3.000],  loss: 874409.106250, mae: 2752.234229, accuracy: 0.193750, mean_q: -3105.571387, mean_eps: 0.377560\n",
      "  6932/50000: episode: 925, duration: 0.049s, episode steps:  13, steps per second: 267, episode reward: -10229.000, mean reward: -786.846 [-999.000, -45.000], mean action: 1.385 [0.000, 3.000],  loss: 738558.091346, mae: 2724.561749, accuracy: 0.221154, mean_q: -3095.234675, mean_eps: 0.376750\n",
      "  6944/50000: episode: 926, duration: 0.050s, episode steps:  12, steps per second: 238, episode reward: -9186.000, mean reward: -765.500 [-999.000, -58.000], mean action: 0.750 [0.000, 3.000],  loss: 681869.757812, mae: 2769.176371, accuracy: 0.177083, mean_q: -3167.412333, mean_eps: 0.375625\n",
      "  6954/50000: episode: 927, duration: 0.037s, episode steps:  10, steps per second: 268, episode reward: -7188.000, mean reward: -718.800 [-999.000, -58.000], mean action: 2.400 [0.000, 3.000],  loss: 778619.300000, mae: 2791.852783, accuracy: 0.215625, mean_q: -3192.997192, mean_eps: 0.374635\n",
      "  6962/50000: episode: 928, duration: 0.031s, episode steps:   8, steps per second: 257, episode reward: -5234.000, mean reward: -654.250 [-999.000, -58.000], mean action: 2.250 [0.000, 3.000],  loss: 741596.437500, mae: 2808.199097, accuracy: 0.152344, mean_q: -3205.276886, mean_eps: 0.373825\n",
      "  6966/50000: episode: 929, duration: 0.018s, episode steps:   4, steps per second: 228, episode reward: -1238.000, mean reward: -309.500 [-999.000, -58.000], mean action: 0.750 [0.000, 2.000],  loss: 742374.468750, mae: 2789.168823, accuracy: 0.156250, mean_q: -3202.374817, mean_eps: 0.373285\n",
      "  6969/50000: episode: 930, duration: 0.015s, episode steps:   3, steps per second: 204, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 1082244.083333, mae: 2838.319255, accuracy: 0.145833, mean_q: -3211.338379, mean_eps: 0.372970\n",
      "  6978/50000: episode: 931, duration: 0.033s, episode steps:   9, steps per second: 271, episode reward: -6189.000, mean reward: -687.667 [-999.000, -58.000], mean action: 1.778 [0.000, 3.000],  loss: 833074.645833, mae: 2802.159288, accuracy: 0.211806, mean_q: -3183.968153, mean_eps: 0.372430\n",
      "  6985/50000: episode: 932, duration: 0.029s, episode steps:   7, steps per second: 244, episode reward: -4220.000, mean reward: -602.857 [-999.000, -32.000], mean action: 0.857 [0.000, 3.000],  loss: 897253.383929, mae: 2793.759277, accuracy: 0.200893, mean_q: -3175.022984, mean_eps: 0.371710\n",
      "  6992/50000: episode: 933, duration: 0.026s, episode steps:   7, steps per second: 268, episode reward: -4191.000, mean reward: -598.714 [-999.000, -58.000], mean action: 1.143 [0.000, 3.000],  loss: 1034083.642857, mae: 2814.400844, accuracy: 0.209821, mean_q: -3161.618827, mean_eps: 0.371080\n",
      "  6998/50000: episode: 934, duration: 0.024s, episode steps:   6, steps per second: 249, episode reward: -3236.000, mean reward: -539.333 [-999.000, -58.000], mean action: 2.000 [0.000, 3.000],  loss: 991630.041667, mae: 2790.579631, accuracy: 0.276042, mean_q: -3132.641968, mean_eps: 0.370495\n",
      "  7001/50000: episode: 935, duration: 0.015s, episode steps:   3, steps per second: 203, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 957548.166667, mae: 2792.333577, accuracy: 0.187500, mean_q: -3132.070638, mean_eps: 0.370090\n",
      "  7016/50000: episode: 936, duration: 0.060s, episode steps:  15, steps per second: 249, episode reward: -12227.000, mean reward: -815.133 [-999.000, -60.000], mean action: 1.733 [0.000, 3.000],  loss: 843398.779167, mae: 2762.931185, accuracy: 0.197917, mean_q: -3119.352979, mean_eps: 0.369280\n",
      "  7019/50000: episode: 937, duration: 0.015s, episode steps:   3, steps per second: 203, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.333 [0.000, 3.000],  loss: 791130.343750, mae: 2780.863851, accuracy: 0.135417, mean_q: -3154.942139, mean_eps: 0.368470\n",
      "  7022/50000: episode: 938, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.333 [0.000, 3.000],  loss: 933822.791667, mae: 2800.964274, accuracy: 0.166667, mean_q: -3152.035238, mean_eps: 0.368200\n",
      "  7027/50000: episode: 939, duration: 0.023s, episode steps:   5, steps per second: 217, episode reward: -2237.000, mean reward: -447.400 [-999.000, -58.000], mean action: 1.200 [0.000, 3.000],  loss: 761955.475000, mae: 2757.749170, accuracy: 0.250000, mean_q: -3149.304932, mean_eps: 0.367840\n",
      "  7049/50000: episode: 940, duration: 0.092s, episode steps:  22, steps per second: 240, episode reward: -19220.000, mean reward: -873.636 [-999.000, -60.000], mean action: 2.045 [0.000, 3.000],  loss: 939815.173295, mae: 2801.211703, accuracy: 0.252841, mean_q: -3171.236772, mean_eps: 0.366625\n",
      "  7053/50000: episode: 941, duration: 0.020s, episode steps:   4, steps per second: 201, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 700132.343750, mae: 2746.936035, accuracy: 0.210938, mean_q: -3148.610779, mean_eps: 0.365455\n",
      "  7057/50000: episode: 942, duration: 0.019s, episode steps:   4, steps per second: 210, episode reward: -1223.000, mean reward: -305.750 [-999.000, -32.000], mean action: 1.750 [1.000, 3.000],  loss: 516752.609375, mae: 2701.729492, accuracy: 0.179688, mean_q: -3142.053284, mean_eps: 0.365095\n",
      "  7061/50000: episode: 943, duration: 0.022s, episode steps:   4, steps per second: 182, episode reward: -1238.000, mean reward: -309.500 [-999.000, -58.000], mean action: 1.250 [0.000, 2.000],  loss: 694110.968750, mae: 2758.356079, accuracy: 0.179688, mean_q: -3156.342102, mean_eps: 0.364735\n",
      "  7065/50000: episode: 944, duration: 0.018s, episode steps:   4, steps per second: 219, episode reward: -1238.000, mean reward: -309.500 [-999.000, -45.000], mean action: 1.750 [0.000, 3.000],  loss: 799645.359375, mae: 2773.259155, accuracy: 0.218750, mean_q: -3165.263489, mean_eps: 0.364375\n",
      "  7071/50000: episode: 945, duration: 0.028s, episode steps:   6, steps per second: 214, episode reward: -3192.000, mean reward: -532.000 [-999.000, -58.000], mean action: 1.000 [0.000, 3.000],  loss: 736690.916667, mae: 2792.457275, accuracy: 0.208333, mean_q: -3202.347168, mean_eps: 0.363925\n",
      "  7075/50000: episode: 946, duration: 0.017s, episode steps:   4, steps per second: 235, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.750 [0.000, 3.000],  loss: 725862.953125, mae: 2792.377563, accuracy: 0.195312, mean_q: -3202.994507, mean_eps: 0.363475\n",
      "  7079/50000: episode: 947, duration: 0.018s, episode steps:   4, steps per second: 223, episode reward: -1238.000, mean reward: -309.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 801584.703125, mae: 2799.858643, accuracy: 0.187500, mean_q: -3199.388184, mean_eps: 0.363115\n",
      "  7088/50000: episode: 948, duration: 0.034s, episode steps:   9, steps per second: 268, episode reward: -6189.000, mean reward: -687.667 [-999.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 749588.399306, mae: 2784.475206, accuracy: 0.170139, mean_q: -3205.657878, mean_eps: 0.362530\n",
      "  7091/50000: episode: 949, duration: 0.014s, episode steps:   3, steps per second: 218, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 546297.041667, mae: 2761.397217, accuracy: 0.177083, mean_q: -3229.933594, mean_eps: 0.361990\n",
      "  7094/50000: episode: 950, duration: 0.013s, episode steps:   3, steps per second: 225, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.333 [0.000, 3.000],  loss: 760207.291667, mae: 2826.332438, accuracy: 0.166667, mean_q: -3205.513184, mean_eps: 0.361720\n",
      "  7097/50000: episode: 951, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.333 [0.000, 3.000],  loss: 565554.718750, mae: 2750.038981, accuracy: 0.281250, mean_q: -3187.895345, mean_eps: 0.361450\n",
      "  7104/50000: episode: 952, duration: 0.025s, episode steps:   7, steps per second: 281, episode reward: -4235.000, mean reward: -605.000 [-999.000, -58.000], mean action: 1.714 [0.000, 3.000],  loss: 731463.598214, mae: 2803.683629, accuracy: 0.142857, mean_q: -3217.904157, mean_eps: 0.361000\n",
      "  7108/50000: episode: 953, duration: 0.016s, episode steps:   4, steps per second: 257, episode reward: -1238.000, mean reward: -309.500 [-999.000, -60.000], mean action: 1.500 [0.000, 3.000],  loss: 548804.890625, mae: 2792.503601, accuracy: 0.179688, mean_q: -3216.263367, mean_eps: 0.360505\n",
      "  7135/50000: episode: 954, duration: 0.084s, episode steps:  27, steps per second: 322, episode reward: -24171.000, mean reward: -895.222 [-999.000, -58.000], mean action: 2.370 [0.000, 3.000],  loss: 809157.960648, mae: 2843.107775, accuracy: 0.165509, mean_q: -3254.370886, mean_eps: 0.359110\n",
      "  7138/50000: episode: 955, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 902560.104167, mae: 2816.803304, accuracy: 0.187500, mean_q: -3190.856527, mean_eps: 0.357760\n",
      "  7159/50000: episode: 956, duration: 0.066s, episode steps:  21, steps per second: 318, episode reward: -18221.000, mean reward: -867.667 [-999.000, -60.000], mean action: 1.286 [0.000, 3.000],  loss: 765207.309524, mae: 2767.005952, accuracy: 0.186012, mean_q: -3125.206369, mean_eps: 0.356680\n",
      "  7182/50000: episode: 957, duration: 0.075s, episode steps:  23, steps per second: 306, episode reward: -20175.000, mean reward: -877.174 [-999.000, -32.000], mean action: 2.130 [0.000, 3.000],  loss: 683956.324728, mae: 2798.727221, accuracy: 0.137228, mean_q: -3188.992007, mean_eps: 0.354700\n",
      "  7190/50000: episode: 958, duration: 0.028s, episode steps:   8, steps per second: 282, episode reward: -5190.000, mean reward: -648.750 [-999.000, -58.000], mean action: 2.000 [0.000, 3.000],  loss: 943817.074219, mae: 2837.924042, accuracy: 0.191406, mean_q: -3196.238739, mean_eps: 0.353305\n",
      "  7201/50000: episode: 959, duration: 0.037s, episode steps:  11, steps per second: 300, episode reward: -8231.000, mean reward: -748.273 [-999.000, -60.000], mean action: 2.182 [0.000, 3.000],  loss: 787314.241477, mae: 2809.314187, accuracy: 0.147727, mean_q: -3177.408270, mean_eps: 0.352450\n",
      "  7227/50000: episode: 960, duration: 0.082s, episode steps:  26, steps per second: 317, episode reward: -23172.000, mean reward: -891.231 [-999.000, -58.000], mean action: 2.462 [0.000, 3.000],  loss: 800260.760817, mae: 2790.313777, accuracy: 0.157452, mean_q: -3172.243127, mean_eps: 0.350785\n",
      "  7231/50000: episode: 961, duration: 0.015s, episode steps:   4, steps per second: 263, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 470770.187500, mae: 2736.676636, accuracy: 0.226562, mean_q: -3199.824890, mean_eps: 0.349435\n",
      "  7235/50000: episode: 962, duration: 0.015s, episode steps:   4, steps per second: 260, episode reward: -1238.000, mean reward: -309.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 734164.328125, mae: 2811.757751, accuracy: 0.226562, mean_q: -3221.154358, mean_eps: 0.349075\n",
      "  7246/50000: episode: 963, duration: 0.039s, episode steps:  11, steps per second: 282, episode reward: -8187.000, mean reward: -744.273 [-999.000, -45.000], mean action: 1.182 [0.000, 3.000],  loss: 740871.164773, mae: 2812.266668, accuracy: 0.227273, mean_q: -3236.196289, mean_eps: 0.348400\n",
      "  7251/50000: episode: 964, duration: 0.018s, episode steps:   5, steps per second: 271, episode reward: -2193.000, mean reward: -438.600 [-999.000, -32.000], mean action: 1.400 [0.000, 3.000],  loss: 965568.125000, mae: 2875.312402, accuracy: 0.137500, mean_q: -3262.845801, mean_eps: 0.347680\n",
      "  7271/50000: episode: 965, duration: 0.064s, episode steps:  20, steps per second: 311, episode reward: -17178.000, mean reward: -858.900 [-999.000, -32.000], mean action: 1.300 [0.000, 3.000],  loss: 835475.639062, mae: 2836.722522, accuracy: 0.239063, mean_q: -3230.146399, mean_eps: 0.346555\n",
      "  7275/50000: episode: 966, duration: 0.015s, episode steps:   4, steps per second: 263, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 673453.562500, mae: 2816.998169, accuracy: 0.210938, mean_q: -3223.627136, mean_eps: 0.345475\n",
      "  7279/50000: episode: 967, duration: 0.016s, episode steps:   4, steps per second: 249, episode reward: -1238.000, mean reward: -309.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 874568.640625, mae: 2831.697205, accuracy: 0.234375, mean_q: -3227.408875, mean_eps: 0.345115\n",
      "  7283/50000: episode: 968, duration: 0.015s, episode steps:   4, steps per second: 264, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 757368.695312, mae: 2807.311096, accuracy: 0.250000, mean_q: -3238.706848, mean_eps: 0.344755\n",
      "  7286/50000: episode: 969, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 910354.750000, mae: 2853.213949, accuracy: 0.218750, mean_q: -3260.599284, mean_eps: 0.344440\n",
      "  7298/50000: episode: 970, duration: 0.040s, episode steps:  12, steps per second: 302, episode reward: -9186.000, mean reward: -765.500 [-999.000, -32.000], mean action: 1.750 [0.000, 3.000],  loss: 879263.432292, mae: 2847.998739, accuracy: 0.195312, mean_q: -3252.944397, mean_eps: 0.343765\n",
      "  7306/50000: episode: 971, duration: 0.028s, episode steps:   8, steps per second: 288, episode reward: -5234.000, mean reward: -654.250 [-999.000, -60.000], mean action: 0.750 [0.000, 3.000],  loss: 837016.101562, mae: 2826.276520, accuracy: 0.246094, mean_q: -3221.894928, mean_eps: 0.342865\n",
      "  7311/50000: episode: 972, duration: 0.019s, episode steps:   5, steps per second: 267, episode reward: -2193.000, mean reward: -438.600 [-999.000, -32.000], mean action: 1.000 [0.000, 3.000],  loss: 757269.275000, mae: 2830.740771, accuracy: 0.193750, mean_q: -3241.844531, mean_eps: 0.342280\n",
      "  7314/50000: episode: 973, duration: 0.013s, episode steps:   3, steps per second: 225, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 951941.187500, mae: 2841.783366, accuracy: 0.229167, mean_q: -3201.469645, mean_eps: 0.341920\n",
      "  7321/50000: episode: 974, duration: 0.024s, episode steps:   7, steps per second: 289, episode reward: -4191.000, mean reward: -598.714 [-999.000, -32.000], mean action: 1.714 [0.000, 3.000],  loss: 713866.383929, mae: 2804.396205, accuracy: 0.205357, mean_q: -3231.281669, mean_eps: 0.341470\n",
      "  7326/50000: episode: 975, duration: 0.018s, episode steps:   5, steps per second: 277, episode reward: -2237.000, mean reward: -447.400 [-999.000, -45.000], mean action: 1.400 [0.000, 3.000],  loss: 605087.462500, mae: 2804.649121, accuracy: 0.156250, mean_q: -3253.357812, mean_eps: 0.340930\n",
      "  7335/50000: episode: 976, duration: 0.030s, episode steps:   9, steps per second: 300, episode reward: -6189.000, mean reward: -687.667 [-999.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 815210.579861, mae: 2830.557237, accuracy: 0.274306, mean_q: -3240.931315, mean_eps: 0.340300\n",
      "  7347/50000: episode: 977, duration: 0.040s, episode steps:  12, steps per second: 297, episode reward: -9215.000, mean reward: -767.917 [-999.000, -32.000], mean action: 0.500 [0.000, 3.000],  loss: 942664.729167, mae: 2859.198832, accuracy: 0.210938, mean_q: -3235.423136, mean_eps: 0.339355\n",
      "  7352/50000: episode: 978, duration: 0.019s, episode steps:   5, steps per second: 270, episode reward: -2193.000, mean reward: -438.600 [-999.000, -32.000], mean action: 1.600 [0.000, 3.000],  loss: 732877.237500, mae: 2824.935840, accuracy: 0.187500, mean_q: -3236.845752, mean_eps: 0.338590\n",
      "  7355/50000: episode: 979, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 735039.333333, mae: 2809.438070, accuracy: 0.281250, mean_q: -3215.732015, mean_eps: 0.338230\n",
      "  7359/50000: episode: 980, duration: 0.015s, episode steps:   4, steps per second: 261, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 0.750 [0.000, 2.000],  loss: 813147.734375, mae: 2850.930481, accuracy: 0.164062, mean_q: -3248.476074, mean_eps: 0.337915\n",
      "  7366/50000: episode: 981, duration: 0.025s, episode steps:   7, steps per second: 286, episode reward: -4235.000, mean reward: -605.000 [-999.000, -45.000], mean action: 1.571 [0.000, 3.000],  loss: 794765.589286, mae: 2842.159215, accuracy: 0.183036, mean_q: -3266.184047, mean_eps: 0.337420\n",
      "  7369/50000: episode: 982, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.333 [0.000, 3.000],  loss: 1000268.687500, mae: 2886.464274, accuracy: 0.177083, mean_q: -3281.750244, mean_eps: 0.336970\n",
      "  7374/50000: episode: 983, duration: 0.019s, episode steps:   5, steps per second: 265, episode reward: -2193.000, mean reward: -438.600 [-999.000, -58.000], mean action: 1.200 [0.000, 3.000],  loss: 822894.268750, mae: 2856.439990, accuracy: 0.225000, mean_q: -3257.819434, mean_eps: 0.336610\n",
      "  7377/50000: episode: 984, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 887465.729167, mae: 2859.389486, accuracy: 0.197917, mean_q: -3260.089600, mean_eps: 0.336250\n",
      "  7386/50000: episode: 985, duration: 0.031s, episode steps:   9, steps per second: 290, episode reward: -6233.000, mean reward: -692.556 [-999.000, -45.000], mean action: 1.222 [0.000, 3.000],  loss: 739162.079861, mae: 2811.617296, accuracy: 0.274306, mean_q: -3240.448052, mean_eps: 0.335710\n",
      "  7391/50000: episode: 986, duration: 0.018s, episode steps:   5, steps per second: 272, episode reward: -2222.000, mean reward: -444.400 [-999.000, -32.000], mean action: 1.400 [0.000, 3.000],  loss: 888286.600000, mae: 2870.273291, accuracy: 0.181250, mean_q: -3263.013037, mean_eps: 0.335080\n",
      "  7402/50000: episode: 987, duration: 0.036s, episode steps:  11, steps per second: 303, episode reward: -8187.000, mean reward: -744.273 [-999.000, -32.000], mean action: 1.273 [0.000, 3.000],  loss: 789271.431818, mae: 2838.019997, accuracy: 0.252841, mean_q: -3247.836936, mean_eps: 0.334360\n",
      "  7405/50000: episode: 988, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 809923.812500, mae: 2844.577311, accuracy: 0.218750, mean_q: -3245.176432, mean_eps: 0.333730\n",
      "  7409/50000: episode: 989, duration: 0.015s, episode steps:   4, steps per second: 264, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.750 [0.000, 3.000],  loss: 869842.335938, mae: 2839.069214, accuracy: 0.179688, mean_q: -3237.278870, mean_eps: 0.333415\n",
      "  7413/50000: episode: 990, duration: 0.015s, episode steps:   4, steps per second: 262, episode reward: -1238.000, mean reward: -309.500 [-999.000, -45.000], mean action: 2.000 [1.000, 3.000],  loss: 883884.031250, mae: 2833.670288, accuracy: 0.226562, mean_q: -3215.004028, mean_eps: 0.333055\n",
      "  7417/50000: episode: 991, duration: 0.015s, episode steps:   4, steps per second: 262, episode reward: -1238.000, mean reward: -309.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 1082692.945312, mae: 2868.890869, accuracy: 0.210938, mean_q: -3200.557556, mean_eps: 0.332695\n",
      "  7420/50000: episode: 992, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 755321.312500, mae: 2792.707194, accuracy: 0.260417, mean_q: -3164.777832, mean_eps: 0.332380\n",
      "  7430/50000: episode: 993, duration: 0.034s, episode steps:  10, steps per second: 296, episode reward: -7232.000, mean reward: -723.200 [-999.000, -45.000], mean action: 1.200 [0.000, 3.000],  loss: 804475.178125, mae: 2797.386694, accuracy: 0.268750, mean_q: -3161.493286, mean_eps: 0.331795\n",
      "  7440/50000: episode: 994, duration: 0.035s, episode steps:  10, steps per second: 285, episode reward: -7188.000, mean reward: -718.800 [-999.000, -32.000], mean action: 1.300 [1.000, 3.000],  loss: 806906.843750, mae: 2803.794971, accuracy: 0.209375, mean_q: -3174.305127, mean_eps: 0.330895\n",
      "  7444/50000: episode: 995, duration: 0.015s, episode steps:   4, steps per second: 263, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 2.000 [0.000, 3.000],  loss: 746608.703125, mae: 2813.590820, accuracy: 0.265625, mean_q: -3200.766724, mean_eps: 0.330265\n",
      "  7457/50000: episode: 996, duration: 0.043s, episode steps:  13, steps per second: 302, episode reward: -10185.000, mean reward: -783.462 [-999.000, -32.000], mean action: 1.231 [1.000, 3.000],  loss: 780946.365385, mae: 2838.076266, accuracy: 0.221154, mean_q: -3254.512357, mean_eps: 0.329500\n",
      "  7463/50000: episode: 997, duration: 0.021s, episode steps:   6, steps per second: 281, episode reward: -3236.000, mean reward: -539.333 [-999.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 818455.468750, mae: 2858.951782, accuracy: 0.250000, mean_q: -3277.691528, mean_eps: 0.328645\n",
      "  7472/50000: episode: 998, duration: 0.030s, episode steps:   9, steps per second: 296, episode reward: -6233.000, mean reward: -692.556 [-999.000, -45.000], mean action: 1.111 [0.000, 2.000],  loss: 931878.597222, mae: 2887.106771, accuracy: 0.180556, mean_q: -3290.131266, mean_eps: 0.327970\n",
      "  7479/50000: episode: 999, duration: 0.025s, episode steps:   7, steps per second: 281, episode reward: -4235.000, mean reward: -605.000 [-999.000, -60.000], mean action: 1.714 [0.000, 3.000],  loss: 971211.696429, mae: 2878.777134, accuracy: 0.232143, mean_q: -3276.912214, mean_eps: 0.327250\n",
      "  7494/50000: episode: 1000, duration: 0.050s, episode steps:  15, steps per second: 301, episode reward: -12212.000, mean reward: -814.133 [-999.000, -32.000], mean action: 0.800 [0.000, 3.000],  loss: 818111.389583, mae: 2855.178906, accuracy: 0.208333, mean_q: -3264.851953, mean_eps: 0.326260\n",
      "  7502/50000: episode: 1001, duration: 0.029s, episode steps:   8, steps per second: 280, episode reward: -5190.000, mean reward: -648.750 [-999.000, -32.000], mean action: 0.625 [0.000, 3.000],  loss: 821616.148438, mae: 2837.016876, accuracy: 0.179688, mean_q: -3248.960938, mean_eps: 0.325225\n",
      "  7505/50000: episode: 1002, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 855616.229167, mae: 2851.155436, accuracy: 0.208333, mean_q: -3238.425863, mean_eps: 0.324730\n",
      "  7512/50000: episode: 1003, duration: 0.025s, episode steps:   7, steps per second: 282, episode reward: -4220.000, mean reward: -602.857 [-999.000, -32.000], mean action: 1.286 [0.000, 3.000],  loss: 924844.370536, mae: 2862.971191, accuracy: 0.209821, mean_q: -3251.137905, mean_eps: 0.324280\n",
      "  7515/50000: episode: 1004, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 667250.135417, mae: 2810.821126, accuracy: 0.166667, mean_q: -3237.343262, mean_eps: 0.323830\n",
      "  7518/50000: episode: 1005, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 786389.000000, mae: 2835.261068, accuracy: 0.270833, mean_q: -3249.734619, mean_eps: 0.323560\n",
      "  7523/50000: episode: 1006, duration: 0.018s, episode steps:   5, steps per second: 282, episode reward: -2193.000, mean reward: -438.600 [-999.000, -45.000], mean action: 1.400 [0.000, 3.000],  loss: 587694.543750, mae: 2812.427783, accuracy: 0.200000, mean_q: -3266.741162, mean_eps: 0.323200\n",
      "  7529/50000: episode: 1007, duration: 0.022s, episode steps:   6, steps per second: 276, episode reward: -3221.000, mean reward: -536.833 [-999.000, -32.000], mean action: 1.000 [0.000, 3.000],  loss: 790778.510417, mae: 2869.961466, accuracy: 0.203125, mean_q: -3290.745199, mean_eps: 0.322705\n",
      "  7534/50000: episode: 1008, duration: 0.019s, episode steps:   5, steps per second: 263, episode reward: -2193.000, mean reward: -438.600 [-999.000, -32.000], mean action: 1.400 [0.000, 3.000],  loss: 930287.012500, mae: 2894.206055, accuracy: 0.268750, mean_q: -3308.098584, mean_eps: 0.322210\n",
      "  7537/50000: episode: 1009, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 786950.229167, mae: 2885.030273, accuracy: 0.166667, mean_q: -3305.164551, mean_eps: 0.321850\n",
      "  7544/50000: episode: 1010, duration: 0.025s, episode steps:   7, steps per second: 282, episode reward: -4191.000, mean reward: -598.714 [-999.000, -32.000], mean action: 1.429 [1.000, 3.000],  loss: 866230.491071, mae: 2876.194092, accuracy: 0.165179, mean_q: -3297.828648, mean_eps: 0.321400\n",
      "  7547/50000: episode: 1011, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 668963.364583, mae: 2833.310547, accuracy: 0.239583, mean_q: -3256.459473, mean_eps: 0.320950\n",
      "  7553/50000: episode: 1012, duration: 0.022s, episode steps:   6, steps per second: 276, episode reward: -3192.000, mean reward: -532.000 [-999.000, -32.000], mean action: 1.167 [0.000, 3.000],  loss: 886211.421875, mae: 2864.053792, accuracy: 0.270833, mean_q: -3284.392131, mean_eps: 0.320545\n",
      "  7557/50000: episode: 1013, duration: 0.017s, episode steps:   4, steps per second: 230, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 2.000 [0.000, 3.000],  loss: 1013264.984375, mae: 2890.296082, accuracy: 0.281250, mean_q: -3258.080872, mean_eps: 0.320095\n",
      "  7560/50000: episode: 1014, duration: 0.016s, episode steps:   3, steps per second: 186, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 954871.583333, mae: 2856.076497, accuracy: 0.333333, mean_q: -3244.844808, mean_eps: 0.319780\n",
      "  7571/50000: episode: 1015, duration: 0.037s, episode steps:  11, steps per second: 299, episode reward: -8187.000, mean reward: -744.273 [-999.000, -32.000], mean action: 1.909 [0.000, 3.000],  loss: 1109374.869318, mae: 2913.677557, accuracy: 0.232955, mean_q: -3264.755460, mean_eps: 0.319150\n",
      "  7574/50000: episode: 1016, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 1153686.625000, mae: 2905.079102, accuracy: 0.177083, mean_q: -3264.386393, mean_eps: 0.318520\n",
      "  7577/50000: episode: 1017, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 755711.312500, mae: 2832.068685, accuracy: 0.197917, mean_q: -3239.791585, mean_eps: 0.318250\n",
      "  7581/50000: episode: 1018, duration: 0.016s, episode steps:   4, steps per second: 257, episode reward: -1238.000, mean reward: -309.500 [-999.000, -45.000], mean action: 1.750 [0.000, 3.000],  loss: 733563.656250, mae: 2823.479614, accuracy: 0.195312, mean_q: -3226.204895, mean_eps: 0.317935\n",
      "  7586/50000: episode: 1019, duration: 0.018s, episode steps:   5, steps per second: 275, episode reward: -2193.000, mean reward: -438.600 [-999.000, -45.000], mean action: 1.600 [0.000, 3.000],  loss: 907212.137500, mae: 2852.735547, accuracy: 0.225000, mean_q: -3221.501709, mean_eps: 0.317530\n",
      "  7589/50000: episode: 1020, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.333 [0.000, 3.000],  loss: 896436.583333, mae: 2846.546224, accuracy: 0.125000, mean_q: -3251.446289, mean_eps: 0.317170\n",
      "  7593/50000: episode: 1021, duration: 0.015s, episode steps:   4, steps per second: 263, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 953877.734375, mae: 2890.326172, accuracy: 0.210938, mean_q: -3261.826965, mean_eps: 0.316855\n",
      "  7597/50000: episode: 1022, duration: 0.015s, episode steps:   4, steps per second: 270, episode reward: -1238.000, mean reward: -309.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 975655.890625, mae: 2865.239380, accuracy: 0.242188, mean_q: -3249.275391, mean_eps: 0.316495\n",
      "  7603/50000: episode: 1023, duration: 0.021s, episode steps:   6, steps per second: 281, episode reward: -3221.000, mean reward: -536.833 [-999.000, -32.000], mean action: 1.167 [0.000, 3.000],  loss: 849082.281250, mae: 2846.171509, accuracy: 0.203125, mean_q: -3250.086344, mean_eps: 0.316045\n",
      "  7609/50000: episode: 1024, duration: 0.021s, episode steps:   6, steps per second: 280, episode reward: -3192.000, mean reward: -532.000 [-999.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 682337.437500, mae: 2823.840902, accuracy: 0.223958, mean_q: -3250.459554, mean_eps: 0.315505\n",
      "  7615/50000: episode: 1025, duration: 0.032s, episode steps:   6, steps per second: 190, episode reward: -3236.000, mean reward: -539.333 [-999.000, -60.000], mean action: 1.000 [0.000, 3.000],  loss: 660641.614583, mae: 2847.693726, accuracy: 0.156250, mean_q: -3292.855794, mean_eps: 0.314965\n",
      "  7618/50000: episode: 1026, duration: 0.018s, episode steps:   3, steps per second: 167, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.333 [0.000, 3.000],  loss: 708267.583333, mae: 2875.002441, accuracy: 0.166667, mean_q: -3311.296549, mean_eps: 0.314560\n",
      "  7622/50000: episode: 1027, duration: 0.019s, episode steps:   4, steps per second: 207, episode reward: -1238.000, mean reward: -309.500 [-999.000, -45.000], mean action: 1.250 [0.000, 3.000],  loss: 961733.484375, mae: 2912.338806, accuracy: 0.203125, mean_q: -3320.389099, mean_eps: 0.314245\n",
      "  7625/50000: episode: 1028, duration: 0.024s, episode steps:   3, steps per second: 123, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 715438.937500, mae: 2878.812826, accuracy: 0.114583, mean_q: -3323.483398, mean_eps: 0.313930\n",
      "  7629/50000: episode: 1029, duration: 0.022s, episode steps:   4, steps per second: 179, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 671781.421875, mae: 2865.423340, accuracy: 0.234375, mean_q: -3330.135254, mean_eps: 0.313615\n",
      "  7632/50000: episode: 1030, duration: 0.017s, episode steps:   3, steps per second: 171, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 629667.812500, mae: 2860.362956, accuracy: 0.177083, mean_q: -3322.073893, mean_eps: 0.313300\n",
      "  7641/50000: episode: 1031, duration: 0.044s, episode steps:   9, steps per second: 205, episode reward: -6233.000, mean reward: -692.556 [-999.000, -60.000], mean action: 1.778 [0.000, 3.000],  loss: 714405.708333, mae: 2896.526123, accuracy: 0.170139, mean_q: -3366.703857, mean_eps: 0.312760\n",
      "  7644/50000: episode: 1032, duration: 0.016s, episode steps:   3, steps per second: 187, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 678978.604167, mae: 2919.094076, accuracy: 0.197917, mean_q: -3386.939860, mean_eps: 0.312220\n",
      "  7648/50000: episode: 1033, duration: 0.021s, episode steps:   4, steps per second: 191, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 2.000 [0.000, 3.000],  loss: 687160.601562, mae: 2901.844971, accuracy: 0.226562, mean_q: -3373.686279, mean_eps: 0.311905\n",
      "  7651/50000: episode: 1034, duration: 0.016s, episode steps:   3, steps per second: 182, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 1106698.166667, mae: 2990.692708, accuracy: 0.156250, mean_q: -3394.011475, mean_eps: 0.311590\n",
      "  7654/50000: episode: 1035, duration: 0.017s, episode steps:   3, steps per second: 176, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 970836.500000, mae: 2969.664632, accuracy: 0.093750, mean_q: -3414.592936, mean_eps: 0.311320\n",
      "  7664/50000: episode: 1036, duration: 0.045s, episode steps:  10, steps per second: 223, episode reward: -7217.000, mean reward: -721.700 [-999.000, -32.000], mean action: 0.600 [0.000, 3.000],  loss: 718686.337500, mae: 2905.538867, accuracy: 0.200000, mean_q: -3352.129150, mean_eps: 0.310735\n",
      "  7670/50000: episode: 1037, duration: 0.024s, episode steps:   6, steps per second: 246, episode reward: -3192.000, mean reward: -532.000 [-999.000, -45.000], mean action: 1.333 [0.000, 3.000],  loss: 1047437.895833, mae: 2927.951131, accuracy: 0.223958, mean_q: -3314.862630, mean_eps: 0.310015\n",
      "  7676/50000: episode: 1038, duration: 0.034s, episode steps:   6, steps per second: 175, episode reward: -3236.000, mean reward: -539.333 [-999.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 616999.510417, mae: 2855.929118, accuracy: 0.192708, mean_q: -3293.305949, mean_eps: 0.309475\n",
      "  7680/50000: episode: 1039, duration: 0.020s, episode steps:   4, steps per second: 204, episode reward: -1238.000, mean reward: -309.500 [-999.000, -45.000], mean action: 2.000 [1.000, 3.000],  loss: 725616.781250, mae: 2858.891785, accuracy: 0.226562, mean_q: -3286.082703, mean_eps: 0.309025\n",
      "  7683/50000: episode: 1040, duration: 0.015s, episode steps:   3, steps per second: 196, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 701507.500000, mae: 2881.476888, accuracy: 0.156250, mean_q: -3295.925618, mean_eps: 0.308710\n",
      "  7699/50000: episode: 1041, duration: 0.077s, episode steps:  16, steps per second: 209, episode reward: -13182.000, mean reward: -823.875 [-999.000, -32.000], mean action: 1.188 [0.000, 3.000],  loss: 758230.966797, mae: 2886.420761, accuracy: 0.228516, mean_q: -3326.515671, mean_eps: 0.307855\n",
      "  7702/50000: episode: 1042, duration: 0.016s, episode steps:   3, steps per second: 184, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 896226.375000, mae: 2941.938558, accuracy: 0.177083, mean_q: -3377.729736, mean_eps: 0.307000\n",
      "  7734/50000: episode: 1043, duration: 0.141s, episode steps:  32, steps per second: 227, episode reward: -29166.000, mean reward: -911.438 [-999.000, -32.000], mean action: 1.625 [0.000, 3.000],  loss: 903871.620117, mae: 2917.200691, accuracy: 0.179688, mean_q: -3330.609818, mean_eps: 0.305425\n",
      "  7766/50000: episode: 1044, duration: 0.101s, episode steps:  32, steps per second: 317, episode reward: -29166.000, mean reward: -911.438 [-999.000, -32.000], mean action: 1.906 [0.000, 3.000],  loss: 797846.630859, mae: 2888.998932, accuracy: 0.208008, mean_q: -3307.245964, mean_eps: 0.302545\n",
      "  7769/50000: episode: 1045, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 954401.052083, mae: 2933.968587, accuracy: 0.145833, mean_q: -3309.188314, mean_eps: 0.300970\n",
      "  7772/50000: episode: 1046, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 729648.416667, mae: 2863.826986, accuracy: 0.239583, mean_q: -3308.585612, mean_eps: 0.300700\n",
      "  7796/50000: episode: 1047, duration: 0.121s, episode steps:  24, steps per second: 198, episode reward: -21174.000, mean reward: -882.250 [-999.000, -45.000], mean action: 1.833 [0.000, 2.000],  loss: 1018170.319010, mae: 2885.684184, accuracy: 0.203125, mean_q: -3244.831055, mean_eps: 0.299485\n",
      "  7846/50000: episode: 1048, duration: 0.219s, episode steps:  50, steps per second: 229, episode reward: -47148.000, mean reward: -942.960 [-999.000, -32.000], mean action: 1.000 [0.000, 3.000],  loss: 780494.575937, mae: 2879.746611, accuracy: 0.223750, mean_q: -3278.102344, mean_eps: 0.296155\n",
      "  7849/50000: episode: 1049, duration: 0.014s, episode steps:   3, steps per second: 208, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 931532.104167, mae: 2948.965983, accuracy: 0.208333, mean_q: -3339.075114, mean_eps: 0.293770\n",
      "  7852/50000: episode: 1050, duration: 0.014s, episode steps:   3, steps per second: 210, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 711391.750000, mae: 2915.153320, accuracy: 0.197917, mean_q: -3329.172282, mean_eps: 0.293500\n",
      "  7877/50000: episode: 1051, duration: 0.122s, episode steps:  25, steps per second: 206, episode reward: -22217.000, mean reward: -888.680 [-999.000, -45.000], mean action: 1.160 [0.000, 3.000],  loss: 905571.661250, mae: 2916.952139, accuracy: 0.235000, mean_q: -3320.259453, mean_eps: 0.292240\n",
      "  7883/50000: episode: 1052, duration: 0.031s, episode steps:   6, steps per second: 193, episode reward: -3192.000, mean reward: -532.000 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 876194.354167, mae: 2894.293172, accuracy: 0.135417, mean_q: -3291.046834, mean_eps: 0.290845\n",
      "  7886/50000: episode: 1053, duration: 0.019s, episode steps:   3, steps per second: 162, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 773073.437500, mae: 2878.593750, accuracy: 0.135417, mean_q: -3293.059733, mean_eps: 0.290440\n",
      "  7891/50000: episode: 1054, duration: 0.027s, episode steps:   5, steps per second: 188, episode reward: -2222.000, mean reward: -444.400 [-999.000, -32.000], mean action: 1.600 [0.000, 3.000],  loss: 1121518.525000, mae: 2916.062305, accuracy: 0.162500, mean_q: -3286.725537, mean_eps: 0.290080\n",
      "  7898/50000: episode: 1055, duration: 0.037s, episode steps:   7, steps per second: 189, episode reward: -4191.000, mean reward: -598.714 [-999.000, -32.000], mean action: 1.571 [0.000, 3.000],  loss: 890250.526786, mae: 2870.068813, accuracy: 0.138393, mean_q: -3265.444999, mean_eps: 0.289540\n",
      "  7902/50000: episode: 1056, duration: 0.023s, episode steps:   4, steps per second: 171, episode reward: -1238.000, mean reward: -309.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 859626.296875, mae: 2873.011536, accuracy: 0.156250, mean_q: -3251.160950, mean_eps: 0.289045\n",
      "  7923/50000: episode: 1057, duration: 0.090s, episode steps:  21, steps per second: 233, episode reward: -18177.000, mean reward: -865.571 [-999.000, -58.000], mean action: 2.238 [0.000, 3.000],  loss: 833463.504464, mae: 2878.710472, accuracy: 0.163690, mean_q: -3290.624674, mean_eps: 0.287920\n",
      "  7926/50000: episode: 1058, duration: 0.013s, episode steps:   3, steps per second: 224, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 825763.729167, mae: 2896.977295, accuracy: 0.166667, mean_q: -3314.217448, mean_eps: 0.286840\n",
      "  7971/50000: episode: 1059, duration: 0.162s, episode steps:  45, steps per second: 278, episode reward: -42182.000, mean reward: -937.378 [-999.000, -58.000], mean action: 0.400 [0.000, 3.000],  loss: 934516.195139, mae: 2883.903418, accuracy: 0.221528, mean_q: -3272.884695, mean_eps: 0.284680\n",
      "  7974/50000: episode: 1060, duration: 0.013s, episode steps:   3, steps per second: 226, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 866085.354167, mae: 2850.697754, accuracy: 0.187500, mean_q: -3248.014242, mean_eps: 0.282520\n",
      "  8020/50000: episode: 1061, duration: 0.214s, episode steps:  46, steps per second: 215, episode reward: -43196.000, mean reward: -939.043 [-999.000, -45.000], mean action: 1.065 [0.000, 3.000],  loss: 849048.703804, mae: 2909.422719, accuracy: 0.194973, mean_q: -3324.390588, mean_eps: 0.280315\n",
      "  8023/50000: episode: 1062, duration: 0.018s, episode steps:   3, steps per second: 164, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 1022021.479167, mae: 2926.957031, accuracy: 0.197917, mean_q: -3307.416260, mean_eps: 0.278110\n",
      "  8026/50000: episode: 1063, duration: 0.017s, episode steps:   3, steps per second: 174, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 774177.187500, mae: 2878.392171, accuracy: 0.177083, mean_q: -3285.129720, mean_eps: 0.277840\n",
      "  8029/50000: episode: 1064, duration: 0.014s, episode steps:   3, steps per second: 207, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 951811.791667, mae: 2896.585205, accuracy: 0.197917, mean_q: -3287.154704, mean_eps: 0.277570\n",
      "  8033/50000: episode: 1065, duration: 0.018s, episode steps:   4, steps per second: 227, episode reward: -1223.000, mean reward: -305.750 [-999.000, -58.000], mean action: 1.750 [0.000, 3.000],  loss: 794435.703125, mae: 2865.547607, accuracy: 0.195312, mean_q: -3264.529419, mean_eps: 0.277255\n",
      "  8036/50000: episode: 1066, duration: 0.015s, episode steps:   3, steps per second: 200, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 766209.281250, mae: 2872.255778, accuracy: 0.218750, mean_q: -3293.104899, mean_eps: 0.276940\n",
      "  8040/50000: episode: 1067, duration: 0.015s, episode steps:   4, steps per second: 262, episode reward: -1223.000, mean reward: -305.750 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 863007.289062, mae: 2882.696045, accuracy: 0.210938, mean_q: -3285.368164, mean_eps: 0.276625\n",
      "  8047/50000: episode: 1068, duration: 0.024s, episode steps:   7, steps per second: 293, episode reward: -4191.000, mean reward: -598.714 [-999.000, -45.000], mean action: 1.857 [0.000, 3.000],  loss: 798238.196429, mae: 2879.714007, accuracy: 0.160714, mean_q: -3295.541504, mean_eps: 0.276130\n",
      "  8052/50000: episode: 1069, duration: 0.021s, episode steps:   5, steps per second: 240, episode reward: -2222.000, mean reward: -444.400 [-999.000, -32.000], mean action: 1.600 [1.000, 3.000],  loss: 1033059.875000, mae: 2931.436963, accuracy: 0.200000, mean_q: -3308.720410, mean_eps: 0.275590\n",
      "  8055/50000: episode: 1070, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 597159.875000, mae: 2842.468669, accuracy: 0.218750, mean_q: -3289.184977, mean_eps: 0.275230\n",
      "  8071/50000: episode: 1071, duration: 0.066s, episode steps:  16, steps per second: 241, episode reward: -13211.000, mean reward: -825.688 [-999.000, -32.000], mean action: 0.625 [0.000, 3.000],  loss: 893068.007812, mae: 2907.937973, accuracy: 0.175781, mean_q: -3312.256805, mean_eps: 0.274375\n",
      "  8074/50000: episode: 1072, duration: 0.014s, episode steps:   3, steps per second: 222, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 903375.125000, mae: 2895.184245, accuracy: 0.166667, mean_q: -3298.159505, mean_eps: 0.273520\n",
      "  8078/50000: episode: 1073, duration: 0.016s, episode steps:   4, steps per second: 245, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.750 [1.000, 3.000],  loss: 983419.125000, mae: 2894.794922, accuracy: 0.210938, mean_q: -3268.252991, mean_eps: 0.273205\n",
      "  8081/50000: episode: 1074, duration: 0.015s, episode steps:   3, steps per second: 197, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 695932.645833, mae: 2867.510986, accuracy: 0.156250, mean_q: -3273.448649, mean_eps: 0.272890\n",
      "  8084/50000: episode: 1075, duration: 0.015s, episode steps:   3, steps per second: 199, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 1408906.937500, mae: 2969.679688, accuracy: 0.166667, mean_q: -3272.539144, mean_eps: 0.272620\n",
      "  8088/50000: episode: 1076, duration: 0.019s, episode steps:   4, steps per second: 210, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 666584.664062, mae: 2834.129700, accuracy: 0.156250, mean_q: -3267.212341, mean_eps: 0.272305\n",
      "  8091/50000: episode: 1077, duration: 0.015s, episode steps:   3, steps per second: 201, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 1055188.416667, mae: 2906.177572, accuracy: 0.125000, mean_q: -3260.523356, mean_eps: 0.271990\n",
      "  8101/50000: episode: 1078, duration: 0.045s, episode steps:  10, steps per second: 222, episode reward: -7217.000, mean reward: -721.700 [-999.000, -58.000], mean action: 1.700 [0.000, 3.000],  loss: 779228.728125, mae: 2832.701123, accuracy: 0.187500, mean_q: -3227.094043, mean_eps: 0.271405\n",
      "  8106/50000: episode: 1079, duration: 0.021s, episode steps:   5, steps per second: 236, episode reward: -2193.000, mean reward: -438.600 [-999.000, -58.000], mean action: 2.000 [0.000, 3.000],  loss: 824982.343750, mae: 2853.864941, accuracy: 0.175000, mean_q: -3253.445068, mean_eps: 0.270730\n",
      "  8121/50000: episode: 1080, duration: 0.059s, episode steps:  15, steps per second: 255, episode reward: -12227.000, mean reward: -815.133 [-999.000, -58.000], mean action: 2.200 [0.000, 3.000],  loss: 816688.554167, mae: 2864.112972, accuracy: 0.168750, mean_q: -3273.171257, mean_eps: 0.269830\n",
      "  8149/50000: episode: 1081, duration: 0.105s, episode steps:  28, steps per second: 267, episode reward: -25170.000, mean reward: -898.929 [-999.000, -58.000], mean action: 2.464 [0.000, 3.000],  loss: 886140.755580, mae: 2895.052333, accuracy: 0.128348, mean_q: -3282.133301, mean_eps: 0.267895\n",
      "  8153/50000: episode: 1082, duration: 0.018s, episode steps:   4, steps per second: 228, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 836447.343750, mae: 2883.897461, accuracy: 0.164062, mean_q: -3282.087891, mean_eps: 0.266455\n",
      "  8156/50000: episode: 1083, duration: 0.015s, episode steps:   3, steps per second: 198, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 996189.520833, mae: 2927.982585, accuracy: 0.135417, mean_q: -3307.258301, mean_eps: 0.266140\n",
      "  8159/50000: episode: 1084, duration: 0.014s, episode steps:   3, steps per second: 212, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 713516.166667, mae: 2860.272624, accuracy: 0.177083, mean_q: -3288.831462, mean_eps: 0.265870\n",
      "  8163/50000: episode: 1085, duration: 0.019s, episode steps:   4, steps per second: 214, episode reward: -1238.000, mean reward: -309.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 1023905.343750, mae: 2904.531494, accuracy: 0.148438, mean_q: -3294.701965, mean_eps: 0.265555\n",
      "  8168/50000: episode: 1086, duration: 0.020s, episode steps:   5, steps per second: 244, episode reward: -2193.000, mean reward: -438.600 [-999.000, -32.000], mean action: 1.600 [0.000, 3.000],  loss: 666286.287500, mae: 2840.619775, accuracy: 0.150000, mean_q: -3273.603662, mean_eps: 0.265150\n",
      "  8172/50000: episode: 1087, duration: 0.018s, episode steps:   4, steps per second: 221, episode reward: -1238.000, mean reward: -309.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 739041.187500, mae: 2861.841125, accuracy: 0.156250, mean_q: -3284.230164, mean_eps: 0.264745\n",
      "  8175/50000: episode: 1088, duration: 0.015s, episode steps:   3, steps per second: 197, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 833657.333333, mae: 2871.857340, accuracy: 0.187500, mean_q: -3292.240234, mean_eps: 0.264430\n",
      "  8178/50000: episode: 1089, duration: 0.017s, episode steps:   3, steps per second: 173, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 848521.041667, mae: 2880.729004, accuracy: 0.125000, mean_q: -3302.237793, mean_eps: 0.264160\n",
      "  8181/50000: episode: 1090, duration: 0.021s, episode steps:   3, steps per second: 146, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 862333.916667, mae: 2884.793864, accuracy: 0.166667, mean_q: -3293.616455, mean_eps: 0.263890\n",
      "  8184/50000: episode: 1091, duration: 0.014s, episode steps:   3, steps per second: 222, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 893124.375000, mae: 2872.140299, accuracy: 0.187500, mean_q: -3267.272868, mean_eps: 0.263620\n",
      "  8187/50000: episode: 1092, duration: 0.014s, episode steps:   3, steps per second: 215, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 796734.937500, mae: 2900.760417, accuracy: 0.083333, mean_q: -3310.408610, mean_eps: 0.263350\n",
      "  8197/50000: episode: 1093, duration: 0.040s, episode steps:  10, steps per second: 249, episode reward: -7188.000, mean reward: -718.800 [-999.000, -58.000], mean action: 2.300 [0.000, 3.000],  loss: 957903.812500, mae: 2894.321265, accuracy: 0.140625, mean_q: -3277.129639, mean_eps: 0.262765\n",
      "  8200/50000: episode: 1094, duration: 0.014s, episode steps:   3, steps per second: 217, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 862241.854167, mae: 2893.742839, accuracy: 0.125000, mean_q: -3275.087484, mean_eps: 0.262180\n",
      "  8203/50000: episode: 1095, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 955521.562500, mae: 2880.235840, accuracy: 0.177083, mean_q: -3256.578695, mean_eps: 0.261910\n",
      "  8212/50000: episode: 1096, duration: 0.037s, episode steps:   9, steps per second: 246, episode reward: -6189.000, mean reward: -687.667 [-999.000, -45.000], mean action: 2.222 [0.000, 3.000],  loss: 926007.506944, mae: 2847.833089, accuracy: 0.291667, mean_q: -3216.275146, mean_eps: 0.261370\n",
      "  8218/50000: episode: 1097, duration: 0.024s, episode steps:   6, steps per second: 252, episode reward: -3221.000, mean reward: -536.833 [-999.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 741917.406250, mae: 2834.147868, accuracy: 0.302083, mean_q: -3227.858887, mean_eps: 0.260695\n",
      "  8225/50000: episode: 1098, duration: 0.029s, episode steps:   7, steps per second: 239, episode reward: -4220.000, mean reward: -602.857 [-999.000, -32.000], mean action: 1.000 [0.000, 3.000],  loss: 721942.303571, mae: 2840.676514, accuracy: 0.392857, mean_q: -3237.791574, mean_eps: 0.260110\n",
      "  8241/50000: episode: 1099, duration: 0.064s, episode steps:  16, steps per second: 250, episode reward: -13182.000, mean reward: -823.875 [-999.000, -32.000], mean action: 0.562 [0.000, 3.000],  loss: 684929.565430, mae: 2901.822815, accuracy: 0.326172, mean_q: -3340.416367, mean_eps: 0.259075\n",
      "  8257/50000: episode: 1100, duration: 0.069s, episode steps:  16, steps per second: 233, episode reward: -13182.000, mean reward: -823.875 [-999.000, -32.000], mean action: 0.750 [0.000, 3.000],  loss: 943149.455078, mae: 2956.060303, accuracy: 0.289062, mean_q: -3368.166473, mean_eps: 0.257635\n",
      "  8281/50000: episode: 1101, duration: 0.102s, episode steps:  24, steps per second: 234, episode reward: -21203.000, mean reward: -883.458 [-999.000, -32.000], mean action: 0.625 [0.000, 3.000],  loss: 940882.125000, mae: 2899.218750, accuracy: 0.339844, mean_q: -3274.320475, mean_eps: 0.255835\n",
      "  8295/50000: episode: 1102, duration: 0.058s, episode steps:  14, steps per second: 242, episode reward: -11184.000, mean reward: -798.857 [-999.000, -32.000], mean action: 0.786 [0.000, 3.000],  loss: 848476.069196, mae: 2843.242013, accuracy: 0.341518, mean_q: -3217.563215, mean_eps: 0.254125\n",
      "  8298/50000: episode: 1103, duration: 0.013s, episode steps:   3, steps per second: 230, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 741320.166667, mae: 2828.795410, accuracy: 0.302083, mean_q: -3228.571533, mean_eps: 0.253360\n",
      "  8301/50000: episode: 1104, duration: 0.013s, episode steps:   3, steps per second: 224, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 1061327.500000, mae: 2897.241374, accuracy: 0.291667, mean_q: -3225.793213, mean_eps: 0.253090\n",
      "  8304/50000: episode: 1105, duration: 0.015s, episode steps:   3, steps per second: 206, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 934756.000000, mae: 2855.676270, accuracy: 0.166667, mean_q: -3254.365560, mean_eps: 0.252820\n",
      "  8308/50000: episode: 1106, duration: 0.017s, episode steps:   4, steps per second: 229, episode reward: -1223.000, mean reward: -305.750 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 769030.078125, mae: 2861.502380, accuracy: 0.117188, mean_q: -3256.195923, mean_eps: 0.252505\n",
      "  8312/50000: episode: 1107, duration: 0.017s, episode steps:   4, steps per second: 230, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 907810.023438, mae: 2877.541443, accuracy: 0.156250, mean_q: -3250.528564, mean_eps: 0.252145\n",
      "  8323/50000: episode: 1108, duration: 0.040s, episode steps:  11, steps per second: 272, episode reward: -8231.000, mean reward: -748.273 [-999.000, -60.000], mean action: 0.909 [0.000, 3.000],  loss: 737761.255682, mae: 2861.564964, accuracy: 0.164773, mean_q: -3260.223145, mean_eps: 0.251470\n",
      "  8327/50000: episode: 1109, duration: 0.015s, episode steps:   4, steps per second: 267, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 971888.437500, mae: 2908.008362, accuracy: 0.117188, mean_q: -3288.670105, mean_eps: 0.250795\n",
      "  8330/50000: episode: 1110, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 871477.375000, mae: 2879.015869, accuracy: 0.135417, mean_q: -3276.427816, mean_eps: 0.250480\n",
      "  8335/50000: episode: 1111, duration: 0.019s, episode steps:   5, steps per second: 258, episode reward: -2222.000, mean reward: -444.400 [-999.000, -58.000], mean action: 1.200 [0.000, 3.000],  loss: 663919.762500, mae: 2858.336816, accuracy: 0.156250, mean_q: -3278.422021, mean_eps: 0.250120\n",
      "  8341/50000: episode: 1112, duration: 0.023s, episode steps:   6, steps per second: 260, episode reward: -3192.000, mean reward: -532.000 [-999.000, -58.000], mean action: 1.000 [0.000, 3.000],  loss: 856484.395833, mae: 2903.252889, accuracy: 0.151042, mean_q: -3298.217244, mean_eps: 0.249625\n",
      "  8347/50000: episode: 1113, duration: 0.022s, episode steps:   6, steps per second: 274, episode reward: -3192.000, mean reward: -532.000 [-999.000, -32.000], mean action: 1.167 [0.000, 3.000],  loss: 708291.531250, mae: 2883.424805, accuracy: 0.135417, mean_q: -3304.542196, mean_eps: 0.249085\n",
      "  8350/50000: episode: 1114, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 1.667 [0.000, 3.000],  loss: 500178.625000, mae: 2887.330648, accuracy: 0.125000, mean_q: -3343.351562, mean_eps: 0.248680\n",
      "  8353/50000: episode: 1115, duration: 0.012s, episode steps:   3, steps per second: 256, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 724643.562500, mae: 2903.843994, accuracy: 0.229167, mean_q: -3326.928955, mean_eps: 0.248410\n",
      "  8356/50000: episode: 1116, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 1.667 [0.000, 3.000],  loss: 748904.875000, mae: 2886.412109, accuracy: 0.208333, mean_q: -3340.436035, mean_eps: 0.248140\n",
      "  8361/50000: episode: 1117, duration: 0.018s, episode steps:   5, steps per second: 284, episode reward: -2193.000, mean reward: -438.600 [-999.000, -32.000], mean action: 1.600 [1.000, 3.000],  loss: 781885.675000, mae: 2912.158887, accuracy: 0.256250, mean_q: -3332.265527, mean_eps: 0.247780\n",
      "  8364/50000: episode: 1118, duration: 0.012s, episode steps:   3, steps per second: 256, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 890428.020833, mae: 2927.264567, accuracy: 0.229167, mean_q: -3335.420654, mean_eps: 0.247420\n",
      "  8368/50000: episode: 1119, duration: 0.015s, episode steps:   4, steps per second: 274, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.750 [1.000, 3.000],  loss: 840859.515625, mae: 2918.206604, accuracy: 0.179688, mean_q: -3323.828674, mean_eps: 0.247105\n",
      "  8371/50000: episode: 1120, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 492004.354167, mae: 2837.259115, accuracy: 0.177083, mean_q: -3303.620768, mean_eps: 0.246790\n",
      "  8374/50000: episode: 1121, duration: 0.012s, episode steps:   3, steps per second: 254, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 813542.437500, mae: 2901.004883, accuracy: 0.177083, mean_q: -3332.040365, mean_eps: 0.246520\n",
      "  8377/50000: episode: 1122, duration: 0.011s, episode steps:   3, steps per second: 261, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 1028105.604167, mae: 2958.339193, accuracy: 0.166667, mean_q: -3333.504232, mean_eps: 0.246250\n",
      "  8381/50000: episode: 1123, duration: 0.015s, episode steps:   4, steps per second: 268, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.750 [0.000, 3.000],  loss: 931578.539062, mae: 2929.539612, accuracy: 0.234375, mean_q: -3331.692444, mean_eps: 0.245935\n",
      "  8384/50000: episode: 1124, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 657474.916667, mae: 2897.749837, accuracy: 0.114583, mean_q: -3340.538086, mean_eps: 0.245620\n",
      "  8387/50000: episode: 1125, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 695821.041667, mae: 2915.436768, accuracy: 0.177083, mean_q: -3343.542399, mean_eps: 0.245350\n",
      "  8390/50000: episode: 1126, duration: 0.013s, episode steps:   3, steps per second: 225, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 748388.979167, mae: 2905.018717, accuracy: 0.135417, mean_q: -3340.658040, mean_eps: 0.245080\n",
      "  8396/50000: episode: 1127, duration: 0.022s, episode steps:   6, steps per second: 275, episode reward: -3192.000, mean reward: -532.000 [-999.000, -32.000], mean action: 0.500 [0.000, 2.000],  loss: 780328.083333, mae: 2921.126424, accuracy: 0.145833, mean_q: -3339.639404, mean_eps: 0.244675\n",
      "  8400/50000: episode: 1128, duration: 0.015s, episode steps:   4, steps per second: 260, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 679100.953125, mae: 2893.641724, accuracy: 0.210938, mean_q: -3342.869019, mean_eps: 0.244225\n",
      "  8403/50000: episode: 1129, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 946395.479167, mae: 2950.471273, accuracy: 0.197917, mean_q: -3334.951986, mean_eps: 0.243910\n",
      "  8407/50000: episode: 1130, duration: 0.015s, episode steps:   4, steps per second: 271, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.750 [0.000, 3.000],  loss: 972977.765625, mae: 2961.170044, accuracy: 0.164062, mean_q: -3358.471069, mean_eps: 0.243595\n",
      "  8410/50000: episode: 1131, duration: 0.012s, episode steps:   3, steps per second: 255, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 1207532.718750, mae: 2979.519368, accuracy: 0.135417, mean_q: -3345.482829, mean_eps: 0.243280\n",
      "  8413/50000: episode: 1132, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 889517.687500, mae: 2910.010905, accuracy: 0.177083, mean_q: -3323.108805, mean_eps: 0.243010\n",
      "  8416/50000: episode: 1133, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 961425.437500, mae: 2900.750407, accuracy: 0.156250, mean_q: -3302.378581, mean_eps: 0.242740\n",
      "  8437/50000: episode: 1134, duration: 0.066s, episode steps:  21, steps per second: 319, episode reward: -18206.000, mean reward: -866.952 [-999.000, -32.000], mean action: 0.524 [0.000, 3.000],  loss: 844507.860863, mae: 2874.249535, accuracy: 0.177083, mean_q: -3250.647089, mean_eps: 0.241660\n",
      "  8457/50000: episode: 1135, duration: 0.073s, episode steps:  20, steps per second: 274, episode reward: -17178.000, mean reward: -858.900 [-999.000, -58.000], mean action: 2.850 [1.000, 3.000],  loss: 783186.225000, mae: 2885.970251, accuracy: 0.109375, mean_q: -3278.232434, mean_eps: 0.239815\n",
      "  8481/50000: episode: 1136, duration: 0.086s, episode steps:  24, steps per second: 278, episode reward: -21174.000, mean reward: -882.250 [-999.000, -45.000], mean action: 2.583 [0.000, 3.000],  loss: 890093.630208, mae: 2905.915090, accuracy: 0.022135, mean_q: -3277.882345, mean_eps: 0.237835\n",
      "  8513/50000: episode: 1137, duration: 0.111s, episode steps:  32, steps per second: 287, episode reward: -29166.000, mean reward: -911.438 [-999.000, -45.000], mean action: 2.031 [0.000, 3.000],  loss: 947588.009766, mae: 2882.409569, accuracy: 0.013672, mean_q: -3226.149956, mean_eps: 0.235315\n",
      "  8516/50000: episode: 1138, duration: 0.015s, episode steps:   3, steps per second: 195, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 1057870.250000, mae: 2860.436361, accuracy: 0.052083, mean_q: -3167.464925, mean_eps: 0.233740\n",
      "  8520/50000: episode: 1139, duration: 0.015s, episode steps:   4, steps per second: 266, episode reward: -1223.000, mean reward: -305.750 [-999.000, -32.000], mean action: 1.750 [1.000, 3.000],  loss: 482648.632812, mae: 2745.850525, accuracy: 0.078125, mean_q: -3155.672180, mean_eps: 0.233425\n",
      "  8524/50000: episode: 1140, duration: 0.017s, episode steps:   4, steps per second: 242, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 826560.773438, mae: 2818.740723, accuracy: 0.109375, mean_q: -3177.139587, mean_eps: 0.233065\n",
      "  8548/50000: episode: 1141, duration: 0.076s, episode steps:  24, steps per second: 318, episode reward: -21203.000, mean reward: -883.458 [-999.000, -32.000], mean action: 0.250 [0.000, 3.000],  loss: 880064.243490, mae: 2850.844564, accuracy: 0.169271, mean_q: -3209.744202, mean_eps: 0.231805\n",
      "  8561/50000: episode: 1142, duration: 0.041s, episode steps:  13, steps per second: 315, episode reward: -10214.000, mean reward: -785.692 [-999.000, -58.000], mean action: 1.846 [0.000, 3.000],  loss: 763272.939904, mae: 2832.535194, accuracy: 0.163462, mean_q: -3212.003963, mean_eps: 0.230140\n",
      "  8598/50000: episode: 1143, duration: 0.116s, episode steps:  37, steps per second: 319, episode reward: -34161.000, mean reward: -923.270 [-999.000, -45.000], mean action: 1.892 [0.000, 3.000],  loss: 813155.713682, mae: 2843.912888, accuracy: 0.177365, mean_q: -3218.618256, mean_eps: 0.227890\n",
      "  8602/50000: episode: 1144, duration: 0.015s, episode steps:   4, steps per second: 268, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 0.750 [0.000, 2.000],  loss: 878537.140625, mae: 2860.096008, accuracy: 0.195312, mean_q: -3228.136230, mean_eps: 0.226045\n",
      "  8605/50000: episode: 1145, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.333 [0.000, 3.000],  loss: 866006.645833, mae: 2860.218424, accuracy: 0.197917, mean_q: -3213.688802, mean_eps: 0.225730\n",
      "  8608/50000: episode: 1146, duration: 0.011s, episode steps:   3, steps per second: 262, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.333 [0.000, 3.000],  loss: 945330.520833, mae: 2865.556803, accuracy: 0.239583, mean_q: -3210.217204, mean_eps: 0.225460\n",
      "  8621/50000: episode: 1147, duration: 0.042s, episode steps:  13, steps per second: 312, episode reward: -10229.000, mean reward: -786.846 [-999.000, -60.000], mean action: 1.077 [0.000, 3.000],  loss: 864530.783654, mae: 2852.898287, accuracy: 0.163462, mean_q: -3191.087834, mean_eps: 0.224740\n",
      "  8624/50000: episode: 1148, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 956470.750000, mae: 2850.668538, accuracy: 0.187500, mean_q: -3210.547282, mean_eps: 0.224020\n",
      "  8630/50000: episode: 1149, duration: 0.021s, episode steps:   6, steps per second: 288, episode reward: -3192.000, mean reward: -532.000 [-999.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 769453.885417, mae: 2831.398641, accuracy: 0.145833, mean_q: -3209.056478, mean_eps: 0.223615\n",
      "  8634/50000: episode: 1150, duration: 0.015s, episode steps:   4, steps per second: 268, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 612163.765625, mae: 2812.725769, accuracy: 0.187500, mean_q: -3206.827576, mean_eps: 0.223165\n",
      "  8637/50000: episode: 1151, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 950261.791667, mae: 2871.701090, accuracy: 0.166667, mean_q: -3232.017171, mean_eps: 0.222850\n",
      "  8646/50000: episode: 1152, duration: 0.030s, episode steps:   9, steps per second: 303, episode reward: -6218.000, mean reward: -690.889 [-999.000, -32.000], mean action: 1.000 [0.000, 3.000],  loss: 987830.635417, mae: 2878.383382, accuracy: 0.163194, mean_q: -3226.886963, mean_eps: 0.222310\n",
      "  8666/50000: episode: 1153, duration: 0.062s, episode steps:  20, steps per second: 325, episode reward: -17222.000, mean reward: -861.100 [-999.000, -60.000], mean action: 0.450 [0.000, 3.000],  loss: 836772.762500, mae: 2824.632483, accuracy: 0.148438, mean_q: -3194.488977, mean_eps: 0.221005\n",
      "  8677/50000: episode: 1154, duration: 0.044s, episode steps:  11, steps per second: 252, episode reward: -8216.000, mean reward: -746.909 [-999.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 807647.090909, mae: 2833.580256, accuracy: 0.090909, mean_q: -3191.084739, mean_eps: 0.219610\n",
      "  8680/50000: episode: 1155, duration: 0.012s, episode steps:   3, steps per second: 258, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 903492.072917, mae: 2842.783122, accuracy: 0.156250, mean_q: -3182.621257, mean_eps: 0.218980\n",
      "  8684/50000: episode: 1156, duration: 0.015s, episode steps:   4, steps per second: 268, episode reward: -1223.000, mean reward: -305.750 [-999.000, -32.000], mean action: 1.750 [1.000, 3.000],  loss: 807333.960938, mae: 2818.840515, accuracy: 0.125000, mean_q: -3186.425171, mean_eps: 0.218665\n",
      "  8687/50000: episode: 1157, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 702353.604167, mae: 2815.582113, accuracy: 0.083333, mean_q: -3191.321452, mean_eps: 0.218350\n",
      "  8690/50000: episode: 1158, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 766037.322917, mae: 2829.332275, accuracy: 0.093750, mean_q: -3195.286458, mean_eps: 0.218080\n",
      "  8693/50000: episode: 1159, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 851475.166667, mae: 2855.523438, accuracy: 0.093750, mean_q: -3213.538005, mean_eps: 0.217810\n",
      "  8696/50000: episode: 1160, duration: 0.012s, episode steps:   3, steps per second: 258, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 820502.145833, mae: 2837.230387, accuracy: 0.135417, mean_q: -3213.110107, mean_eps: 0.217540\n",
      "  8701/50000: episode: 1161, duration: 0.018s, episode steps:   5, steps per second: 273, episode reward: -2193.000, mean reward: -438.600 [-999.000, -32.000], mean action: 1.600 [1.000, 3.000],  loss: 903715.137500, mae: 2856.131592, accuracy: 0.093750, mean_q: -3204.871338, mean_eps: 0.217180\n",
      "  8704/50000: episode: 1162, duration: 0.012s, episode steps:   3, steps per second: 255, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 895208.062500, mae: 2835.892985, accuracy: 0.062500, mean_q: -3202.525960, mean_eps: 0.216820\n",
      "  8708/50000: episode: 1163, duration: 0.015s, episode steps:   4, steps per second: 269, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 947255.062500, mae: 2819.640320, accuracy: 0.085938, mean_q: -3181.507446, mean_eps: 0.216505\n",
      "  8711/50000: episode: 1164, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 932324.354167, mae: 2832.262288, accuracy: 0.156250, mean_q: -3177.938802, mean_eps: 0.216190\n",
      "  8715/50000: episode: 1165, duration: 0.015s, episode steps:   4, steps per second: 270, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.750 [0.000, 3.000],  loss: 809905.921875, mae: 2823.667175, accuracy: 0.125000, mean_q: -3181.128723, mean_eps: 0.215875\n",
      "  8718/50000: episode: 1166, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 553783.750000, mae: 2771.896077, accuracy: 0.125000, mean_q: -3173.035563, mean_eps: 0.215560\n",
      "  8721/50000: episode: 1167, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 1.667 [0.000, 3.000],  loss: 813981.229167, mae: 2847.338379, accuracy: 0.135417, mean_q: -3192.641683, mean_eps: 0.215290\n",
      "  8724/50000: episode: 1168, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 1.667 [0.000, 3.000],  loss: 708823.520833, mae: 2801.277100, accuracy: 0.166667, mean_q: -3180.329915, mean_eps: 0.215020\n",
      "  8727/50000: episode: 1169, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 1.667 [0.000, 3.000],  loss: 875947.854167, mae: 2863.867513, accuracy: 0.135417, mean_q: -3208.647868, mean_eps: 0.214750\n",
      "  8731/50000: episode: 1170, duration: 0.017s, episode steps:   4, steps per second: 242, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.750 [0.000, 3.000],  loss: 784318.796875, mae: 2826.939697, accuracy: 0.179688, mean_q: -3198.852600, mean_eps: 0.214435\n",
      "  8747/50000: episode: 1171, duration: 0.051s, episode steps:  16, steps per second: 314, episode reward: -13182.000, mean reward: -823.875 [-999.000, -32.000], mean action: 2.625 [0.000, 3.000],  loss: 742508.587891, mae: 2840.382172, accuracy: 0.195312, mean_q: -3220.901566, mean_eps: 0.213535\n",
      "  8754/50000: episode: 1172, duration: 0.024s, episode steps:   7, steps per second: 296, episode reward: -4220.000, mean reward: -602.857 [-999.000, -58.000], mean action: 2.143 [0.000, 3.000],  loss: 928322.758929, mae: 2866.950788, accuracy: 0.183036, mean_q: -3226.352644, mean_eps: 0.212500\n",
      "  8757/50000: episode: 1173, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 1255194.125000, mae: 2884.141113, accuracy: 0.145833, mean_q: -3203.013997, mean_eps: 0.212050\n",
      "  8778/50000: episode: 1174, duration: 0.065s, episode steps:  21, steps per second: 323, episode reward: -18206.000, mean reward: -866.952 [-999.000, -32.000], mean action: 0.429 [0.000, 3.000],  loss: 840966.820685, mae: 2802.564267, accuracy: 0.135417, mean_q: -3150.804501, mean_eps: 0.210970\n",
      "  8781/50000: episode: 1175, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 773044.541667, mae: 2776.596842, accuracy: 0.093750, mean_q: -3154.083415, mean_eps: 0.209890\n",
      "  8784/50000: episode: 1176, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 898612.583333, mae: 2820.483398, accuracy: 0.104167, mean_q: -3152.566243, mean_eps: 0.209620\n",
      "  8788/50000: episode: 1177, duration: 0.015s, episode steps:   4, steps per second: 258, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 2.000 [0.000, 3.000],  loss: 828403.265625, mae: 2807.867798, accuracy: 0.085938, mean_q: -3154.953796, mean_eps: 0.209305\n",
      "  8792/50000: episode: 1178, duration: 0.016s, episode steps:   4, steps per second: 248, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.750 [0.000, 3.000],  loss: 936921.046875, mae: 2828.956543, accuracy: 0.109375, mean_q: -3157.615295, mean_eps: 0.208945\n",
      "  8801/50000: episode: 1179, duration: 0.030s, episode steps:   9, steps per second: 298, episode reward: -6218.000, mean reward: -690.889 [-999.000, -32.000], mean action: 0.667 [0.000, 3.000],  loss: 1055826.923611, mae: 2835.439480, accuracy: 0.128472, mean_q: -3150.957655, mean_eps: 0.208360\n",
      "  8826/50000: episode: 1180, duration: 0.077s, episode steps:  25, steps per second: 323, episode reward: -22202.000, mean reward: -888.080 [-999.000, -32.000], mean action: 0.640 [0.000, 3.000],  loss: 816270.923750, mae: 2769.802598, accuracy: 0.175000, mean_q: -3104.360547, mean_eps: 0.206830\n",
      "  8829/50000: episode: 1181, duration: 0.012s, episode steps:   3, steps per second: 254, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 844896.437500, mae: 2802.031657, accuracy: 0.250000, mean_q: -3112.436930, mean_eps: 0.205570\n",
      "  8833/50000: episode: 1182, duration: 0.015s, episode steps:   4, steps per second: 270, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.000 [0.000, 3.000],  loss: 804673.203125, mae: 2794.450195, accuracy: 0.312500, mean_q: -3148.673035, mean_eps: 0.205255\n",
      "  8842/50000: episode: 1183, duration: 0.030s, episode steps:   9, steps per second: 301, episode reward: -6233.000, mean reward: -692.556 [-999.000, -60.000], mean action: 1.111 [0.000, 3.000],  loss: 863104.895833, mae: 2837.377577, accuracy: 0.218750, mean_q: -3184.951362, mean_eps: 0.204670\n",
      "  8863/50000: episode: 1184, duration: 0.072s, episode steps:  21, steps per second: 293, episode reward: -18206.000, mean reward: -866.952 [-999.000, -58.000], mean action: 2.667 [0.000, 3.000],  loss: 914155.141369, mae: 2813.835031, accuracy: 0.218750, mean_q: -3165.946022, mean_eps: 0.203320\n",
      "  8866/50000: episode: 1185, duration: 0.012s, episode steps:   3, steps per second: 257, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 402367.656250, mae: 2693.045573, accuracy: 0.197917, mean_q: -3111.705160, mean_eps: 0.202240\n",
      "  8870/50000: episode: 1186, duration: 0.016s, episode steps:   4, steps per second: 257, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 928395.125000, mae: 2797.705750, accuracy: 0.218750, mean_q: -3104.074036, mean_eps: 0.201925\n",
      "  8873/50000: episode: 1187, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 837985.583333, mae: 2778.213867, accuracy: 0.229167, mean_q: -3111.644043, mean_eps: 0.201610\n",
      "  8876/50000: episode: 1188, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 769690.687500, mae: 2757.215902, accuracy: 0.218750, mean_q: -3086.495199, mean_eps: 0.201340\n",
      "  8882/50000: episode: 1189, duration: 0.022s, episode steps:   6, steps per second: 272, episode reward: -3236.000, mean reward: -539.333 [-999.000, -45.000], mean action: 2.000 [0.000, 3.000],  loss: 784342.677083, mae: 2786.370361, accuracy: 0.203125, mean_q: -3108.617635, mean_eps: 0.200935\n",
      "  8893/50000: episode: 1190, duration: 0.037s, episode steps:  11, steps per second: 301, episode reward: -8231.000, mean reward: -748.273 [-999.000, -60.000], mean action: 1.091 [0.000, 3.000],  loss: 544185.573864, mae: 2755.021573, accuracy: 0.272727, mean_q: -3143.232511, mean_eps: 0.200170\n",
      "  8896/50000: episode: 1191, duration: 0.012s, episode steps:   3, steps per second: 261, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.333 [0.000, 3.000],  loss: 833099.020833, mae: 2818.576742, accuracy: 0.250000, mean_q: -3180.299316, mean_eps: 0.199540\n",
      "  8911/50000: episode: 1192, duration: 0.049s, episode steps:  15, steps per second: 305, episode reward: -12212.000, mean reward: -814.133 [-999.000, -32.000], mean action: 0.800 [0.000, 3.000],  loss: 641732.683333, mae: 2834.138802, accuracy: 0.283333, mean_q: -3231.202799, mean_eps: 0.198730\n",
      "  8919/50000: episode: 1193, duration: 0.027s, episode steps:   8, steps per second: 293, episode reward: -5190.000, mean reward: -648.750 [-999.000, -32.000], mean action: 1.000 [0.000, 3.000],  loss: 807580.210938, mae: 2867.153564, accuracy: 0.296875, mean_q: -3257.837097, mean_eps: 0.197695\n",
      "  8922/50000: episode: 1194, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 638078.604167, mae: 2842.075928, accuracy: 0.312500, mean_q: -3228.323405, mean_eps: 0.197200\n",
      "  8925/50000: episode: 1195, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 891322.708333, mae: 2875.180501, accuracy: 0.239583, mean_q: -3219.715739, mean_eps: 0.196930\n",
      "  8929/50000: episode: 1196, duration: 0.016s, episode steps:   4, steps per second: 257, episode reward: -1223.000, mean reward: -305.750 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 734552.421875, mae: 2834.414185, accuracy: 0.265625, mean_q: -3217.408813, mean_eps: 0.196615\n",
      "  8932/50000: episode: 1197, duration: 0.015s, episode steps:   3, steps per second: 204, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 758747.750000, mae: 2827.731689, accuracy: 0.270833, mean_q: -3212.308187, mean_eps: 0.196300\n",
      "  8936/50000: episode: 1198, duration: 0.016s, episode steps:   4, steps per second: 247, episode reward: -1238.000, mean reward: -309.500 [-999.000, -60.000], mean action: 1.500 [0.000, 3.000],  loss: 650385.796875, mae: 2819.875122, accuracy: 0.210938, mean_q: -3216.019226, mean_eps: 0.195985\n",
      "  8939/50000: episode: 1199, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 746204.750000, mae: 2849.900553, accuracy: 0.218750, mean_q: -3220.757080, mean_eps: 0.195670\n",
      "  8942/50000: episode: 1200, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 563150.822917, mae: 2820.005941, accuracy: 0.312500, mean_q: -3231.085368, mean_eps: 0.195400\n",
      "  8945/50000: episode: 1201, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 814702.458333, mae: 2867.749674, accuracy: 0.218750, mean_q: -3236.701172, mean_eps: 0.195130\n",
      "  8949/50000: episode: 1202, duration: 0.015s, episode steps:   4, steps per second: 275, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 780532.312500, mae: 2861.625671, accuracy: 0.289062, mean_q: -3258.153442, mean_eps: 0.194815\n",
      "  8952/50000: episode: 1203, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 746797.270833, mae: 2864.521566, accuracy: 0.291667, mean_q: -3257.134521, mean_eps: 0.194500\n",
      "  8955/50000: episode: 1204, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 742260.895833, mae: 2852.254313, accuracy: 0.281250, mean_q: -3249.471273, mean_eps: 0.194230\n",
      "  8959/50000: episode: 1205, duration: 0.015s, episode steps:   4, steps per second: 263, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.750 [0.000, 3.000],  loss: 734480.585938, mae: 2849.016479, accuracy: 0.265625, mean_q: -3254.462524, mean_eps: 0.193915\n",
      "  8962/50000: episode: 1206, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 792093.541667, mae: 2873.920329, accuracy: 0.250000, mean_q: -3250.803467, mean_eps: 0.193600\n",
      "  8965/50000: episode: 1207, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 483138.067708, mae: 2792.693522, accuracy: 0.218750, mean_q: -3231.889567, mean_eps: 0.193330\n",
      "  8968/50000: episode: 1208, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 834202.500000, mae: 2853.175293, accuracy: 0.125000, mean_q: -3245.737467, mean_eps: 0.193060\n",
      "  8971/50000: episode: 1209, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 642344.250000, mae: 2835.711182, accuracy: 0.125000, mean_q: -3240.713867, mean_eps: 0.192790\n",
      "  8974/50000: episode: 1210, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 1079658.000000, mae: 2879.767660, accuracy: 0.072917, mean_q: -3222.782715, mean_eps: 0.192520\n",
      "  8990/50000: episode: 1211, duration: 0.055s, episode steps:  16, steps per second: 289, episode reward: -13182.000, mean reward: -823.875 [-999.000, -58.000], mean action: 1.875 [0.000, 3.000],  loss: 765738.947266, mae: 2834.552948, accuracy: 0.070312, mean_q: -3198.264587, mean_eps: 0.191665\n",
      "  9007/50000: episode: 1212, duration: 0.053s, episode steps:  17, steps per second: 320, episode reward: -14181.000, mean reward: -834.176 [-999.000, -45.000], mean action: 1.941 [0.000, 3.000],  loss: 817845.297794, mae: 2838.961239, accuracy: 0.042279, mean_q: -3198.019086, mean_eps: 0.190180\n",
      "  9010/50000: episode: 1213, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 679642.041667, mae: 2815.198486, accuracy: 0.000000, mean_q: -3188.878743, mean_eps: 0.189280\n",
      "  9015/50000: episode: 1214, duration: 0.018s, episode steps:   5, steps per second: 277, episode reward: -2193.000, mean reward: -438.600 [-999.000, -58.000], mean action: 1.600 [0.000, 3.000],  loss: 851607.737500, mae: 2848.564697, accuracy: 0.025000, mean_q: -3188.718506, mean_eps: 0.188920\n",
      "  9025/50000: episode: 1215, duration: 0.033s, episode steps:  10, steps per second: 300, episode reward: -7217.000, mean reward: -721.700 [-999.000, -58.000], mean action: 2.600 [0.000, 3.000],  loss: 923899.656250, mae: 2856.836328, accuracy: 0.059375, mean_q: -3185.848218, mean_eps: 0.188245\n",
      "  9032/50000: episode: 1216, duration: 0.023s, episode steps:   7, steps per second: 299, episode reward: -4191.000, mean reward: -598.714 [-999.000, -58.000], mean action: 1.857 [0.000, 3.000],  loss: 779904.879464, mae: 2801.506243, accuracy: 0.044643, mean_q: -3140.428816, mean_eps: 0.187480\n",
      "  9039/50000: episode: 1217, duration: 0.024s, episode steps:   7, steps per second: 295, episode reward: -4191.000, mean reward: -598.714 [-999.000, -45.000], mean action: 1.429 [0.000, 2.000],  loss: 899863.160714, mae: 2808.757150, accuracy: 0.017857, mean_q: -3131.257603, mean_eps: 0.186850\n",
      "  9055/50000: episode: 1218, duration: 0.058s, episode steps:  16, steps per second: 274, episode reward: -13182.000, mean reward: -823.875 [-999.000, -58.000], mean action: 1.875 [0.000, 3.000],  loss: 835824.195312, mae: 2788.497726, accuracy: 0.058594, mean_q: -3123.080597, mean_eps: 0.185815\n",
      "  9058/50000: episode: 1219, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 968085.614583, mae: 2789.384766, accuracy: 0.125000, mean_q: -3118.528890, mean_eps: 0.184960\n",
      "  9061/50000: episode: 1220, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 948807.125000, mae: 2781.520915, accuracy: 0.177083, mean_q: -3099.754476, mean_eps: 0.184690\n",
      "  9065/50000: episode: 1221, duration: 0.015s, episode steps:   4, steps per second: 275, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.750 [1.000, 3.000],  loss: 606792.390625, mae: 2720.733521, accuracy: 0.203125, mean_q: -3101.835876, mean_eps: 0.184375\n",
      "  9068/50000: episode: 1222, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 808511.687500, mae: 2767.541911, accuracy: 0.229167, mean_q: -3119.002441, mean_eps: 0.184060\n",
      "  9076/50000: episode: 1223, duration: 0.027s, episode steps:   8, steps per second: 301, episode reward: -5190.000, mean reward: -648.750 [-999.000, -45.000], mean action: 1.500 [0.000, 2.000],  loss: 916017.992188, mae: 2799.425598, accuracy: 0.132812, mean_q: -3109.411652, mean_eps: 0.183565\n",
      "  9079/50000: episode: 1224, duration: 0.012s, episode steps:   3, steps per second: 255, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 964823.479167, mae: 2805.324788, accuracy: 0.364583, mean_q: -3107.748617, mean_eps: 0.183070\n",
      "  9082/50000: episode: 1225, duration: 0.012s, episode steps:   3, steps per second: 259, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 821105.770833, mae: 2788.251465, accuracy: 0.218750, mean_q: -3100.594727, mean_eps: 0.182800\n",
      "  9085/50000: episode: 1226, duration: 0.012s, episode steps:   3, steps per second: 256, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 1062045.312500, mae: 2816.753662, accuracy: 0.218750, mean_q: -3104.002035, mean_eps: 0.182530\n",
      "  9088/50000: episode: 1227, duration: 0.012s, episode steps:   3, steps per second: 255, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 921982.937500, mae: 2778.534831, accuracy: 0.156250, mean_q: -3087.239990, mean_eps: 0.182260\n",
      "  9110/50000: episode: 1228, duration: 0.074s, episode steps:  22, steps per second: 296, episode reward: -19176.000, mean reward: -871.636 [-999.000, -45.000], mean action: 1.909 [0.000, 3.000],  loss: 754965.585227, mae: 2772.250610, accuracy: 0.144886, mean_q: -3092.034990, mean_eps: 0.181135\n",
      "  9113/50000: episode: 1229, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -224.000, mean reward: -74.667 [-116.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 1080851.708333, mae: 2854.406413, accuracy: 0.125000, mean_q: -3129.916178, mean_eps: 0.180010\n",
      "  9119/50000: episode: 1230, duration: 0.022s, episode steps:   6, steps per second: 275, episode reward: -3192.000, mean reward: -532.000 [-999.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 703188.739583, mae: 2788.822510, accuracy: 0.187500, mean_q: -3118.323161, mean_eps: 0.179605\n",
      "  9142/50000: episode: 1231, duration: 0.074s, episode steps:  23, steps per second: 311, episode reward: -20175.000, mean reward: -877.174 [-999.000, -32.000], mean action: 1.217 [1.000, 3.000],  loss: 753995.038723, mae: 2834.531823, accuracy: 0.256793, mean_q: -3182.062840, mean_eps: 0.178300\n",
      "  9157/50000: episode: 1232, duration: 0.051s, episode steps:  15, steps per second: 297, episode reward: -12227.000, mean reward: -815.133 [-999.000, -45.000], mean action: 0.867 [0.000, 3.000],  loss: 822682.050000, mae: 2826.234229, accuracy: 0.302083, mean_q: -3177.377572, mean_eps: 0.176590\n",
      "  9160/50000: episode: 1233, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 738005.854167, mae: 2773.158691, accuracy: 0.416667, mean_q: -3143.744873, mean_eps: 0.175780\n",
      "  9165/50000: episode: 1234, duration: 0.019s, episode steps:   5, steps per second: 268, episode reward: -2222.000, mean reward: -444.400 [-999.000, -32.000], mean action: 1.400 [0.000, 3.000],  loss: 715727.593750, mae: 2788.588770, accuracy: 0.356250, mean_q: -3141.351855, mean_eps: 0.175420\n",
      "  9168/50000: episode: 1235, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 886682.812500, mae: 2801.705322, accuracy: 0.354167, mean_q: -3118.810791, mean_eps: 0.175060\n",
      "  9171/50000: episode: 1236, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 731544.989583, mae: 2786.107178, accuracy: 0.322917, mean_q: -3135.692952, mean_eps: 0.174790\n",
      "  9185/50000: episode: 1237, duration: 0.050s, episode steps:  14, steps per second: 280, episode reward: -11184.000, mean reward: -798.857 [-999.000, -32.000], mean action: 0.714 [0.000, 3.000],  loss: 775882.229911, mae: 2808.604318, accuracy: 0.368304, mean_q: -3145.128941, mean_eps: 0.174025\n",
      "  9188/50000: episode: 1238, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 981026.041667, mae: 2828.256917, accuracy: 0.416667, mean_q: -3142.826904, mean_eps: 0.173260\n",
      "  9191/50000: episode: 1239, duration: 0.013s, episode steps:   3, steps per second: 225, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 1130725.312500, mae: 2833.684896, accuracy: 0.364583, mean_q: -3109.399740, mean_eps: 0.172990\n",
      "  9221/50000: episode: 1240, duration: 0.106s, episode steps:  30, steps per second: 283, episode reward: -27168.000, mean reward: -905.600 [-999.000, -32.000], mean action: 1.067 [0.000, 3.000],  loss: 934765.829167, mae: 2761.006372, accuracy: 0.381250, mean_q: -3028.892424, mean_eps: 0.171505\n",
      "  9238/50000: episode: 1241, duration: 0.078s, episode steps:  17, steps per second: 219, episode reward: -14210.000, mean reward: -835.882 [-999.000, -32.000], mean action: 0.353 [0.000, 3.000],  loss: 653531.450368, mae: 2706.151726, accuracy: 0.323529, mean_q: -3026.339557, mean_eps: 0.169390\n",
      "  9249/50000: episode: 1242, duration: 0.038s, episode steps:  11, steps per second: 288, episode reward: -8187.000, mean reward: -744.273 [-999.000, -32.000], mean action: 0.909 [0.000, 3.000],  loss: 726728.664773, mae: 2778.246227, accuracy: 0.380682, mean_q: -3115.985329, mean_eps: 0.168130\n",
      "  9261/50000: episode: 1243, duration: 0.042s, episode steps:  12, steps per second: 283, episode reward: -9186.000, mean reward: -765.500 [-999.000, -32.000], mean action: 0.333 [0.000, 2.000],  loss: 782103.649740, mae: 2817.396586, accuracy: 0.356771, mean_q: -3149.561422, mean_eps: 0.167095\n",
      "  9281/50000: episode: 1244, duration: 0.072s, episode steps:  20, steps per second: 279, episode reward: -17207.000, mean reward: -860.350 [-999.000, -32.000], mean action: 1.050 [0.000, 3.000],  loss: 793534.251563, mae: 2798.658264, accuracy: 0.254688, mean_q: -3126.457727, mean_eps: 0.165655\n",
      "  9287/50000: episode: 1245, duration: 0.023s, episode steps:   6, steps per second: 261, episode reward: -3192.000, mean reward: -532.000 [-999.000, -32.000], mean action: 1.000 [0.000, 3.000],  loss: 897786.687500, mae: 2801.184977, accuracy: 0.109375, mean_q: -3097.928060, mean_eps: 0.164485\n",
      "  9301/50000: episode: 1246, duration: 0.050s, episode steps:  14, steps per second: 283, episode reward: -11184.000, mean reward: -798.857 [-999.000, -45.000], mean action: 1.786 [0.000, 2.000],  loss: 862781.937500, mae: 2750.278181, accuracy: 0.091518, mean_q: -3055.455828, mean_eps: 0.163585\n",
      "  9306/50000: episode: 1247, duration: 0.021s, episode steps:   5, steps per second: 237, episode reward: -2193.000, mean reward: -438.600 [-999.000, -45.000], mean action: 2.000 [0.000, 3.000],  loss: 755694.612500, mae: 2723.521045, accuracy: 0.225000, mean_q: -3042.472266, mean_eps: 0.162730\n",
      "  9329/50000: episode: 1248, duration: 0.079s, episode steps:  23, steps per second: 292, episode reward: -20175.000, mean reward: -877.174 [-999.000, -32.000], mean action: 0.696 [0.000, 3.000],  loss: 797306.351902, mae: 2728.870839, accuracy: 0.349185, mean_q: -3049.095894, mean_eps: 0.161470\n",
      "  9345/50000: episode: 1249, duration: 0.050s, episode steps:  16, steps per second: 323, episode reward: -13211.000, mean reward: -825.688 [-999.000, -32.000], mean action: 0.375 [0.000, 3.000],  loss: 811084.769531, mae: 2739.737473, accuracy: 0.392578, mean_q: -3018.909912, mean_eps: 0.159715\n",
      "  9380/50000: episode: 1250, duration: 0.111s, episode steps:  35, steps per second: 316, episode reward: -32163.000, mean reward: -918.943 [-999.000, -32.000], mean action: 0.371 [0.000, 3.000],  loss: 844849.488393, mae: 2762.267746, accuracy: 0.342857, mean_q: -3050.368841, mean_eps: 0.157420\n",
      "  9387/50000: episode: 1251, duration: 0.024s, episode steps:   7, steps per second: 286, episode reward: -4191.000, mean reward: -598.714 [-999.000, -32.000], mean action: 0.857 [0.000, 3.000],  loss: 678326.607143, mae: 2707.884208, accuracy: 0.339286, mean_q: -3030.716867, mean_eps: 0.155530\n",
      "  9391/50000: episode: 1252, duration: 0.016s, episode steps:   4, steps per second: 248, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.750 [0.000, 3.000],  loss: 780429.421875, mae: 2704.663574, accuracy: 0.390625, mean_q: -3038.438660, mean_eps: 0.155035\n",
      "  9394/50000: episode: 1253, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 608578.697917, mae: 2721.898844, accuracy: 0.364583, mean_q: -3050.201497, mean_eps: 0.154720\n",
      "  9398/50000: episode: 1254, duration: 0.016s, episode steps:   4, steps per second: 257, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 785146.703125, mae: 2747.295105, accuracy: 0.398438, mean_q: -3064.955811, mean_eps: 0.154405\n",
      "  9407/50000: episode: 1255, duration: 0.030s, episode steps:   9, steps per second: 304, episode reward: -6189.000, mean reward: -687.667 [-999.000, -32.000], mean action: 1.222 [0.000, 3.000],  loss: 719682.909722, mae: 2746.504395, accuracy: 0.378472, mean_q: -3073.672201, mean_eps: 0.153820\n",
      "  9434/50000: episode: 1256, duration: 0.088s, episode steps:  27, steps per second: 309, episode reward: -24200.000, mean reward: -896.296 [-999.000, -32.000], mean action: 0.296 [0.000, 3.000],  loss: 757326.035880, mae: 2742.795989, accuracy: 0.337963, mean_q: -3068.360234, mean_eps: 0.152200\n",
      "  9462/50000: episode: 1257, duration: 0.094s, episode steps:  28, steps per second: 298, episode reward: -25170.000, mean reward: -898.929 [-999.000, -58.000], mean action: 2.000 [0.000, 3.000],  loss: 732058.530134, mae: 2724.402082, accuracy: 0.181920, mean_q: -3054.732797, mean_eps: 0.149725\n",
      "  9467/50000: episode: 1258, duration: 0.018s, episode steps:   5, steps per second: 273, episode reward: -2193.000, mean reward: -438.600 [-999.000, -45.000], mean action: 1.800 [0.000, 3.000],  loss: 867942.662500, mae: 2757.014453, accuracy: 0.162500, mean_q: -3050.951123, mean_eps: 0.148240\n",
      "  9474/50000: episode: 1259, duration: 0.024s, episode steps:   7, steps per second: 293, episode reward: -4220.000, mean reward: -602.857 [-999.000, -32.000], mean action: 1.286 [0.000, 3.000],  loss: 953384.285714, mae: 2775.248884, accuracy: 0.160714, mean_q: -3056.389823, mean_eps: 0.147700\n",
      "  9477/50000: episode: 1260, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 640639.968750, mae: 2691.978271, accuracy: 0.197917, mean_q: -3028.247152, mean_eps: 0.147250\n",
      "  9482/50000: episode: 1261, duration: 0.018s, episode steps:   5, steps per second: 279, episode reward: -2193.000, mean reward: -438.600 [-999.000, -32.000], mean action: 1.400 [0.000, 3.000],  loss: 696694.887500, mae: 2713.953369, accuracy: 0.225000, mean_q: -3034.161133, mean_eps: 0.146890\n",
      "  9498/50000: episode: 1262, duration: 0.051s, episode steps:  16, steps per second: 311, episode reward: -13211.000, mean reward: -825.688 [-999.000, -58.000], mean action: 2.625 [0.000, 3.000],  loss: 817710.611328, mae: 2741.937073, accuracy: 0.197266, mean_q: -3037.358017, mean_eps: 0.145945\n",
      "  9501/50000: episode: 1263, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -224.000, mean reward: -74.667 [-116.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 1002742.604167, mae: 2768.061849, accuracy: 0.229167, mean_q: -3034.207357, mean_eps: 0.145090\n",
      "  9504/50000: episode: 1264, duration: 0.012s, episode steps:   3, steps per second: 258, episode reward: -224.000, mean reward: -74.667 [-116.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 703556.052083, mae: 2715.529867, accuracy: 0.145833, mean_q: -3003.824788, mean_eps: 0.144820\n",
      "  9521/50000: episode: 1265, duration: 0.053s, episode steps:  17, steps per second: 319, episode reward: -14181.000, mean reward: -834.176 [-999.000, -45.000], mean action: 2.647 [0.000, 3.000],  loss: 847348.139706, mae: 2720.660759, accuracy: 0.200368, mean_q: -3020.038646, mean_eps: 0.143920\n",
      "  9524/50000: episode: 1266, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 550380.635417, mae: 2655.327881, accuracy: 0.062500, mean_q: -2993.966227, mean_eps: 0.143020\n",
      "  9541/50000: episode: 1267, duration: 0.059s, episode steps:  17, steps per second: 291, episode reward: -14210.000, mean reward: -835.882 [-999.000, -58.000], mean action: 1.882 [0.000, 3.000],  loss: 669500.417279, mae: 2711.364100, accuracy: 0.137868, mean_q: -3024.974911, mean_eps: 0.142120\n",
      "  9545/50000: episode: 1268, duration: 0.015s, episode steps:   4, steps per second: 260, episode reward: -1223.000, mean reward: -305.750 [-999.000, -32.000], mean action: 1.750 [1.000, 3.000],  loss: 710893.554688, mae: 2727.872986, accuracy: 0.132812, mean_q: -3069.438232, mean_eps: 0.141175\n",
      "  9549/50000: episode: 1269, duration: 0.015s, episode steps:   4, steps per second: 267, episode reward: -1223.000, mean reward: -305.750 [-999.000, -32.000], mean action: 1.750 [1.000, 3.000],  loss: 672631.281250, mae: 2729.147888, accuracy: 0.179688, mean_q: -3076.769897, mean_eps: 0.140815\n",
      "  9557/50000: episode: 1270, duration: 0.026s, episode steps:   8, steps per second: 303, episode reward: -5219.000, mean reward: -652.375 [-999.000, -58.000], mean action: 2.250 [0.000, 3.000],  loss: 737348.351562, mae: 2748.797058, accuracy: 0.234375, mean_q: -3076.550354, mean_eps: 0.140275\n",
      "  9560/50000: episode: 1271, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 704051.260417, mae: 2755.181641, accuracy: 0.312500, mean_q: -3083.150065, mean_eps: 0.139780\n",
      "  9623/50000: episode: 1272, duration: 0.201s, episode steps:  63, steps per second: 313, episode reward: -60164.000, mean reward: -954.984 [-999.000, -58.000], mean action: 2.000 [0.000, 3.000],  loss: 757492.068700, mae: 2749.631270, accuracy: 0.186508, mean_q: -3075.953695, mean_eps: 0.136810\n",
      "  9626/50000: episode: 1273, duration: 0.013s, episode steps:   3, steps per second: 224, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 719479.770833, mae: 2718.324137, accuracy: 0.135417, mean_q: -3021.798503, mean_eps: 0.133840\n",
      "  9629/50000: episode: 1274, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 870794.208333, mae: 2736.463135, accuracy: 0.156250, mean_q: -3012.137207, mean_eps: 0.133570\n",
      "  9632/50000: episode: 1275, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 629030.229167, mae: 2701.192871, accuracy: 0.208333, mean_q: -3014.418701, mean_eps: 0.133300\n",
      "  9641/50000: episode: 1276, duration: 0.034s, episode steps:   9, steps per second: 264, episode reward: -6218.000, mean reward: -690.889 [-999.000, -58.000], mean action: 1.778 [0.000, 3.000],  loss: 900573.645833, mae: 2744.279650, accuracy: 0.163194, mean_q: -3019.306234, mean_eps: 0.132760\n",
      "  9644/50000: episode: 1277, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 848132.166667, mae: 2720.808431, accuracy: 0.177083, mean_q: -3004.587321, mean_eps: 0.132220\n",
      "  9647/50000: episode: 1278, duration: 0.014s, episode steps:   3, steps per second: 218, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 814255.625000, mae: 2717.147054, accuracy: 0.260417, mean_q: -3001.440918, mean_eps: 0.131950\n",
      "  9650/50000: episode: 1279, duration: 0.013s, episode steps:   3, steps per second: 224, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 515600.875000, mae: 2680.324056, accuracy: 0.156250, mean_q: -3007.947103, mean_eps: 0.131680\n",
      "  9653/50000: episode: 1280, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 711082.562500, mae: 2708.824951, accuracy: 0.135417, mean_q: -3007.536865, mean_eps: 0.131410\n",
      "  9662/50000: episode: 1281, duration: 0.029s, episode steps:   9, steps per second: 310, episode reward: -6189.000, mean reward: -687.667 [-999.000, -45.000], mean action: 1.889 [0.000, 3.000],  loss: 693193.590278, mae: 2710.659695, accuracy: 0.145833, mean_q: -3031.099067, mean_eps: 0.130870\n",
      "  9665/50000: episode: 1282, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 665398.593750, mae: 2721.052897, accuracy: 0.166667, mean_q: -3031.624756, mean_eps: 0.130330\n",
      "  9668/50000: episode: 1283, duration: 0.012s, episode steps:   3, steps per second: 258, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 911640.750000, mae: 2746.855225, accuracy: 0.239583, mean_q: -3046.582275, mean_eps: 0.130060\n",
      "  9676/50000: episode: 1284, duration: 0.027s, episode steps:   8, steps per second: 298, episode reward: -5190.000, mean reward: -648.750 [-999.000, -58.000], mean action: 1.875 [0.000, 3.000],  loss: 650787.398438, mae: 2714.550262, accuracy: 0.281250, mean_q: -3031.322144, mean_eps: 0.129565\n",
      "  9679/50000: episode: 1285, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 772976.166667, mae: 2753.263021, accuracy: 0.375000, mean_q: -3042.414876, mean_eps: 0.129070\n",
      "  9685/50000: episode: 1286, duration: 0.023s, episode steps:   6, steps per second: 258, episode reward: -3221.000, mean reward: -536.833 [-999.000, -32.000], mean action: 1.000 [0.000, 3.000],  loss: 493942.953125, mae: 2681.706421, accuracy: 0.380208, mean_q: -3054.954956, mean_eps: 0.128665\n",
      "  9688/50000: episode: 1287, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 841054.020833, mae: 2768.161865, accuracy: 0.354167, mean_q: -3063.878662, mean_eps: 0.128260\n",
      "  9757/50000: episode: 1288, duration: 0.213s, episode steps:  69, steps per second: 324, episode reward: -66129.000, mean reward: -958.391 [-999.000, -32.000], mean action: 0.623 [0.000, 3.000],  loss: 786749.245018, mae: 2719.799352, accuracy: 0.269928, mean_q: -3005.262709, mean_eps: 0.125020\n",
      "  9770/50000: episode: 1289, duration: 0.044s, episode steps:  13, steps per second: 296, episode reward: -10185.000, mean reward: -783.462 [-999.000, -32.000], mean action: 0.231 [0.000, 2.000],  loss: 733611.673077, mae: 2734.722957, accuracy: 0.360577, mean_q: -3040.168739, mean_eps: 0.121330\n",
      "  9797/50000: episode: 1290, duration: 0.084s, episode steps:  27, steps per second: 321, episode reward: -24171.000, mean reward: -895.222 [-999.000, -32.000], mean action: 0.148 [0.000, 2.000],  loss: 625031.109954, mae: 2744.229935, accuracy: 0.334491, mean_q: -3081.719094, mean_eps: 0.119530\n",
      "  9800/50000: episode: 1291, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 626230.333333, mae: 2789.600667, accuracy: 0.500000, mean_q: -3109.256348, mean_eps: 0.118180\n",
      "  9815/50000: episode: 1292, duration: 0.047s, episode steps:  15, steps per second: 317, episode reward: -12183.000, mean reward: -812.200 [-999.000, -32.000], mean action: 0.400 [0.000, 3.000],  loss: 761527.875000, mae: 2763.720264, accuracy: 0.435417, mean_q: -3077.646224, mean_eps: 0.117370\n",
      "  9878/50000: episode: 1293, duration: 0.200s, episode steps:  63, steps per second: 315, episode reward: -60135.000, mean reward: -954.524 [-999.000, -32.000], mean action: 0.984 [0.000, 3.000],  loss: 741570.013393, mae: 2743.257444, accuracy: 0.455853, mean_q: -3014.140028, mean_eps: 0.113860\n",
      "  9925/50000: episode: 1294, duration: 0.143s, episode steps:  47, steps per second: 330, episode reward: -44151.000, mean reward: -939.383 [-999.000, -32.000], mean action: 1.043 [0.000, 3.000],  loss: 801897.212766, mae: 2715.576032, accuracy: 0.411569, mean_q: -2973.869276, mean_eps: 0.108910\n",
      "  9929/50000: episode: 1295, duration: 0.015s, episode steps:   4, steps per second: 266, episode reward: -1223.000, mean reward: -305.750 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 899805.500000, mae: 2722.707458, accuracy: 0.320312, mean_q: -3001.153137, mean_eps: 0.106615\n",
      "  9932/50000: episode: 1296, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 580913.520833, mae: 2683.109131, accuracy: 0.302083, mean_q: -3014.028320, mean_eps: 0.106300\n",
      "  9935/50000: episode: 1297, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -224.000, mean reward: -74.667 [-116.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 983427.104167, mae: 2755.409587, accuracy: 0.270833, mean_q: -3010.400960, mean_eps: 0.106030\n",
      "  9988/50000: episode: 1298, duration: 0.165s, episode steps:  53, steps per second: 321, episode reward: -50174.000, mean reward: -946.679 [-999.000, -32.000], mean action: 0.528 [0.000, 3.000],  loss: 771740.510613, mae: 2709.775782, accuracy: 0.250590, mean_q: -2999.865631, mean_eps: 0.103510\n",
      "  9991/50000: episode: 1299, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 548177.625000, mae: 2659.594564, accuracy: 0.281250, mean_q: -2971.660970, mean_eps: 0.100990\n",
      " 10002/50000: episode: 1300, duration: 0.036s, episode steps:  11, steps per second: 309, episode reward: -8187.000, mean reward: -744.273 [-999.000, -32.000], mean action: 0.545 [0.000, 3.000],  loss: 886569.400568, mae: 2734.390403, accuracy: 0.295455, mean_q: -2989.474654, mean_eps: 0.100368\n",
      " 10005/50000: episode: 1301, duration: 0.012s, episode steps:   3, steps per second: 256, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 648239.708333, mae: 2714.487386, accuracy: 0.312500, mean_q: -2998.442790, mean_eps: 0.100000\n",
      " 10008/50000: episode: 1302, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 859659.708333, mae: 2744.035889, accuracy: 0.250000, mean_q: -2997.538330, mean_eps: 0.100000\n",
      " 10012/50000: episode: 1303, duration: 0.015s, episode steps:   4, steps per second: 265, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 743507.140625, mae: 2740.037598, accuracy: 0.250000, mean_q: -3008.281616, mean_eps: 0.100000\n",
      " 10015/50000: episode: 1304, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 977095.145833, mae: 2766.782145, accuracy: 0.333333, mean_q: -3004.377116, mean_eps: 0.100000\n",
      " 10018/50000: episode: 1305, duration: 0.012s, episode steps:   3, steps per second: 258, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 838814.166667, mae: 2714.888590, accuracy: 0.197917, mean_q: -3001.212484, mean_eps: 0.100000\n",
      " 10021/50000: episode: 1306, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 464151.072917, mae: 2666.003906, accuracy: 0.364583, mean_q: -3006.398844, mean_eps: 0.100000\n",
      " 10042/50000: episode: 1307, duration: 0.064s, episode steps:  21, steps per second: 327, episode reward: -18221.000, mean reward: -867.667 [-999.000, -58.000], mean action: 2.667 [0.000, 3.000],  loss: 763214.260417, mae: 2726.339135, accuracy: 0.330357, mean_q: -3026.611130, mean_eps: 0.100000\n",
      " 10057/50000: episode: 1308, duration: 0.055s, episode steps:  15, steps per second: 270, episode reward: -12183.000, mean reward: -812.200 [-999.000, -32.000], mean action: 1.133 [0.000, 3.000],  loss: 770060.935417, mae: 2736.833724, accuracy: 0.306250, mean_q: -3016.631152, mean_eps: 0.100000\n",
      " 10066/50000: episode: 1309, duration: 0.040s, episode steps:   9, steps per second: 226, episode reward: -6218.000, mean reward: -690.889 [-999.000, -32.000], mean action: 1.222 [0.000, 3.000],  loss: 738026.895833, mae: 2709.322374, accuracy: 0.354167, mean_q: -3001.207574, mean_eps: 0.100000\n",
      " 10069/50000: episode: 1310, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 891791.229167, mae: 2734.032064, accuracy: 0.281250, mean_q: -2968.008708, mean_eps: 0.100000\n",
      " 10072/50000: episode: 1311, duration: 0.012s, episode steps:   3, steps per second: 257, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 787455.562500, mae: 2713.333008, accuracy: 0.343750, mean_q: -2962.417969, mean_eps: 0.100000\n",
      " 10089/50000: episode: 1312, duration: 0.055s, episode steps:  17, steps per second: 310, episode reward: -14181.000, mean reward: -834.176 [-999.000, -32.000], mean action: 0.176 [0.000, 2.000],  loss: 806608.588235, mae: 2713.368940, accuracy: 0.319853, mean_q: -2970.245821, mean_eps: 0.100000\n",
      " 10116/50000: episode: 1313, duration: 0.083s, episode steps:  27, steps per second: 324, episode reward: -24200.000, mean reward: -896.296 [-999.000, -32.000], mean action: 1.074 [0.000, 3.000],  loss: 702032.607639, mae: 2718.972774, accuracy: 0.245370, mean_q: -3012.892777, mean_eps: 0.100000\n",
      " 10119/50000: episode: 1314, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -224.000, mean reward: -74.667 [-116.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 757118.604167, mae: 2762.593343, accuracy: 0.166667, mean_q: -3039.694580, mean_eps: 0.100000\n",
      " 10122/50000: episode: 1315, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 492951.552083, mae: 2708.793457, accuracy: 0.302083, mean_q: -3035.285889, mean_eps: 0.100000\n",
      " 10125/50000: episode: 1316, duration: 0.014s, episode steps:   3, steps per second: 214, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 857632.208333, mae: 2761.410400, accuracy: 0.260417, mean_q: -3046.541585, mean_eps: 0.100000\n",
      " 10164/50000: episode: 1317, duration: 0.122s, episode steps:  39, steps per second: 320, episode reward: -36159.000, mean reward: -927.154 [-999.000, -32.000], mean action: 0.179 [0.000, 3.000],  loss: 756370.656250, mae: 2750.266796, accuracy: 0.369391, mean_q: -3028.478635, mean_eps: 0.100000\n",
      " 10167/50000: episode: 1318, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 719328.020833, mae: 2755.089111, accuracy: 0.375000, mean_q: -3029.956217, mean_eps: 0.100000\n",
      " 10173/50000: episode: 1319, duration: 0.021s, episode steps:   6, steps per second: 285, episode reward: -3192.000, mean reward: -532.000 [-999.000, -32.000], mean action: 0.667 [0.000, 2.000],  loss: 783633.260417, mae: 2746.634806, accuracy: 0.401042, mean_q: -3030.621908, mean_eps: 0.100000\n",
      " 10187/50000: episode: 1320, duration: 0.046s, episode steps:  14, steps per second: 304, episode reward: -11184.000, mean reward: -798.857 [-999.000, -32.000], mean action: 0.500 [0.000, 3.000],  loss: 658754.854911, mae: 2730.412598, accuracy: 0.390625, mean_q: -3030.128645, mean_eps: 0.100000\n",
      " 10201/50000: episode: 1321, duration: 0.048s, episode steps:  14, steps per second: 294, episode reward: -11184.000, mean reward: -798.857 [-999.000, -32.000], mean action: 0.429 [0.000, 3.000],  loss: 726590.017857, mae: 2763.551147, accuracy: 0.383929, mean_q: -3067.320417, mean_eps: 0.100000\n",
      " 10204/50000: episode: 1322, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 802441.666667, mae: 2773.437093, accuracy: 0.375000, mean_q: -3078.252197, mean_eps: 0.100000\n",
      " 10219/50000: episode: 1323, duration: 0.051s, episode steps:  15, steps per second: 294, episode reward: -12183.000, mean reward: -812.200 [-999.000, -32.000], mean action: 0.533 [0.000, 3.000],  loss: 644615.039583, mae: 2752.793717, accuracy: 0.318750, mean_q: -3070.586849, mean_eps: 0.100000\n",
      " 10256/50000: episode: 1324, duration: 0.114s, episode steps:  37, steps per second: 325, episode reward: -34190.000, mean reward: -924.054 [-999.000, -58.000], mean action: 2.811 [0.000, 3.000],  loss: 795016.744088, mae: 2778.534411, accuracy: 0.249155, mean_q: -3078.539927, mean_eps: 0.100000\n",
      " 10259/50000: episode: 1325, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 841142.937500, mae: 2768.348551, accuracy: 0.145833, mean_q: -3017.094157, mean_eps: 0.100000\n",
      " 10310/50000: episode: 1326, duration: 0.159s, episode steps:  51, steps per second: 320, episode reward: -48147.000, mean reward: -944.059 [-999.000, -45.000], mean action: 2.000 [0.000, 3.000],  loss: 749088.966299, mae: 2765.372219, accuracy: 0.054534, mean_q: -3047.653541, mean_eps: 0.100000\n",
      " 10338/50000: episode: 1327, duration: 0.090s, episode steps:  28, steps per second: 313, episode reward: -25199.000, mean reward: -899.964 [-999.000, -58.000], mean action: 2.786 [0.000, 3.000],  loss: 810635.210938, mae: 2775.080697, accuracy: 0.191964, mean_q: -3073.205784, mean_eps: 0.100000\n",
      " 10341/50000: episode: 1328, duration: 0.014s, episode steps:   3, steps per second: 214, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 793897.791667, mae: 2734.117269, accuracy: 0.187500, mean_q: -3039.114665, mean_eps: 0.100000\n",
      " 10344/50000: episode: 1329, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 829858.656250, mae: 2748.475911, accuracy: 0.114583, mean_q: -3021.039469, mean_eps: 0.100000\n",
      " 10376/50000: episode: 1330, duration: 0.098s, episode steps:  32, steps per second: 327, episode reward: -29195.000, mean reward: -912.344 [-999.000, -58.000], mean action: 2.750 [0.000, 3.000],  loss: 698012.083984, mae: 2767.691780, accuracy: 0.189453, mean_q: -3077.800110, mean_eps: 0.100000\n",
      " 10380/50000: episode: 1331, duration: 0.015s, episode steps:   4, steps per second: 265, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 755461.718750, mae: 2825.545837, accuracy: 0.187500, mean_q: -3138.233154, mean_eps: 0.100000\n",
      " 10395/50000: episode: 1332, duration: 0.048s, episode steps:  15, steps per second: 314, episode reward: -12227.000, mean reward: -815.133 [-999.000, -45.000], mean action: 2.600 [0.000, 3.000],  loss: 865067.412500, mae: 2822.519287, accuracy: 0.239583, mean_q: -3119.626351, mean_eps: 0.100000\n",
      " 10398/50000: episode: 1333, duration: 0.012s, episode steps:   3, steps per second: 257, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 597338.500000, mae: 2735.332357, accuracy: 0.406250, mean_q: -3057.261800, mean_eps: 0.100000\n",
      " 10420/50000: episode: 1334, duration: 0.074s, episode steps:  22, steps per second: 296, episode reward: -19205.000, mean reward: -872.955 [-999.000, -32.000], mean action: 0.273 [0.000, 3.000],  loss: 782040.707386, mae: 2774.328880, accuracy: 0.367898, mean_q: -3059.499123, mean_eps: 0.100000\n",
      " 10423/50000: episode: 1335, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 646831.000000, mae: 2766.804525, accuracy: 0.302083, mean_q: -3069.349040, mean_eps: 0.100000\n",
      " 10430/50000: episode: 1336, duration: 0.024s, episode steps:   7, steps per second: 296, episode reward: -4220.000, mean reward: -602.857 [-999.000, -58.000], mean action: 2.000 [0.000, 3.000],  loss: 703000.964286, mae: 2759.956229, accuracy: 0.272321, mean_q: -3077.866420, mean_eps: 0.100000\n",
      " 10457/50000: episode: 1337, duration: 0.084s, episode steps:  27, steps per second: 320, episode reward: -24200.000, mean reward: -896.296 [-999.000, -58.000], mean action: 2.778 [0.000, 3.000],  loss: 794936.018519, mae: 2810.465350, accuracy: 0.282407, mean_q: -3109.806731, mean_eps: 0.100000\n",
      " 10460/50000: episode: 1338, duration: 0.012s, episode steps:   3, steps per second: 253, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 663367.239583, mae: 2777.782633, accuracy: 0.187500, mean_q: -3086.035970, mean_eps: 0.100000\n",
      " 10463/50000: episode: 1339, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 666202.177083, mae: 2746.997233, accuracy: 0.239583, mean_q: -3085.742676, mean_eps: 0.100000\n",
      " 10466/50000: episode: 1340, duration: 0.012s, episode steps:   3, steps per second: 259, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 617675.666667, mae: 2753.406006, accuracy: 0.145833, mean_q: -3085.378011, mean_eps: 0.100000\n",
      " 10469/50000: episode: 1341, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 801728.791667, mae: 2781.300863, accuracy: 0.072917, mean_q: -3081.228841, mean_eps: 0.100000\n",
      " 10472/50000: episode: 1342, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 982657.875000, mae: 2792.206136, accuracy: 0.156250, mean_q: -3073.260905, mean_eps: 0.100000\n",
      " 10475/50000: episode: 1343, duration: 0.012s, episode steps:   3, steps per second: 253, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 975521.145833, mae: 2829.379069, accuracy: 0.125000, mean_q: -3081.378906, mean_eps: 0.100000\n",
      " 10491/50000: episode: 1344, duration: 0.058s, episode steps:  16, steps per second: 277, episode reward: -13211.000, mean reward: -825.688 [-999.000, -58.000], mean action: 2.625 [0.000, 3.000],  loss: 683697.281250, mae: 2775.285919, accuracy: 0.041016, mean_q: -3071.582626, mean_eps: 0.100000\n",
      " 10521/50000: episode: 1345, duration: 0.097s, episode steps:  30, steps per second: 310, episode reward: -27197.000, mean reward: -906.567 [-999.000, -58.000], mean action: 1.933 [0.000, 3.000],  loss: 699427.210417, mae: 2791.054858, accuracy: 0.087500, mean_q: -3091.928451, mean_eps: 0.100000\n",
      " 10525/50000: episode: 1346, duration: 0.016s, episode steps:   4, steps per second: 256, episode reward: -1223.000, mean reward: -305.750 [-999.000, -58.000], mean action: 1.750 [0.000, 3.000],  loss: 738835.265625, mae: 2805.841248, accuracy: 0.203125, mean_q: -3137.511719, mean_eps: 0.100000\n",
      " 10557/50000: episode: 1347, duration: 0.099s, episode steps:  32, steps per second: 325, episode reward: -29210.000, mean reward: -912.812 [-999.000, -60.000], mean action: 1.094 [0.000, 3.000],  loss: 676274.478516, mae: 2820.132095, accuracy: 0.207031, mean_q: -3144.578949, mean_eps: 0.100000\n",
      " 10586/50000: episode: 1348, duration: 0.092s, episode steps:  29, steps per second: 316, episode reward: -26198.000, mean reward: -903.379 [-999.000, -58.000], mean action: 1.931 [0.000, 3.000],  loss: 715418.877155, mae: 2832.974837, accuracy: 0.091595, mean_q: -3159.967016, mean_eps: 0.100000\n",
      " 10593/50000: episode: 1349, duration: 0.025s, episode steps:   7, steps per second: 285, episode reward: -4220.000, mean reward: -602.857 [-999.000, -58.000], mean action: 2.143 [0.000, 3.000],  loss: 737006.035714, mae: 2831.212960, accuracy: 0.120536, mean_q: -3167.963797, mean_eps: 0.100000\n",
      " 10597/50000: episode: 1350, duration: 0.015s, episode steps:   4, steps per second: 269, episode reward: -1238.000, mean reward: -309.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 642822.718750, mae: 2801.369446, accuracy: 0.195312, mean_q: -3167.796021, mean_eps: 0.100000\n",
      " 10600/50000: episode: 1351, duration: 0.012s, episode steps:   3, steps per second: 257, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 742565.854167, mae: 2856.563558, accuracy: 0.166667, mean_q: -3184.621501, mean_eps: 0.100000\n",
      " 10603/50000: episode: 1352, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 659470.416667, mae: 2838.807373, accuracy: 0.166667, mean_q: -3187.524495, mean_eps: 0.100000\n",
      " 10606/50000: episode: 1353, duration: 0.012s, episode steps:   3, steps per second: 253, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 466709.635417, mae: 2826.880941, accuracy: 0.166667, mean_q: -3201.337402, mean_eps: 0.100000\n",
      " 10645/50000: episode: 1354, duration: 0.124s, episode steps:  39, steps per second: 315, episode reward: -36159.000, mean reward: -927.154 [-999.000, -58.000], mean action: 1.949 [0.000, 3.000],  loss: 813426.746795, mae: 2853.935578, accuracy: 0.051282, mean_q: -3159.052340, mean_eps: 0.100000\n",
      " 10656/50000: episode: 1355, duration: 0.036s, episode steps:  11, steps per second: 308, episode reward: -8187.000, mean reward: -744.273 [-999.000, -45.000], mean action: 2.455 [0.000, 3.000],  loss: 704329.880682, mae: 2803.203147, accuracy: 0.173295, mean_q: -3123.613969, mean_eps: 0.100000\n",
      " 10659/50000: episode: 1356, duration: 0.012s, episode steps:   3, steps per second: 255, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 607081.645833, mae: 2825.576497, accuracy: 0.291667, mean_q: -3152.402100, mean_eps: 0.100000\n",
      " 10662/50000: episode: 1357, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 816046.708333, mae: 2865.768473, accuracy: 0.197917, mean_q: -3168.540120, mean_eps: 0.100000\n",
      " 10665/50000: episode: 1358, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 704608.239583, mae: 2849.111165, accuracy: 0.260417, mean_q: -3170.319336, mean_eps: 0.100000\n",
      " 10668/50000: episode: 1359, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 794444.541667, mae: 2859.230306, accuracy: 0.218750, mean_q: -3187.069987, mean_eps: 0.100000\n",
      " 10671/50000: episode: 1360, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 746386.135417, mae: 2851.449626, accuracy: 0.270833, mean_q: -3193.239176, mean_eps: 0.100000\n",
      " 10674/50000: episode: 1361, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 665498.552083, mae: 2847.857910, accuracy: 0.281250, mean_q: -3190.526855, mean_eps: 0.100000\n",
      " 10677/50000: episode: 1362, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.333 [0.000, 3.000],  loss: 574704.562500, mae: 2799.710612, accuracy: 0.395833, mean_q: -3169.181641, mean_eps: 0.100000\n",
      " 10680/50000: episode: 1363, duration: 0.012s, episode steps:   3, steps per second: 258, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 525816.239583, mae: 2812.610352, accuracy: 0.333333, mean_q: -3187.673503, mean_eps: 0.100000\n",
      " 10683/50000: episode: 1364, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 922163.770833, mae: 2880.735921, accuracy: 0.270833, mean_q: -3182.736165, mean_eps: 0.100000\n",
      " 10686/50000: episode: 1365, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 754410.041667, mae: 2844.722900, accuracy: 0.302083, mean_q: -3168.349528, mean_eps: 0.100000\n",
      " 10689/50000: episode: 1366, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 1005269.041667, mae: 2910.364339, accuracy: 0.239583, mean_q: -3196.052572, mean_eps: 0.100000\n",
      " 10692/50000: episode: 1367, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 1026894.187500, mae: 2894.114502, accuracy: 0.302083, mean_q: -3176.756917, mean_eps: 0.100000\n",
      " 10696/50000: episode: 1368, duration: 0.016s, episode steps:   4, steps per second: 246, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.750 [0.000, 3.000],  loss: 718430.343750, mae: 2843.336914, accuracy: 0.312500, mean_q: -3162.139282, mean_eps: 0.100000\n",
      " 10699/50000: episode: 1369, duration: 0.014s, episode steps:   3, steps per second: 209, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 776403.843750, mae: 2840.728516, accuracy: 0.385417, mean_q: -3153.319987, mean_eps: 0.100000\n",
      " 10702/50000: episode: 1370, duration: 0.014s, episode steps:   3, steps per second: 218, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 762445.708333, mae: 2822.573324, accuracy: 0.291667, mean_q: -3142.177816, mean_eps: 0.100000\n",
      " 10705/50000: episode: 1371, duration: 0.015s, episode steps:   3, steps per second: 196, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 982940.937500, mae: 2848.478678, accuracy: 0.302083, mean_q: -3134.050130, mean_eps: 0.100000\n",
      " 10708/50000: episode: 1372, duration: 0.016s, episode steps:   3, steps per second: 186, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 887998.062500, mae: 2828.681396, accuracy: 0.229167, mean_q: -3108.227783, mean_eps: 0.100000\n",
      " 10711/50000: episode: 1373, duration: 0.016s, episode steps:   3, steps per second: 190, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 860151.333333, mae: 2820.243734, accuracy: 0.229167, mean_q: -3100.279215, mean_eps: 0.100000\n",
      " 10714/50000: episode: 1374, duration: 0.015s, episode steps:   3, steps per second: 199, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 822321.645833, mae: 2824.775065, accuracy: 0.333333, mean_q: -3113.241781, mean_eps: 0.100000\n",
      " 10718/50000: episode: 1375, duration: 0.018s, episode steps:   4, steps per second: 221, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 728041.562500, mae: 2788.160339, accuracy: 0.257812, mean_q: -3110.501709, mean_eps: 0.100000\n",
      " 10721/50000: episode: 1376, duration: 0.014s, episode steps:   3, steps per second: 212, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 670418.343750, mae: 2811.129476, accuracy: 0.208333, mean_q: -3123.940023, mean_eps: 0.100000\n",
      " 10724/50000: episode: 1377, duration: 0.015s, episode steps:   3, steps per second: 202, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 776590.895833, mae: 2838.674316, accuracy: 0.343750, mean_q: -3149.784831, mean_eps: 0.100000\n",
      " 10728/50000: episode: 1378, duration: 0.018s, episode steps:   4, steps per second: 225, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 737705.718750, mae: 2808.915405, accuracy: 0.085938, mean_q: -3148.851990, mean_eps: 0.100000\n",
      " 10731/50000: episode: 1379, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 1029537.791667, mae: 2901.814941, accuracy: 0.052083, mean_q: -3166.204427, mean_eps: 0.100000\n",
      " 10734/50000: episode: 1380, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 667091.395833, mae: 2829.946289, accuracy: 0.052083, mean_q: -3158.027913, mean_eps: 0.100000\n",
      " 10741/50000: episode: 1381, duration: 0.026s, episode steps:   7, steps per second: 269, episode reward: -4191.000, mean reward: -598.714 [-999.000, -45.000], mean action: 1.571 [0.000, 2.000],  loss: 680864.366071, mae: 2837.861712, accuracy: 0.026786, mean_q: -3149.384417, mean_eps: 0.100000\n",
      " 10749/50000: episode: 1382, duration: 0.030s, episode steps:   8, steps per second: 263, episode reward: -5190.000, mean reward: -648.750 [-999.000, -45.000], mean action: 1.625 [0.000, 2.000],  loss: 782842.074219, mae: 2871.963165, accuracy: 0.054688, mean_q: -3163.640472, mean_eps: 0.100000\n",
      " 10871/50000: episode: 1383, duration: 0.385s, episode steps: 122, steps per second: 317, episode reward: -119076.000, mean reward: -976.033 [-999.000, -45.000], mean action: 1.984 [0.000, 3.000],  loss: 733762.512039, mae: 2869.232514, accuracy: 0.036885, mean_q: -3165.907309, mean_eps: 0.100000\n",
      " 10877/50000: episode: 1384, duration: 0.021s, episode steps:   6, steps per second: 285, episode reward: -3192.000, mean reward: -532.000 [-999.000, -45.000], mean action: 1.500 [0.000, 2.000],  loss: 822646.322917, mae: 2893.175130, accuracy: 0.109375, mean_q: -3230.331543, mean_eps: 0.100000\n",
      " 10880/50000: episode: 1385, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 717219.604167, mae: 2881.851969, accuracy: 0.197917, mean_q: -3226.660400, mean_eps: 0.100000\n",
      " 10883/50000: episode: 1386, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 694476.395833, mae: 2857.661458, accuracy: 0.250000, mean_q: -3205.728678, mean_eps: 0.100000\n",
      " 10886/50000: episode: 1387, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 821827.562500, mae: 2868.622396, accuracy: 0.156250, mean_q: -3204.841878, mean_eps: 0.100000\n",
      " 10890/50000: episode: 1388, duration: 0.014s, episode steps:   4, steps per second: 279, episode reward: -1223.000, mean reward: -305.750 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 659204.484375, mae: 2845.680542, accuracy: 0.304688, mean_q: -3190.499695, mean_eps: 0.100000\n",
      " 10893/50000: episode: 1389, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 1007819.916667, mae: 2870.918050, accuracy: 0.281250, mean_q: -3167.880778, mean_eps: 0.100000\n",
      " 10896/50000: episode: 1390, duration: 0.012s, episode steps:   3, steps per second: 257, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 840436.812500, mae: 2868.383789, accuracy: 0.302083, mean_q: -3173.839681, mean_eps: 0.100000\n",
      " 10899/50000: episode: 1391, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 561513.500000, mae: 2812.685628, accuracy: 0.333333, mean_q: -3172.085938, mean_eps: 0.100000\n",
      " 10902/50000: episode: 1392, duration: 0.011s, episode steps:   3, steps per second: 263, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 856008.145833, mae: 2867.960938, accuracy: 0.281250, mean_q: -3144.274740, mean_eps: 0.100000\n",
      " 10905/50000: episode: 1393, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 936362.333333, mae: 2853.874674, accuracy: 0.229167, mean_q: -3149.758464, mean_eps: 0.100000\n",
      " 10908/50000: episode: 1394, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 770798.625000, mae: 2852.486084, accuracy: 0.281250, mean_q: -3151.991130, mean_eps: 0.100000\n",
      " 10911/50000: episode: 1395, duration: 0.012s, episode steps:   3, steps per second: 253, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 1131918.875000, mae: 2893.679118, accuracy: 0.187500, mean_q: -3149.802002, mean_eps: 0.100000\n",
      " 10915/50000: episode: 1396, duration: 0.015s, episode steps:   4, steps per second: 267, episode reward: -1223.000, mean reward: -305.750 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 769538.156250, mae: 2821.299072, accuracy: 0.226562, mean_q: -3141.389893, mean_eps: 0.100000\n",
      " 10918/50000: episode: 1397, duration: 0.011s, episode steps:   3, steps per second: 262, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 843959.666667, mae: 2836.193604, accuracy: 0.145833, mean_q: -3135.243978, mean_eps: 0.100000\n",
      " 10937/50000: episode: 1398, duration: 0.067s, episode steps:  19, steps per second: 285, episode reward: -16208.000, mean reward: -853.053 [-999.000, -58.000], mean action: 1.947 [0.000, 3.000],  loss: 776398.000000, mae: 2844.691470, accuracy: 0.182566, mean_q: -3150.358745, mean_eps: 0.100000\n",
      " 10940/50000: episode: 1399, duration: 0.014s, episode steps:   3, steps per second: 215, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 487313.072917, mae: 2798.485352, accuracy: 0.270833, mean_q: -3169.085449, mean_eps: 0.100000\n",
      " 10943/50000: episode: 1400, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 853222.041667, mae: 2849.652181, accuracy: 0.250000, mean_q: -3163.283773, mean_eps: 0.100000\n",
      " 10946/50000: episode: 1401, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 813176.666667, mae: 2871.077230, accuracy: 0.291667, mean_q: -3165.403564, mean_eps: 0.100000\n",
      " 10950/50000: episode: 1402, duration: 0.015s, episode steps:   4, steps per second: 270, episode reward: -1223.000, mean reward: -305.750 [-999.000, -32.000], mean action: 1.750 [0.000, 3.000],  loss: 1330819.531250, mae: 2938.470520, accuracy: 0.281250, mean_q: -3161.415710, mean_eps: 0.100000\n",
      " 10953/50000: episode: 1403, duration: 0.012s, episode steps:   3, steps per second: 253, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 927719.479167, mae: 2849.525879, accuracy: 0.395833, mean_q: -3115.343180, mean_eps: 0.100000\n",
      " 10956/50000: episode: 1404, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 574645.958333, mae: 2755.173096, accuracy: 0.447917, mean_q: -3074.168620, mean_eps: 0.100000\n",
      " 10959/50000: episode: 1405, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 681890.333333, mae: 2792.694336, accuracy: 0.312500, mean_q: -3089.692057, mean_eps: 0.100000\n",
      " 10963/50000: episode: 1406, duration: 0.015s, episode steps:   4, steps per second: 268, episode reward: -1223.000, mean reward: -305.750 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 957894.234375, mae: 2864.927063, accuracy: 0.273438, mean_q: -3120.541260, mean_eps: 0.100000\n",
      " 10966/50000: episode: 1407, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 993706.208333, mae: 2866.871094, accuracy: 0.354167, mean_q: -3129.702311, mean_eps: 0.100000\n",
      " 10982/50000: episode: 1408, duration: 0.052s, episode steps:  16, steps per second: 310, episode reward: -13182.000, mean reward: -823.875 [-999.000, -32.000], mean action: 1.125 [0.000, 3.000],  loss: 717183.375000, mae: 2818.291992, accuracy: 0.277344, mean_q: -3138.032898, mean_eps: 0.100000\n",
      " 10985/50000: episode: 1409, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 829630.208333, mae: 2862.133789, accuracy: 0.312500, mean_q: -3156.786540, mean_eps: 0.100000\n",
      " 10988/50000: episode: 1410, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 732081.916667, mae: 2832.383464, accuracy: 0.427083, mean_q: -3155.978516, mean_eps: 0.100000\n",
      " 10992/50000: episode: 1411, duration: 0.016s, episode steps:   4, steps per second: 257, episode reward: -1223.000, mean reward: -305.750 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 1108159.546875, mae: 2897.532227, accuracy: 0.367188, mean_q: -3166.856262, mean_eps: 0.100000\n",
      " 10995/50000: episode: 1412, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 854152.270833, mae: 2837.568685, accuracy: 0.322917, mean_q: -3134.706136, mean_eps: 0.100000\n",
      " 11000/50000: episode: 1413, duration: 0.020s, episode steps:   5, steps per second: 250, episode reward: -2193.000, mean reward: -438.600 [-999.000, -32.000], mean action: 0.600 [0.000, 2.000],  loss: 734147.300000, mae: 2817.962842, accuracy: 0.343750, mean_q: -3115.329785, mean_eps: 0.100000\n",
      " 11033/50000: episode: 1414, duration: 0.105s, episode steps:  33, steps per second: 315, episode reward: -30165.000, mean reward: -914.091 [-999.000, -45.000], mean action: 2.788 [0.000, 3.000],  loss: 793547.857481, mae: 2837.756163, accuracy: 0.358902, mean_q: -3127.531583, mean_eps: 0.100000\n",
      " 11043/50000: episode: 1415, duration: 0.033s, episode steps:  10, steps per second: 305, episode reward: -7188.000, mean reward: -718.800 [-999.000, -32.000], mean action: 0.300 [0.000, 2.000],  loss: 733319.434375, mae: 2840.426196, accuracy: 0.293750, mean_q: -3160.121606, mean_eps: 0.100000\n",
      " 11104/50000: episode: 1416, duration: 0.208s, episode steps:  61, steps per second: 293, episode reward: -58166.000, mean reward: -953.541 [-999.000, -32.000], mean action: 0.131 [0.000, 3.000],  loss: 746059.835553, mae: 2857.242684, accuracy: 0.322746, mean_q: -3168.164963, mean_eps: 0.100000\n",
      " 11107/50000: episode: 1417, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 634126.750000, mae: 2842.898682, accuracy: 0.156250, mean_q: -3223.334391, mean_eps: 0.100000\n",
      " 11137/50000: episode: 1418, duration: 0.094s, episode steps:  30, steps per second: 320, episode reward: -27197.000, mean reward: -906.567 [-999.000, -58.000], mean action: 1.933 [0.000, 3.000],  loss: 795039.790625, mae: 2877.694816, accuracy: 0.118750, mean_q: -3181.489372, mean_eps: 0.100000\n",
      " 11174/50000: episode: 1419, duration: 0.114s, episode steps:  37, steps per second: 324, episode reward: -34205.000, mean reward: -924.459 [-999.000, -45.000], mean action: 1.027 [0.000, 3.000],  loss: 773552.972128, mae: 2862.521121, accuracy: 0.181588, mean_q: -3171.609494, mean_eps: 0.100000\n",
      " 11177/50000: episode: 1420, duration: 0.014s, episode steps:   3, steps per second: 213, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 697321.687500, mae: 2886.025065, accuracy: 0.302083, mean_q: -3227.513509, mean_eps: 0.100000\n",
      " 11255/50000: episode: 1421, duration: 0.245s, episode steps:  78, steps per second: 318, episode reward: -75149.000, mean reward: -963.449 [-999.000, -32.000], mean action: 0.128 [0.000, 3.000],  loss: 728391.212540, mae: 2887.512849, accuracy: 0.340946, mean_q: -3206.364076, mean_eps: 0.100000\n",
      " 11258/50000: episode: 1422, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 576465.026042, mae: 2867.405518, accuracy: 0.270833, mean_q: -3245.653646, mean_eps: 0.100000\n",
      " 11261/50000: episode: 1423, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 811425.833333, mae: 2927.680176, accuracy: 0.250000, mean_q: -3268.518799, mean_eps: 0.100000\n",
      " 11264/50000: episode: 1424, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 674997.500000, mae: 2917.038981, accuracy: 0.322917, mean_q: -3256.282796, mean_eps: 0.100000\n",
      " 11267/50000: episode: 1425, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 646624.072917, mae: 2897.189128, accuracy: 0.281250, mean_q: -3259.601074, mean_eps: 0.100000\n",
      " 11270/50000: episode: 1426, duration: 0.012s, episode steps:   3, steps per second: 257, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 675147.479167, mae: 2909.004964, accuracy: 0.229167, mean_q: -3257.872477, mean_eps: 0.100000\n",
      " 11273/50000: episode: 1427, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 568238.958333, mae: 2876.392660, accuracy: 0.239583, mean_q: -3243.593831, mean_eps: 0.100000\n",
      " 11276/50000: episode: 1428, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 924871.604167, mae: 2939.307292, accuracy: 0.145833, mean_q: -3242.872884, mean_eps: 0.100000\n",
      " 11279/50000: episode: 1429, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 799039.645833, mae: 2929.771891, accuracy: 0.187500, mean_q: -3266.066813, mean_eps: 0.100000\n",
      " 11310/50000: episode: 1430, duration: 0.095s, episode steps:  31, steps per second: 326, episode reward: -28211.000, mean reward: -910.032 [-999.000, -45.000], mean action: 2.806 [0.000, 3.000],  loss: 781810.293347, mae: 2912.311760, accuracy: 0.143145, mean_q: -3227.865565, mean_eps: 0.100000\n",
      " 11313/50000: episode: 1431, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 895461.062500, mae: 2919.106445, accuracy: 0.156250, mean_q: -3211.998698, mean_eps: 0.100000\n",
      " 11316/50000: episode: 1432, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 768365.395833, mae: 2920.955404, accuracy: 0.125000, mean_q: -3208.889811, mean_eps: 0.100000\n",
      " 11319/50000: episode: 1433, duration: 0.017s, episode steps:   3, steps per second: 179, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 833365.531250, mae: 2913.068278, accuracy: 0.104167, mean_q: -3214.004639, mean_eps: 0.100000\n",
      " 11322/50000: episode: 1434, duration: 0.017s, episode steps:   3, steps per second: 180, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 646589.541667, mae: 2893.340820, accuracy: 0.093750, mean_q: -3212.772054, mean_eps: 0.100000\n",
      " 11325/50000: episode: 1435, duration: 0.014s, episode steps:   3, steps per second: 211, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 848268.083333, mae: 2924.520833, accuracy: 0.135417, mean_q: -3240.714030, mean_eps: 0.100000\n",
      " 11328/50000: episode: 1436, duration: 0.014s, episode steps:   3, steps per second: 216, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 484341.041667, mae: 2851.689941, accuracy: 0.114583, mean_q: -3228.308512, mean_eps: 0.100000\n",
      " 11331/50000: episode: 1437, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 591130.802083, mae: 2898.144287, accuracy: 0.125000, mean_q: -3229.945068, mean_eps: 0.100000\n",
      " 11334/50000: episode: 1438, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 965246.770833, mae: 2971.679932, accuracy: 0.072917, mean_q: -3237.289551, mean_eps: 0.100000\n",
      " 11372/50000: episode: 1439, duration: 0.117s, episode steps:  38, steps per second: 324, episode reward: -35160.000, mean reward: -925.263 [-999.000, -45.000], mean action: 2.053 [0.000, 3.000],  loss: 741961.141447, mae: 2935.401939, accuracy: 0.124178, mean_q: -3254.891158, mean_eps: 0.100000\n",
      " 11398/50000: episode: 1440, duration: 0.085s, episode steps:  26, steps per second: 307, episode reward: -23201.000, mean reward: -892.346 [-999.000, -32.000], mean action: 0.808 [0.000, 3.000],  loss: 775817.213942, mae: 2946.025372, accuracy: 0.248798, mean_q: -3256.132061, mean_eps: 0.100000\n",
      " 11401/50000: episode: 1441, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 600547.291667, mae: 2922.813639, accuracy: 0.291667, mean_q: -3229.420492, mean_eps: 0.100000\n",
      " 11404/50000: episode: 1442, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 659766.854167, mae: 2938.235677, accuracy: 0.312500, mean_q: -3270.286458, mean_eps: 0.100000\n",
      " 11480/50000: episode: 1443, duration: 0.236s, episode steps:  76, steps per second: 322, episode reward: -73151.000, mean reward: -962.513 [-999.000, -32.000], mean action: 0.158 [0.000, 3.000],  loss: 802707.079153, mae: 2919.900567, accuracy: 0.325658, mean_q: -3231.905746, mean_eps: 0.100000\n",
      " 11483/50000: episode: 1444, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 554795.906250, mae: 2898.239909, accuracy: 0.187500, mean_q: -3245.057292, mean_eps: 0.100000\n",
      " 11486/50000: episode: 1445, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 787706.333333, mae: 2960.783447, accuracy: 0.156250, mean_q: -3279.984619, mean_eps: 0.100000\n",
      " 11538/50000: episode: 1446, duration: 0.158s, episode steps:  52, steps per second: 328, episode reward: -49146.000, mean reward: -945.115 [-999.000, -45.000], mean action: 1.923 [0.000, 3.000],  loss: 889949.379207, mae: 2901.715017, accuracy: 0.146034, mean_q: -3185.022442, mean_eps: 0.100000\n",
      " 11541/50000: episode: 1447, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 731423.281250, mae: 2828.748861, accuracy: 0.229167, mean_q: -3120.549805, mean_eps: 0.100000\n",
      " 11544/50000: episode: 1448, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 673491.635417, mae: 2855.954834, accuracy: 0.260417, mean_q: -3136.550944, mean_eps: 0.100000\n",
      " 11547/50000: episode: 1449, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 491896.385417, mae: 2819.902181, accuracy: 0.239583, mean_q: -3169.857585, mean_eps: 0.100000\n",
      " 11550/50000: episode: 1450, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 740447.291667, mae: 2880.707926, accuracy: 0.322917, mean_q: -3198.736410, mean_eps: 0.100000\n",
      " 11553/50000: episode: 1451, duration: 0.014s, episode steps:   3, steps per second: 215, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 795433.125000, mae: 2933.156006, accuracy: 0.114583, mean_q: -3213.689128, mean_eps: 0.100000\n",
      " 11556/50000: episode: 1452, duration: 0.014s, episode steps:   3, steps per second: 221, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 650474.520833, mae: 2919.967692, accuracy: 0.135417, mean_q: -3241.058838, mean_eps: 0.100000\n",
      " 11559/50000: episode: 1453, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 653410.083333, mae: 2914.701579, accuracy: 0.135417, mean_q: -3253.360270, mean_eps: 0.100000\n",
      " 11562/50000: episode: 1454, duration: 0.012s, episode steps:   3, steps per second: 258, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 1037052.166667, mae: 3002.826660, accuracy: 0.177083, mean_q: -3262.469564, mean_eps: 0.100000\n",
      " 11582/50000: episode: 1455, duration: 0.062s, episode steps:  20, steps per second: 321, episode reward: -17222.000, mean reward: -861.100 [-999.000, -45.000], mean action: 2.000 [1.000, 3.000],  loss: 740779.796875, mae: 2938.236475, accuracy: 0.243750, mean_q: -3251.948413, mean_eps: 0.100000\n",
      " 11585/50000: episode: 1456, duration: 0.012s, episode steps:   3, steps per second: 253, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 889383.041667, mae: 2950.419596, accuracy: 0.250000, mean_q: -3233.938232, mean_eps: 0.100000\n",
      " 11588/50000: episode: 1457, duration: 0.012s, episode steps:   3, steps per second: 255, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 816970.937500, mae: 2935.255778, accuracy: 0.385417, mean_q: -3240.783366, mean_eps: 0.100000\n",
      " 11591/50000: episode: 1458, duration: 0.012s, episode steps:   3, steps per second: 254, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 871828.062500, mae: 2962.449707, accuracy: 0.354167, mean_q: -3240.214193, mean_eps: 0.100000\n",
      " 11600/50000: episode: 1459, duration: 0.030s, episode steps:   9, steps per second: 295, episode reward: -6189.000, mean reward: -687.667 [-999.000, -32.000], mean action: 1.111 [0.000, 3.000],  loss: 708591.118056, mae: 2908.117350, accuracy: 0.361111, mean_q: -3222.180013, mean_eps: 0.100000\n",
      " 11603/50000: episode: 1460, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 1056294.750000, mae: 2959.143962, accuracy: 0.270833, mean_q: -3201.159098, mean_eps: 0.100000\n",
      " 11606/50000: episode: 1461, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 823555.791667, mae: 2940.241862, accuracy: 0.270833, mean_q: -3225.096517, mean_eps: 0.100000\n",
      " 11609/50000: episode: 1462, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 789617.708333, mae: 2903.432536, accuracy: 0.385417, mean_q: -3191.563395, mean_eps: 0.100000\n",
      " 11612/50000: episode: 1463, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 716320.302083, mae: 2914.392008, accuracy: 0.354167, mean_q: -3196.537028, mean_eps: 0.100000\n",
      " 11617/50000: episode: 1464, duration: 0.020s, episode steps:   5, steps per second: 255, episode reward: -2193.000, mean reward: -438.600 [-999.000, -32.000], mean action: 1.400 [0.000, 3.000],  loss: 582331.312500, mae: 2872.818555, accuracy: 0.337500, mean_q: -3208.149121, mean_eps: 0.100000\n",
      " 11620/50000: episode: 1465, duration: 0.014s, episode steps:   3, steps per second: 217, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 795371.635417, mae: 2914.761637, accuracy: 0.312500, mean_q: -3218.561523, mean_eps: 0.100000\n",
      " 11632/50000: episode: 1466, duration: 0.039s, episode steps:  12, steps per second: 308, episode reward: -9186.000, mean reward: -765.500 [-999.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 779473.205729, mae: 2933.028605, accuracy: 0.398438, mean_q: -3212.288086, mean_eps: 0.100000\n",
      " 11635/50000: episode: 1467, duration: 0.012s, episode steps:   3, steps per second: 256, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 613149.218750, mae: 2920.595052, accuracy: 0.479167, mean_q: -3225.386068, mean_eps: 0.100000\n",
      " 11638/50000: episode: 1468, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 557041.145833, mae: 2911.523763, accuracy: 0.468750, mean_q: -3244.505697, mean_eps: 0.100000\n",
      " 11641/50000: episode: 1469, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 1085698.229167, mae: 2994.179362, accuracy: 0.427083, mean_q: -3240.987874, mean_eps: 0.100000\n",
      " 11686/50000: episode: 1470, duration: 0.149s, episode steps:  45, steps per second: 302, episode reward: -42153.000, mean reward: -936.733 [-999.000, -32.000], mean action: 0.133 [0.000, 3.000],  loss: 818213.775694, mae: 2906.714676, accuracy: 0.435417, mean_q: -3174.691770, mean_eps: 0.100000\n",
      " 11689/50000: episode: 1471, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 846205.500000, mae: 2918.841878, accuracy: 0.500000, mean_q: -3177.742920, mean_eps: 0.100000\n",
      " 11692/50000: episode: 1472, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 776935.145833, mae: 2892.027425, accuracy: 0.270833, mean_q: -3202.260661, mean_eps: 0.100000\n",
      " 11696/50000: episode: 1473, duration: 0.015s, episode steps:   4, steps per second: 265, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.750 [1.000, 3.000],  loss: 861800.593750, mae: 2917.923096, accuracy: 0.335938, mean_q: -3190.819031, mean_eps: 0.100000\n",
      " 11699/50000: episode: 1474, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 804352.312500, mae: 2907.745117, accuracy: 0.322917, mean_q: -3204.985107, mean_eps: 0.100000\n",
      " 11702/50000: episode: 1475, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 1182744.270833, mae: 2952.467611, accuracy: 0.260417, mean_q: -3191.863200, mean_eps: 0.100000\n",
      " 11705/50000: episode: 1476, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 737073.375000, mae: 2876.216797, accuracy: 0.281250, mean_q: -3193.227051, mean_eps: 0.100000\n",
      " 11708/50000: episode: 1477, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 532493.932292, mae: 2835.704590, accuracy: 0.197917, mean_q: -3189.407959, mean_eps: 0.100000\n",
      " 11711/50000: episode: 1478, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 903794.541667, mae: 2925.467773, accuracy: 0.218750, mean_q: -3209.827718, mean_eps: 0.100000\n",
      " 11714/50000: episode: 1479, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 620778.062500, mae: 2902.270101, accuracy: 0.302083, mean_q: -3217.310465, mean_eps: 0.100000\n",
      " 11728/50000: episode: 1480, duration: 0.045s, episode steps:  14, steps per second: 313, episode reward: -11184.000, mean reward: -798.857 [-999.000, -45.000], mean action: 1.786 [0.000, 2.000],  loss: 917710.580357, mae: 2929.590437, accuracy: 0.107143, mean_q: -3206.101737, mean_eps: 0.100000\n",
      " 11731/50000: episode: 1481, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 696419.208333, mae: 2859.378743, accuracy: 0.208333, mean_q: -3198.294922, mean_eps: 0.100000\n",
      " 11734/50000: episode: 1482, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 544735.364583, mae: 2856.031738, accuracy: 0.229167, mean_q: -3207.490072, mean_eps: 0.100000\n",
      " 11737/50000: episode: 1483, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 732254.625000, mae: 2926.092041, accuracy: 0.302083, mean_q: -3220.455892, mean_eps: 0.100000\n",
      " 11740/50000: episode: 1484, duration: 0.012s, episode steps:   3, steps per second: 256, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 1124454.604167, mae: 2972.049805, accuracy: 0.177083, mean_q: -3226.055420, mean_eps: 0.100000\n",
      " 11743/50000: episode: 1485, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 1.667 [0.000, 3.000],  loss: 474443.364583, mae: 2868.666423, accuracy: 0.291667, mean_q: -3243.873535, mean_eps: 0.100000\n",
      " 11746/50000: episode: 1486, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 673275.854167, mae: 2907.921224, accuracy: 0.250000, mean_q: -3243.956543, mean_eps: 0.100000\n",
      " 11781/50000: episode: 1487, duration: 0.111s, episode steps:  35, steps per second: 314, episode reward: -32192.000, mean reward: -919.771 [-999.000, -32.000], mean action: 0.257 [0.000, 3.000],  loss: 689648.287500, mae: 2937.337884, accuracy: 0.285714, mean_q: -3273.682457, mean_eps: 0.100000\n",
      " 11784/50000: episode: 1488, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 911328.270833, mae: 2986.413818, accuracy: 0.322917, mean_q: -3286.334798, mean_eps: 0.100000\n",
      " 11788/50000: episode: 1489, duration: 0.015s, episode steps:   4, steps per second: 271, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 837006.890625, mae: 2962.580811, accuracy: 0.265625, mean_q: -3265.992798, mean_eps: 0.100000\n",
      " 11791/50000: episode: 1490, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 862154.479167, mae: 2946.213704, accuracy: 0.208333, mean_q: -3235.606364, mean_eps: 0.100000\n",
      " 11795/50000: episode: 1491, duration: 0.016s, episode steps:   4, steps per second: 256, episode reward: -1223.000, mean reward: -305.750 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 1017747.687500, mae: 2963.940430, accuracy: 0.328125, mean_q: -3238.382996, mean_eps: 0.100000\n",
      " 11819/50000: episode: 1492, duration: 0.076s, episode steps:  24, steps per second: 315, episode reward: -21174.000, mean reward: -882.250 [-999.000, -32.000], mean action: 2.750 [0.000, 3.000],  loss: 924735.192708, mae: 2919.788025, accuracy: 0.220052, mean_q: -3182.239482, mean_eps: 0.100000\n",
      " 11822/50000: episode: 1493, duration: 0.014s, episode steps:   3, steps per second: 215, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 824672.770833, mae: 2878.123617, accuracy: 0.260417, mean_q: -3150.535075, mean_eps: 0.100000\n",
      " 11825/50000: episode: 1494, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 860970.041667, mae: 2869.476400, accuracy: 0.218750, mean_q: -3150.564860, mean_eps: 0.100000\n",
      " 11855/50000: episode: 1495, duration: 0.094s, episode steps:  30, steps per second: 319, episode reward: -27197.000, mean reward: -906.567 [-999.000, -32.000], mean action: 1.133 [0.000, 3.000],  loss: 795745.762500, mae: 2885.198039, accuracy: 0.282292, mean_q: -3161.307560, mean_eps: 0.100000\n",
      " 11858/50000: episode: 1496, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -224.000, mean reward: -74.667 [-116.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 766555.708333, mae: 2936.086426, accuracy: 0.145833, mean_q: -3223.031820, mean_eps: 0.100000\n",
      " 11861/50000: episode: 1497, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -224.000, mean reward: -74.667 [-116.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 894896.937500, mae: 2908.857096, accuracy: 0.104167, mean_q: -3191.379801, mean_eps: 0.100000\n",
      " 11864/50000: episode: 1498, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 640527.427083, mae: 2891.121989, accuracy: 0.083333, mean_q: -3198.345133, mean_eps: 0.100000\n",
      " 11871/50000: episode: 1499, duration: 0.024s, episode steps:   7, steps per second: 296, episode reward: -4220.000, mean reward: -602.857 [-999.000, -58.000], mean action: 2.143 [0.000, 3.000],  loss: 744163.428571, mae: 2903.368827, accuracy: 0.080357, mean_q: -3201.390974, mean_eps: 0.100000\n",
      " 11874/50000: episode: 1500, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 853474.312500, mae: 2939.376872, accuracy: 0.041667, mean_q: -3218.195150, mean_eps: 0.100000\n",
      " 11881/50000: episode: 1501, duration: 0.025s, episode steps:   7, steps per second: 282, episode reward: -4191.000, mean reward: -598.714 [-999.000, -45.000], mean action: 1.571 [0.000, 2.000],  loss: 573996.321429, mae: 2896.333601, accuracy: 0.116071, mean_q: -3222.335868, mean_eps: 0.100000\n",
      " 11888/50000: episode: 1502, duration: 0.026s, episode steps:   7, steps per second: 267, episode reward: -4191.000, mean reward: -598.714 [-999.000, -45.000], mean action: 1.857 [0.000, 3.000],  loss: 885356.625000, mae: 2952.014648, accuracy: 0.107143, mean_q: -3235.006069, mean_eps: 0.100000\n",
      " 11891/50000: episode: 1503, duration: 0.014s, episode steps:   3, steps per second: 222, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 883640.520833, mae: 2956.787679, accuracy: 0.083333, mean_q: -3246.944824, mean_eps: 0.100000\n",
      " 11894/50000: episode: 1504, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 609583.364583, mae: 2909.862467, accuracy: 0.156250, mean_q: -3252.727865, mean_eps: 0.100000\n",
      " 11897/50000: episode: 1505, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 827443.416667, mae: 2931.473470, accuracy: 0.177083, mean_q: -3236.612223, mean_eps: 0.100000\n",
      " 11900/50000: episode: 1506, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 717765.729167, mae: 2922.931315, accuracy: 0.135417, mean_q: -3218.601400, mean_eps: 0.100000\n",
      " 11904/50000: episode: 1507, duration: 0.015s, episode steps:   4, steps per second: 264, episode reward: -1223.000, mean reward: -305.750 [-999.000, -58.000], mean action: 1.250 [0.000, 3.000],  loss: 1003624.656250, mae: 2943.321655, accuracy: 0.125000, mean_q: -3212.265015, mean_eps: 0.100000\n",
      " 11911/50000: episode: 1508, duration: 0.025s, episode steps:   7, steps per second: 284, episode reward: -4220.000, mean reward: -602.857 [-999.000, -58.000], mean action: 2.143 [0.000, 3.000],  loss: 640688.933036, mae: 2893.470564, accuracy: 0.178571, mean_q: -3226.906738, mean_eps: 0.100000\n",
      " 11914/50000: episode: 1509, duration: 0.011s, episode steps:   3, steps per second: 262, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 777863.916667, mae: 2921.399089, accuracy: 0.177083, mean_q: -3230.402181, mean_eps: 0.100000\n",
      " 11917/50000: episode: 1510, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 1.667 [0.000, 3.000],  loss: 638602.645833, mae: 2894.903564, accuracy: 0.197917, mean_q: -3233.983317, mean_eps: 0.100000\n",
      " 11920/50000: episode: 1511, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 724112.156250, mae: 2916.627035, accuracy: 0.250000, mean_q: -3217.960124, mean_eps: 0.100000\n",
      " 11923/50000: episode: 1512, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 858676.104167, mae: 2965.680176, accuracy: 0.312500, mean_q: -3243.988932, mean_eps: 0.100000\n",
      " 11926/50000: episode: 1513, duration: 0.012s, episode steps:   3, steps per second: 257, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 741188.625000, mae: 2925.351481, accuracy: 0.218750, mean_q: -3250.166911, mean_eps: 0.100000\n",
      " 11929/50000: episode: 1514, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -224.000, mean reward: -74.667 [-116.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 820964.729167, mae: 2959.509440, accuracy: 0.260417, mean_q: -3253.657064, mean_eps: 0.100000\n",
      " 11932/50000: episode: 1515, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 698979.687500, mae: 2913.972900, accuracy: 0.250000, mean_q: -3245.276286, mean_eps: 0.100000\n",
      " 11935/50000: episode: 1516, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 967117.708333, mae: 2973.008870, accuracy: 0.218750, mean_q: -3247.745605, mean_eps: 0.100000\n",
      " 11938/50000: episode: 1517, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 814599.145833, mae: 2941.153158, accuracy: 0.250000, mean_q: -3257.819580, mean_eps: 0.100000\n",
      " 11941/50000: episode: 1518, duration: 0.013s, episode steps:   3, steps per second: 224, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 912575.364583, mae: 2957.323161, accuracy: 0.156250, mean_q: -3236.097249, mean_eps: 0.100000\n",
      " 11944/50000: episode: 1519, duration: 0.014s, episode steps:   3, steps per second: 210, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 1178817.916667, mae: 2984.925456, accuracy: 0.166667, mean_q: -3230.143392, mean_eps: 0.100000\n",
      " 11947/50000: episode: 1520, duration: 0.014s, episode steps:   3, steps per second: 220, episode reward: -224.000, mean reward: -74.667 [-116.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 736582.239583, mae: 2872.745850, accuracy: 0.218750, mean_q: -3194.976156, mean_eps: 0.100000\n",
      " 11950/50000: episode: 1521, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 526560.343750, mae: 2869.964681, accuracy: 0.333333, mean_q: -3182.355550, mean_eps: 0.100000\n",
      " 11953/50000: episode: 1522, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 531207.875000, mae: 2847.868408, accuracy: 0.281250, mean_q: -3190.182617, mean_eps: 0.100000\n",
      " 11956/50000: episode: 1523, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 656605.604167, mae: 2920.696208, accuracy: 0.208333, mean_q: -3222.011637, mean_eps: 0.100000\n",
      " 11959/50000: episode: 1524, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 795117.416667, mae: 2940.129476, accuracy: 0.239583, mean_q: -3245.544922, mean_eps: 0.100000\n",
      " 11962/50000: episode: 1525, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 489850.791667, mae: 2891.222575, accuracy: 0.343750, mean_q: -3259.246908, mean_eps: 0.100000\n",
      " 11965/50000: episode: 1526, duration: 0.012s, episode steps:   3, steps per second: 256, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 794398.510417, mae: 2958.399251, accuracy: 0.416667, mean_q: -3273.825439, mean_eps: 0.100000\n",
      " 11968/50000: episode: 1527, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 781783.812500, mae: 2958.692139, accuracy: 0.395833, mean_q: -3277.482910, mean_eps: 0.100000\n",
      " 11972/50000: episode: 1528, duration: 0.015s, episode steps:   4, steps per second: 268, episode reward: -1223.000, mean reward: -305.750 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 736112.828125, mae: 2964.128418, accuracy: 0.335938, mean_q: -3284.708008, mean_eps: 0.100000\n",
      " 11976/50000: episode: 1529, duration: 0.016s, episode steps:   4, steps per second: 253, episode reward: -1223.000, mean reward: -305.750 [-999.000, -58.000], mean action: 1.250 [0.000, 2.000],  loss: 706322.765625, mae: 2939.236755, accuracy: 0.335938, mean_q: -3274.358887, mean_eps: 0.100000\n",
      " 11979/50000: episode: 1530, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 777711.041667, mae: 2950.630941, accuracy: 0.322917, mean_q: -3261.181234, mean_eps: 0.100000\n",
      " 11994/50000: episode: 1531, duration: 0.050s, episode steps:  15, steps per second: 300, episode reward: -12227.000, mean reward: -815.133 [-999.000, -58.000], mean action: 2.600 [0.000, 3.000],  loss: 754876.916667, mae: 2947.779362, accuracy: 0.310417, mean_q: -3269.751139, mean_eps: 0.100000\n",
      " 11998/50000: episode: 1532, duration: 0.017s, episode steps:   4, steps per second: 240, episode reward: -1223.000, mean reward: -305.750 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 649991.859375, mae: 2935.299622, accuracy: 0.343750, mean_q: -3273.905518, mean_eps: 0.100000\n",
      " 12001/50000: episode: 1533, duration: 0.019s, episode steps:   3, steps per second: 160, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 925597.562500, mae: 2944.494792, accuracy: 0.250000, mean_q: -3222.312337, mean_eps: 0.100000\n",
      " 12004/50000: episode: 1534, duration: 0.015s, episode steps:   3, steps per second: 205, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 501904.739583, mae: 2910.552572, accuracy: 0.395833, mean_q: -3240.924967, mean_eps: 0.100000\n",
      " 12007/50000: episode: 1535, duration: 0.013s, episode steps:   3, steps per second: 230, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 914603.875000, mae: 2954.835124, accuracy: 0.270833, mean_q: -3226.562663, mean_eps: 0.100000\n",
      " 12010/50000: episode: 1536, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 856496.666667, mae: 2969.484294, accuracy: 0.406250, mean_q: -3240.649007, mean_eps: 0.100000\n",
      " 12013/50000: episode: 1537, duration: 0.014s, episode steps:   3, steps per second: 213, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 778558.895833, mae: 2916.623942, accuracy: 0.416667, mean_q: -3244.317057, mean_eps: 0.100000\n",
      " 12016/50000: episode: 1538, duration: 0.016s, episode steps:   3, steps per second: 191, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 645700.166667, mae: 2917.278158, accuracy: 0.302083, mean_q: -3252.553060, mean_eps: 0.100000\n",
      " 12019/50000: episode: 1539, duration: 0.016s, episode steps:   3, steps per second: 187, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 509548.135417, mae: 2921.465576, accuracy: 0.354167, mean_q: -3253.364502, mean_eps: 0.100000\n",
      " 12036/50000: episode: 1540, duration: 0.075s, episode steps:  17, steps per second: 227, episode reward: -14210.000, mean reward: -835.882 [-999.000, -32.000], mean action: 0.353 [0.000, 3.000],  loss: 865259.626838, mae: 2959.960866, accuracy: 0.316176, mean_q: -3256.512738, mean_eps: 0.100000\n",
      " 12039/50000: episode: 1541, duration: 0.016s, episode steps:   3, steps per second: 192, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 740915.166667, mae: 2938.101807, accuracy: 0.395833, mean_q: -3246.500895, mean_eps: 0.100000\n",
      " 12042/50000: episode: 1542, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 479422.927083, mae: 2909.121826, accuracy: 0.343750, mean_q: -3260.857259, mean_eps: 0.100000\n",
      " 12045/50000: episode: 1543, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 710053.520833, mae: 2934.847819, accuracy: 0.395833, mean_q: -3253.209147, mean_eps: 0.100000\n",
      " 12049/50000: episode: 1544, duration: 0.016s, episode steps:   4, steps per second: 244, episode reward: -1223.000, mean reward: -305.750 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 1004086.281250, mae: 2973.809998, accuracy: 0.359375, mean_q: -3259.842834, mean_eps: 0.100000\n",
      " 12053/50000: episode: 1545, duration: 0.018s, episode steps:   4, steps per second: 224, episode reward: -1238.000, mean reward: -309.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 650258.460938, mae: 2941.225281, accuracy: 0.343750, mean_q: -3282.744629, mean_eps: 0.100000\n",
      " 12056/50000: episode: 1546, duration: 0.014s, episode steps:   3, steps per second: 209, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 2.000 [1.000, 3.000],  loss: 800996.145833, mae: 2996.530924, accuracy: 0.447917, mean_q: -3266.179281, mean_eps: 0.100000\n",
      " 12059/50000: episode: 1547, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 945334.000000, mae: 2987.315267, accuracy: 0.291667, mean_q: -3268.637126, mean_eps: 0.100000\n",
      " 12062/50000: episode: 1548, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 1086385.000000, mae: 3013.041097, accuracy: 0.229167, mean_q: -3269.668213, mean_eps: 0.100000\n",
      " 12065/50000: episode: 1549, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 725542.937500, mae: 2979.526693, accuracy: 0.406250, mean_q: -3294.127930, mean_eps: 0.100000\n",
      " 12074/50000: episode: 1550, duration: 0.031s, episode steps:   9, steps per second: 288, episode reward: -6189.000, mean reward: -687.667 [-999.000, -32.000], mean action: 1.778 [0.000, 3.000],  loss: 812154.767361, mae: 2963.027995, accuracy: 0.305556, mean_q: -3269.899604, mean_eps: 0.100000\n",
      " 12077/50000: episode: 1551, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 877259.229167, mae: 2956.530111, accuracy: 0.208333, mean_q: -3259.257812, mean_eps: 0.100000\n",
      " 12080/50000: episode: 1552, duration: 0.014s, episode steps:   3, steps per second: 210, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 682541.000000, mae: 2947.375488, accuracy: 0.260417, mean_q: -3287.669596, mean_eps: 0.100000\n",
      " 12083/50000: episode: 1553, duration: 0.015s, episode steps:   3, steps per second: 199, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 704234.729167, mae: 2927.780436, accuracy: 0.354167, mean_q: -3259.278483, mean_eps: 0.100000\n",
      " 12086/50000: episode: 1554, duration: 0.016s, episode steps:   3, steps per second: 187, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 851173.500000, mae: 2968.904785, accuracy: 0.312500, mean_q: -3281.876302, mean_eps: 0.100000\n",
      " 12089/50000: episode: 1555, duration: 0.019s, episode steps:   3, steps per second: 159, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 1110765.833333, mae: 2988.595296, accuracy: 0.322917, mean_q: -3255.441732, mean_eps: 0.100000\n",
      " 12092/50000: episode: 1556, duration: 0.019s, episode steps:   3, steps per second: 154, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 941210.333333, mae: 2947.827799, accuracy: 0.312500, mean_q: -3214.913167, mean_eps: 0.100000\n",
      " 12095/50000: episode: 1557, duration: 0.018s, episode steps:   3, steps per second: 162, episode reward: -224.000, mean reward: -74.667 [-116.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 828317.645833, mae: 2911.614421, accuracy: 0.322917, mean_q: -3211.725586, mean_eps: 0.100000\n",
      " 12098/50000: episode: 1558, duration: 0.018s, episode steps:   3, steps per second: 163, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 1152374.583333, mae: 2974.088704, accuracy: 0.197917, mean_q: -3210.252930, mean_eps: 0.100000\n",
      " 12101/50000: episode: 1559, duration: 0.017s, episode steps:   3, steps per second: 173, episode reward: -224.000, mean reward: -74.667 [-116.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 926614.062500, mae: 2891.568766, accuracy: 0.302083, mean_q: -3175.105469, mean_eps: 0.100000\n",
      " 12135/50000: episode: 1560, duration: 0.133s, episode steps:  34, steps per second: 256, episode reward: -31164.000, mean reward: -916.588 [-999.000, -32.000], mean action: 2.824 [0.000, 3.000],  loss: 875375.321691, mae: 2909.879509, accuracy: 0.214154, mean_q: -3162.745203, mean_eps: 0.100000\n",
      " 12138/50000: episode: 1561, duration: 0.014s, episode steps:   3, steps per second: 208, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 655630.062500, mae: 2834.403646, accuracy: 0.270833, mean_q: -3142.693604, mean_eps: 0.100000\n",
      " 12142/50000: episode: 1562, duration: 0.021s, episode steps:   4, steps per second: 188, episode reward: -1223.000, mean reward: -305.750 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 834541.234375, mae: 2892.853149, accuracy: 0.234375, mean_q: -3167.721252, mean_eps: 0.100000\n",
      " 12146/50000: episode: 1563, duration: 0.020s, episode steps:   4, steps per second: 200, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 874388.734375, mae: 2888.823669, accuracy: 0.296875, mean_q: -3169.464233, mean_eps: 0.100000\n",
      " 12149/50000: episode: 1564, duration: 0.016s, episode steps:   3, steps per second: 184, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 593065.114583, mae: 2879.260579, accuracy: 0.343750, mean_q: -3185.955322, mean_eps: 0.100000\n",
      " 12152/50000: episode: 1565, duration: 0.017s, episode steps:   3, steps per second: 175, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 964376.916667, mae: 2909.878092, accuracy: 0.260417, mean_q: -3161.524821, mean_eps: 0.100000\n",
      " 12155/50000: episode: 1566, duration: 0.017s, episode steps:   3, steps per second: 174, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 725947.781250, mae: 2898.310384, accuracy: 0.239583, mean_q: -3165.271973, mean_eps: 0.100000\n",
      " 12205/50000: episode: 1567, duration: 0.191s, episode steps:  50, steps per second: 262, episode reward: -47177.000, mean reward: -943.540 [-999.000, -32.000], mean action: 1.180 [1.000, 3.000],  loss: 762611.151875, mae: 2893.150439, accuracy: 0.245000, mean_q: -3165.737939, mean_eps: 0.100000\n",
      " 12219/50000: episode: 1568, duration: 0.045s, episode steps:  14, steps per second: 312, episode reward: -11213.000, mean reward: -800.929 [-999.000, -32.000], mean action: 1.143 [0.000, 3.000],  loss: 900473.964286, mae: 2920.609462, accuracy: 0.241071, mean_q: -3173.681606, mean_eps: 0.100000\n",
      " 12223/50000: episode: 1569, duration: 0.016s, episode steps:   4, steps per second: 249, episode reward: -1223.000, mean reward: -305.750 [-999.000, -58.000], mean action: 1.750 [0.000, 3.000],  loss: 726967.250000, mae: 2892.524719, accuracy: 0.117188, mean_q: -3164.697327, mean_eps: 0.100000\n",
      " 12237/50000: episode: 1570, duration: 0.045s, episode steps:  14, steps per second: 309, episode reward: -11228.000, mean reward: -802.000 [-999.000, -45.000], mean action: 2.571 [0.000, 3.000],  loss: 930412.593750, mae: 2921.823905, accuracy: 0.136161, mean_q: -3184.158779, mean_eps: 0.100000\n",
      " 12240/50000: episode: 1571, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 1.667 [0.000, 3.000],  loss: 726422.104167, mae: 2894.256836, accuracy: 0.208333, mean_q: -3180.676432, mean_eps: 0.100000\n",
      " 12247/50000: episode: 1572, duration: 0.025s, episode steps:   7, steps per second: 280, episode reward: -4191.000, mean reward: -598.714 [-999.000, -45.000], mean action: 2.143 [0.000, 3.000],  loss: 729955.241071, mae: 2893.386300, accuracy: 0.232143, mean_q: -3178.677281, mean_eps: 0.100000\n",
      " 12251/50000: episode: 1573, duration: 0.018s, episode steps:   4, steps per second: 221, episode reward: -1223.000, mean reward: -305.750 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 639541.296875, mae: 2883.157288, accuracy: 0.265625, mean_q: -3178.283020, mean_eps: 0.100000\n",
      " 12254/50000: episode: 1574, duration: 0.017s, episode steps:   3, steps per second: 180, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 720198.760417, mae: 2904.784180, accuracy: 0.218750, mean_q: -3199.356689, mean_eps: 0.100000\n",
      " 12257/50000: episode: 1575, duration: 0.017s, episode steps:   3, steps per second: 175, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 519122.739583, mae: 2916.327393, accuracy: 0.312500, mean_q: -3255.122070, mean_eps: 0.100000\n",
      " 12260/50000: episode: 1576, duration: 0.016s, episode steps:   3, steps per second: 192, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 812304.145833, mae: 2965.074463, accuracy: 0.270833, mean_q: -3263.776774, mean_eps: 0.100000\n",
      " 12263/50000: episode: 1577, duration: 0.019s, episode steps:   3, steps per second: 162, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 466123.229167, mae: 2915.353027, accuracy: 0.302083, mean_q: -3267.801514, mean_eps: 0.100000\n",
      " 12266/50000: episode: 1578, duration: 0.023s, episode steps:   3, steps per second: 132, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 608190.125000, mae: 2916.685954, accuracy: 0.239583, mean_q: -3253.030192, mean_eps: 0.100000\n",
      " 12278/50000: episode: 1579, duration: 0.046s, episode steps:  12, steps per second: 260, episode reward: -9186.000, mean reward: -765.500 [-999.000, -45.000], mean action: 2.500 [0.000, 3.000],  loss: 642193.520833, mae: 2928.592509, accuracy: 0.190104, mean_q: -3249.998128, mean_eps: 0.100000\n",
      " 12281/50000: episode: 1580, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 680187.791667, mae: 2942.134115, accuracy: 0.187500, mean_q: -3271.205404, mean_eps: 0.100000\n",
      " 12284/50000: episode: 1581, duration: 0.012s, episode steps:   3, steps per second: 254, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 843609.250000, mae: 2987.261882, accuracy: 0.125000, mean_q: -3265.418864, mean_eps: 0.100000\n",
      " 12288/50000: episode: 1582, duration: 0.016s, episode steps:   4, steps per second: 257, episode reward: -1238.000, mean reward: -309.500 [-999.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 808529.640625, mae: 2944.478760, accuracy: 0.062500, mean_q: -3239.646362, mean_eps: 0.100000\n",
      " 12291/50000: episode: 1583, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 2.000 [1.000, 3.000],  loss: 675744.833333, mae: 2917.485352, accuracy: 0.072917, mean_q: -3202.382243, mean_eps: 0.100000\n",
      " 12294/50000: episode: 1584, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 1059979.645833, mae: 2960.456950, accuracy: 0.083333, mean_q: -3210.033122, mean_eps: 0.100000\n",
      " 12297/50000: episode: 1585, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 699679.020833, mae: 2911.874023, accuracy: 0.322917, mean_q: -3211.015137, mean_eps: 0.100000\n",
      " 12300/50000: episode: 1586, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 834834.333333, mae: 2914.089274, accuracy: 0.281250, mean_q: -3193.068766, mean_eps: 0.100000\n",
      " 12306/50000: episode: 1587, duration: 0.021s, episode steps:   6, steps per second: 282, episode reward: -3192.000, mean reward: -532.000 [-999.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 727302.531250, mae: 2909.009318, accuracy: 0.187500, mean_q: -3177.330607, mean_eps: 0.100000\n",
      " 12309/50000: episode: 1588, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 757972.541667, mae: 2918.427734, accuracy: 0.250000, mean_q: -3201.944661, mean_eps: 0.100000\n",
      " 12312/50000: episode: 1589, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 860696.229167, mae: 2945.008626, accuracy: 0.208333, mean_q: -3204.603841, mean_eps: 0.100000\n",
      " 12318/50000: episode: 1590, duration: 0.023s, episode steps:   6, steps per second: 261, episode reward: -3236.000, mean reward: -539.333 [-999.000, -45.000], mean action: 1.333 [0.000, 3.000],  loss: 725766.156250, mae: 2915.239461, accuracy: 0.239583, mean_q: -3227.354655, mean_eps: 0.100000\n",
      " 12321/50000: episode: 1591, duration: 0.014s, episode steps:   3, steps per second: 220, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 617479.750000, mae: 2913.511637, accuracy: 0.208333, mean_q: -3227.312012, mean_eps: 0.100000\n",
      " 12376/50000: episode: 1592, duration: 0.170s, episode steps:  55, steps per second: 324, episode reward: -52187.000, mean reward: -948.855 [-999.000, -45.000], mean action: 1.964 [0.000, 3.000],  loss: 816556.141477, mae: 2915.601851, accuracy: 0.257955, mean_q: -3182.359606, mean_eps: 0.100000\n",
      " 12471/50000: episode: 1593, duration: 0.313s, episode steps:  95, steps per second: 303, episode reward: -92147.000, mean reward: -969.968 [-999.000, -45.000], mean action: 1.926 [0.000, 3.000],  loss: 820220.899507, mae: 2877.526832, accuracy: 0.281908, mean_q: -3129.857419, mean_eps: 0.100000\n",
      " 12474/50000: episode: 1594, duration: 0.012s, episode steps:   3, steps per second: 255, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 668757.416667, mae: 2840.311279, accuracy: 0.395833, mean_q: -3108.148031, mean_eps: 0.100000\n",
      " 12478/50000: episode: 1595, duration: 0.015s, episode steps:   4, steps per second: 258, episode reward: -1223.000, mean reward: -305.750 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 776882.546875, mae: 2851.572449, accuracy: 0.335938, mean_q: -3103.506226, mean_eps: 0.100000\n",
      " 12481/50000: episode: 1596, duration: 0.012s, episode steps:   3, steps per second: 254, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 606020.645833, mae: 2844.049154, accuracy: 0.354167, mean_q: -3147.041667, mean_eps: 0.100000\n",
      " 12588/50000: episode: 1597, duration: 0.331s, episode steps: 107, steps per second: 323, episode reward: -104091.000, mean reward: -972.813 [-999.000, -32.000], mean action: 1.626 [0.000, 3.000],  loss: 741455.760222, mae: 2888.704113, accuracy: 0.312208, mean_q: -3153.743039, mean_eps: 0.100000\n",
      " 12591/50000: episode: 1598, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 2.000 [1.000, 3.000],  loss: 993777.083333, mae: 2938.686361, accuracy: 0.354167, mean_q: -3166.407959, mean_eps: 0.100000\n",
      " 12594/50000: episode: 1599, duration: 0.012s, episode steps:   3, steps per second: 256, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 1095146.312500, mae: 2957.233724, accuracy: 0.312500, mean_q: -3143.376139, mean_eps: 0.100000\n",
      " 12597/50000: episode: 1600, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 771758.791667, mae: 2866.040120, accuracy: 0.458333, mean_q: -3115.705322, mean_eps: 0.100000\n",
      " 12601/50000: episode: 1601, duration: 0.017s, episode steps:   4, steps per second: 241, episode reward: -1238.000, mean reward: -309.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 628304.515625, mae: 2835.549805, accuracy: 0.421875, mean_q: -3088.528687, mean_eps: 0.100000\n",
      " 12605/50000: episode: 1602, duration: 0.015s, episode steps:   4, steps per second: 260, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 856588.343750, mae: 2882.131653, accuracy: 0.429688, mean_q: -3082.965149, mean_eps: 0.100000\n",
      " 12637/50000: episode: 1603, duration: 0.102s, episode steps:  32, steps per second: 315, episode reward: -29166.000, mean reward: -911.438 [-999.000, -32.000], mean action: 0.188 [0.000, 3.000],  loss: 751738.545898, mae: 2872.393906, accuracy: 0.478516, mean_q: -3101.920128, mean_eps: 0.100000\n",
      " 12640/50000: episode: 1604, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 963607.104167, mae: 2893.029622, accuracy: 0.479167, mean_q: -3096.054199, mean_eps: 0.100000\n",
      " 12643/50000: episode: 1605, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 612881.416667, mae: 2854.956299, accuracy: 0.395833, mean_q: -3116.929362, mean_eps: 0.100000\n",
      " 12646/50000: episode: 1606, duration: 0.012s, episode steps:   3, steps per second: 257, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 866661.333333, mae: 2883.425293, accuracy: 0.500000, mean_q: -3125.315755, mean_eps: 0.100000\n",
      " 12656/50000: episode: 1607, duration: 0.036s, episode steps:  10, steps per second: 280, episode reward: -7217.000, mean reward: -721.700 [-999.000, -32.000], mean action: 0.700 [0.000, 3.000],  loss: 668684.731250, mae: 2869.416357, accuracy: 0.350000, mean_q: -3134.322021, mean_eps: 0.100000\n",
      " 12659/50000: episode: 1608, duration: 0.014s, episode steps:   3, steps per second: 212, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 614919.666667, mae: 2869.256429, accuracy: 0.208333, mean_q: -3158.878337, mean_eps: 0.100000\n",
      " 12663/50000: episode: 1609, duration: 0.019s, episode steps:   4, steps per second: 209, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 0.750 [0.000, 2.000],  loss: 798842.195312, mae: 2903.770874, accuracy: 0.296875, mean_q: -3156.090637, mean_eps: 0.100000\n",
      " 12666/50000: episode: 1610, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 603784.437500, mae: 2881.067383, accuracy: 0.250000, mean_q: -3163.053060, mean_eps: 0.100000\n",
      " 12669/50000: episode: 1611, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 512319.916667, mae: 2865.820150, accuracy: 0.250000, mean_q: -3174.534993, mean_eps: 0.100000\n",
      " 12672/50000: episode: 1612, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 943293.270833, mae: 2905.852132, accuracy: 0.197917, mean_q: -3170.223714, mean_eps: 0.100000\n",
      " 12675/50000: episode: 1613, duration: 0.014s, episode steps:   3, steps per second: 222, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 779844.479167, mae: 2924.111735, accuracy: 0.218750, mean_q: -3172.027751, mean_eps: 0.100000\n",
      " 12678/50000: episode: 1614, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 910053.958333, mae: 2927.379313, accuracy: 0.104167, mean_q: -3161.135173, mean_eps: 0.100000\n",
      " 12681/50000: episode: 1615, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 2.000 [1.000, 3.000],  loss: 927720.604167, mae: 2919.745117, accuracy: 0.218750, mean_q: -3181.301025, mean_eps: 0.100000\n",
      " 12684/50000: episode: 1616, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 604531.072917, mae: 2863.127848, accuracy: 0.250000, mean_q: -3137.792969, mean_eps: 0.100000\n",
      " 12687/50000: episode: 1617, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 739846.447917, mae: 2897.363281, accuracy: 0.229167, mean_q: -3140.267660, mean_eps: 0.100000\n",
      " 12690/50000: episode: 1618, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 805807.937500, mae: 2889.306071, accuracy: 0.197917, mean_q: -3145.326416, mean_eps: 0.100000\n",
      " 12693/50000: episode: 1619, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 2.000 [1.000, 3.000],  loss: 567286.895833, mae: 2836.143311, accuracy: 0.229167, mean_q: -3154.470703, mean_eps: 0.100000\n",
      " 12696/50000: episode: 1620, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 740049.489583, mae: 2868.109294, accuracy: 0.302083, mean_q: -3121.546712, mean_eps: 0.100000\n",
      " 12702/50000: episode: 1621, duration: 0.021s, episode steps:   6, steps per second: 285, episode reward: -3192.000, mean reward: -532.000 [-999.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 714003.348958, mae: 2894.895589, accuracy: 0.312500, mean_q: -3156.112508, mean_eps: 0.100000\n",
      " 12705/50000: episode: 1622, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 827637.833333, mae: 2938.294515, accuracy: 0.250000, mean_q: -3188.387288, mean_eps: 0.100000\n",
      " 12708/50000: episode: 1623, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 885561.687500, mae: 2938.768066, accuracy: 0.239583, mean_q: -3191.034017, mean_eps: 0.100000\n",
      " 12711/50000: episode: 1624, duration: 0.016s, episode steps:   3, steps per second: 190, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 747526.145833, mae: 2898.167562, accuracy: 0.302083, mean_q: -3173.441488, mean_eps: 0.100000\n",
      " 12715/50000: episode: 1625, duration: 0.019s, episode steps:   4, steps per second: 215, episode reward: -1238.000, mean reward: -309.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 914297.359375, mae: 2931.481689, accuracy: 0.343750, mean_q: -3178.120178, mean_eps: 0.100000\n",
      " 12718/50000: episode: 1626, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 974215.145833, mae: 2937.671143, accuracy: 0.270833, mean_q: -3140.903890, mean_eps: 0.100000\n",
      " 12721/50000: episode: 1627, duration: 0.012s, episode steps:   3, steps per second: 258, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 1143045.979167, mae: 2927.416178, accuracy: 0.260417, mean_q: -3119.701416, mean_eps: 0.100000\n",
      " 12724/50000: episode: 1628, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 725424.729167, mae: 2851.112956, accuracy: 0.322917, mean_q: -3113.977946, mean_eps: 0.100000\n",
      " 12727/50000: episode: 1629, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 778128.083333, mae: 2818.343343, accuracy: 0.333333, mean_q: -3052.793945, mean_eps: 0.100000\n",
      " 12730/50000: episode: 1630, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 768761.270833, mae: 2836.078451, accuracy: 0.416667, mean_q: -3079.975993, mean_eps: 0.100000\n",
      " 12733/50000: episode: 1631, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 2.000 [1.000, 3.000],  loss: 736419.291667, mae: 2792.763021, accuracy: 0.302083, mean_q: -3064.110840, mean_eps: 0.100000\n",
      " 12736/50000: episode: 1632, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 2.000 [1.000, 3.000],  loss: 830736.729167, mae: 2858.830648, accuracy: 0.229167, mean_q: -3063.749268, mean_eps: 0.100000\n",
      " 12741/50000: episode: 1633, duration: 0.020s, episode steps:   5, steps per second: 246, episode reward: -2222.000, mean reward: -444.400 [-999.000, -58.000], mean action: 1.600 [0.000, 3.000],  loss: 736174.900000, mae: 2839.088916, accuracy: 0.243750, mean_q: -3085.633252, mean_eps: 0.100000\n",
      " 12744/50000: episode: 1634, duration: 0.015s, episode steps:   3, steps per second: 206, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 2.000 [1.000, 3.000],  loss: 984352.395833, mae: 2884.103271, accuracy: 0.156250, mean_q: -3104.474772, mean_eps: 0.100000\n",
      " 12747/50000: episode: 1635, duration: 0.017s, episode steps:   3, steps per second: 180, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 741074.458333, mae: 2847.281576, accuracy: 0.260417, mean_q: -3115.143392, mean_eps: 0.100000\n",
      " 12750/50000: episode: 1636, duration: 0.017s, episode steps:   3, steps per second: 173, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 580185.072917, mae: 2824.735270, accuracy: 0.239583, mean_q: -3088.521159, mean_eps: 0.100000\n",
      " 12753/50000: episode: 1637, duration: 0.016s, episode steps:   3, steps per second: 187, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 790538.770833, mae: 2849.217611, accuracy: 0.229167, mean_q: -3076.488037, mean_eps: 0.100000\n",
      " 12761/50000: episode: 1638, duration: 0.036s, episode steps:   8, steps per second: 219, episode reward: -5190.000, mean reward: -648.750 [-999.000, -32.000], mean action: 1.750 [0.000, 3.000],  loss: 814817.511719, mae: 2842.070099, accuracy: 0.296875, mean_q: -3070.484406, mean_eps: 0.100000\n",
      " 12764/50000: episode: 1639, duration: 0.014s, episode steps:   3, steps per second: 210, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 766992.750000, mae: 2857.083496, accuracy: 0.218750, mean_q: -3095.893473, mean_eps: 0.100000\n",
      " 12767/50000: episode: 1640, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -224.000, mean reward: -74.667 [-116.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 934280.541667, mae: 2847.811605, accuracy: 0.260417, mean_q: -3046.780843, mean_eps: 0.100000\n",
      " 12770/50000: episode: 1641, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 716826.093750, mae: 2814.706543, accuracy: 0.281250, mean_q: -3074.229574, mean_eps: 0.100000\n",
      " 12773/50000: episode: 1642, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 777749.729167, mae: 2846.784668, accuracy: 0.229167, mean_q: -3090.380127, mean_eps: 0.100000\n",
      " 12776/50000: episode: 1643, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 777083.270833, mae: 2829.911702, accuracy: 0.322917, mean_q: -3073.679362, mean_eps: 0.100000\n",
      " 12779/50000: episode: 1644, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 757396.500000, mae: 2832.250081, accuracy: 0.343750, mean_q: -3060.821859, mean_eps: 0.100000\n",
      " 12782/50000: episode: 1645, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 673107.375000, mae: 2827.800049, accuracy: 0.291667, mean_q: -3071.815918, mean_eps: 0.100000\n",
      " 12793/50000: episode: 1646, duration: 0.037s, episode steps:  11, steps per second: 295, episode reward: -8216.000, mean reward: -746.909 [-999.000, -32.000], mean action: 0.545 [0.000, 3.000],  loss: 739317.306818, mae: 2823.102317, accuracy: 0.321023, mean_q: -3083.818049, mean_eps: 0.100000\n",
      " 12796/50000: episode: 1647, duration: 0.012s, episode steps:   3, steps per second: 255, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 763540.906250, mae: 2822.841715, accuracy: 0.197917, mean_q: -3064.345947, mean_eps: 0.100000\n",
      " 12821/50000: episode: 1648, duration: 0.085s, episode steps:  25, steps per second: 295, episode reward: -22173.000, mean reward: -886.920 [-999.000, -32.000], mean action: 1.880 [0.000, 3.000],  loss: 711753.886250, mae: 2847.482725, accuracy: 0.295000, mean_q: -3095.248379, mean_eps: 0.100000\n",
      " 12824/50000: episode: 1649, duration: 0.015s, episode steps:   3, steps per second: 200, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 650890.875000, mae: 2848.707194, accuracy: 0.312500, mean_q: -3137.985596, mean_eps: 0.100000\n",
      " 12827/50000: episode: 1650, duration: 0.015s, episode steps:   3, steps per second: 198, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 597362.906250, mae: 2844.128011, accuracy: 0.281250, mean_q: -3109.396647, mean_eps: 0.100000\n",
      " 12830/50000: episode: 1651, duration: 0.016s, episode steps:   3, steps per second: 185, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 472059.697917, mae: 2858.818604, accuracy: 0.291667, mean_q: -3155.865560, mean_eps: 0.100000\n",
      " 12834/50000: episode: 1652, duration: 0.019s, episode steps:   4, steps per second: 213, episode reward: -1238.000, mean reward: -309.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 567108.296875, mae: 2882.028748, accuracy: 0.343750, mean_q: -3183.713806, mean_eps: 0.100000\n",
      " 12838/50000: episode: 1653, duration: 0.020s, episode steps:   4, steps per second: 202, episode reward: -1223.000, mean reward: -305.750 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 802901.000000, mae: 2925.702881, accuracy: 0.296875, mean_q: -3197.984192, mean_eps: 0.100000\n",
      " 12841/50000: episode: 1654, duration: 0.014s, episode steps:   3, steps per second: 215, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 691783.458333, mae: 2934.372884, accuracy: 0.250000, mean_q: -3203.681152, mean_eps: 0.100000\n",
      " 12844/50000: episode: 1655, duration: 0.016s, episode steps:   3, steps per second: 193, episode reward: -224.000, mean reward: -74.667 [-116.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 704854.010417, mae: 2902.086914, accuracy: 0.322917, mean_q: -3167.529378, mean_eps: 0.100000\n",
      " 12847/50000: episode: 1656, duration: 0.016s, episode steps:   3, steps per second: 186, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 601352.947917, mae: 2871.604085, accuracy: 0.312500, mean_q: -3177.512044, mean_eps: 0.100000\n",
      " 12850/50000: episode: 1657, duration: 0.015s, episode steps:   3, steps per second: 205, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 1029204.083333, mae: 2943.278646, accuracy: 0.354167, mean_q: -3169.668132, mean_eps: 0.100000\n",
      " 12853/50000: episode: 1658, duration: 0.015s, episode steps:   3, steps per second: 206, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 668356.239583, mae: 2872.994710, accuracy: 0.291667, mean_q: -3118.210612, mean_eps: 0.100000\n",
      " 12867/50000: episode: 1659, duration: 0.047s, episode steps:  14, steps per second: 300, episode reward: -11213.000, mean reward: -800.929 [-999.000, -32.000], mean action: 0.714 [0.000, 3.000],  loss: 819976.741071, mae: 2849.500140, accuracy: 0.330357, mean_q: -3070.470180, mean_eps: 0.100000\n",
      " 12870/50000: episode: 1660, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 1007592.270833, mae: 2867.347168, accuracy: 0.343750, mean_q: -3042.710775, mean_eps: 0.100000\n",
      " 12873/50000: episode: 1661, duration: 0.018s, episode steps:   3, steps per second: 165, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 695450.083333, mae: 2820.868245, accuracy: 0.354167, mean_q: -3034.752116, mean_eps: 0.100000\n",
      " 12876/50000: episode: 1662, duration: 0.015s, episode steps:   3, steps per second: 206, episode reward: -224.000, mean reward: -74.667 [-116.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 853333.375000, mae: 2821.586589, accuracy: 0.395833, mean_q: -3029.584717, mean_eps: 0.100000\n",
      " 12879/50000: episode: 1663, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 526231.281250, mae: 2787.258057, accuracy: 0.447917, mean_q: -3040.167480, mean_eps: 0.100000\n",
      " 12896/50000: episode: 1664, duration: 0.056s, episode steps:  17, steps per second: 304, episode reward: -14210.000, mean reward: -835.882 [-999.000, -32.000], mean action: 1.176 [1.000, 3.000],  loss: 813702.661765, mae: 2842.768339, accuracy: 0.325368, mean_q: -3062.654168, mean_eps: 0.100000\n",
      " 12899/50000: episode: 1665, duration: 0.015s, episode steps:   3, steps per second: 201, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 807970.083333, mae: 2827.870850, accuracy: 0.270833, mean_q: -3062.047607, mean_eps: 0.100000\n",
      " 12906/50000: episode: 1666, duration: 0.033s, episode steps:   7, steps per second: 215, episode reward: -4220.000, mean reward: -602.857 [-999.000, -32.000], mean action: 1.000 [0.000, 3.000],  loss: 778595.223214, mae: 2827.688442, accuracy: 0.352679, mean_q: -3062.724505, mean_eps: 0.100000\n",
      " 12910/50000: episode: 1667, duration: 0.020s, episode steps:   4, steps per second: 204, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 650977.515625, mae: 2807.786682, accuracy: 0.296875, mean_q: -3052.855164, mean_eps: 0.100000\n",
      " 12913/50000: episode: 1668, duration: 0.015s, episode steps:   3, steps per second: 201, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 602950.708333, mae: 2796.612467, accuracy: 0.281250, mean_q: -3051.297282, mean_eps: 0.100000\n",
      " 12916/50000: episode: 1669, duration: 0.013s, episode steps:   3, steps per second: 230, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 865837.250000, mae: 2822.529460, accuracy: 0.239583, mean_q: -3044.062907, mean_eps: 0.100000\n",
      " 12919/50000: episode: 1670, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 1027685.895833, mae: 2876.463542, accuracy: 0.229167, mean_q: -3051.496908, mean_eps: 0.100000\n",
      " 12922/50000: episode: 1671, duration: 0.015s, episode steps:   3, steps per second: 197, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 894618.333333, mae: 2835.588053, accuracy: 0.239583, mean_q: -3061.738037, mean_eps: 0.100000\n",
      " 12926/50000: episode: 1672, duration: 0.021s, episode steps:   4, steps per second: 190, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 630586.968750, mae: 2767.919434, accuracy: 0.281250, mean_q: -3021.523926, mean_eps: 0.100000\n",
      " 12929/50000: episode: 1673, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 892755.083333, mae: 2849.412679, accuracy: 0.322917, mean_q: -3044.313721, mean_eps: 0.100000\n",
      " 12932/50000: episode: 1674, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 539685.885417, mae: 2789.155192, accuracy: 0.218750, mean_q: -3040.213623, mean_eps: 0.100000\n",
      " 12935/50000: episode: 1675, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 876144.500000, mae: 2815.541097, accuracy: 0.312500, mean_q: -3034.844076, mean_eps: 0.100000\n",
      " 12938/50000: episode: 1676, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 917626.500000, mae: 2860.906413, accuracy: 0.260417, mean_q: -3048.687337, mean_eps: 0.100000\n",
      " 12941/50000: episode: 1677, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 603959.906250, mae: 2829.823812, accuracy: 0.281250, mean_q: -3075.271566, mean_eps: 0.100000\n",
      " 12944/50000: episode: 1678, duration: 0.012s, episode steps:   3, steps per second: 256, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 713193.229167, mae: 2833.552246, accuracy: 0.302083, mean_q: -3058.434977, mean_eps: 0.100000\n",
      " 12947/50000: episode: 1679, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 594745.270833, mae: 2778.113607, accuracy: 0.239583, mean_q: -3038.284180, mean_eps: 0.100000\n",
      " 12950/50000: episode: 1680, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 624992.781250, mae: 2780.474772, accuracy: 0.260417, mean_q: -3062.052897, mean_eps: 0.100000\n",
      " 12953/50000: episode: 1681, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 713294.833333, mae: 2837.445475, accuracy: 0.270833, mean_q: -3102.194987, mean_eps: 0.100000\n",
      " 12956/50000: episode: 1682, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 770819.562500, mae: 2863.023356, accuracy: 0.281250, mean_q: -3113.519613, mean_eps: 0.100000\n",
      " 12960/50000: episode: 1683, duration: 0.015s, episode steps:   4, steps per second: 264, episode reward: -1238.000, mean reward: -309.500 [-999.000, -60.000], mean action: 2.250 [1.000, 3.000],  loss: 638087.953125, mae: 2817.053589, accuracy: 0.351562, mean_q: -3079.042847, mean_eps: 0.100000\n",
      " 12963/50000: episode: 1684, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 705366.729167, mae: 2809.536621, accuracy: 0.322917, mean_q: -3036.452393, mean_eps: 0.100000\n",
      " 12966/50000: episode: 1685, duration: 0.012s, episode steps:   3, steps per second: 255, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 781689.583333, mae: 2842.423584, accuracy: 0.187500, mean_q: -3083.852702, mean_eps: 0.100000\n",
      " 12969/50000: episode: 1686, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 755921.270833, mae: 2854.825195, accuracy: 0.281250, mean_q: -3066.206217, mean_eps: 0.100000\n",
      " 12972/50000: episode: 1687, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 631090.572917, mae: 2807.874349, accuracy: 0.250000, mean_q: -3064.964274, mean_eps: 0.100000\n",
      " 12975/50000: episode: 1688, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 882268.000000, mae: 2842.252279, accuracy: 0.322917, mean_q: -3047.165365, mean_eps: 0.100000\n",
      " 12978/50000: episode: 1689, duration: 0.014s, episode steps:   3, steps per second: 209, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 554900.770833, mae: 2794.888102, accuracy: 0.218750, mean_q: -3079.483398, mean_eps: 0.100000\n",
      " 12988/50000: episode: 1690, duration: 0.038s, episode steps:  10, steps per second: 266, episode reward: -7217.000, mean reward: -721.700 [-999.000, -58.000], mean action: 2.400 [0.000, 3.000],  loss: 637043.612500, mae: 2800.553491, accuracy: 0.187500, mean_q: -3045.746387, mean_eps: 0.100000\n",
      " 12991/50000: episode: 1691, duration: 0.015s, episode steps:   3, steps per second: 203, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 1.667 [0.000, 3.000],  loss: 901499.541667, mae: 2846.072428, accuracy: 0.156250, mean_q: -3057.056559, mean_eps: 0.100000\n",
      " 12994/50000: episode: 1692, duration: 0.015s, episode steps:   3, steps per second: 204, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 628414.520833, mae: 2811.075358, accuracy: 0.156250, mean_q: -3062.938395, mean_eps: 0.100000\n",
      " 12998/50000: episode: 1693, duration: 0.020s, episode steps:   4, steps per second: 198, episode reward: -1223.000, mean reward: -305.750 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 769837.687500, mae: 2840.909912, accuracy: 0.234375, mean_q: -3060.183350, mean_eps: 0.100000\n",
      " 13001/50000: episode: 1694, duration: 0.015s, episode steps:   3, steps per second: 199, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 673977.416667, mae: 2794.337240, accuracy: 0.177083, mean_q: -3057.027913, mean_eps: 0.100000\n",
      " 13004/50000: episode: 1695, duration: 0.015s, episode steps:   3, steps per second: 204, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 479210.385417, mae: 2800.070312, accuracy: 0.281250, mean_q: -3076.488525, mean_eps: 0.100000\n",
      " 13007/50000: episode: 1696, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 687328.145833, mae: 2820.749674, accuracy: 0.229167, mean_q: -3075.546305, mean_eps: 0.100000\n",
      " 13010/50000: episode: 1697, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 963841.812500, mae: 2849.632243, accuracy: 0.218750, mean_q: -3057.947917, mean_eps: 0.100000\n",
      " 13013/50000: episode: 1698, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 838529.083333, mae: 2813.685384, accuracy: 0.302083, mean_q: -3038.863444, mean_eps: 0.100000\n",
      " 13016/50000: episode: 1699, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 683699.166667, mae: 2788.964193, accuracy: 0.229167, mean_q: -3037.168945, mean_eps: 0.100000\n",
      " 13019/50000: episode: 1700, duration: 0.015s, episode steps:   3, steps per second: 198, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 776274.166667, mae: 2823.552327, accuracy: 0.281250, mean_q: -3032.725260, mean_eps: 0.100000\n",
      " 13022/50000: episode: 1701, duration: 0.015s, episode steps:   3, steps per second: 199, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 635309.302083, mae: 2797.235352, accuracy: 0.291667, mean_q: -3028.646647, mean_eps: 0.100000\n",
      " 13026/50000: episode: 1702, duration: 0.020s, episode steps:   4, steps per second: 203, episode reward: -1238.000, mean reward: -309.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 724770.609375, mae: 2814.309082, accuracy: 0.281250, mean_q: -3008.114746, mean_eps: 0.100000\n",
      " 13029/50000: episode: 1703, duration: 0.015s, episode steps:   3, steps per second: 200, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 664124.875000, mae: 2821.058757, accuracy: 0.270833, mean_q: -3034.581787, mean_eps: 0.100000\n",
      " 13032/50000: episode: 1704, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 607738.229167, mae: 2824.166178, accuracy: 0.187500, mean_q: -3048.245768, mean_eps: 0.100000\n",
      " 13035/50000: episode: 1705, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 2.000 [1.000, 3.000],  loss: 568551.916667, mae: 2806.191976, accuracy: 0.177083, mean_q: -3065.271159, mean_eps: 0.100000\n",
      " 13092/50000: episode: 1706, duration: 0.180s, episode steps:  57, steps per second: 316, episode reward: -54185.000, mean reward: -950.614 [-999.000, -45.000], mean action: 1.860 [0.000, 3.000],  loss: 697500.792763, mae: 2823.522024, accuracy: 0.194627, mean_q: -3055.227278, mean_eps: 0.100000\n",
      " 13095/50000: episode: 1707, duration: 0.014s, episode steps:   3, steps per second: 219, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 682512.895833, mae: 2865.237549, accuracy: 0.281250, mean_q: -3098.369385, mean_eps: 0.100000\n",
      " 13099/50000: episode: 1708, duration: 0.015s, episode steps:   4, steps per second: 263, episode reward: -1238.000, mean reward: -309.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 854148.265625, mae: 2895.483398, accuracy: 0.234375, mean_q: -3107.503174, mean_eps: 0.100000\n",
      " 13102/50000: episode: 1709, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 750824.166667, mae: 2842.925456, accuracy: 0.291667, mean_q: -3083.573893, mean_eps: 0.100000\n",
      " 13105/50000: episode: 1710, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 707087.687500, mae: 2851.631104, accuracy: 0.260417, mean_q: -3091.222412, mean_eps: 0.100000\n",
      " 13108/50000: episode: 1711, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 358566.229167, mae: 2794.493164, accuracy: 0.354167, mean_q: -3089.248210, mean_eps: 0.100000\n",
      " 13111/50000: episode: 1712, duration: 0.012s, episode steps:   3, steps per second: 255, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 653658.489583, mae: 2844.668945, accuracy: 0.250000, mean_q: -3095.093424, mean_eps: 0.100000\n",
      " 13114/50000: episode: 1713, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 850368.437500, mae: 2885.847738, accuracy: 0.250000, mean_q: -3131.904704, mean_eps: 0.100000\n",
      " 13117/50000: episode: 1714, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 630361.322917, mae: 2856.535563, accuracy: 0.218750, mean_q: -3116.566976, mean_eps: 0.100000\n",
      " 13120/50000: episode: 1715, duration: 0.011s, episode steps:   3, steps per second: 262, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 791198.208333, mae: 2843.713949, accuracy: 0.218750, mean_q: -3114.479899, mean_eps: 0.100000\n",
      " 13123/50000: episode: 1716, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -224.000, mean reward: -74.667 [-116.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 720065.958333, mae: 2867.230713, accuracy: 0.208333, mean_q: -3133.996501, mean_eps: 0.100000\n",
      " 13126/50000: episode: 1717, duration: 0.012s, episode steps:   3, steps per second: 258, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 852483.645833, mae: 2880.064453, accuracy: 0.260417, mean_q: -3085.194173, mean_eps: 0.100000\n",
      " 13129/50000: episode: 1718, duration: 0.012s, episode steps:   3, steps per second: 256, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 700270.666667, mae: 2854.427409, accuracy: 0.291667, mean_q: -3097.530762, mean_eps: 0.100000\n",
      " 13132/50000: episode: 1719, duration: 0.012s, episode steps:   3, steps per second: 254, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 2.000 [1.000, 3.000],  loss: 668419.000000, mae: 2849.103027, accuracy: 0.302083, mean_q: -3096.993815, mean_eps: 0.100000\n",
      " 13136/50000: episode: 1720, duration: 0.015s, episode steps:   4, steps per second: 272, episode reward: -1223.000, mean reward: -305.750 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 876637.484375, mae: 2873.291687, accuracy: 0.210938, mean_q: -3107.836975, mean_eps: 0.100000\n",
      " 13140/50000: episode: 1721, duration: 0.015s, episode steps:   4, steps per second: 265, episode reward: -1223.000, mean reward: -305.750 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 610496.500000, mae: 2813.271179, accuracy: 0.218750, mean_q: -3100.555908, mean_eps: 0.100000\n",
      " 13143/50000: episode: 1722, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 793874.375000, mae: 2843.056071, accuracy: 0.270833, mean_q: -3066.118815, mean_eps: 0.100000\n",
      " 13146/50000: episode: 1723, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 898193.729167, mae: 2898.100179, accuracy: 0.177083, mean_q: -3098.656250, mean_eps: 0.100000\n",
      " 13149/50000: episode: 1724, duration: 0.013s, episode steps:   3, steps per second: 225, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 573332.968750, mae: 2801.333171, accuracy: 0.260417, mean_q: -3072.466797, mean_eps: 0.100000\n",
      " 13152/50000: episode: 1725, duration: 0.014s, episode steps:   3, steps per second: 210, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 826491.125000, mae: 2836.434977, accuracy: 0.218750, mean_q: -3072.308757, mean_eps: 0.100000\n",
      " 13155/50000: episode: 1726, duration: 0.014s, episode steps:   3, steps per second: 212, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 901763.458333, mae: 2853.908773, accuracy: 0.208333, mean_q: -3019.963460, mean_eps: 0.100000\n",
      " 13158/50000: episode: 1727, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 740763.000000, mae: 2809.045492, accuracy: 0.218750, mean_q: -3045.800944, mean_eps: 0.100000\n",
      " 13161/50000: episode: 1728, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 725496.500000, mae: 2828.844645, accuracy: 0.291667, mean_q: -3035.860352, mean_eps: 0.100000\n",
      " 13164/50000: episode: 1729, duration: 0.012s, episode steps:   3, steps per second: 259, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 652824.416667, mae: 2781.556315, accuracy: 0.197917, mean_q: -3005.011719, mean_eps: 0.100000\n",
      " 13167/50000: episode: 1730, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 642784.645833, mae: 2784.484538, accuracy: 0.250000, mean_q: -3032.609456, mean_eps: 0.100000\n",
      " 13170/50000: episode: 1731, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 2.000 [1.000, 3.000],  loss: 568610.083333, mae: 2768.764486, accuracy: 0.270833, mean_q: -3018.509521, mean_eps: 0.100000\n",
      " 13173/50000: episode: 1732, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 1181197.125000, mae: 2886.489502, accuracy: 0.208333, mean_q: -3016.427653, mean_eps: 0.100000\n",
      " 13179/50000: episode: 1733, duration: 0.021s, episode steps:   6, steps per second: 291, episode reward: -3192.000, mean reward: -532.000 [-999.000, -32.000], mean action: 1.167 [0.000, 3.000],  loss: 571548.119792, mae: 2788.323364, accuracy: 0.223958, mean_q: -3016.744385, mean_eps: 0.100000\n",
      " 13246/50000: episode: 1734, duration: 0.209s, episode steps:  67, steps per second: 320, episode reward: -64131.000, mean reward: -957.179 [-999.000, -32.000], mean action: 1.254 [0.000, 3.000],  loss: 730647.725280, mae: 2808.755346, accuracy: 0.236940, mean_q: -3016.969238, mean_eps: 0.100000\n",
      " 13249/50000: episode: 1735, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 543552.989583, mae: 2771.044434, accuracy: 0.322917, mean_q: -3033.265625, mean_eps: 0.100000\n",
      " 13252/50000: episode: 1736, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 596750.197917, mae: 2777.421956, accuracy: 0.322917, mean_q: -3029.988118, mean_eps: 0.100000\n",
      " 13256/50000: episode: 1737, duration: 0.015s, episode steps:   4, steps per second: 265, episode reward: -1223.000, mean reward: -305.750 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 992940.687500, mae: 2870.783813, accuracy: 0.234375, mean_q: -3061.019714, mean_eps: 0.100000\n",
      " 13259/50000: episode: 1738, duration: 0.012s, episode steps:   3, steps per second: 253, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 734199.395833, mae: 2814.632975, accuracy: 0.208333, mean_q: -3045.750651, mean_eps: 0.100000\n",
      " 13262/50000: episode: 1739, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 2.000 [1.000, 3.000],  loss: 584587.375000, mae: 2776.994873, accuracy: 0.260417, mean_q: -3028.773600, mean_eps: 0.100000\n",
      " 13265/50000: episode: 1740, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -224.000, mean reward: -74.667 [-116.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 923585.750000, mae: 2843.826904, accuracy: 0.250000, mean_q: -3015.263753, mean_eps: 0.100000\n",
      " 13268/50000: episode: 1741, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -224.000, mean reward: -74.667 [-116.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 696897.187500, mae: 2783.395589, accuracy: 0.270833, mean_q: -3015.102376, mean_eps: 0.100000\n",
      " 13271/50000: episode: 1742, duration: 0.012s, episode steps:   3, steps per second: 257, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 718544.687500, mae: 2795.723470, accuracy: 0.343750, mean_q: -3004.516276, mean_eps: 0.100000\n",
      " 13274/50000: episode: 1743, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 706218.875000, mae: 2777.278076, accuracy: 0.218750, mean_q: -2986.382487, mean_eps: 0.100000\n",
      " 13278/50000: episode: 1744, duration: 0.016s, episode steps:   4, steps per second: 256, episode reward: -1223.000, mean reward: -305.750 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 692899.671875, mae: 2775.409363, accuracy: 0.250000, mean_q: -2991.272827, mean_eps: 0.100000\n",
      " 13283/50000: episode: 1745, duration: 0.018s, episode steps:   5, steps per second: 278, episode reward: -2237.000, mean reward: -447.400 [-999.000, -45.000], mean action: 1.400 [0.000, 3.000],  loss: 526346.843750, mae: 2776.599951, accuracy: 0.312500, mean_q: -3027.225293, mean_eps: 0.100000\n",
      " 13286/50000: episode: 1746, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 766753.020833, mae: 2845.017253, accuracy: 0.281250, mean_q: -3084.803630, mean_eps: 0.100000\n",
      " 13289/50000: episode: 1747, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 549176.833333, mae: 2811.805257, accuracy: 0.302083, mean_q: -3082.483236, mean_eps: 0.100000\n",
      " 13292/50000: episode: 1748, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 525558.375000, mae: 2793.893636, accuracy: 0.322917, mean_q: -3067.050212, mean_eps: 0.100000\n",
      " 13295/50000: episode: 1749, duration: 0.012s, episode steps:   3, steps per second: 254, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 612697.166667, mae: 2828.435221, accuracy: 0.333333, mean_q: -3082.567790, mean_eps: 0.100000\n",
      " 13298/50000: episode: 1750, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 1.667 [0.000, 3.000],  loss: 494991.614583, mae: 2807.873291, accuracy: 0.270833, mean_q: -3092.748372, mean_eps: 0.100000\n",
      " 13301/50000: episode: 1751, duration: 0.014s, episode steps:   3, steps per second: 221, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 711332.322917, mae: 2852.184489, accuracy: 0.208333, mean_q: -3064.622152, mean_eps: 0.100000\n",
      " 13304/50000: episode: 1752, duration: 0.014s, episode steps:   3, steps per second: 214, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 571047.437500, mae: 2809.660645, accuracy: 0.437500, mean_q: -3085.386882, mean_eps: 0.100000\n",
      " 13307/50000: episode: 1753, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 822069.062500, mae: 2872.353434, accuracy: 0.281250, mean_q: -3089.662760, mean_eps: 0.100000\n",
      " 13310/50000: episode: 1754, duration: 0.015s, episode steps:   3, steps per second: 206, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 818150.791667, mae: 2846.641927, accuracy: 0.260417, mean_q: -3062.890951, mean_eps: 0.100000\n",
      " 13314/50000: episode: 1755, duration: 0.015s, episode steps:   4, steps per second: 264, episode reward: -1223.000, mean reward: -305.750 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 721776.234375, mae: 2837.424988, accuracy: 0.226562, mean_q: -3073.112305, mean_eps: 0.100000\n",
      " 13317/50000: episode: 1756, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 666050.937500, mae: 2840.644857, accuracy: 0.270833, mean_q: -3074.085856, mean_eps: 0.100000\n",
      " 13320/50000: episode: 1757, duration: 0.012s, episode steps:   3, steps per second: 253, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 840168.895833, mae: 2834.202962, accuracy: 0.270833, mean_q: -3022.477946, mean_eps: 0.100000\n",
      " 13323/50000: episode: 1758, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 711051.250000, mae: 2822.175781, accuracy: 0.239583, mean_q: -3025.270264, mean_eps: 0.100000\n",
      " 13327/50000: episode: 1759, duration: 0.015s, episode steps:   4, steps per second: 273, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 753545.171875, mae: 2829.500549, accuracy: 0.242188, mean_q: -3073.844604, mean_eps: 0.100000\n",
      " 13330/50000: episode: 1760, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 799666.458333, mae: 2815.601318, accuracy: 0.239583, mean_q: -3007.915120, mean_eps: 0.100000\n",
      " 13341/50000: episode: 1761, duration: 0.036s, episode steps:  11, steps per second: 303, episode reward: -8231.000, mean reward: -748.273 [-999.000, -45.000], mean action: 1.909 [0.000, 3.000],  loss: 594235.734375, mae: 2800.747825, accuracy: 0.232955, mean_q: -3037.306752, mean_eps: 0.100000\n",
      " 13344/50000: episode: 1762, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 537614.750000, mae: 2797.090169, accuracy: 0.229167, mean_q: -3057.246989, mean_eps: 0.100000\n",
      " 13347/50000: episode: 1763, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 387660.166667, mae: 2793.050700, accuracy: 0.177083, mean_q: -3088.562419, mean_eps: 0.100000\n",
      " 13350/50000: episode: 1764, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 555574.989583, mae: 2849.126302, accuracy: 0.135417, mean_q: -3114.342041, mean_eps: 0.100000\n",
      " 13353/50000: episode: 1765, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 1034180.458333, mae: 2933.058431, accuracy: 0.145833, mean_q: -3112.214355, mean_eps: 0.100000\n",
      " 13358/50000: episode: 1766, duration: 0.018s, episode steps:   5, steps per second: 273, episode reward: -2237.000, mean reward: -447.400 [-999.000, -45.000], mean action: 1.400 [0.000, 3.000],  loss: 844190.968750, mae: 2915.028027, accuracy: 0.131250, mean_q: -3132.748828, mean_eps: 0.100000\n",
      " 13361/50000: episode: 1767, duration: 0.015s, episode steps:   3, steps per second: 205, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 753675.020833, mae: 2854.050781, accuracy: 0.083333, mean_q: -3094.810384, mean_eps: 0.100000\n",
      " 13364/50000: episode: 1768, duration: 0.020s, episode steps:   3, steps per second: 147, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 2.000 [1.000, 3.000],  loss: 1046087.979167, mae: 2883.413818, accuracy: 0.031250, mean_q: -3072.156413, mean_eps: 0.100000\n",
      " 13367/50000: episode: 1769, duration: 0.014s, episode steps:   3, steps per second: 216, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 770887.312500, mae: 2881.037354, accuracy: 0.062500, mean_q: -3089.835775, mean_eps: 0.100000\n",
      " 13370/50000: episode: 1770, duration: 0.014s, episode steps:   3, steps per second: 221, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 1031338.916667, mae: 2905.600911, accuracy: 0.145833, mean_q: -3065.835286, mean_eps: 0.100000\n",
      " 13373/50000: episode: 1771, duration: 0.014s, episode steps:   3, steps per second: 212, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 864100.333333, mae: 2832.405111, accuracy: 0.093750, mean_q: -3043.962646, mean_eps: 0.100000\n",
      " 13376/50000: episode: 1772, duration: 0.013s, episode steps:   3, steps per second: 230, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 496979.802083, mae: 2743.198486, accuracy: 0.125000, mean_q: -3005.820475, mean_eps: 0.100000\n",
      " 13379/50000: episode: 1773, duration: 0.014s, episode steps:   3, steps per second: 219, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 732014.645833, mae: 2780.905273, accuracy: 0.166667, mean_q: -2974.675456, mean_eps: 0.100000\n",
      " 13382/50000: episode: 1774, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 732132.187500, mae: 2811.070964, accuracy: 0.093750, mean_q: -3000.078695, mean_eps: 0.100000\n",
      " 13398/50000: episode: 1775, duration: 0.057s, episode steps:  16, steps per second: 280, episode reward: -13226.000, mean reward: -826.625 [-999.000, -45.000], mean action: 1.938 [0.000, 3.000],  loss: 691073.179688, mae: 2791.941849, accuracy: 0.183594, mean_q: -3002.020065, mean_eps: 0.100000\n",
      " 13401/50000: episode: 1776, duration: 0.014s, episode steps:   3, steps per second: 221, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 2.000 [1.000, 3.000],  loss: 609040.281250, mae: 2805.580078, accuracy: 0.166667, mean_q: -3000.988200, mean_eps: 0.100000\n",
      " 13405/50000: episode: 1777, duration: 0.020s, episode steps:   4, steps per second: 200, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 642328.601562, mae: 2779.193909, accuracy: 0.109375, mean_q: -3006.000916, mean_eps: 0.100000\n",
      " 13408/50000: episode: 1778, duration: 0.015s, episode steps:   3, steps per second: 196, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 740690.791667, mae: 2864.469889, accuracy: 0.135417, mean_q: -3043.489665, mean_eps: 0.100000\n",
      " 13411/50000: episode: 1779, duration: 0.014s, episode steps:   3, steps per second: 215, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 2.000 [1.000, 3.000],  loss: 748703.354167, mae: 2838.494466, accuracy: 0.135417, mean_q: -3068.027507, mean_eps: 0.100000\n",
      " 13419/50000: episode: 1780, duration: 0.030s, episode steps:   8, steps per second: 268, episode reward: -5219.000, mean reward: -652.375 [-999.000, -58.000], mean action: 1.750 [0.000, 3.000],  loss: 776598.843750, mae: 2854.732391, accuracy: 0.121094, mean_q: -3074.311493, mean_eps: 0.100000\n",
      " 13422/50000: episode: 1781, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 768807.416667, mae: 2838.397786, accuracy: 0.156250, mean_q: -3071.551758, mean_eps: 0.100000\n",
      " 13425/50000: episode: 1782, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 835291.166667, mae: 2858.580322, accuracy: 0.135417, mean_q: -3049.881022, mean_eps: 0.100000\n",
      " 13430/50000: episode: 1783, duration: 0.021s, episode steps:   5, steps per second: 233, episode reward: -2222.000, mean reward: -444.400 [-999.000, -58.000], mean action: 1.600 [0.000, 3.000],  loss: 735760.612500, mae: 2823.447363, accuracy: 0.250000, mean_q: -3019.729639, mean_eps: 0.100000\n",
      " 13433/50000: episode: 1784, duration: 0.014s, episode steps:   3, steps per second: 220, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 656569.958333, mae: 2812.939453, accuracy: 0.312500, mean_q: -3033.822754, mean_eps: 0.100000\n",
      " 13436/50000: episode: 1785, duration: 0.014s, episode steps:   3, steps per second: 215, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 492457.562500, mae: 2765.040283, accuracy: 0.239583, mean_q: -3007.260091, mean_eps: 0.100000\n",
      " 13439/50000: episode: 1786, duration: 0.013s, episode steps:   3, steps per second: 224, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 774241.302083, mae: 2841.121012, accuracy: 0.197917, mean_q: -3036.169515, mean_eps: 0.100000\n",
      " 13442/50000: episode: 1787, duration: 0.014s, episode steps:   3, steps per second: 222, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 665754.958333, mae: 2798.806234, accuracy: 0.260417, mean_q: -3021.244141, mean_eps: 0.100000\n",
      " 13447/50000: episode: 1788, duration: 0.021s, episode steps:   5, steps per second: 241, episode reward: -2193.000, mean reward: -438.600 [-999.000, -32.000], mean action: 1.600 [1.000, 3.000],  loss: 698966.562500, mae: 2812.880420, accuracy: 0.293750, mean_q: -3021.919727, mean_eps: 0.100000\n",
      " 13450/50000: episode: 1789, duration: 0.014s, episode steps:   3, steps per second: 219, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 570875.104167, mae: 2760.088053, accuracy: 0.218750, mean_q: -3007.429525, mean_eps: 0.100000\n",
      " 13453/50000: episode: 1790, duration: 0.016s, episode steps:   3, steps per second: 183, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 589202.354167, mae: 2790.743327, accuracy: 0.239583, mean_q: -3024.553630, mean_eps: 0.100000\n",
      " 13456/50000: episode: 1791, duration: 0.016s, episode steps:   3, steps per second: 190, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 749336.083333, mae: 2862.465413, accuracy: 0.250000, mean_q: -3076.792155, mean_eps: 0.100000\n",
      " 13459/50000: episode: 1792, duration: 0.014s, episode steps:   3, steps per second: 221, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 906108.541667, mae: 2853.096273, accuracy: 0.208333, mean_q: -3043.983968, mean_eps: 0.100000\n",
      " 13462/50000: episode: 1793, duration: 0.014s, episode steps:   3, steps per second: 221, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 782496.500000, mae: 2807.724284, accuracy: 0.260417, mean_q: -3032.083008, mean_eps: 0.100000\n",
      " 13465/50000: episode: 1794, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 717117.250000, mae: 2786.476725, accuracy: 0.177083, mean_q: -3002.400553, mean_eps: 0.100000\n",
      " 13468/50000: episode: 1795, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 599474.135417, mae: 2770.780192, accuracy: 0.239583, mean_q: -3009.675944, mean_eps: 0.100000\n",
      " 13471/50000: episode: 1796, duration: 0.014s, episode steps:   3, steps per second: 208, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 552121.885417, mae: 2788.471191, accuracy: 0.322917, mean_q: -3025.396973, mean_eps: 0.100000\n",
      " 13474/50000: episode: 1797, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 734913.125000, mae: 2837.035726, accuracy: 0.197917, mean_q: -3050.547282, mean_eps: 0.100000\n",
      " 13477/50000: episode: 1798, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 532870.479167, mae: 2765.374919, accuracy: 0.208333, mean_q: -2997.773519, mean_eps: 0.100000\n",
      " 13480/50000: episode: 1799, duration: 0.012s, episode steps:   3, steps per second: 254, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 691434.458333, mae: 2842.814941, accuracy: 0.218750, mean_q: -3072.063721, mean_eps: 0.100000\n",
      " 13483/50000: episode: 1800, duration: 0.012s, episode steps:   3, steps per second: 253, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 770298.791667, mae: 2864.333822, accuracy: 0.208333, mean_q: -3066.318441, mean_eps: 0.100000\n",
      " 13486/50000: episode: 1801, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 797909.416667, mae: 2801.436442, accuracy: 0.187500, mean_q: -2994.723958, mean_eps: 0.100000\n",
      " 13489/50000: episode: 1802, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 1108443.708333, mae: 2860.978760, accuracy: 0.177083, mean_q: -3020.128906, mean_eps: 0.100000\n",
      " 13492/50000: episode: 1803, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 594226.520833, mae: 2765.470703, accuracy: 0.250000, mean_q: -2989.193522, mean_eps: 0.100000\n",
      " 13495/50000: episode: 1804, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 623197.968750, mae: 2732.673014, accuracy: 0.312500, mean_q: -2942.388835, mean_eps: 0.100000\n",
      " 13498/50000: episode: 1805, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 529066.552083, mae: 2740.376465, accuracy: 0.229167, mean_q: -2977.680583, mean_eps: 0.100000\n",
      " 13501/50000: episode: 1806, duration: 0.013s, episode steps:   3, steps per second: 226, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 878377.875000, mae: 2786.330648, accuracy: 0.229167, mean_q: -2921.474121, mean_eps: 0.100000\n",
      " 13504/50000: episode: 1807, duration: 0.014s, episode steps:   3, steps per second: 211, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 680425.354167, mae: 2777.486247, accuracy: 0.104167, mean_q: -2969.865804, mean_eps: 0.100000\n",
      " 13507/50000: episode: 1808, duration: 0.014s, episode steps:   3, steps per second: 222, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 732055.333333, mae: 2791.562093, accuracy: 0.197917, mean_q: -2965.529378, mean_eps: 0.100000\n",
      " 13510/50000: episode: 1809, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 759080.541667, mae: 2835.627848, accuracy: 0.218750, mean_q: -3034.325439, mean_eps: 0.100000\n",
      " 13513/50000: episode: 1810, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 778239.083333, mae: 2800.635091, accuracy: 0.260417, mean_q: -2997.129150, mean_eps: 0.100000\n",
      " 13516/50000: episode: 1811, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 456997.062500, mae: 2746.282145, accuracy: 0.250000, mean_q: -3018.193034, mean_eps: 0.100000\n",
      " 13519/50000: episode: 1812, duration: 0.012s, episode steps:   3, steps per second: 260, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 994454.666667, mae: 2829.905273, accuracy: 0.208333, mean_q: -3001.862386, mean_eps: 0.100000\n",
      " 13522/50000: episode: 1813, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 805203.062500, mae: 2792.338542, accuracy: 0.156250, mean_q: -3008.664388, mean_eps: 0.100000\n",
      " 13525/50000: episode: 1814, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 915602.895833, mae: 2846.661621, accuracy: 0.250000, mean_q: -3022.815999, mean_eps: 0.100000\n",
      " 13528/50000: episode: 1815, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 579372.052083, mae: 2765.850016, accuracy: 0.218750, mean_q: -2997.399740, mean_eps: 0.100000\n",
      " 13531/50000: episode: 1816, duration: 0.011s, episode steps:   3, steps per second: 267, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 815482.166667, mae: 2811.487630, accuracy: 0.218750, mean_q: -2993.562093, mean_eps: 0.100000\n",
      " 13534/50000: episode: 1817, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 881717.687500, mae: 2804.596761, accuracy: 0.270833, mean_q: -3001.928385, mean_eps: 0.100000\n",
      " 13537/50000: episode: 1818, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 538381.989583, mae: 2745.265462, accuracy: 0.270833, mean_q: -2952.967285, mean_eps: 0.100000\n",
      " 13540/50000: episode: 1819, duration: 0.012s, episode steps:   3, steps per second: 256, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 742373.125000, mae: 2767.828451, accuracy: 0.375000, mean_q: -2957.154867, mean_eps: 0.100000\n",
      " 13543/50000: episode: 1820, duration: 0.012s, episode steps:   3, steps per second: 255, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 594206.354167, mae: 2724.670329, accuracy: 0.427083, mean_q: -2919.729492, mean_eps: 0.100000\n",
      " 13548/50000: episode: 1821, duration: 0.020s, episode steps:   5, steps per second: 252, episode reward: -2193.000, mean reward: -438.600 [-999.000, -58.000], mean action: 2.200 [1.000, 3.000],  loss: 795026.725000, mae: 2773.717383, accuracy: 0.275000, mean_q: -2931.125098, mean_eps: 0.100000\n",
      " 13551/50000: episode: 1822, duration: 0.014s, episode steps:   3, steps per second: 211, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 806967.708333, mae: 2782.697917, accuracy: 0.395833, mean_q: -2912.841878, mean_eps: 0.100000\n",
      " 13554/50000: episode: 1823, duration: 0.017s, episode steps:   3, steps per second: 177, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 794294.416667, mae: 2787.666016, accuracy: 0.437500, mean_q: -2920.618408, mean_eps: 0.100000\n",
      " 13557/50000: episode: 1824, duration: 0.014s, episode steps:   3, steps per second: 217, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 618866.781250, mae: 2736.970052, accuracy: 0.343750, mean_q: -2945.906738, mean_eps: 0.100000\n",
      " 13561/50000: episode: 1825, duration: 0.015s, episode steps:   4, steps per second: 262, episode reward: -1238.000, mean reward: -309.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 877519.625000, mae: 2777.699219, accuracy: 0.351562, mean_q: -2928.873352, mean_eps: 0.100000\n",
      " 13564/50000: episode: 1826, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 704965.468750, mae: 2765.100423, accuracy: 0.354167, mean_q: -2913.007731, mean_eps: 0.100000\n",
      " 13567/50000: episode: 1827, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 645340.010417, mae: 2717.824463, accuracy: 0.322917, mean_q: -2931.880371, mean_eps: 0.100000\n",
      " 13570/50000: episode: 1828, duration: 0.012s, episode steps:   3, steps per second: 258, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 712163.958333, mae: 2764.459880, accuracy: 0.239583, mean_q: -2967.603190, mean_eps: 0.100000\n",
      " 13573/50000: episode: 1829, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 421428.187500, mae: 2709.374919, accuracy: 0.333333, mean_q: -2929.097900, mean_eps: 0.100000\n",
      " 13576/50000: episode: 1830, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 480517.906250, mae: 2730.332031, accuracy: 0.291667, mean_q: -2971.087484, mean_eps: 0.100000\n",
      " 13579/50000: episode: 1831, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 639090.270833, mae: 2769.932292, accuracy: 0.229167, mean_q: -2981.914388, mean_eps: 0.100000\n",
      " 13582/50000: episode: 1832, duration: 0.012s, episode steps:   3, steps per second: 257, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 679128.145833, mae: 2773.969482, accuracy: 0.250000, mean_q: -2982.810872, mean_eps: 0.100000\n",
      " 13585/50000: episode: 1833, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 542420.458333, mae: 2779.399007, accuracy: 0.250000, mean_q: -2985.792318, mean_eps: 0.100000\n",
      " 13588/50000: episode: 1834, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 732952.770833, mae: 2792.770589, accuracy: 0.270833, mean_q: -2978.664795, mean_eps: 0.100000\n",
      " 13591/50000: episode: 1835, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 787836.875000, mae: 2826.016032, accuracy: 0.229167, mean_q: -3030.629964, mean_eps: 0.100000\n",
      " 13594/50000: episode: 1836, duration: 0.012s, episode steps:   3, steps per second: 254, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 758652.645833, mae: 2802.902995, accuracy: 0.270833, mean_q: -3012.999186, mean_eps: 0.100000\n",
      " 13598/50000: episode: 1837, duration: 0.015s, episode steps:   4, steps per second: 272, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 824267.718750, mae: 2824.609558, accuracy: 0.210938, mean_q: -2999.702515, mean_eps: 0.100000\n",
      " 13601/50000: episode: 1838, duration: 0.013s, episode steps:   3, steps per second: 223, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 543790.822917, mae: 2743.738444, accuracy: 0.302083, mean_q: -2950.549398, mean_eps: 0.100000\n",
      " 13606/50000: episode: 1839, duration: 0.018s, episode steps:   5, steps per second: 273, episode reward: -2193.000, mean reward: -438.600 [-999.000, -32.000], mean action: 1.800 [0.000, 3.000],  loss: 800453.237500, mae: 2784.214648, accuracy: 0.300000, mean_q: -2969.098437, mean_eps: 0.100000\n",
      " 13609/50000: episode: 1840, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 548367.145833, mae: 2697.930664, accuracy: 0.270833, mean_q: -2927.092611, mean_eps: 0.100000\n",
      " 13612/50000: episode: 1841, duration: 0.014s, episode steps:   3, steps per second: 214, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 539042.145833, mae: 2722.843424, accuracy: 0.322917, mean_q: -2938.049723, mean_eps: 0.100000\n",
      " 13615/50000: episode: 1842, duration: 0.013s, episode steps:   3, steps per second: 226, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 643584.822917, mae: 2727.944580, accuracy: 0.281250, mean_q: -2929.611572, mean_eps: 0.100000\n",
      " 13618/50000: episode: 1843, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 529177.781250, mae: 2741.249674, accuracy: 0.291667, mean_q: -2955.498454, mean_eps: 0.100000\n",
      " 13621/50000: episode: 1844, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 721879.625000, mae: 2779.398193, accuracy: 0.208333, mean_q: -2939.816325, mean_eps: 0.100000\n",
      " 13625/50000: episode: 1845, duration: 0.016s, episode steps:   4, steps per second: 255, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 886911.515625, mae: 2770.479492, accuracy: 0.179688, mean_q: -2920.005615, mean_eps: 0.100000\n",
      " 13628/50000: episode: 1846, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 848654.145833, mae: 2761.976156, accuracy: 0.281250, mean_q: -2926.595296, mean_eps: 0.100000\n",
      " 13631/50000: episode: 1847, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 653542.312500, mae: 2721.066813, accuracy: 0.239583, mean_q: -2911.620931, mean_eps: 0.100000\n",
      " 13634/50000: episode: 1848, duration: 0.012s, episode steps:   3, steps per second: 259, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 562674.270833, mae: 2716.261393, accuracy: 0.239583, mean_q: -2917.391846, mean_eps: 0.100000\n",
      " 13637/50000: episode: 1849, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 822718.062500, mae: 2768.770996, accuracy: 0.114583, mean_q: -2916.911784, mean_eps: 0.100000\n",
      " 13640/50000: episode: 1850, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 695322.854167, mae: 2730.255778, accuracy: 0.208333, mean_q: -2899.040690, mean_eps: 0.100000\n",
      " 13644/50000: episode: 1851, duration: 0.015s, episode steps:   4, steps per second: 275, episode reward: -1238.000, mean reward: -309.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 609510.710938, mae: 2738.543274, accuracy: 0.140625, mean_q: -2927.516357, mean_eps: 0.100000\n",
      " 13647/50000: episode: 1852, duration: 0.011s, episode steps:   3, steps per second: 262, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 826534.916667, mae: 2738.608887, accuracy: 0.166667, mean_q: -2905.707438, mean_eps: 0.100000\n",
      " 13650/50000: episode: 1853, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 2.000 [1.000, 3.000],  loss: 676969.375000, mae: 2756.391032, accuracy: 0.145833, mean_q: -2940.507568, mean_eps: 0.100000\n",
      " 13653/50000: episode: 1854, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 719497.697917, mae: 2738.715088, accuracy: 0.260417, mean_q: -2910.205648, mean_eps: 0.100000\n",
      " 13656/50000: episode: 1855, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 555977.187500, mae: 2724.921224, accuracy: 0.239583, mean_q: -2927.208171, mean_eps: 0.100000\n",
      " 13659/50000: episode: 1856, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 546404.750000, mae: 2732.384766, accuracy: 0.270833, mean_q: -2908.679443, mean_eps: 0.100000\n",
      " 13662/50000: episode: 1857, duration: 0.014s, episode steps:   3, steps per second: 210, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 764167.625000, mae: 2746.892171, accuracy: 0.260417, mean_q: -2921.809489, mean_eps: 0.100000\n",
      " 13666/50000: episode: 1858, duration: 0.017s, episode steps:   4, steps per second: 232, episode reward: -1223.000, mean reward: -305.750 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 680711.203125, mae: 2762.902649, accuracy: 0.242188, mean_q: -2942.599060, mean_eps: 0.100000\n",
      " 13669/50000: episode: 1859, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 844275.333333, mae: 2784.968750, accuracy: 0.270833, mean_q: -2939.030273, mean_eps: 0.100000\n",
      " 13672/50000: episode: 1860, duration: 0.013s, episode steps:   3, steps per second: 223, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 823236.416667, mae: 2771.980957, accuracy: 0.208333, mean_q: -2928.861084, mean_eps: 0.100000\n",
      " 13677/50000: episode: 1861, duration: 0.021s, episode steps:   5, steps per second: 236, episode reward: -2237.000, mean reward: -447.400 [-999.000, -60.000], mean action: 1.400 [0.000, 3.000],  loss: 654579.806250, mae: 2744.626563, accuracy: 0.281250, mean_q: -2934.383838, mean_eps: 0.100000\n",
      " 13680/50000: episode: 1862, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 718779.395833, mae: 2736.219482, accuracy: 0.208333, mean_q: -2903.742513, mean_eps: 0.100000\n",
      " 13683/50000: episode: 1863, duration: 0.014s, episode steps:   3, steps per second: 212, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 659460.250000, mae: 2709.226969, accuracy: 0.333333, mean_q: -2906.072021, mean_eps: 0.100000\n",
      " 13686/50000: episode: 1864, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 674609.531250, mae: 2754.967448, accuracy: 0.322917, mean_q: -2934.212891, mean_eps: 0.100000\n",
      " 13689/50000: episode: 1865, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 828049.541667, mae: 2756.497396, accuracy: 0.281250, mean_q: -2903.862061, mean_eps: 0.100000\n",
      " 13695/50000: episode: 1866, duration: 0.022s, episode steps:   6, steps per second: 267, episode reward: -3236.000, mean reward: -539.333 [-999.000, -60.000], mean action: 1.000 [0.000, 3.000],  loss: 531452.166667, mae: 2683.758097, accuracy: 0.234375, mean_q: -2881.605957, mean_eps: 0.100000\n",
      " 13698/50000: episode: 1867, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 647386.500000, mae: 2708.422201, accuracy: 0.364583, mean_q: -2883.272949, mean_eps: 0.100000\n",
      " 13701/50000: episode: 1868, duration: 0.013s, episode steps:   3, steps per second: 227, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 568112.437500, mae: 2714.861084, accuracy: 0.322917, mean_q: -2909.588867, mean_eps: 0.100000\n",
      " 13704/50000: episode: 1869, duration: 0.014s, episode steps:   3, steps per second: 215, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 550709.593750, mae: 2717.651774, accuracy: 0.260417, mean_q: -2910.289388, mean_eps: 0.100000\n",
      " 13707/50000: episode: 1870, duration: 0.014s, episode steps:   3, steps per second: 218, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 749702.583333, mae: 2773.349202, accuracy: 0.208333, mean_q: -2934.545898, mean_eps: 0.100000\n",
      " 13710/50000: episode: 1871, duration: 0.015s, episode steps:   3, steps per second: 196, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 524727.854167, mae: 2731.297119, accuracy: 0.187500, mean_q: -2969.540202, mean_eps: 0.100000\n",
      " 13713/50000: episode: 1872, duration: 0.014s, episode steps:   3, steps per second: 216, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 710912.916667, mae: 2716.082926, accuracy: 0.291667, mean_q: -2885.146810, mean_eps: 0.100000\n",
      " 13716/50000: episode: 1873, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -224.000, mean reward: -74.667 [-116.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 468188.895833, mae: 2715.319987, accuracy: 0.302083, mean_q: -2936.663005, mean_eps: 0.100000\n",
      " 13720/50000: episode: 1874, duration: 0.015s, episode steps:   4, steps per second: 259, episode reward: -1223.000, mean reward: -305.750 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 704424.375000, mae: 2766.389587, accuracy: 0.257812, mean_q: -2965.076538, mean_eps: 0.100000\n",
      " 13723/50000: episode: 1875, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -224.000, mean reward: -74.667 [-116.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 592131.239583, mae: 2734.903483, accuracy: 0.270833, mean_q: -2948.338542, mean_eps: 0.100000\n",
      " 13726/50000: episode: 1876, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 760313.916667, mae: 2781.058919, accuracy: 0.156250, mean_q: -2945.770671, mean_eps: 0.100000\n",
      " 13729/50000: episode: 1877, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 717276.583333, mae: 2777.869385, accuracy: 0.197917, mean_q: -2932.861328, mean_eps: 0.100000\n",
      " 13732/50000: episode: 1878, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 743449.937500, mae: 2736.888102, accuracy: 0.239583, mean_q: -2897.109701, mean_eps: 0.100000\n",
      " 13736/50000: episode: 1879, duration: 0.015s, episode steps:   4, steps per second: 271, episode reward: -1223.000, mean reward: -305.750 [-999.000, -32.000], mean action: 0.750 [0.000, 2.000],  loss: 584072.125000, mae: 2689.044250, accuracy: 0.273438, mean_q: -2885.423462, mean_eps: 0.100000\n",
      " 13740/50000: episode: 1880, duration: 0.015s, episode steps:   4, steps per second: 265, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 596752.304688, mae: 2724.743958, accuracy: 0.281250, mean_q: -2914.971252, mean_eps: 0.100000\n",
      " 13743/50000: episode: 1881, duration: 0.012s, episode steps:   3, steps per second: 255, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 780070.895833, mae: 2709.736328, accuracy: 0.281250, mean_q: -2884.318034, mean_eps: 0.100000\n",
      " 13746/50000: episode: 1882, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 779916.520833, mae: 2743.234456, accuracy: 0.229167, mean_q: -2900.545329, mean_eps: 0.100000\n",
      " 13750/50000: episode: 1883, duration: 0.016s, episode steps:   4, steps per second: 250, episode reward: -1238.000, mean reward: -309.500 [-999.000, -60.000], mean action: 1.750 [1.000, 3.000],  loss: 544451.093750, mae: 2682.705811, accuracy: 0.250000, mean_q: -2872.593872, mean_eps: 0.100000\n",
      " 13753/50000: episode: 1884, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 800253.187500, mae: 2756.405192, accuracy: 0.187500, mean_q: -2895.783773, mean_eps: 0.100000\n",
      " 13756/50000: episode: 1885, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 576386.750000, mae: 2689.168376, accuracy: 0.312500, mean_q: -2874.904785, mean_eps: 0.100000\n",
      " 13759/50000: episode: 1886, duration: 0.014s, episode steps:   3, steps per second: 213, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.333 [0.000, 3.000],  loss: 688457.020833, mae: 2717.772949, accuracy: 0.229167, mean_q: -2868.156657, mean_eps: 0.100000\n",
      " 13762/50000: episode: 1887, duration: 0.018s, episode steps:   3, steps per second: 163, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 607440.250000, mae: 2675.076497, accuracy: 0.239583, mean_q: -2874.271973, mean_eps: 0.100000\n",
      " 13765/50000: episode: 1888, duration: 0.014s, episode steps:   3, steps per second: 208, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 593514.718750, mae: 2674.789469, accuracy: 0.218750, mean_q: -2847.638835, mean_eps: 0.100000\n",
      " 13768/50000: episode: 1889, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 778156.250000, mae: 2682.913574, accuracy: 0.187500, mean_q: -2838.193441, mean_eps: 0.100000\n",
      " 13771/50000: episode: 1890, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 605164.364583, mae: 2689.104899, accuracy: 0.156250, mean_q: -2853.774984, mean_eps: 0.100000\n",
      " 13774/50000: episode: 1891, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 711064.208333, mae: 2693.164876, accuracy: 0.177083, mean_q: -2856.039632, mean_eps: 0.100000\n",
      " 13777/50000: episode: 1892, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 786167.291667, mae: 2712.529622, accuracy: 0.208333, mean_q: -2855.846924, mean_eps: 0.100000\n",
      " 13786/50000: episode: 1893, duration: 0.030s, episode steps:   9, steps per second: 302, episode reward: -6233.000, mean reward: -692.556 [-999.000, -45.000], mean action: 0.889 [0.000, 3.000],  loss: 630764.322917, mae: 2674.337429, accuracy: 0.267361, mean_q: -2843.503445, mean_eps: 0.100000\n",
      " 13789/50000: episode: 1894, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 723083.343750, mae: 2711.728434, accuracy: 0.260417, mean_q: -2845.179525, mean_eps: 0.100000\n",
      " 13792/50000: episode: 1895, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.333 [0.000, 3.000],  loss: 656690.583333, mae: 2714.307536, accuracy: 0.208333, mean_q: -2861.890218, mean_eps: 0.100000\n",
      " 13795/50000: episode: 1896, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 722359.062500, mae: 2717.104167, accuracy: 0.218750, mean_q: -2861.499268, mean_eps: 0.100000\n",
      " 13798/50000: episode: 1897, duration: 0.012s, episode steps:   3, steps per second: 258, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 654187.437500, mae: 2711.110433, accuracy: 0.260417, mean_q: -2858.589518, mean_eps: 0.100000\n",
      " 13802/50000: episode: 1898, duration: 0.016s, episode steps:   4, steps per second: 255, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 526612.062500, mae: 2669.580017, accuracy: 0.265625, mean_q: -2853.766418, mean_eps: 0.100000\n",
      " 13805/50000: episode: 1899, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 733153.500000, mae: 2734.530192, accuracy: 0.250000, mean_q: -2886.740804, mean_eps: 0.100000\n",
      " 13808/50000: episode: 1900, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 740126.000000, mae: 2736.909180, accuracy: 0.270833, mean_q: -2895.626383, mean_eps: 0.100000\n",
      " 13811/50000: episode: 1901, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 626691.427083, mae: 2688.191813, accuracy: 0.260417, mean_q: -2885.135905, mean_eps: 0.100000\n",
      " 13814/50000: episode: 1902, duration: 0.014s, episode steps:   3, steps per second: 214, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 807956.875000, mae: 2731.623047, accuracy: 0.250000, mean_q: -2887.159505, mean_eps: 0.100000\n",
      " 13817/50000: episode: 1903, duration: 0.014s, episode steps:   3, steps per second: 214, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 840216.812500, mae: 2719.660400, accuracy: 0.177083, mean_q: -2890.164795, mean_eps: 0.100000\n",
      " 13820/50000: episode: 1904, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 849979.812500, mae: 2691.706868, accuracy: 0.208333, mean_q: -2803.007975, mean_eps: 0.100000\n",
      " 13823/50000: episode: 1905, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 741251.375000, mae: 2688.618001, accuracy: 0.177083, mean_q: -2843.798991, mean_eps: 0.100000\n",
      " 13826/50000: episode: 1906, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 465228.000000, mae: 2634.585124, accuracy: 0.187500, mean_q: -2830.353027, mean_eps: 0.100000\n",
      " 13830/50000: episode: 1907, duration: 0.015s, episode steps:   4, steps per second: 267, episode reward: -1238.000, mean reward: -309.500 [-999.000, -60.000], mean action: 1.500 [0.000, 3.000],  loss: 863033.671875, mae: 2727.300232, accuracy: 0.210938, mean_q: -2836.821411, mean_eps: 0.100000\n",
      " 13833/50000: episode: 1908, duration: 0.012s, episode steps:   3, steps per second: 255, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 2.000 [1.000, 3.000],  loss: 650118.812500, mae: 2664.975260, accuracy: 0.187500, mean_q: -2810.344727, mean_eps: 0.100000\n",
      " 13836/50000: episode: 1909, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 661675.729167, mae: 2677.025635, accuracy: 0.281250, mean_q: -2806.874349, mean_eps: 0.100000\n",
      " 13839/50000: episode: 1910, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 541980.125000, mae: 2634.533691, accuracy: 0.218750, mean_q: -2817.804850, mean_eps: 0.100000\n",
      " 13842/50000: episode: 1911, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 611995.260417, mae: 2658.026530, accuracy: 0.229167, mean_q: -2841.644531, mean_eps: 0.100000\n",
      " 13845/50000: episode: 1912, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 839773.875000, mae: 2692.688883, accuracy: 0.229167, mean_q: -2810.429850, mean_eps: 0.100000\n",
      " 13848/50000: episode: 1913, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 713874.395833, mae: 2680.085205, accuracy: 0.239583, mean_q: -2846.548177, mean_eps: 0.100000\n",
      " 13851/50000: episode: 1914, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 683261.812500, mae: 2676.527832, accuracy: 0.333333, mean_q: -2809.518799, mean_eps: 0.100000\n",
      " 13854/50000: episode: 1915, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 676656.250000, mae: 2704.768880, accuracy: 0.260417, mean_q: -2844.347738, mean_eps: 0.100000\n",
      " 13857/50000: episode: 1916, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 603318.052083, mae: 2626.262451, accuracy: 0.302083, mean_q: -2798.782145, mean_eps: 0.100000\n",
      " 13860/50000: episode: 1917, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 653051.385417, mae: 2673.210856, accuracy: 0.218750, mean_q: -2812.365234, mean_eps: 0.100000\n",
      " 13863/50000: episode: 1918, duration: 0.013s, episode steps:   3, steps per second: 224, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 538121.093750, mae: 2639.633138, accuracy: 0.250000, mean_q: -2814.476807, mean_eps: 0.100000\n",
      " 13866/50000: episode: 1919, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 575542.000000, mae: 2654.178385, accuracy: 0.250000, mean_q: -2816.717529, mean_eps: 0.100000\n",
      " 13869/50000: episode: 1920, duration: 0.013s, episode steps:   3, steps per second: 230, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 457423.177083, mae: 2666.949951, accuracy: 0.166667, mean_q: -2874.448568, mean_eps: 0.100000\n",
      " 13872/50000: episode: 1921, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 737259.697917, mae: 2715.393880, accuracy: 0.208333, mean_q: -2849.269857, mean_eps: 0.100000\n",
      " 13875/50000: episode: 1922, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 364849.375000, mae: 2662.338623, accuracy: 0.343750, mean_q: -2863.286621, mean_eps: 0.100000\n",
      " 13878/50000: episode: 1923, duration: 0.014s, episode steps:   3, steps per second: 222, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 806363.833333, mae: 2716.685710, accuracy: 0.322917, mean_q: -2850.182129, mean_eps: 0.100000\n",
      " 13881/50000: episode: 1924, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 454006.770833, mae: 2678.493490, accuracy: 0.250000, mean_q: -2887.101562, mean_eps: 0.100000\n",
      " 13885/50000: episode: 1925, duration: 0.016s, episode steps:   4, steps per second: 250, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.750 [1.000, 3.000],  loss: 738232.203125, mae: 2744.099548, accuracy: 0.164062, mean_q: -2893.593384, mean_eps: 0.100000\n",
      " 13888/50000: episode: 1926, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 768560.395833, mae: 2717.477376, accuracy: 0.187500, mean_q: -2857.004639, mean_eps: 0.100000\n",
      " 13891/50000: episode: 1927, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 663207.666667, mae: 2709.151042, accuracy: 0.135417, mean_q: -2864.420329, mean_eps: 0.100000\n",
      " 13894/50000: episode: 1928, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 633762.270833, mae: 2681.295247, accuracy: 0.260417, mean_q: -2830.398600, mean_eps: 0.100000\n",
      " 13897/50000: episode: 1929, duration: 0.012s, episode steps:   3, steps per second: 256, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 909509.437500, mae: 2746.224772, accuracy: 0.145833, mean_q: -2840.759521, mean_eps: 0.100000\n",
      " 13900/50000: episode: 1930, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 452994.218750, mae: 2650.725505, accuracy: 0.114583, mean_q: -2823.351888, mean_eps: 0.100000\n",
      " 13903/50000: episode: 1931, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 621890.031250, mae: 2663.694010, accuracy: 0.135417, mean_q: -2818.987386, mean_eps: 0.100000\n",
      " 13906/50000: episode: 1932, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 744033.479167, mae: 2670.500163, accuracy: 0.135417, mean_q: -2787.577067, mean_eps: 0.100000\n",
      " 13909/50000: episode: 1933, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 509445.520833, mae: 2632.422526, accuracy: 0.250000, mean_q: -2797.640869, mean_eps: 0.100000\n",
      " 13912/50000: episode: 1934, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 756118.229167, mae: 2673.006999, accuracy: 0.229167, mean_q: -2798.646403, mean_eps: 0.100000\n",
      " 13915/50000: episode: 1935, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 472677.677083, mae: 2636.249349, accuracy: 0.177083, mean_q: -2777.843424, mean_eps: 0.100000\n",
      " 13918/50000: episode: 1936, duration: 0.014s, episode steps:   3, steps per second: 208, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 705572.833333, mae: 2672.050944, accuracy: 0.156250, mean_q: -2786.238932, mean_eps: 0.100000\n",
      " 13921/50000: episode: 1937, duration: 0.014s, episode steps:   3, steps per second: 209, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 709529.750000, mae: 2636.443848, accuracy: 0.197917, mean_q: -2761.291097, mean_eps: 0.100000\n",
      " 13924/50000: episode: 1938, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 688616.666667, mae: 2676.920817, accuracy: 0.166667, mean_q: -2817.639730, mean_eps: 0.100000\n",
      " 13927/50000: episode: 1939, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 658472.625000, mae: 2668.780436, accuracy: 0.187500, mean_q: -2816.874512, mean_eps: 0.100000\n",
      " 13930/50000: episode: 1940, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 498092.208333, mae: 2654.162760, accuracy: 0.166667, mean_q: -2817.454020, mean_eps: 0.100000\n",
      " 13933/50000: episode: 1941, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 515821.687500, mae: 2625.746745, accuracy: 0.156250, mean_q: -2824.895426, mean_eps: 0.100000\n",
      " 13936/50000: episode: 1942, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 606202.734375, mae: 2668.089681, accuracy: 0.177083, mean_q: -2851.269531, mean_eps: 0.100000\n",
      " 13939/50000: episode: 1943, duration: 0.011s, episode steps:   3, steps per second: 263, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 604459.260417, mae: 2665.738200, accuracy: 0.239583, mean_q: -2834.516113, mean_eps: 0.100000\n",
      " 13942/50000: episode: 1944, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 574559.354167, mae: 2646.966797, accuracy: 0.197917, mean_q: -2829.894450, mean_eps: 0.100000\n",
      " 13945/50000: episode: 1945, duration: 0.012s, episode steps:   3, steps per second: 254, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 706822.708333, mae: 2682.299316, accuracy: 0.166667, mean_q: -2839.059977, mean_eps: 0.100000\n",
      " 13949/50000: episode: 1946, duration: 0.015s, episode steps:   4, steps per second: 264, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 2.250 [1.000, 3.000],  loss: 726986.671875, mae: 2692.814941, accuracy: 0.195312, mean_q: -2822.207581, mean_eps: 0.100000\n",
      " 13952/50000: episode: 1947, duration: 0.012s, episode steps:   3, steps per second: 257, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 730088.458333, mae: 2673.071370, accuracy: 0.156250, mean_q: -2814.577393, mean_eps: 0.100000\n",
      " 13955/50000: episode: 1948, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 651122.031250, mae: 2697.643636, accuracy: 0.156250, mean_q: -2842.799886, mean_eps: 0.100000\n",
      " 13958/50000: episode: 1949, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 759385.250000, mae: 2650.183757, accuracy: 0.177083, mean_q: -2776.736084, mean_eps: 0.100000\n",
      " 13961/50000: episode: 1950, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 540206.942708, mae: 2623.221191, accuracy: 0.260417, mean_q: -2772.590739, mean_eps: 0.100000\n",
      " 13965/50000: episode: 1951, duration: 0.021s, episode steps:   4, steps per second: 188, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 504887.644531, mae: 2624.850281, accuracy: 0.226562, mean_q: -2790.170898, mean_eps: 0.100000\n",
      " 13968/50000: episode: 1952, duration: 0.014s, episode steps:   3, steps per second: 207, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 582903.437500, mae: 2644.430908, accuracy: 0.197917, mean_q: -2792.110677, mean_eps: 0.100000\n",
      " 13971/50000: episode: 1953, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 659754.375000, mae: 2683.830892, accuracy: 0.187500, mean_q: -2816.390869, mean_eps: 0.100000\n",
      " 13974/50000: episode: 1954, duration: 0.012s, episode steps:   3, steps per second: 254, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 610802.312500, mae: 2667.425212, accuracy: 0.197917, mean_q: -2830.997803, mean_eps: 0.100000\n",
      " 13978/50000: episode: 1955, duration: 0.015s, episode steps:   4, steps per second: 268, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.750 [1.000, 3.000],  loss: 734526.843750, mae: 2683.501343, accuracy: 0.195312, mean_q: -2817.178345, mean_eps: 0.100000\n",
      " 13981/50000: episode: 1956, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 556175.479167, mae: 2643.369222, accuracy: 0.291667, mean_q: -2799.146403, mean_eps: 0.100000\n",
      " 13984/50000: episode: 1957, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 771511.750000, mae: 2691.129720, accuracy: 0.187500, mean_q: -2839.277995, mean_eps: 0.100000\n",
      " 13987/50000: episode: 1958, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 584488.458333, mae: 2623.897949, accuracy: 0.187500, mean_q: -2781.480143, mean_eps: 0.100000\n",
      " 13990/50000: episode: 1959, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 665283.656250, mae: 2660.325521, accuracy: 0.270833, mean_q: -2773.984294, mean_eps: 0.100000\n",
      " 13995/50000: episode: 1960, duration: 0.018s, episode steps:   5, steps per second: 271, episode reward: -2193.000, mean reward: -438.600 [-999.000, -45.000], mean action: 1.800 [0.000, 3.000],  loss: 757247.337500, mae: 2642.997998, accuracy: 0.206250, mean_q: -2807.870361, mean_eps: 0.100000\n",
      " 13998/50000: episode: 1961, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 774571.104167, mae: 2636.064372, accuracy: 0.218750, mean_q: -2762.435303, mean_eps: 0.100000\n",
      " 14002/50000: episode: 1962, duration: 0.016s, episode steps:   4, steps per second: 257, episode reward: -1223.000, mean reward: -305.750 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 575813.265625, mae: 2638.186157, accuracy: 0.187500, mean_q: -2775.681580, mean_eps: 0.100000\n",
      " 14005/50000: episode: 1963, duration: 0.012s, episode steps:   3, steps per second: 254, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 722064.458333, mae: 2634.750000, accuracy: 0.239583, mean_q: -2723.172526, mean_eps: 0.100000\n",
      " 14008/50000: episode: 1964, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 704650.000000, mae: 2644.678141, accuracy: 0.197917, mean_q: -2777.933187, mean_eps: 0.100000\n",
      " 14011/50000: episode: 1965, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 567329.062500, mae: 2596.152425, accuracy: 0.187500, mean_q: -2743.195150, mean_eps: 0.100000\n",
      " 14014/50000: episode: 1966, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 667446.562500, mae: 2650.038005, accuracy: 0.270833, mean_q: -2773.346436, mean_eps: 0.100000\n",
      " 14017/50000: episode: 1967, duration: 0.014s, episode steps:   3, steps per second: 208, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 877377.479167, mae: 2679.929199, accuracy: 0.270833, mean_q: -2742.358236, mean_eps: 0.100000\n",
      " 14020/50000: episode: 1968, duration: 0.014s, episode steps:   3, steps per second: 209, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 803353.781250, mae: 2667.531006, accuracy: 0.218750, mean_q: -2728.580729, mean_eps: 0.100000\n",
      " 14024/50000: episode: 1969, duration: 0.016s, episode steps:   4, steps per second: 245, episode reward: -1238.000, mean reward: -309.500 [-999.000, -60.000], mean action: 2.250 [1.000, 3.000],  loss: 614688.437500, mae: 2608.308777, accuracy: 0.250000, mean_q: -2731.892334, mean_eps: 0.100000\n",
      " 14027/50000: episode: 1970, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 562587.697917, mae: 2576.402507, accuracy: 0.218750, mean_q: -2716.592367, mean_eps: 0.100000\n",
      " 14030/50000: episode: 1971, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 521219.833333, mae: 2574.002197, accuracy: 0.229167, mean_q: -2701.874268, mean_eps: 0.100000\n",
      " 14033/50000: episode: 1972, duration: 0.012s, episode steps:   3, steps per second: 256, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 573199.531250, mae: 2592.621257, accuracy: 0.239583, mean_q: -2742.996501, mean_eps: 0.100000\n",
      " 14037/50000: episode: 1973, duration: 0.015s, episode steps:   4, steps per second: 264, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 684847.281250, mae: 2615.694885, accuracy: 0.234375, mean_q: -2731.759155, mean_eps: 0.100000\n",
      " 14040/50000: episode: 1974, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 612568.437500, mae: 2595.000407, accuracy: 0.125000, mean_q: -2739.181396, mean_eps: 0.100000\n",
      " 14044/50000: episode: 1975, duration: 0.015s, episode steps:   4, steps per second: 262, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 724863.937500, mae: 2657.898926, accuracy: 0.242188, mean_q: -2761.274170, mean_eps: 0.100000\n",
      " 14047/50000: episode: 1976, duration: 0.012s, episode steps:   3, steps per second: 259, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 678386.052083, mae: 2605.220540, accuracy: 0.229167, mean_q: -2736.490397, mean_eps: 0.100000\n",
      " 14050/50000: episode: 1977, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 579340.958333, mae: 2610.608561, accuracy: 0.229167, mean_q: -2744.800130, mean_eps: 0.100000\n",
      " 14053/50000: episode: 1978, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 694688.000000, mae: 2642.841634, accuracy: 0.229167, mean_q: -2743.641357, mean_eps: 0.100000\n",
      " 14056/50000: episode: 1979, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 766458.177083, mae: 2616.304362, accuracy: 0.229167, mean_q: -2732.800781, mean_eps: 0.100000\n",
      " 14059/50000: episode: 1980, duration: 0.012s, episode steps:   3, steps per second: 256, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 618693.458333, mae: 2641.512695, accuracy: 0.177083, mean_q: -2755.327881, mean_eps: 0.100000\n",
      " 14062/50000: episode: 1981, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 694193.500000, mae: 2629.480713, accuracy: 0.333333, mean_q: -2744.204753, mean_eps: 0.100000\n",
      " 14065/50000: episode: 1982, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 605836.208333, mae: 2583.386882, accuracy: 0.229167, mean_q: -2722.445068, mean_eps: 0.100000\n",
      " 14068/50000: episode: 1983, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 607513.187500, mae: 2585.560791, accuracy: 0.354167, mean_q: -2695.167074, mean_eps: 0.100000\n",
      " 14071/50000: episode: 1984, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 542122.604167, mae: 2589.294271, accuracy: 0.260417, mean_q: -2705.451497, mean_eps: 0.100000\n",
      " 14075/50000: episode: 1985, duration: 0.018s, episode steps:   4, steps per second: 217, episode reward: -1223.000, mean reward: -305.750 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 702369.656250, mae: 2632.825989, accuracy: 0.304688, mean_q: -2708.581970, mean_eps: 0.100000\n",
      " 14078/50000: episode: 1986, duration: 0.015s, episode steps:   3, steps per second: 206, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 569156.708333, mae: 2599.688558, accuracy: 0.375000, mean_q: -2714.279378, mean_eps: 0.100000\n",
      " 14081/50000: episode: 1987, duration: 0.013s, episode steps:   3, steps per second: 225, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 535259.979167, mae: 2601.757894, accuracy: 0.322917, mean_q: -2725.815430, mean_eps: 0.100000\n",
      " 14093/50000: episode: 1988, duration: 0.039s, episode steps:  12, steps per second: 308, episode reward: -9215.000, mean reward: -767.917 [-999.000, -32.000], mean action: 0.333 [0.000, 3.000],  loss: 609826.164062, mae: 2596.751180, accuracy: 0.322917, mean_q: -2711.295736, mean_eps: 0.100000\n",
      " 14096/50000: episode: 1989, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 858703.770833, mae: 2618.028727, accuracy: 0.312500, mean_q: -2679.420573, mean_eps: 0.100000\n",
      " 14099/50000: episode: 1990, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 628429.458333, mae: 2603.954997, accuracy: 0.312500, mean_q: -2718.871826, mean_eps: 0.100000\n",
      " 14110/50000: episode: 1991, duration: 0.036s, episode steps:  11, steps per second: 305, episode reward: -8187.000, mean reward: -744.273 [-999.000, -32.000], mean action: 0.545 [0.000, 3.000],  loss: 551983.625000, mae: 2572.224432, accuracy: 0.301136, mean_q: -2722.513406, mean_eps: 0.100000\n",
      " 14113/50000: episode: 1992, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 531136.270833, mae: 2514.246745, accuracy: 0.281250, mean_q: -2639.779134, mean_eps: 0.100000\n",
      " 14116/50000: episode: 1993, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 673070.093750, mae: 2580.636068, accuracy: 0.208333, mean_q: -2669.986247, mean_eps: 0.100000\n",
      " 14122/50000: episode: 1994, duration: 0.022s, episode steps:   6, steps per second: 274, episode reward: -3221.000, mean reward: -536.833 [-999.000, -32.000], mean action: 1.000 [0.000, 3.000],  loss: 583336.286458, mae: 2560.606852, accuracy: 0.229167, mean_q: -2681.044067, mean_eps: 0.100000\n",
      " 14126/50000: episode: 1995, duration: 0.017s, episode steps:   4, steps per second: 241, episode reward: -1223.000, mean reward: -305.750 [-999.000, -58.000], mean action: 1.750 [0.000, 3.000],  loss: 671790.828125, mae: 2575.477051, accuracy: 0.250000, mean_q: -2658.608582, mean_eps: 0.100000\n",
      " 14129/50000: episode: 1996, duration: 0.014s, episode steps:   3, steps per second: 214, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 615386.000000, mae: 2547.345378, accuracy: 0.229167, mean_q: -2665.523519, mean_eps: 0.100000\n",
      " 14132/50000: episode: 1997, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 415746.312500, mae: 2520.177165, accuracy: 0.281250, mean_q: -2643.419759, mean_eps: 0.100000\n",
      " 14135/50000: episode: 1998, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 359958.093750, mae: 2496.510661, accuracy: 0.312500, mean_q: -2647.340983, mean_eps: 0.100000\n",
      " 14138/50000: episode: 1999, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 536287.156250, mae: 2548.019206, accuracy: 0.260417, mean_q: -2687.721191, mean_eps: 0.100000\n",
      " 14141/50000: episode: 2000, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 689866.812500, mae: 2603.357422, accuracy: 0.250000, mean_q: -2685.329183, mean_eps: 0.100000\n",
      " 14144/50000: episode: 2001, duration: 0.012s, episode steps:   3, steps per second: 257, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 761244.927083, mae: 2624.347656, accuracy: 0.281250, mean_q: -2723.697754, mean_eps: 0.100000\n",
      " 14147/50000: episode: 2002, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 628235.437500, mae: 2579.545166, accuracy: 0.166667, mean_q: -2668.551514, mean_eps: 0.100000\n",
      " 14150/50000: episode: 2003, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 609023.875000, mae: 2584.429850, accuracy: 0.250000, mean_q: -2717.629150, mean_eps: 0.100000\n",
      " 14153/50000: episode: 2004, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 737648.687500, mae: 2598.984619, accuracy: 0.281250, mean_q: -2704.879720, mean_eps: 0.100000\n",
      " 14156/50000: episode: 2005, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 660263.447917, mae: 2598.392985, accuracy: 0.229167, mean_q: -2708.925049, mean_eps: 0.100000\n",
      " 14159/50000: episode: 2006, duration: 0.012s, episode steps:   3, steps per second: 256, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 722805.697917, mae: 2606.648438, accuracy: 0.250000, mean_q: -2685.169596, mean_eps: 0.100000\n",
      " 14164/50000: episode: 2007, duration: 0.018s, episode steps:   5, steps per second: 280, episode reward: -2222.000, mean reward: -444.400 [-999.000, -58.000], mean action: 2.000 [0.000, 3.000],  loss: 489052.768750, mae: 2533.704248, accuracy: 0.212500, mean_q: -2675.070508, mean_eps: 0.100000\n",
      " 14167/50000: episode: 2008, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 671521.989583, mae: 2575.906413, accuracy: 0.239583, mean_q: -2672.308757, mean_eps: 0.100000\n",
      " 14170/50000: episode: 2009, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 574038.114583, mae: 2577.725911, accuracy: 0.218750, mean_q: -2681.931559, mean_eps: 0.100000\n",
      " 14174/50000: episode: 2010, duration: 0.015s, episode steps:   4, steps per second: 258, episode reward: -1223.000, mean reward: -305.750 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 619454.789062, mae: 2567.823547, accuracy: 0.210938, mean_q: -2680.539978, mean_eps: 0.100000\n",
      " 14177/50000: episode: 2011, duration: 0.014s, episode steps:   3, steps per second: 216, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 446063.770833, mae: 2560.205404, accuracy: 0.218750, mean_q: -2683.366130, mean_eps: 0.100000\n",
      " 14180/50000: episode: 2012, duration: 0.018s, episode steps:   3, steps per second: 171, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 625475.958333, mae: 2596.721517, accuracy: 0.145833, mean_q: -2686.109131, mean_eps: 0.100000\n",
      " 14183/50000: episode: 2013, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 514886.572917, mae: 2577.974935, accuracy: 0.135417, mean_q: -2706.323893, mean_eps: 0.100000\n",
      " 14196/50000: episode: 2014, duration: 0.044s, episode steps:  13, steps per second: 295, episode reward: -10214.000, mean reward: -785.692 [-999.000, -58.000], mean action: 1.846 [0.000, 3.000],  loss: 572067.427885, mae: 2573.939153, accuracy: 0.156250, mean_q: -2703.967811, mean_eps: 0.100000\n",
      " 14199/50000: episode: 2015, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 773486.145833, mae: 2590.868815, accuracy: 0.218750, mean_q: -2696.959880, mean_eps: 0.100000\n",
      " 14203/50000: episode: 2016, duration: 0.015s, episode steps:   4, steps per second: 270, episode reward: -1223.000, mean reward: -305.750 [-999.000, -58.000], mean action: 1.250 [0.000, 3.000],  loss: 649187.976562, mae: 2578.663208, accuracy: 0.210938, mean_q: -2658.842163, mean_eps: 0.100000\n",
      " 14206/50000: episode: 2017, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 546282.208333, mae: 2536.069173, accuracy: 0.250000, mean_q: -2650.683838, mean_eps: 0.100000\n",
      " 14209/50000: episode: 2018, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 650968.104167, mae: 2567.660807, accuracy: 0.197917, mean_q: -2665.262939, mean_eps: 0.100000\n",
      " 14212/50000: episode: 2019, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 561952.375000, mae: 2544.182210, accuracy: 0.166667, mean_q: -2637.313477, mean_eps: 0.100000\n",
      " 14216/50000: episode: 2020, duration: 0.015s, episode steps:   4, steps per second: 263, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 2.000 [0.000, 3.000],  loss: 575975.250000, mae: 2541.082092, accuracy: 0.203125, mean_q: -2645.239502, mean_eps: 0.100000\n",
      " 14219/50000: episode: 2021, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 546795.572917, mae: 2510.566081, accuracy: 0.166667, mean_q: -2637.089111, mean_eps: 0.100000\n",
      " 14222/50000: episode: 2022, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 684113.020833, mae: 2561.048340, accuracy: 0.218750, mean_q: -2633.704264, mean_eps: 0.100000\n",
      " 14225/50000: episode: 2023, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 641056.770833, mae: 2547.551514, accuracy: 0.177083, mean_q: -2636.484294, mean_eps: 0.100000\n",
      " 14229/50000: episode: 2024, duration: 0.016s, episode steps:   4, steps per second: 244, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 482977.882812, mae: 2503.889404, accuracy: 0.250000, mean_q: -2620.092407, mean_eps: 0.100000\n",
      " 14233/50000: episode: 2025, duration: 0.018s, episode steps:   4, steps per second: 228, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 497657.226562, mae: 2486.904968, accuracy: 0.242188, mean_q: -2615.087463, mean_eps: 0.100000\n",
      " 14236/50000: episode: 2026, duration: 0.014s, episode steps:   3, steps per second: 211, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 562372.125000, mae: 2556.426595, accuracy: 0.177083, mean_q: -2644.185628, mean_eps: 0.100000\n",
      " 14239/50000: episode: 2027, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 721135.645833, mae: 2597.405111, accuracy: 0.218750, mean_q: -2665.057780, mean_eps: 0.100000\n",
      " 14242/50000: episode: 2028, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 708997.645833, mae: 2575.269938, accuracy: 0.177083, mean_q: -2651.563151, mean_eps: 0.100000\n",
      " 14245/50000: episode: 2029, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 539885.927083, mae: 2504.942057, accuracy: 0.208333, mean_q: -2613.387614, mean_eps: 0.100000\n",
      " 14248/50000: episode: 2030, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 526446.489583, mae: 2515.327637, accuracy: 0.197917, mean_q: -2635.486328, mean_eps: 0.100000\n",
      " 14251/50000: episode: 2031, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -224.000, mean reward: -74.667 [-116.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 597927.052083, mae: 2539.028320, accuracy: 0.156250, mean_q: -2625.254801, mean_eps: 0.100000\n",
      " 14254/50000: episode: 2032, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 598909.895833, mae: 2546.262777, accuracy: 0.177083, mean_q: -2633.658773, mean_eps: 0.100000\n",
      " 14257/50000: episode: 2033, duration: 0.012s, episode steps:   3, steps per second: 253, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 391017.500000, mae: 2493.175537, accuracy: 0.145833, mean_q: -2634.556641, mean_eps: 0.100000\n",
      " 14260/50000: episode: 2034, duration: 0.012s, episode steps:   3, steps per second: 255, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 613067.281250, mae: 2517.597819, accuracy: 0.135417, mean_q: -2619.109212, mean_eps: 0.100000\n",
      " 14264/50000: episode: 2035, duration: 0.015s, episode steps:   4, steps per second: 265, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 426880.671875, mae: 2507.343262, accuracy: 0.179688, mean_q: -2631.677917, mean_eps: 0.100000\n",
      " 14267/50000: episode: 2036, duration: 0.012s, episode steps:   3, steps per second: 253, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 601190.447917, mae: 2531.435059, accuracy: 0.218750, mean_q: -2623.573730, mean_eps: 0.100000\n",
      " 14271/50000: episode: 2037, duration: 0.015s, episode steps:   4, steps per second: 270, episode reward: -1223.000, mean reward: -305.750 [-999.000, -32.000], mean action: 1.750 [0.000, 3.000],  loss: 537030.273438, mae: 2524.864380, accuracy: 0.203125, mean_q: -2627.301331, mean_eps: 0.100000\n",
      " 14274/50000: episode: 2038, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 723887.062500, mae: 2561.697510, accuracy: 0.156250, mean_q: -2643.099284, mean_eps: 0.100000\n",
      " 14277/50000: episode: 2039, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 431721.713542, mae: 2535.768392, accuracy: 0.197917, mean_q: -2646.775146, mean_eps: 0.100000\n",
      " 14281/50000: episode: 2040, duration: 0.016s, episode steps:   4, steps per second: 255, episode reward: -1223.000, mean reward: -305.750 [-999.000, -32.000], mean action: 1.000 [0.000, 3.000],  loss: 503844.039062, mae: 2537.899597, accuracy: 0.171875, mean_q: -2667.739075, mean_eps: 0.100000\n",
      " 14284/50000: episode: 2041, duration: 0.014s, episode steps:   3, steps per second: 208, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 671898.041667, mae: 2574.532715, accuracy: 0.156250, mean_q: -2661.190267, mean_eps: 0.100000\n",
      " 14287/50000: episode: 2042, duration: 0.014s, episode steps:   3, steps per second: 220, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 636071.364583, mae: 2614.077311, accuracy: 0.166667, mean_q: -2692.621419, mean_eps: 0.100000\n",
      " 14290/50000: episode: 2043, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 524625.416667, mae: 2527.503174, accuracy: 0.156250, mean_q: -2659.437174, mean_eps: 0.100000\n",
      " 14294/50000: episode: 2044, duration: 0.015s, episode steps:   4, steps per second: 263, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 613035.578125, mae: 2515.007568, accuracy: 0.156250, mean_q: -2635.947571, mean_eps: 0.100000\n",
      " 14297/50000: episode: 2045, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 583290.687500, mae: 2502.608643, accuracy: 0.197917, mean_q: -2600.815186, mean_eps: 0.100000\n",
      " 14301/50000: episode: 2046, duration: 0.015s, episode steps:   4, steps per second: 269, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 482322.609375, mae: 2501.164795, accuracy: 0.187500, mean_q: -2606.998352, mean_eps: 0.100000\n",
      " 14304/50000: episode: 2047, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 472305.031250, mae: 2504.420817, accuracy: 0.197917, mean_q: -2613.393799, mean_eps: 0.100000\n",
      " 14307/50000: episode: 2048, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 667565.447917, mae: 2530.132894, accuracy: 0.239583, mean_q: -2611.217122, mean_eps: 0.100000\n",
      " 14310/50000: episode: 2049, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 648143.020833, mae: 2520.088704, accuracy: 0.218750, mean_q: -2590.061768, mean_eps: 0.100000\n",
      " 14313/50000: episode: 2050, duration: 0.012s, episode steps:   3, steps per second: 255, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 679426.322917, mae: 2525.132243, accuracy: 0.250000, mean_q: -2589.084391, mean_eps: 0.100000\n",
      " 14317/50000: episode: 2051, duration: 0.015s, episode steps:   4, steps per second: 271, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 640280.390625, mae: 2526.923218, accuracy: 0.242188, mean_q: -2592.570679, mean_eps: 0.100000\n",
      " 14320/50000: episode: 2052, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 620830.854167, mae: 2488.489665, accuracy: 0.260417, mean_q: -2564.536540, mean_eps: 0.100000\n",
      " 14323/50000: episode: 2053, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 610275.854167, mae: 2492.552409, accuracy: 0.177083, mean_q: -2566.381592, mean_eps: 0.100000\n",
      " 14326/50000: episode: 2054, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 579736.812500, mae: 2484.792074, accuracy: 0.239583, mean_q: -2572.932617, mean_eps: 0.100000\n",
      " 14329/50000: episode: 2055, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 562303.375000, mae: 2462.387695, accuracy: 0.229167, mean_q: -2528.348796, mean_eps: 0.100000\n",
      " 14332/50000: episode: 2056, duration: 0.015s, episode steps:   3, steps per second: 203, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 640728.375000, mae: 2456.149333, accuracy: 0.239583, mean_q: -2542.202637, mean_eps: 0.100000\n",
      " 14335/50000: episode: 2057, duration: 0.014s, episode steps:   3, steps per second: 216, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 604069.937500, mae: 2488.236165, accuracy: 0.177083, mean_q: -2579.567139, mean_eps: 0.100000\n",
      " 14339/50000: episode: 2058, duration: 0.015s, episode steps:   4, steps per second: 264, episode reward: -1223.000, mean reward: -305.750 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 622146.671875, mae: 2472.225403, accuracy: 0.203125, mean_q: -2533.040894, mean_eps: 0.100000\n",
      " 14343/50000: episode: 2059, duration: 0.016s, episode steps:   4, steps per second: 257, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 518801.179688, mae: 2458.805542, accuracy: 0.250000, mean_q: -2545.780273, mean_eps: 0.100000\n",
      " 14370/50000: episode: 2060, duration: 0.085s, episode steps:  27, steps per second: 317, episode reward: -24200.000, mean reward: -896.296 [-999.000, -58.000], mean action: 1.926 [0.000, 3.000],  loss: 605489.907407, mae: 2486.172101, accuracy: 0.181713, mean_q: -2549.489167, mean_eps: 0.100000\n",
      " 14373/50000: episode: 2061, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 748638.375000, mae: 2498.171794, accuracy: 0.135417, mean_q: -2521.983887, mean_eps: 0.100000\n",
      " 14376/50000: episode: 2062, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.667 [0.000, 3.000],  loss: 553952.416667, mae: 2474.110840, accuracy: 0.156250, mean_q: -2541.940430, mean_eps: 0.100000\n",
      " 14379/50000: episode: 2063, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 549903.968750, mae: 2443.180664, accuracy: 0.218750, mean_q: -2522.520589, mean_eps: 0.100000\n",
      " 14382/50000: episode: 2064, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 472617.625000, mae: 2434.116130, accuracy: 0.145833, mean_q: -2518.166097, mean_eps: 0.100000\n",
      " 14386/50000: episode: 2065, duration: 0.019s, episode steps:   4, steps per second: 206, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 672368.671875, mae: 2471.247803, accuracy: 0.140625, mean_q: -2516.303284, mean_eps: 0.100000\n",
      " 14389/50000: episode: 2066, duration: 0.016s, episode steps:   3, steps per second: 188, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 534789.187500, mae: 2444.565592, accuracy: 0.125000, mean_q: -2515.715902, mean_eps: 0.100000\n",
      " 14392/50000: episode: 2067, duration: 0.014s, episode steps:   3, steps per second: 216, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 485621.760417, mae: 2417.367350, accuracy: 0.104167, mean_q: -2538.710531, mean_eps: 0.100000\n",
      " 14396/50000: episode: 2068, duration: 0.015s, episode steps:   4, steps per second: 259, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 554182.257812, mae: 2433.666748, accuracy: 0.257812, mean_q: -2489.638428, mean_eps: 0.100000\n",
      " 14399/50000: episode: 2069, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 611909.687500, mae: 2443.427979, accuracy: 0.145833, mean_q: -2529.919108, mean_eps: 0.100000\n",
      " 14402/50000: episode: 2070, duration: 0.014s, episode steps:   3, steps per second: 218, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 607319.312500, mae: 2465.871012, accuracy: 0.135417, mean_q: -2518.340169, mean_eps: 0.100000\n",
      " 14408/50000: episode: 2071, duration: 0.021s, episode steps:   6, steps per second: 288, episode reward: -3192.000, mean reward: -532.000 [-999.000, -58.000], mean action: 1.667 [0.000, 3.000],  loss: 528128.656250, mae: 2481.969645, accuracy: 0.151042, mean_q: -2545.730998, mean_eps: 0.100000\n",
      " 14411/50000: episode: 2072, duration: 0.012s, episode steps:   3, steps per second: 253, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 607214.343750, mae: 2464.728760, accuracy: 0.187500, mean_q: -2519.474609, mean_eps: 0.100000\n",
      " 14415/50000: episode: 2073, duration: 0.015s, episode steps:   4, steps per second: 273, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 313402.210938, mae: 2398.371399, accuracy: 0.218750, mean_q: -2529.054810, mean_eps: 0.100000\n",
      " 14418/50000: episode: 2074, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 508314.531250, mae: 2449.576986, accuracy: 0.166667, mean_q: -2537.803223, mean_eps: 0.100000\n",
      " 14425/50000: episode: 2075, duration: 0.024s, episode steps:   7, steps per second: 292, episode reward: -4220.000, mean reward: -602.857 [-999.000, -58.000], mean action: 1.714 [0.000, 3.000],  loss: 574933.558036, mae: 2453.646484, accuracy: 0.129464, mean_q: -2542.463309, mean_eps: 0.100000\n",
      " 14429/50000: episode: 2076, duration: 0.015s, episode steps:   4, steps per second: 263, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 550058.480469, mae: 2469.172180, accuracy: 0.179688, mean_q: -2525.152832, mean_eps: 0.100000\n",
      " 14433/50000: episode: 2077, duration: 0.015s, episode steps:   4, steps per second: 264, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 466054.070312, mae: 2445.550171, accuracy: 0.187500, mean_q: -2508.847351, mean_eps: 0.100000\n",
      " 14437/50000: episode: 2078, duration: 0.016s, episode steps:   4, steps per second: 247, episode reward: -1223.000, mean reward: -305.750 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 584129.734375, mae: 2468.809021, accuracy: 0.195312, mean_q: -2523.789795, mean_eps: 0.100000\n",
      " 14440/50000: episode: 2079, duration: 0.014s, episode steps:   3, steps per second: 214, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 617144.635417, mae: 2479.414307, accuracy: 0.197917, mean_q: -2516.119466, mean_eps: 0.100000\n",
      " 14444/50000: episode: 2080, duration: 0.017s, episode steps:   4, steps per second: 236, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 2.250 [1.000, 3.000],  loss: 560670.250000, mae: 2438.542175, accuracy: 0.164062, mean_q: -2503.081177, mean_eps: 0.100000\n",
      " 14447/50000: episode: 2081, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 787319.562500, mae: 2491.973796, accuracy: 0.145833, mean_q: -2499.009766, mean_eps: 0.100000\n",
      " 14450/50000: episode: 2082, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 542716.843750, mae: 2434.124756, accuracy: 0.218750, mean_q: -2495.284831, mean_eps: 0.100000\n",
      " 14453/50000: episode: 2083, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 543126.713542, mae: 2428.116211, accuracy: 0.260417, mean_q: -2502.450928, mean_eps: 0.100000\n",
      " 14456/50000: episode: 2084, duration: 0.012s, episode steps:   3, steps per second: 257, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 455491.000000, mae: 2392.277100, accuracy: 0.229167, mean_q: -2464.630859, mean_eps: 0.100000\n",
      " 14459/50000: episode: 2085, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 638287.458333, mae: 2431.244710, accuracy: 0.218750, mean_q: -2451.582601, mean_eps: 0.100000\n",
      " 14462/50000: episode: 2086, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 531736.760417, mae: 2448.787598, accuracy: 0.156250, mean_q: -2533.836019, mean_eps: 0.100000\n",
      " 14465/50000: episode: 2087, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 496447.739583, mae: 2434.351807, accuracy: 0.177083, mean_q: -2507.011719, mean_eps: 0.100000\n",
      " 14468/50000: episode: 2088, duration: 0.012s, episode steps:   3, steps per second: 257, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 574739.291667, mae: 2442.812337, accuracy: 0.270833, mean_q: -2474.426595, mean_eps: 0.100000\n",
      " 14472/50000: episode: 2089, duration: 0.015s, episode steps:   4, steps per second: 269, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 424460.335938, mae: 2427.677856, accuracy: 0.210938, mean_q: -2515.037415, mean_eps: 0.100000\n",
      " 14475/50000: episode: 2090, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 661615.708333, mae: 2474.226318, accuracy: 0.208333, mean_q: -2518.734945, mean_eps: 0.100000\n",
      " 14478/50000: episode: 2091, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 577431.083333, mae: 2440.893717, accuracy: 0.291667, mean_q: -2515.260661, mean_eps: 0.100000\n",
      " 14481/50000: episode: 2092, duration: 0.012s, episode steps:   3, steps per second: 255, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 420340.666667, mae: 2374.302734, accuracy: 0.239583, mean_q: -2463.171956, mean_eps: 0.100000\n",
      " 14484/50000: episode: 2093, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 556284.458333, mae: 2415.882568, accuracy: 0.250000, mean_q: -2477.824870, mean_eps: 0.100000\n",
      " 14488/50000: episode: 2094, duration: 0.018s, episode steps:   4, steps per second: 225, episode reward: -1223.000, mean reward: -305.750 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 729768.687500, mae: 2473.097961, accuracy: 0.242188, mean_q: -2477.733765, mean_eps: 0.100000\n",
      " 14492/50000: episode: 2095, duration: 0.017s, episode steps:   4, steps per second: 233, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 530078.507812, mae: 2417.385559, accuracy: 0.289062, mean_q: -2484.396912, mean_eps: 0.100000\n",
      " 14495/50000: episode: 2096, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 748538.177083, mae: 2443.271403, accuracy: 0.229167, mean_q: -2440.090007, mean_eps: 0.100000\n",
      " 14498/50000: episode: 2097, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 408629.156250, mae: 2349.010579, accuracy: 0.260417, mean_q: -2426.919678, mean_eps: 0.100000\n",
      " 14501/50000: episode: 2098, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 445029.562500, mae: 2373.061198, accuracy: 0.239583, mean_q: -2443.927246, mean_eps: 0.100000\n",
      " 14523/50000: episode: 2099, duration: 0.069s, episode steps:  22, steps per second: 320, episode reward: -19205.000, mean reward: -872.955 [-999.000, -32.000], mean action: 0.318 [0.000, 3.000],  loss: 523108.423295, mae: 2396.931319, accuracy: 0.333807, mean_q: -2451.705023, mean_eps: 0.100000\n",
      " 14526/50000: episode: 2100, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 764938.375000, mae: 2465.772624, accuracy: 0.260417, mean_q: -2462.866130, mean_eps: 0.100000\n",
      " 14529/50000: episode: 2101, duration: 0.012s, episode steps:   3, steps per second: 254, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 522588.270833, mae: 2421.736247, accuracy: 0.250000, mean_q: -2458.234375, mean_eps: 0.100000\n",
      " 14532/50000: episode: 2102, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 585758.604167, mae: 2400.802653, accuracy: 0.343750, mean_q: -2444.227783, mean_eps: 0.100000\n",
      " 14535/50000: episode: 2103, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 577215.395833, mae: 2400.227539, accuracy: 0.302083, mean_q: -2418.128906, mean_eps: 0.100000\n",
      " 14538/50000: episode: 2104, duration: 0.012s, episode steps:   3, steps per second: 256, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 466348.666667, mae: 2400.734619, accuracy: 0.177083, mean_q: -2456.103353, mean_eps: 0.100000\n",
      " 14541/50000: episode: 2105, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 521281.343750, mae: 2380.732422, accuracy: 0.291667, mean_q: -2446.787760, mean_eps: 0.100000\n",
      " 14544/50000: episode: 2106, duration: 0.015s, episode steps:   3, steps per second: 201, episode reward: -224.000, mean reward: -74.667 [-116.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 480828.041667, mae: 2401.766032, accuracy: 0.197917, mean_q: -2475.607096, mean_eps: 0.100000\n",
      " 14547/50000: episode: 2107, duration: 0.014s, episode steps:   3, steps per second: 221, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 539130.770833, mae: 2421.696370, accuracy: 0.229167, mean_q: -2476.661377, mean_eps: 0.100000\n",
      " 14550/50000: episode: 2108, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 451370.270833, mae: 2381.280436, accuracy: 0.156250, mean_q: -2436.277018, mean_eps: 0.100000\n",
      " 14553/50000: episode: 2109, duration: 0.012s, episode steps:   3, steps per second: 257, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 568584.635417, mae: 2396.642008, accuracy: 0.177083, mean_q: -2442.376953, mean_eps: 0.100000\n",
      " 14556/50000: episode: 2110, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 570161.354167, mae: 2376.439941, accuracy: 0.177083, mean_q: -2431.318034, mean_eps: 0.100000\n",
      " 14559/50000: episode: 2111, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 572325.968750, mae: 2380.943766, accuracy: 0.260417, mean_q: -2394.185221, mean_eps: 0.100000\n",
      " 14562/50000: episode: 2112, duration: 0.013s, episode steps:   3, steps per second: 225, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 443680.031250, mae: 2333.734782, accuracy: 0.166667, mean_q: -2410.992594, mean_eps: 0.100000\n",
      " 14565/50000: episode: 2113, duration: 0.014s, episode steps:   3, steps per second: 215, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 555985.770833, mae: 2375.406576, accuracy: 0.114583, mean_q: -2401.544189, mean_eps: 0.100000\n",
      " 14569/50000: episode: 2114, duration: 0.016s, episode steps:   4, steps per second: 244, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 491289.820312, mae: 2377.713196, accuracy: 0.210938, mean_q: -2423.903198, mean_eps: 0.100000\n",
      " 14577/50000: episode: 2115, duration: 0.030s, episode steps:   8, steps per second: 264, episode reward: -5219.000, mean reward: -652.375 [-999.000, -58.000], mean action: 1.750 [0.000, 3.000],  loss: 624038.183594, mae: 2381.449341, accuracy: 0.175781, mean_q: -2413.426910, mean_eps: 0.100000\n",
      " 14580/50000: episode: 2116, duration: 0.013s, episode steps:   3, steps per second: 230, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 523218.479167, mae: 2368.785645, accuracy: 0.197917, mean_q: -2399.272217, mean_eps: 0.100000\n",
      " 14583/50000: episode: 2117, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 347569.625000, mae: 2336.909831, accuracy: 0.250000, mean_q: -2417.649821, mean_eps: 0.100000\n",
      " 14586/50000: episode: 2118, duration: 0.015s, episode steps:   3, steps per second: 196, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 567301.395833, mae: 2377.140137, accuracy: 0.218750, mean_q: -2413.310872, mean_eps: 0.100000\n",
      " 14589/50000: episode: 2119, duration: 0.018s, episode steps:   3, steps per second: 163, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 523134.223958, mae: 2357.490967, accuracy: 0.187500, mean_q: -2400.197917, mean_eps: 0.100000\n",
      " 14593/50000: episode: 2120, duration: 0.018s, episode steps:   4, steps per second: 216, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 2.250 [1.000, 3.000],  loss: 525804.460938, mae: 2391.807800, accuracy: 0.132812, mean_q: -2453.172791, mean_eps: 0.100000\n",
      " 14600/50000: episode: 2121, duration: 0.028s, episode steps:   7, steps per second: 248, episode reward: -4220.000, mean reward: -602.857 [-999.000, -58.000], mean action: 1.714 [0.000, 3.000],  loss: 538325.316964, mae: 2373.241246, accuracy: 0.187500, mean_q: -2404.702253, mean_eps: 0.100000\n",
      " 14603/50000: episode: 2122, duration: 0.014s, episode steps:   3, steps per second: 220, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 383867.520833, mae: 2340.436361, accuracy: 0.218750, mean_q: -2422.518392, mean_eps: 0.100000\n",
      " 14607/50000: episode: 2123, duration: 0.016s, episode steps:   4, steps per second: 248, episode reward: -1223.000, mean reward: -305.750 [-999.000, -58.000], mean action: 1.750 [0.000, 3.000],  loss: 638278.375000, mae: 2429.333069, accuracy: 0.171875, mean_q: -2441.172729, mean_eps: 0.100000\n",
      " 14610/50000: episode: 2124, duration: 0.014s, episode steps:   3, steps per second: 220, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 574611.927083, mae: 2407.886637, accuracy: 0.135417, mean_q: -2451.944987, mean_eps: 0.100000\n",
      " 14613/50000: episode: 2125, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 576060.697917, mae: 2402.260254, accuracy: 0.208333, mean_q: -2431.257243, mean_eps: 0.100000\n",
      " 14616/50000: episode: 2126, duration: 0.013s, episode steps:   3, steps per second: 230, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 533385.000000, mae: 2382.924561, accuracy: 0.114583, mean_q: -2434.127441, mean_eps: 0.100000\n",
      " 14619/50000: episode: 2127, duration: 0.013s, episode steps:   3, steps per second: 230, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 594388.333333, mae: 2391.595785, accuracy: 0.166667, mean_q: -2418.742594, mean_eps: 0.100000\n",
      " 14622/50000: episode: 2128, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 552571.500000, mae: 2386.419108, accuracy: 0.229167, mean_q: -2397.690674, mean_eps: 0.100000\n",
      " 14625/50000: episode: 2129, duration: 0.014s, episode steps:   3, steps per second: 218, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 571580.062500, mae: 2386.497396, accuracy: 0.156250, mean_q: -2428.081380, mean_eps: 0.100000\n",
      " 14628/50000: episode: 2130, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 651043.156250, mae: 2388.766357, accuracy: 0.239583, mean_q: -2397.447591, mean_eps: 0.100000\n",
      " 14631/50000: episode: 2131, duration: 0.014s, episode steps:   3, steps per second: 208, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 529479.541667, mae: 2373.934570, accuracy: 0.229167, mean_q: -2400.164714, mean_eps: 0.100000\n",
      " 14634/50000: episode: 2132, duration: 0.015s, episode steps:   3, steps per second: 196, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 644482.718750, mae: 2383.059408, accuracy: 0.250000, mean_q: -2386.109701, mean_eps: 0.100000\n",
      " 14637/50000: episode: 2133, duration: 0.013s, episode steps:   3, steps per second: 223, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 405899.385417, mae: 2333.857829, accuracy: 0.166667, mean_q: -2375.933187, mean_eps: 0.100000\n",
      " 14640/50000: episode: 2134, duration: 0.013s, episode steps:   3, steps per second: 225, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 535587.197917, mae: 2325.191325, accuracy: 0.114583, mean_q: -2338.564535, mean_eps: 0.100000\n",
      " 14643/50000: episode: 2135, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 611982.541667, mae: 2357.790934, accuracy: 0.197917, mean_q: -2371.350016, mean_eps: 0.100000\n",
      " 14646/50000: episode: 2136, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 462153.416667, mae: 2306.266764, accuracy: 0.208333, mean_q: -2348.883870, mean_eps: 0.100000\n",
      " 14650/50000: episode: 2137, duration: 0.015s, episode steps:   4, steps per second: 264, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 417323.328125, mae: 2308.200562, accuracy: 0.171875, mean_q: -2370.219666, mean_eps: 0.100000\n",
      " 14653/50000: episode: 2138, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 589213.229167, mae: 2368.877930, accuracy: 0.104167, mean_q: -2367.336914, mean_eps: 0.100000\n",
      " 14656/50000: episode: 2139, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 522187.625000, mae: 2363.174235, accuracy: 0.125000, mean_q: -2361.675863, mean_eps: 0.100000\n",
      " 14660/50000: episode: 2140, duration: 0.015s, episode steps:   4, steps per second: 266, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 432479.148438, mae: 2293.631836, accuracy: 0.093750, mean_q: -2348.933594, mean_eps: 0.100000\n",
      " 14678/50000: episode: 2141, duration: 0.058s, episode steps:  18, steps per second: 309, episode reward: -15209.000, mean reward: -844.944 [-999.000, -58.000], mean action: 1.944 [0.000, 3.000],  loss: 482201.449653, mae: 2331.541273, accuracy: 0.145833, mean_q: -2356.732992, mean_eps: 0.100000\n",
      " 14682/50000: episode: 2142, duration: 0.016s, episode steps:   4, steps per second: 245, episode reward: -1223.000, mean reward: -305.750 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 465665.640625, mae: 2326.653992, accuracy: 0.156250, mean_q: -2379.538635, mean_eps: 0.100000\n",
      " 14685/50000: episode: 2143, duration: 0.013s, episode steps:   3, steps per second: 223, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 474126.947917, mae: 2316.233643, accuracy: 0.229167, mean_q: -2358.827474, mean_eps: 0.100000\n",
      " 14688/50000: episode: 2144, duration: 0.015s, episode steps:   3, steps per second: 198, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 549271.083333, mae: 2355.013997, accuracy: 0.229167, mean_q: -2395.877441, mean_eps: 0.100000\n",
      " 14691/50000: episode: 2145, duration: 0.014s, episode steps:   3, steps per second: 208, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 424790.958333, mae: 2314.300212, accuracy: 0.208333, mean_q: -2380.886719, mean_eps: 0.100000\n",
      " 14694/50000: episode: 2146, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 517438.406250, mae: 2331.808431, accuracy: 0.260417, mean_q: -2347.780843, mean_eps: 0.100000\n",
      " 14697/50000: episode: 2147, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 559165.718750, mae: 2318.410645, accuracy: 0.260417, mean_q: -2341.624593, mean_eps: 0.100000\n",
      " 14700/50000: episode: 2148, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 606425.458333, mae: 2330.607585, accuracy: 0.187500, mean_q: -2324.825195, mean_eps: 0.100000\n",
      " 14703/50000: episode: 2149, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 507775.312500, mae: 2296.548584, accuracy: 0.135417, mean_q: -2319.583171, mean_eps: 0.100000\n",
      " 14706/50000: episode: 2150, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 430374.093750, mae: 2294.936361, accuracy: 0.177083, mean_q: -2304.978271, mean_eps: 0.100000\n",
      " 14710/50000: episode: 2151, duration: 0.015s, episode steps:   4, steps per second: 268, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 524442.062500, mae: 2309.259460, accuracy: 0.250000, mean_q: -2333.029358, mean_eps: 0.100000\n",
      " 14713/50000: episode: 2152, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 355661.187500, mae: 2273.484294, accuracy: 0.208333, mean_q: -2324.024821, mean_eps: 0.100000\n",
      " 14716/50000: episode: 2153, duration: 0.012s, episode steps:   3, steps per second: 256, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 407589.145833, mae: 2310.575928, accuracy: 0.197917, mean_q: -2338.474854, mean_eps: 0.100000\n",
      " 14719/50000: episode: 2154, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 559819.031250, mae: 2340.466553, accuracy: 0.229167, mean_q: -2340.283447, mean_eps: 0.100000\n",
      " 14722/50000: episode: 2155, duration: 0.012s, episode steps:   3, steps per second: 253, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 548403.541667, mae: 2358.236328, accuracy: 0.135417, mean_q: -2348.521566, mean_eps: 0.100000\n",
      " 14725/50000: episode: 2156, duration: 0.012s, episode steps:   3, steps per second: 259, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 554780.281250, mae: 2314.958415, accuracy: 0.187500, mean_q: -2346.548340, mean_eps: 0.100000\n",
      " 14728/50000: episode: 2157, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 514476.541667, mae: 2310.288086, accuracy: 0.229167, mean_q: -2314.496908, mean_eps: 0.100000\n",
      " 14731/50000: episode: 2158, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 424482.906250, mae: 2285.905355, accuracy: 0.322917, mean_q: -2334.013509, mean_eps: 0.100000\n",
      " 14734/50000: episode: 2159, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 498519.333333, mae: 2277.364339, accuracy: 0.197917, mean_q: -2322.973145, mean_eps: 0.100000\n",
      " 14737/50000: episode: 2160, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 471176.177083, mae: 2310.369222, accuracy: 0.208333, mean_q: -2336.576904, mean_eps: 0.100000\n",
      " 14740/50000: episode: 2161, duration: 0.015s, episode steps:   3, steps per second: 202, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 622465.708333, mae: 2364.211100, accuracy: 0.166667, mean_q: -2315.714193, mean_eps: 0.100000\n",
      " 14743/50000: episode: 2162, duration: 0.014s, episode steps:   3, steps per second: 213, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 477254.645833, mae: 2291.684977, accuracy: 0.260417, mean_q: -2306.354574, mean_eps: 0.100000\n",
      " 14746/50000: episode: 2163, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 635997.479167, mae: 2331.513753, accuracy: 0.229167, mean_q: -2332.048503, mean_eps: 0.100000\n",
      " 14749/50000: episode: 2164, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 395221.260417, mae: 2280.183919, accuracy: 0.177083, mean_q: -2304.040283, mean_eps: 0.100000\n",
      " 14752/50000: episode: 2165, duration: 0.011s, episode steps:   3, steps per second: 262, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 2.000 [1.000, 3.000],  loss: 473222.979167, mae: 2308.168213, accuracy: 0.218750, mean_q: -2310.153320, mean_eps: 0.100000\n",
      " 14756/50000: episode: 2166, duration: 0.015s, episode steps:   4, steps per second: 269, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 498784.250000, mae: 2302.956726, accuracy: 0.203125, mean_q: -2310.663696, mean_eps: 0.100000\n",
      " 14759/50000: episode: 2167, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 534934.968750, mae: 2335.400635, accuracy: 0.239583, mean_q: -2304.440348, mean_eps: 0.100000\n",
      " 14762/50000: episode: 2168, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 617720.927083, mae: 2339.949707, accuracy: 0.166667, mean_q: -2339.250081, mean_eps: 0.100000\n",
      " 14765/50000: episode: 2169, duration: 0.012s, episode steps:   3, steps per second: 258, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 537567.260417, mae: 2306.935791, accuracy: 0.208333, mean_q: -2298.515218, mean_eps: 0.100000\n",
      " 14768/50000: episode: 2170, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 564008.541667, mae: 2309.706299, accuracy: 0.197917, mean_q: -2309.843669, mean_eps: 0.100000\n",
      " 14771/50000: episode: 2171, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 436445.031250, mae: 2280.726156, accuracy: 0.208333, mean_q: -2294.925130, mean_eps: 0.100000\n",
      " 14774/50000: episode: 2172, duration: 0.012s, episode steps:   3, steps per second: 257, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 554252.010417, mae: 2318.235026, accuracy: 0.218750, mean_q: -2299.646077, mean_eps: 0.100000\n",
      " 14777/50000: episode: 2173, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 460281.854167, mae: 2259.751628, accuracy: 0.291667, mean_q: -2278.510173, mean_eps: 0.100000\n",
      " 14780/50000: episode: 2174, duration: 0.012s, episode steps:   3, steps per second: 253, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 562886.229167, mae: 2296.674723, accuracy: 0.166667, mean_q: -2304.112793, mean_eps: 0.100000\n",
      " 14783/50000: episode: 2175, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 460697.406250, mae: 2272.819336, accuracy: 0.322917, mean_q: -2280.637044, mean_eps: 0.100000\n",
      " 14786/50000: episode: 2176, duration: 0.014s, episode steps:   3, steps per second: 219, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 479984.072917, mae: 2299.468424, accuracy: 0.239583, mean_q: -2297.147624, mean_eps: 0.100000\n",
      " 14790/50000: episode: 2177, duration: 0.020s, episode steps:   4, steps per second: 197, episode reward: -1238.000, mean reward: -309.500 [-999.000, -60.000], mean action: 2.250 [1.000, 3.000],  loss: 492031.156250, mae: 2292.584961, accuracy: 0.250000, mean_q: -2309.624512, mean_eps: 0.100000\n",
      " 14793/50000: episode: 2178, duration: 0.014s, episode steps:   3, steps per second: 210, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 403248.890625, mae: 2286.486816, accuracy: 0.250000, mean_q: -2325.177572, mean_eps: 0.100000\n",
      " 14796/50000: episode: 2179, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 498530.687500, mae: 2280.754476, accuracy: 0.250000, mean_q: -2288.033040, mean_eps: 0.100000\n",
      " 14800/50000: episode: 2180, duration: 0.015s, episode steps:   4, steps per second: 261, episode reward: -1223.000, mean reward: -305.750 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 517145.195312, mae: 2270.414062, accuracy: 0.195312, mean_q: -2282.193542, mean_eps: 0.100000\n",
      " 14803/50000: episode: 2181, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 515716.958333, mae: 2299.382650, accuracy: 0.260417, mean_q: -2305.022868, mean_eps: 0.100000\n",
      " 14806/50000: episode: 2182, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 436077.125000, mae: 2266.174642, accuracy: 0.145833, mean_q: -2272.228597, mean_eps: 0.100000\n",
      " 14809/50000: episode: 2183, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 567123.833333, mae: 2284.370117, accuracy: 0.229167, mean_q: -2266.214681, mean_eps: 0.100000\n",
      " 14812/50000: episode: 2184, duration: 0.014s, episode steps:   3, steps per second: 212, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 426532.552083, mae: 2250.782959, accuracy: 0.229167, mean_q: -2281.154378, mean_eps: 0.100000\n",
      " 14815/50000: episode: 2185, duration: 0.013s, episode steps:   3, steps per second: 225, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 677450.687500, mae: 2302.229574, accuracy: 0.145833, mean_q: -2263.594645, mean_eps: 0.100000\n",
      " 14818/50000: episode: 2186, duration: 0.012s, episode steps:   3, steps per second: 253, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 396490.354167, mae: 2239.067220, accuracy: 0.166667, mean_q: -2262.268392, mean_eps: 0.100000\n",
      " 14821/50000: episode: 2187, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 431228.135417, mae: 2255.346436, accuracy: 0.104167, mean_q: -2268.981852, mean_eps: 0.100000\n",
      " 14824/50000: episode: 2188, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 542260.312500, mae: 2261.133301, accuracy: 0.208333, mean_q: -2244.175618, mean_eps: 0.100000\n",
      " 14827/50000: episode: 2189, duration: 0.013s, episode steps:   3, steps per second: 226, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 449631.729167, mae: 2258.341309, accuracy: 0.229167, mean_q: -2256.004557, mean_eps: 0.100000\n",
      " 14831/50000: episode: 2190, duration: 0.016s, episode steps:   4, steps per second: 243, episode reward: -1223.000, mean reward: -305.750 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 584010.390625, mae: 2254.469727, accuracy: 0.179688, mean_q: -2223.257874, mean_eps: 0.100000\n",
      " 14834/50000: episode: 2191, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 515432.375000, mae: 2214.676432, accuracy: 0.114583, mean_q: -2212.401693, mean_eps: 0.100000\n",
      " 14837/50000: episode: 2192, duration: 0.015s, episode steps:   3, steps per second: 202, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 578791.854167, mae: 2261.393229, accuracy: 0.156250, mean_q: -2209.405355, mean_eps: 0.100000\n",
      " 14841/50000: episode: 2193, duration: 0.017s, episode steps:   4, steps per second: 229, episode reward: -1223.000, mean reward: -305.750 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 440558.710938, mae: 2209.320251, accuracy: 0.242188, mean_q: -2198.866516, mean_eps: 0.100000\n",
      " 14845/50000: episode: 2194, duration: 0.016s, episode steps:   4, steps per second: 257, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 570173.171875, mae: 2228.034790, accuracy: 0.218750, mean_q: -2173.991089, mean_eps: 0.100000\n",
      " 14848/50000: episode: 2195, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 446539.937500, mae: 2193.112386, accuracy: 0.166667, mean_q: -2197.722656, mean_eps: 0.100000\n",
      " 14851/50000: episode: 2196, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 514286.020833, mae: 2198.998861, accuracy: 0.177083, mean_q: -2152.976237, mean_eps: 0.100000\n",
      " 14855/50000: episode: 2197, duration: 0.015s, episode steps:   4, steps per second: 266, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.000 [0.000, 3.000],  loss: 465584.546875, mae: 2180.318298, accuracy: 0.195312, mean_q: -2170.088013, mean_eps: 0.100000\n",
      " 14858/50000: episode: 2198, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 419622.572917, mae: 2191.553874, accuracy: 0.270833, mean_q: -2155.510661, mean_eps: 0.100000\n",
      " 14861/50000: episode: 2199, duration: 0.012s, episode steps:   3, steps per second: 253, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 592421.270833, mae: 2239.565104, accuracy: 0.062500, mean_q: -2194.994548, mean_eps: 0.100000\n",
      " 14864/50000: episode: 2200, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 454650.791667, mae: 2221.687419, accuracy: 0.093750, mean_q: -2198.080078, mean_eps: 0.100000\n",
      " 14867/50000: episode: 2201, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 2.000 [1.000, 3.000],  loss: 388213.250000, mae: 2223.208984, accuracy: 0.104167, mean_q: -2235.998128, mean_eps: 0.100000\n",
      " 14870/50000: episode: 2202, duration: 0.012s, episode steps:   3, steps per second: 253, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 435523.114583, mae: 2228.089600, accuracy: 0.187500, mean_q: -2207.170329, mean_eps: 0.100000\n",
      " 14873/50000: episode: 2203, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 382783.979167, mae: 2192.558268, accuracy: 0.218750, mean_q: -2205.306559, mean_eps: 0.100000\n",
      " 14876/50000: episode: 2204, duration: 0.013s, episode steps:   3, steps per second: 225, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 2.000 [1.000, 3.000],  loss: 473200.375000, mae: 2245.734049, accuracy: 0.197917, mean_q: -2237.212077, mean_eps: 0.100000\n",
      " 14879/50000: episode: 2205, duration: 0.015s, episode steps:   3, steps per second: 200, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 297045.421875, mae: 2201.188477, accuracy: 0.239583, mean_q: -2236.281413, mean_eps: 0.100000\n",
      " 14882/50000: episode: 2206, duration: 0.015s, episode steps:   3, steps per second: 201, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 486217.385417, mae: 2273.071289, accuracy: 0.187500, mean_q: -2228.057129, mean_eps: 0.100000\n",
      " 14885/50000: episode: 2207, duration: 0.016s, episode steps:   3, steps per second: 187, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 417471.802083, mae: 2249.451823, accuracy: 0.145833, mean_q: -2274.482422, mean_eps: 0.100000\n",
      " 14888/50000: episode: 2208, duration: 0.015s, episode steps:   3, steps per second: 203, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 404261.520833, mae: 2240.779134, accuracy: 0.229167, mean_q: -2255.451904, mean_eps: 0.100000\n",
      " 14891/50000: episode: 2209, duration: 0.014s, episode steps:   3, steps per second: 210, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 578222.260417, mae: 2308.993571, accuracy: 0.156250, mean_q: -2275.586182, mean_eps: 0.100000\n",
      " 14894/50000: episode: 2210, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 335796.531250, mae: 2228.551758, accuracy: 0.208333, mean_q: -2280.414225, mean_eps: 0.100000\n",
      " 14897/50000: episode: 2211, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 442061.041667, mae: 2277.235758, accuracy: 0.135417, mean_q: -2276.659749, mean_eps: 0.100000\n",
      " 14900/50000: episode: 2212, duration: 0.014s, episode steps:   3, steps per second: 217, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 564628.333333, mae: 2260.704020, accuracy: 0.208333, mean_q: -2258.103271, mean_eps: 0.100000\n",
      " 14903/50000: episode: 2213, duration: 0.013s, episode steps:   3, steps per second: 222, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.667 [0.000, 3.000],  loss: 497877.687500, mae: 2261.177246, accuracy: 0.197917, mean_q: -2256.995199, mean_eps: 0.100000\n",
      " 14906/50000: episode: 2214, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 430627.645833, mae: 2214.970052, accuracy: 0.218750, mean_q: -2193.497477, mean_eps: 0.100000\n",
      " 14909/50000: episode: 2215, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 374737.093750, mae: 2209.814697, accuracy: 0.156250, mean_q: -2225.497640, mean_eps: 0.100000\n",
      " 14913/50000: episode: 2216, duration: 0.015s, episode steps:   4, steps per second: 265, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 392871.804688, mae: 2208.196350, accuracy: 0.195312, mean_q: -2220.846313, mean_eps: 0.100000\n",
      " 14916/50000: episode: 2217, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 339024.354167, mae: 2203.250163, accuracy: 0.177083, mean_q: -2250.916423, mean_eps: 0.100000\n",
      " 14919/50000: episode: 2218, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 545477.989583, mae: 2267.645589, accuracy: 0.229167, mean_q: -2225.590169, mean_eps: 0.100000\n",
      " 14922/50000: episode: 2219, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 401409.708333, mae: 2211.619222, accuracy: 0.197917, mean_q: -2207.851644, mean_eps: 0.100000\n",
      " 14926/50000: episode: 2220, duration: 0.015s, episode steps:   4, steps per second: 267, episode reward: -1238.000, mean reward: -309.500 [-999.000, -60.000], mean action: 1.500 [0.000, 3.000],  loss: 535762.156250, mae: 2248.960876, accuracy: 0.218750, mean_q: -2234.408264, mean_eps: 0.100000\n",
      " 14929/50000: episode: 2221, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 544056.281250, mae: 2244.098633, accuracy: 0.239583, mean_q: -2225.853271, mean_eps: 0.100000\n",
      " 14932/50000: episode: 2222, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 349911.135417, mae: 2197.941406, accuracy: 0.197917, mean_q: -2211.801758, mean_eps: 0.100000\n",
      " 14935/50000: episode: 2223, duration: 0.014s, episode steps:   3, steps per second: 210, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 369306.718750, mae: 2204.274740, accuracy: 0.229167, mean_q: -2214.561035, mean_eps: 0.100000\n",
      " 14938/50000: episode: 2224, duration: 0.014s, episode steps:   3, steps per second: 215, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 518703.479167, mae: 2251.769450, accuracy: 0.177083, mean_q: -2229.831950, mean_eps: 0.100000\n",
      " 14941/50000: episode: 2225, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 373387.463542, mae: 2227.984945, accuracy: 0.218750, mean_q: -2246.254150, mean_eps: 0.100000\n",
      " 14944/50000: episode: 2226, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 541936.104167, mae: 2256.305013, accuracy: 0.281250, mean_q: -2225.024984, mean_eps: 0.100000\n",
      " 14947/50000: episode: 2227, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.333 [0.000, 3.000],  loss: 507013.625000, mae: 2263.957113, accuracy: 0.114583, mean_q: -2240.832926, mean_eps: 0.100000\n",
      " 14950/50000: episode: 2228, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 442366.968750, mae: 2267.664551, accuracy: 0.166667, mean_q: -2277.360026, mean_eps: 0.100000\n",
      " 14953/50000: episode: 2229, duration: 0.013s, episode steps:   3, steps per second: 223, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 477115.656250, mae: 2246.218180, accuracy: 0.197917, mean_q: -2236.255046, mean_eps: 0.100000\n",
      " 14956/50000: episode: 2230, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 389456.145833, mae: 2211.821370, accuracy: 0.208333, mean_q: -2208.840902, mean_eps: 0.100000\n",
      " 14959/50000: episode: 2231, duration: 0.012s, episode steps:   3, steps per second: 256, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 517828.770833, mae: 2248.237549, accuracy: 0.218750, mean_q: -2211.012370, mean_eps: 0.100000\n",
      " 14963/50000: episode: 2232, duration: 0.015s, episode steps:   4, steps per second: 270, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 429794.375000, mae: 2233.352722, accuracy: 0.296875, mean_q: -2212.990112, mean_eps: 0.100000\n",
      " 14966/50000: episode: 2233, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 373572.114583, mae: 2185.117594, accuracy: 0.312500, mean_q: -2186.363118, mean_eps: 0.100000\n",
      " 14969/50000: episode: 2234, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 476193.385417, mae: 2225.115479, accuracy: 0.250000, mean_q: -2195.496826, mean_eps: 0.100000\n",
      " 14972/50000: episode: 2235, duration: 0.012s, episode steps:   3, steps per second: 258, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 380661.791667, mae: 2205.564372, accuracy: 0.229167, mean_q: -2211.356608, mean_eps: 0.100000\n",
      " 14975/50000: episode: 2236, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 344177.250000, mae: 2201.628499, accuracy: 0.302083, mean_q: -2216.500163, mean_eps: 0.100000\n",
      " 14979/50000: episode: 2237, duration: 0.016s, episode steps:   4, steps per second: 257, episode reward: -1223.000, mean reward: -305.750 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 318314.648438, mae: 2214.110291, accuracy: 0.226562, mean_q: -2241.061218, mean_eps: 0.100000\n",
      " 14982/50000: episode: 2238, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 520758.447917, mae: 2242.812581, accuracy: 0.312500, mean_q: -2211.888590, mean_eps: 0.100000\n",
      " 14985/50000: episode: 2239, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 297324.380208, mae: 2202.826416, accuracy: 0.333333, mean_q: -2222.456380, mean_eps: 0.100000\n",
      " 14988/50000: episode: 2240, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 385670.088542, mae: 2239.240397, accuracy: 0.260417, mean_q: -2264.224691, mean_eps: 0.100000\n",
      " 14992/50000: episode: 2241, duration: 0.022s, episode steps:   4, steps per second: 184, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 495804.046875, mae: 2260.478210, accuracy: 0.359375, mean_q: -2244.160095, mean_eps: 0.100000\n",
      " 14995/50000: episode: 2242, duration: 0.013s, episode steps:   3, steps per second: 224, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 516487.395833, mae: 2262.438314, accuracy: 0.250000, mean_q: -2242.832926, mean_eps: 0.100000\n",
      " 14998/50000: episode: 2243, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 382238.052083, mae: 2191.237223, accuracy: 0.270833, mean_q: -2199.963867, mean_eps: 0.100000\n",
      " 15001/50000: episode: 2244, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 467230.708333, mae: 2267.603109, accuracy: 0.260417, mean_q: -2231.615316, mean_eps: 0.100000\n",
      " 15004/50000: episode: 2245, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 496374.458333, mae: 2223.587891, accuracy: 0.281250, mean_q: -2197.836263, mean_eps: 0.100000\n",
      " 15018/50000: episode: 2246, duration: 0.045s, episode steps:  14, steps per second: 313, episode reward: -11228.000, mean reward: -802.000 [-999.000, -60.000], mean action: 0.429 [0.000, 3.000],  loss: 428952.294643, mae: 2233.207241, accuracy: 0.308036, mean_q: -2225.914568, mean_eps: 0.100000\n",
      " 15055/50000: episode: 2247, duration: 0.121s, episode steps:  37, steps per second: 307, episode reward: -34205.000, mean reward: -924.459 [-999.000, -60.000], mean action: 0.162 [0.000, 3.000],  loss: 471635.938767, mae: 2220.613611, accuracy: 0.271115, mean_q: -2188.222834, mean_eps: 0.100000\n",
      " 15058/50000: episode: 2248, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 532760.458333, mae: 2208.705973, accuracy: 0.312500, mean_q: -2177.842285, mean_eps: 0.100000\n",
      " 15062/50000: episode: 2249, duration: 0.016s, episode steps:   4, steps per second: 250, episode reward: -1238.000, mean reward: -309.500 [-999.000, -60.000], mean action: 1.500 [0.000, 3.000],  loss: 495960.269531, mae: 2202.099182, accuracy: 0.156250, mean_q: -2163.427979, mean_eps: 0.100000\n",
      " 15065/50000: episode: 2250, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 492704.750000, mae: 2220.590739, accuracy: 0.177083, mean_q: -2202.462484, mean_eps: 0.100000\n",
      " 15068/50000: episode: 2251, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 506064.947917, mae: 2190.397298, accuracy: 0.229167, mean_q: -2146.403076, mean_eps: 0.100000\n",
      " 15071/50000: episode: 2252, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 374361.489583, mae: 2182.271891, accuracy: 0.208333, mean_q: -2187.632487, mean_eps: 0.100000\n",
      " 15074/50000: episode: 2253, duration: 0.012s, episode steps:   3, steps per second: 258, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 527308.562500, mae: 2210.351156, accuracy: 0.135417, mean_q: -2182.282064, mean_eps: 0.100000\n",
      " 15077/50000: episode: 2254, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 438990.156250, mae: 2166.719238, accuracy: 0.187500, mean_q: -2168.558431, mean_eps: 0.100000\n",
      " 15081/50000: episode: 2255, duration: 0.015s, episode steps:   4, steps per second: 274, episode reward: -1223.000, mean reward: -305.750 [-999.000, -32.000], mean action: 1.750 [0.000, 3.000],  loss: 441066.648438, mae: 2207.706787, accuracy: 0.187500, mean_q: -2165.322510, mean_eps: 0.100000\n",
      " 15084/50000: episode: 2256, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 381894.770833, mae: 2169.941813, accuracy: 0.166667, mean_q: -2157.293376, mean_eps: 0.100000\n",
      " 15087/50000: episode: 2257, duration: 0.012s, episode steps:   3, steps per second: 260, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 454234.520833, mae: 2212.905843, accuracy: 0.156250, mean_q: -2179.289062, mean_eps: 0.100000\n",
      " 15090/50000: episode: 2258, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 371296.114583, mae: 2182.419434, accuracy: 0.156250, mean_q: -2182.108236, mean_eps: 0.100000\n",
      " 15093/50000: episode: 2259, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 349966.119792, mae: 2198.375163, accuracy: 0.177083, mean_q: -2198.175618, mean_eps: 0.100000\n",
      " 15096/50000: episode: 2260, duration: 0.014s, episode steps:   3, steps per second: 222, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 372512.557292, mae: 2174.332764, accuracy: 0.177083, mean_q: -2176.147624, mean_eps: 0.100000\n",
      " 15099/50000: episode: 2261, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 373815.395833, mae: 2222.386637, accuracy: 0.114583, mean_q: -2226.179606, mean_eps: 0.100000\n",
      " 15103/50000: episode: 2262, duration: 0.015s, episode steps:   4, steps per second: 259, episode reward: -1223.000, mean reward: -305.750 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 524711.109375, mae: 2240.721375, accuracy: 0.148438, mean_q: -2215.520203, mean_eps: 0.100000\n",
      " 15106/50000: episode: 2263, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 413858.125000, mae: 2219.593506, accuracy: 0.187500, mean_q: -2215.949707, mean_eps: 0.100000\n",
      " 15109/50000: episode: 2264, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 405424.807292, mae: 2211.880697, accuracy: 0.177083, mean_q: -2205.606852, mean_eps: 0.100000\n",
      " 15112/50000: episode: 2265, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 374036.447917, mae: 2197.010905, accuracy: 0.218750, mean_q: -2199.262695, mean_eps: 0.100000\n",
      " 15115/50000: episode: 2266, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 435396.364583, mae: 2230.720459, accuracy: 0.177083, mean_q: -2228.313639, mean_eps: 0.100000\n",
      " 15118/50000: episode: 2267, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 444247.916667, mae: 2219.240641, accuracy: 0.250000, mean_q: -2194.190837, mean_eps: 0.100000\n",
      " 15121/50000: episode: 2268, duration: 0.014s, episode steps:   3, steps per second: 210, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 404198.583333, mae: 2198.645508, accuracy: 0.197917, mean_q: -2209.637370, mean_eps: 0.100000\n",
      " 15124/50000: episode: 2269, duration: 0.015s, episode steps:   3, steps per second: 201, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 464973.093750, mae: 2217.250326, accuracy: 0.229167, mean_q: -2182.987467, mean_eps: 0.100000\n",
      " 15127/50000: episode: 2270, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 460393.854167, mae: 2209.847656, accuracy: 0.145833, mean_q: -2201.420980, mean_eps: 0.100000\n",
      " 15130/50000: episode: 2271, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 481610.562500, mae: 2217.601237, accuracy: 0.166667, mean_q: -2189.544596, mean_eps: 0.100000\n",
      " 15133/50000: episode: 2272, duration: 0.012s, episode steps:   3, steps per second: 253, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 506567.656250, mae: 2191.897054, accuracy: 0.208333, mean_q: -2155.279215, mean_eps: 0.100000\n",
      " 15136/50000: episode: 2273, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 355950.114583, mae: 2175.229329, accuracy: 0.239583, mean_q: -2157.760986, mean_eps: 0.100000\n",
      " 15140/50000: episode: 2274, duration: 0.015s, episode steps:   4, steps per second: 268, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 308204.117188, mae: 2121.628296, accuracy: 0.250000, mean_q: -2117.113708, mean_eps: 0.100000\n",
      " 15143/50000: episode: 2275, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 393645.010417, mae: 2157.831136, accuracy: 0.166667, mean_q: -2169.715983, mean_eps: 0.100000\n",
      " 15146/50000: episode: 2276, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 463363.791667, mae: 2192.029785, accuracy: 0.218750, mean_q: -2154.789632, mean_eps: 0.100000\n",
      " 15149/50000: episode: 2277, duration: 0.012s, episode steps:   3, steps per second: 253, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 497704.031250, mae: 2201.316406, accuracy: 0.239583, mean_q: -2151.907064, mean_eps: 0.100000\n",
      " 15152/50000: episode: 2278, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 332070.015625, mae: 2157.089681, accuracy: 0.250000, mean_q: -2152.301676, mean_eps: 0.100000\n",
      " 15155/50000: episode: 2279, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -224.000, mean reward: -74.667 [-116.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 525705.416667, mae: 2193.747070, accuracy: 0.166667, mean_q: -2139.814616, mean_eps: 0.100000\n",
      " 15158/50000: episode: 2280, duration: 0.012s, episode steps:   3, steps per second: 257, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 469471.604167, mae: 2177.384928, accuracy: 0.187500, mean_q: -2110.129557, mean_eps: 0.100000\n",
      " 15162/50000: episode: 2281, duration: 0.015s, episode steps:   4, steps per second: 261, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 445906.687500, mae: 2188.870483, accuracy: 0.203125, mean_q: -2133.019348, mean_eps: 0.100000\n",
      " 15165/50000: episode: 2282, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 416693.541667, mae: 2190.944499, accuracy: 0.197917, mean_q: -2131.293132, mean_eps: 0.100000\n",
      " 15168/50000: episode: 2283, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 408847.385417, mae: 2172.771484, accuracy: 0.208333, mean_q: -2153.131917, mean_eps: 0.100000\n",
      " 15172/50000: episode: 2284, duration: 0.018s, episode steps:   4, steps per second: 227, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 427629.937500, mae: 2151.855103, accuracy: 0.203125, mean_q: -2130.921753, mean_eps: 0.100000\n",
      " 15175/50000: episode: 2285, duration: 0.014s, episode steps:   3, steps per second: 213, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 356802.645833, mae: 2131.083984, accuracy: 0.229167, mean_q: -2104.629395, mean_eps: 0.100000\n",
      " 15178/50000: episode: 2286, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 432688.250000, mae: 2170.466309, accuracy: 0.281250, mean_q: -2152.333008, mean_eps: 0.100000\n",
      " 15181/50000: episode: 2287, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 389657.208333, mae: 2130.618164, accuracy: 0.239583, mean_q: -2120.029704, mean_eps: 0.100000\n",
      " 15184/50000: episode: 2288, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 462393.968750, mae: 2181.512207, accuracy: 0.218750, mean_q: -2135.164551, mean_eps: 0.100000\n",
      " 15188/50000: episode: 2289, duration: 0.015s, episode steps:   4, steps per second: 269, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 449763.867188, mae: 2155.531067, accuracy: 0.218750, mean_q: -2117.400574, mean_eps: 0.100000\n",
      " 15191/50000: episode: 2290, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 514173.770833, mae: 2173.183919, accuracy: 0.229167, mean_q: -2101.676188, mean_eps: 0.100000\n",
      " 15194/50000: episode: 2291, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 469706.041667, mae: 2151.287923, accuracy: 0.197917, mean_q: -2102.909017, mean_eps: 0.100000\n",
      " 15197/50000: episode: 2292, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 386296.302083, mae: 2154.630859, accuracy: 0.250000, mean_q: -2114.145752, mean_eps: 0.100000\n",
      " 15201/50000: episode: 2293, duration: 0.017s, episode steps:   4, steps per second: 241, episode reward: -1238.000, mean reward: -309.500 [-999.000, -60.000], mean action: 1.500 [0.000, 3.000],  loss: 449211.132812, mae: 2155.451233, accuracy: 0.296875, mean_q: -2094.795349, mean_eps: 0.100000\n",
      " 15204/50000: episode: 2294, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 393677.979167, mae: 2136.436930, accuracy: 0.239583, mean_q: -2110.997640, mean_eps: 0.100000\n",
      " 15207/50000: episode: 2295, duration: 0.013s, episode steps:   3, steps per second: 226, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 462033.864583, mae: 2134.491374, accuracy: 0.260417, mean_q: -2096.125000, mean_eps: 0.100000\n",
      " 15210/50000: episode: 2296, duration: 0.014s, episode steps:   3, steps per second: 214, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 490679.614583, mae: 2166.218018, accuracy: 0.229167, mean_q: -2108.362223, mean_eps: 0.100000\n",
      " 15213/50000: episode: 2297, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 356857.510417, mae: 2134.402018, accuracy: 0.208333, mean_q: -2122.967692, mean_eps: 0.100000\n",
      " 15216/50000: episode: 2298, duration: 0.022s, episode steps:   3, steps per second: 138, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 492924.114583, mae: 2165.069661, accuracy: 0.250000, mean_q: -2110.697184, mean_eps: 0.100000\n",
      " 15219/50000: episode: 2299, duration: 0.020s, episode steps:   3, steps per second: 151, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 511111.145833, mae: 2160.652425, accuracy: 0.135417, mean_q: -2092.101807, mean_eps: 0.100000\n",
      " 15222/50000: episode: 2300, duration: 0.014s, episode steps:   3, steps per second: 210, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 441881.395833, mae: 2165.476400, accuracy: 0.229167, mean_q: -2122.844157, mean_eps: 0.100000\n",
      " 15225/50000: episode: 2301, duration: 0.014s, episode steps:   3, steps per second: 218, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 399794.697917, mae: 2120.330892, accuracy: 0.156250, mean_q: -2088.686768, mean_eps: 0.100000\n",
      " 15228/50000: episode: 2302, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 396966.937500, mae: 2122.952067, accuracy: 0.229167, mean_q: -2099.777425, mean_eps: 0.100000\n",
      " 15231/50000: episode: 2303, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 432375.760417, mae: 2157.640381, accuracy: 0.156250, mean_q: -2128.729004, mean_eps: 0.100000\n",
      " 15234/50000: episode: 2304, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 353125.000000, mae: 2133.350911, accuracy: 0.177083, mean_q: -2117.186279, mean_eps: 0.100000\n",
      " 15237/50000: episode: 2305, duration: 0.014s, episode steps:   3, steps per second: 207, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 397694.708333, mae: 2133.539062, accuracy: 0.197917, mean_q: -2111.837484, mean_eps: 0.100000\n",
      " 15261/50000: episode: 2306, duration: 0.081s, episode steps:  24, steps per second: 296, episode reward: -21174.000, mean reward: -882.250 [-999.000, -58.000], mean action: 1.958 [0.000, 3.000],  loss: 426748.061849, mae: 2161.679749, accuracy: 0.196615, mean_q: -2117.830302, mean_eps: 0.100000\n",
      " 15296/50000: episode: 2307, duration: 0.117s, episode steps:  35, steps per second: 299, episode reward: -32192.000, mean reward: -919.771 [-999.000, -58.000], mean action: 2.000 [0.000, 3.000],  loss: 441128.722321, mae: 2153.048298, accuracy: 0.190179, mean_q: -2096.126151, mean_eps: 0.100000\n",
      " 15299/50000: episode: 2308, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 505704.406250, mae: 2156.356689, accuracy: 0.125000, mean_q: -2098.570312, mean_eps: 0.100000\n",
      " 15302/50000: episode: 2309, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 344961.052083, mae: 2108.279541, accuracy: 0.218750, mean_q: -2068.034383, mean_eps: 0.100000\n",
      " 15305/50000: episode: 2310, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 382483.244792, mae: 2126.365885, accuracy: 0.208333, mean_q: -2080.890137, mean_eps: 0.100000\n",
      " 15308/50000: episode: 2311, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 463447.656250, mae: 2159.033854, accuracy: 0.239583, mean_q: -2086.648926, mean_eps: 0.100000\n",
      " 15311/50000: episode: 2312, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 473533.947917, mae: 2156.414551, accuracy: 0.218750, mean_q: -2075.561361, mean_eps: 0.100000\n",
      " 15314/50000: episode: 2313, duration: 0.012s, episode steps:   3, steps per second: 253, episode reward: -224.000, mean reward: -74.667 [-116.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 374852.520833, mae: 2143.652995, accuracy: 0.156250, mean_q: -2103.073079, mean_eps: 0.100000\n",
      " 15317/50000: episode: 2314, duration: 0.012s, episode steps:   3, steps per second: 253, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 428173.500000, mae: 2151.035563, accuracy: 0.239583, mean_q: -2093.669189, mean_eps: 0.100000\n",
      " 15320/50000: episode: 2315, duration: 0.014s, episode steps:   3, steps per second: 221, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 449567.739583, mae: 2138.995036, accuracy: 0.145833, mean_q: -2077.313477, mean_eps: 0.100000\n",
      " 15323/50000: episode: 2316, duration: 0.013s, episode steps:   3, steps per second: 224, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 400642.260417, mae: 2136.520508, accuracy: 0.218750, mean_q: -2088.393473, mean_eps: 0.100000\n",
      " 15326/50000: episode: 2317, duration: 0.014s, episode steps:   3, steps per second: 213, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 353791.166667, mae: 2103.182048, accuracy: 0.177083, mean_q: -2074.690755, mean_eps: 0.100000\n",
      " 15329/50000: episode: 2318, duration: 0.013s, episode steps:   3, steps per second: 227, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 399582.979167, mae: 2130.222005, accuracy: 0.270833, mean_q: -2061.839437, mean_eps: 0.100000\n",
      " 15332/50000: episode: 2319, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 487706.281250, mae: 2153.649251, accuracy: 0.250000, mean_q: -2072.971924, mean_eps: 0.100000\n",
      " 15335/50000: episode: 2320, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 516725.072917, mae: 2167.776611, accuracy: 0.145833, mean_q: -2069.104655, mean_eps: 0.100000\n",
      " 15338/50000: episode: 2321, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 474102.833333, mae: 2144.664307, accuracy: 0.260417, mean_q: -2054.356567, mean_eps: 0.100000\n",
      " 15341/50000: episode: 2322, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 486460.593750, mae: 2150.437337, accuracy: 0.125000, mean_q: -2071.489746, mean_eps: 0.100000\n",
      " 15344/50000: episode: 2323, duration: 0.012s, episode steps:   3, steps per second: 255, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 439420.916667, mae: 2121.094889, accuracy: 0.177083, mean_q: -2043.907633, mean_eps: 0.100000\n",
      " 15347/50000: episode: 2324, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 373913.437500, mae: 2079.880615, accuracy: 0.208333, mean_q: -2047.186035, mean_eps: 0.100000\n",
      " 15350/50000: episode: 2325, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 456935.864583, mae: 2129.484619, accuracy: 0.270833, mean_q: -2042.171061, mean_eps: 0.100000\n",
      " 15354/50000: episode: 2326, duration: 0.015s, episode steps:   4, steps per second: 266, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 415025.468750, mae: 2090.038391, accuracy: 0.171875, mean_q: -2033.783600, mean_eps: 0.100000\n",
      " 15357/50000: episode: 2327, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 420995.833333, mae: 2109.512695, accuracy: 0.135417, mean_q: -2039.475871, mean_eps: 0.100000\n",
      " 15360/50000: episode: 2328, duration: 0.012s, episode steps:   3, steps per second: 256, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 471328.416667, mae: 2093.255290, accuracy: 0.260417, mean_q: -2016.918538, mean_eps: 0.100000\n",
      " 15363/50000: episode: 2329, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 476088.864583, mae: 2108.566813, accuracy: 0.156250, mean_q: -2042.144775, mean_eps: 0.100000\n",
      " 15366/50000: episode: 2330, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 2.000 [1.000, 3.000],  loss: 350946.979167, mae: 2069.699504, accuracy: 0.177083, mean_q: -2009.281942, mean_eps: 0.100000\n",
      " 15369/50000: episode: 2331, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 331040.812500, mae: 2062.380697, accuracy: 0.197917, mean_q: -2027.314901, mean_eps: 0.100000\n",
      " 15372/50000: episode: 2332, duration: 0.012s, episode steps:   3, steps per second: 256, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 370935.468750, mae: 2096.801270, accuracy: 0.104167, mean_q: -2039.140828, mean_eps: 0.100000\n",
      " 15375/50000: episode: 2333, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 436531.864583, mae: 2116.868286, accuracy: 0.250000, mean_q: -2048.784220, mean_eps: 0.100000\n",
      " 15378/50000: episode: 2334, duration: 0.014s, episode steps:   3, steps per second: 208, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 489723.833333, mae: 2136.692708, accuracy: 0.145833, mean_q: -2070.910075, mean_eps: 0.100000\n",
      " 15381/50000: episode: 2335, duration: 0.015s, episode steps:   3, steps per second: 206, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 360299.234375, mae: 2092.863118, accuracy: 0.156250, mean_q: -2073.721110, mean_eps: 0.100000\n",
      " 15384/50000: episode: 2336, duration: 0.013s, episode steps:   3, steps per second: 224, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 485539.645833, mae: 2142.080485, accuracy: 0.166667, mean_q: -2061.748820, mean_eps: 0.100000\n",
      " 15387/50000: episode: 2337, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 452361.541667, mae: 2117.556071, accuracy: 0.229167, mean_q: -2029.956746, mean_eps: 0.100000\n",
      " 15390/50000: episode: 2338, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 513759.770833, mae: 2139.054118, accuracy: 0.156250, mean_q: -2046.904785, mean_eps: 0.100000\n",
      " 15393/50000: episode: 2339, duration: 0.013s, episode steps:   3, steps per second: 225, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 427622.656250, mae: 2104.626058, accuracy: 0.208333, mean_q: -2042.485026, mean_eps: 0.100000\n",
      " 15396/50000: episode: 2340, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 495859.062500, mae: 2146.097005, accuracy: 0.208333, mean_q: -2033.386027, mean_eps: 0.100000\n",
      " 15399/50000: episode: 2341, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 474728.927083, mae: 2118.032227, accuracy: 0.187500, mean_q: -2045.457235, mean_eps: 0.100000\n",
      " 15402/50000: episode: 2342, duration: 0.013s, episode steps:   3, steps per second: 224, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 354559.697917, mae: 2079.410400, accuracy: 0.260417, mean_q: -2023.983398, mean_eps: 0.100000\n",
      " 15405/50000: episode: 2343, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 223269.276042, mae: 2041.292236, accuracy: 0.114583, mean_q: -2020.836548, mean_eps: 0.100000\n",
      " 15408/50000: episode: 2344, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 468440.375000, mae: 2118.165283, accuracy: 0.093750, mean_q: -2045.368164, mean_eps: 0.100000\n",
      " 15411/50000: episode: 2345, duration: 0.014s, episode steps:   3, steps per second: 222, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 427944.020833, mae: 2110.698730, accuracy: 0.114583, mean_q: -2012.819906, mean_eps: 0.100000\n",
      " 15414/50000: episode: 2346, duration: 0.013s, episode steps:   3, steps per second: 225, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 374959.354167, mae: 2114.869466, accuracy: 0.093750, mean_q: -2063.092936, mean_eps: 0.100000\n",
      " 15417/50000: episode: 2347, duration: 0.014s, episode steps:   3, steps per second: 212, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 399925.656250, mae: 2085.578044, accuracy: 0.166667, mean_q: -2011.184570, mean_eps: 0.100000\n",
      " 15420/50000: episode: 2348, duration: 0.014s, episode steps:   3, steps per second: 211, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 314032.385417, mae: 2066.191488, accuracy: 0.208333, mean_q: -2003.793905, mean_eps: 0.100000\n",
      " 15423/50000: episode: 2349, duration: 0.013s, episode steps:   3, steps per second: 224, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 410971.093750, mae: 2087.625163, accuracy: 0.135417, mean_q: -2007.672445, mean_eps: 0.100000\n",
      " 15426/50000: episode: 2350, duration: 0.014s, episode steps:   3, steps per second: 210, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 413637.281250, mae: 2087.978882, accuracy: 0.208333, mean_q: -2007.215088, mean_eps: 0.100000\n",
      " 15429/50000: episode: 2351, duration: 0.019s, episode steps:   3, steps per second: 159, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 395663.135417, mae: 2101.067057, accuracy: 0.187500, mean_q: -2023.953573, mean_eps: 0.100000\n",
      " 15432/50000: episode: 2352, duration: 0.018s, episode steps:   3, steps per second: 164, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 360353.020833, mae: 2096.653239, accuracy: 0.177083, mean_q: -2014.135295, mean_eps: 0.100000\n",
      " 15435/50000: episode: 2353, duration: 0.015s, episode steps:   3, steps per second: 201, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 418117.447917, mae: 2078.859904, accuracy: 0.177083, mean_q: -2001.238322, mean_eps: 0.100000\n",
      " 15438/50000: episode: 2354, duration: 0.015s, episode steps:   3, steps per second: 202, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 420583.343750, mae: 2082.743978, accuracy: 0.156250, mean_q: -1998.347087, mean_eps: 0.100000\n",
      " 15442/50000: episode: 2355, duration: 0.016s, episode steps:   4, steps per second: 244, episode reward: -1223.000, mean reward: -305.750 [-999.000, -58.000], mean action: 1.250 [0.000, 3.000],  loss: 366920.773438, mae: 2055.422333, accuracy: 0.179688, mean_q: -2000.119904, mean_eps: 0.100000\n",
      " 15447/50000: episode: 2356, duration: 0.020s, episode steps:   5, steps per second: 250, episode reward: -2222.000, mean reward: -444.400 [-999.000, -58.000], mean action: 1.800 [0.000, 3.000],  loss: 458078.031250, mae: 2095.219238, accuracy: 0.200000, mean_q: -2013.874390, mean_eps: 0.100000\n",
      " 15451/50000: episode: 2357, duration: 0.016s, episode steps:   4, steps per second: 244, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 318349.980469, mae: 2047.432739, accuracy: 0.156250, mean_q: -2005.842224, mean_eps: 0.100000\n",
      " 15454/50000: episode: 2358, duration: 0.014s, episode steps:   3, steps per second: 212, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 313717.760417, mae: 2036.067261, accuracy: 0.239583, mean_q: -2019.803263, mean_eps: 0.100000\n",
      " 15457/50000: episode: 2359, duration: 0.014s, episode steps:   3, steps per second: 216, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 362797.822917, mae: 2063.884888, accuracy: 0.208333, mean_q: -1990.973918, mean_eps: 0.100000\n",
      " 15460/50000: episode: 2360, duration: 0.014s, episode steps:   3, steps per second: 208, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 512906.458333, mae: 2102.121908, accuracy: 0.135417, mean_q: -2001.280233, mean_eps: 0.100000\n",
      " 15463/50000: episode: 2361, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.667 [0.000, 3.000],  loss: 344588.718750, mae: 2093.405192, accuracy: 0.072917, mean_q: -2025.899699, mean_eps: 0.100000\n",
      " 15466/50000: episode: 2362, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 476958.208333, mae: 2106.885417, accuracy: 0.177083, mean_q: -2008.643270, mean_eps: 0.100000\n",
      " 15469/50000: episode: 2363, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 382420.583333, mae: 2070.257731, accuracy: 0.187500, mean_q: -2001.388428, mean_eps: 0.100000\n",
      " 15472/50000: episode: 2364, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 337441.401042, mae: 2037.113281, accuracy: 0.302083, mean_q: -1968.581014, mean_eps: 0.100000\n",
      " 15475/50000: episode: 2365, duration: 0.015s, episode steps:   3, steps per second: 200, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 406445.291667, mae: 2058.434489, accuracy: 0.177083, mean_q: -1990.693766, mean_eps: 0.100000\n",
      " 15478/50000: episode: 2366, duration: 0.015s, episode steps:   3, steps per second: 202, episode reward: -224.000, mean reward: -74.667 [-116.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 398731.750000, mae: 2049.224487, accuracy: 0.260417, mean_q: -1967.404541, mean_eps: 0.100000\n",
      " 15481/50000: episode: 2367, duration: 0.015s, episode steps:   3, steps per second: 197, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 452794.083333, mae: 2076.371419, accuracy: 0.177083, mean_q: -1960.850952, mean_eps: 0.100000\n",
      " 15484/50000: episode: 2368, duration: 0.015s, episode steps:   3, steps per second: 198, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 383244.463542, mae: 2058.434977, accuracy: 0.322917, mean_q: -1963.167074, mean_eps: 0.100000\n",
      " 15487/50000: episode: 2369, duration: 0.013s, episode steps:   3, steps per second: 223, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 337392.588542, mae: 2024.130778, accuracy: 0.250000, mean_q: -1964.746704, mean_eps: 0.100000\n",
      " 15491/50000: episode: 2370, duration: 0.017s, episode steps:   4, steps per second: 240, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 414105.164062, mae: 2066.812805, accuracy: 0.179688, mean_q: -1975.725555, mean_eps: 0.100000\n",
      " 15494/50000: episode: 2371, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 438787.697917, mae: 2075.535075, accuracy: 0.218750, mean_q: -1972.493856, mean_eps: 0.100000\n",
      " 15497/50000: episode: 2372, duration: 0.012s, episode steps:   3, steps per second: 253, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 319978.812500, mae: 2029.588338, accuracy: 0.177083, mean_q: -1962.711060, mean_eps: 0.100000\n",
      " 15500/50000: episode: 2373, duration: 0.013s, episode steps:   3, steps per second: 227, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 524240.552083, mae: 2078.177165, accuracy: 0.156250, mean_q: -1946.771444, mean_eps: 0.100000\n",
      " 15503/50000: episode: 2374, duration: 0.013s, episode steps:   3, steps per second: 226, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 385913.541667, mae: 2072.588460, accuracy: 0.145833, mean_q: -1987.675741, mean_eps: 0.100000\n",
      " 15507/50000: episode: 2375, duration: 0.018s, episode steps:   4, steps per second: 228, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 375806.328125, mae: 2041.852173, accuracy: 0.226562, mean_q: -1961.648376, mean_eps: 0.100000\n",
      " 15510/50000: episode: 2376, duration: 0.014s, episode steps:   3, steps per second: 217, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 337320.854167, mae: 1992.802816, accuracy: 0.197917, mean_q: -1915.566366, mean_eps: 0.100000\n",
      " 15513/50000: episode: 2377, duration: 0.014s, episode steps:   3, steps per second: 215, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 412416.020833, mae: 2051.694906, accuracy: 0.229167, mean_q: -1948.747925, mean_eps: 0.100000\n",
      " 15516/50000: episode: 2378, duration: 0.014s, episode steps:   3, steps per second: 218, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 309484.088542, mae: 2008.524373, accuracy: 0.229167, mean_q: -1951.829183, mean_eps: 0.100000\n",
      " 15519/50000: episode: 2379, duration: 0.015s, episode steps:   3, steps per second: 195, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 334970.072917, mae: 2012.664144, accuracy: 0.250000, mean_q: -1936.349813, mean_eps: 0.100000\n",
      " 15523/50000: episode: 2380, duration: 0.016s, episode steps:   4, steps per second: 251, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 353994.585938, mae: 2048.599304, accuracy: 0.109375, mean_q: -1993.129578, mean_eps: 0.100000\n",
      " 15526/50000: episode: 2381, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 337950.708333, mae: 2037.727295, accuracy: 0.177083, mean_q: -1981.894613, mean_eps: 0.100000\n",
      " 15529/50000: episode: 2382, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 359953.989583, mae: 2039.922241, accuracy: 0.145833, mean_q: -1988.288371, mean_eps: 0.100000\n",
      " 15532/50000: episode: 2383, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 378933.281250, mae: 2056.227865, accuracy: 0.114583, mean_q: -1973.324137, mean_eps: 0.100000\n",
      " 15535/50000: episode: 2384, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 452716.104167, mae: 2075.405192, accuracy: 0.229167, mean_q: -1983.916829, mean_eps: 0.100000\n",
      " 15538/50000: episode: 2385, duration: 0.012s, episode steps:   3, steps per second: 254, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 197805.135417, mae: 2009.148763, accuracy: 0.218750, mean_q: -1993.008911, mean_eps: 0.100000\n",
      " 15541/50000: episode: 2386, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 418928.937500, mae: 2074.325928, accuracy: 0.208333, mean_q: -1982.366130, mean_eps: 0.100000\n",
      " 15544/50000: episode: 2387, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 363929.145833, mae: 2049.629964, accuracy: 0.187500, mean_q: -1971.907633, mean_eps: 0.100000\n",
      " 15547/50000: episode: 2388, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 380215.000000, mae: 2059.130737, accuracy: 0.239583, mean_q: -1981.048136, mean_eps: 0.100000\n",
      " 15550/50000: episode: 2389, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 403590.536458, mae: 2070.259969, accuracy: 0.125000, mean_q: -2003.447754, mean_eps: 0.100000\n",
      " 15553/50000: episode: 2390, duration: 0.014s, episode steps:   3, steps per second: 217, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 2.000 [1.000, 3.000],  loss: 403404.197917, mae: 2078.494873, accuracy: 0.197917, mean_q: -1994.985107, mean_eps: 0.100000\n",
      " 15556/50000: episode: 2391, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 361600.583333, mae: 2066.598592, accuracy: 0.145833, mean_q: -1983.352376, mean_eps: 0.100000\n",
      " 15559/50000: episode: 2392, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 352672.458333, mae: 2046.004150, accuracy: 0.250000, mean_q: -1978.650879, mean_eps: 0.100000\n",
      " 15563/50000: episode: 2393, duration: 0.017s, episode steps:   4, steps per second: 231, episode reward: -1223.000, mean reward: -305.750 [-999.000, -58.000], mean action: 1.750 [0.000, 3.000],  loss: 434401.812500, mae: 2060.611206, accuracy: 0.195312, mean_q: -1954.202972, mean_eps: 0.100000\n",
      " 15566/50000: episode: 2394, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 389583.218750, mae: 2048.861165, accuracy: 0.177083, mean_q: -1965.160441, mean_eps: 0.100000\n",
      " 15569/50000: episode: 2395, duration: 0.013s, episode steps:   3, steps per second: 226, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 476996.812500, mae: 2060.262288, accuracy: 0.239583, mean_q: -1940.712484, mean_eps: 0.100000\n",
      " 15572/50000: episode: 2396, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 387938.968750, mae: 2006.850342, accuracy: 0.177083, mean_q: -1939.250895, mean_eps: 0.100000\n",
      " 15575/50000: episode: 2397, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 346256.093750, mae: 2026.145264, accuracy: 0.156250, mean_q: -1944.782064, mean_eps: 0.100000\n",
      " 15578/50000: episode: 2398, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 246433.229167, mae: 1957.134766, accuracy: 0.208333, mean_q: -1922.818726, mean_eps: 0.100000\n",
      " 15581/50000: episode: 2399, duration: 0.012s, episode steps:   3, steps per second: 259, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 374787.791667, mae: 2011.260050, accuracy: 0.229167, mean_q: -1909.944336, mean_eps: 0.100000\n",
      " 15584/50000: episode: 2400, duration: 0.012s, episode steps:   3, steps per second: 255, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 355753.614583, mae: 2013.484049, accuracy: 0.229167, mean_q: -1934.009644, mean_eps: 0.100000\n",
      " 15587/50000: episode: 2401, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 388393.677083, mae: 2009.769043, accuracy: 0.177083, mean_q: -1928.365194, mean_eps: 0.100000\n",
      " 15590/50000: episode: 2402, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 393763.593750, mae: 2026.464355, accuracy: 0.250000, mean_q: -1926.070801, mean_eps: 0.100000\n",
      " 15593/50000: episode: 2403, duration: 0.012s, episode steps:   3, steps per second: 256, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 408014.427083, mae: 2059.370524, accuracy: 0.208333, mean_q: -1937.974447, mean_eps: 0.100000\n",
      " 15596/50000: episode: 2404, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 387517.864583, mae: 2021.380737, accuracy: 0.145833, mean_q: -1946.190145, mean_eps: 0.100000\n",
      " 15599/50000: episode: 2405, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 354133.447917, mae: 2013.706828, accuracy: 0.104167, mean_q: -1944.519572, mean_eps: 0.100000\n",
      " 15602/50000: episode: 2406, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 281215.526042, mae: 1975.480835, accuracy: 0.145833, mean_q: -1935.408447, mean_eps: 0.100000\n",
      " 15606/50000: episode: 2407, duration: 0.015s, episode steps:   4, steps per second: 259, episode reward: -1223.000, mean reward: -305.750 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 324956.593750, mae: 1995.273956, accuracy: 0.195312, mean_q: -1914.527740, mean_eps: 0.100000\n",
      " 15609/50000: episode: 2408, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 417115.333333, mae: 2021.004476, accuracy: 0.208333, mean_q: -1920.398560, mean_eps: 0.100000\n",
      " 15613/50000: episode: 2409, duration: 0.021s, episode steps:   4, steps per second: 189, episode reward: -1223.000, mean reward: -305.750 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 361204.015625, mae: 2019.131836, accuracy: 0.226562, mean_q: -1917.744080, mean_eps: 0.100000\n",
      " 15616/50000: episode: 2410, duration: 0.013s, episode steps:   3, steps per second: 224, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 326014.937500, mae: 2006.698771, accuracy: 0.208333, mean_q: -1934.005086, mean_eps: 0.100000\n",
      " 15619/50000: episode: 2411, duration: 0.014s, episode steps:   3, steps per second: 221, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 405446.552083, mae: 2030.864909, accuracy: 0.145833, mean_q: -1911.253743, mean_eps: 0.100000\n",
      " 15622/50000: episode: 2412, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 336714.395833, mae: 2004.308838, accuracy: 0.260417, mean_q: -1928.382406, mean_eps: 0.100000\n",
      " 15625/50000: episode: 2413, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -224.000, mean reward: -74.667 [-116.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 295979.015625, mae: 2009.446126, accuracy: 0.208333, mean_q: -1904.090332, mean_eps: 0.100000\n",
      " 15628/50000: episode: 2414, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 311682.562500, mae: 2012.545858, accuracy: 0.104167, mean_q: -1924.171346, mean_eps: 0.100000\n",
      " 15631/50000: episode: 2415, duration: 0.012s, episode steps:   3, steps per second: 256, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 328695.416667, mae: 1985.169678, accuracy: 0.218750, mean_q: -1896.943604, mean_eps: 0.100000\n",
      " 15634/50000: episode: 2416, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 392209.916667, mae: 2038.782878, accuracy: 0.260417, mean_q: -1935.361816, mean_eps: 0.100000\n",
      " 15637/50000: episode: 2417, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 372280.958333, mae: 2014.670125, accuracy: 0.197917, mean_q: -1925.149414, mean_eps: 0.100000\n",
      " 15640/50000: episode: 2418, duration: 0.012s, episode steps:   3, steps per second: 254, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 397154.927083, mae: 2019.236979, accuracy: 0.187500, mean_q: -1921.654907, mean_eps: 0.100000\n",
      " 15643/50000: episode: 2419, duration: 0.012s, episode steps:   3, steps per second: 259, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 366622.604167, mae: 1985.278117, accuracy: 0.260417, mean_q: -1875.480103, mean_eps: 0.100000\n",
      " 15647/50000: episode: 2420, duration: 0.016s, episode steps:   4, steps per second: 258, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 2.000 [0.000, 3.000],  loss: 295801.578125, mae: 1989.086792, accuracy: 0.187500, mean_q: -1931.847809, mean_eps: 0.100000\n",
      " 15652/50000: episode: 2421, duration: 0.019s, episode steps:   5, steps per second: 258, episode reward: -2193.000, mean reward: -438.600 [-999.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 373240.918750, mae: 2016.292676, accuracy: 0.162500, mean_q: -1911.330688, mean_eps: 0.100000\n",
      " 15655/50000: episode: 2422, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 430740.604167, mae: 2016.203735, accuracy: 0.229167, mean_q: -1882.079793, mean_eps: 0.100000\n",
      " 15658/50000: episode: 2423, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 296601.234375, mae: 1971.943278, accuracy: 0.229167, mean_q: -1895.706462, mean_eps: 0.100000\n",
      " 15661/50000: episode: 2424, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 260008.317708, mae: 1946.788656, accuracy: 0.218750, mean_q: -1904.472860, mean_eps: 0.100000\n",
      " 15664/50000: episode: 2425, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 376088.218750, mae: 1997.419963, accuracy: 0.166667, mean_q: -1891.419881, mean_eps: 0.100000\n",
      " 15667/50000: episode: 2426, duration: 0.014s, episode steps:   3, steps per second: 207, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 334649.760417, mae: 1991.938314, accuracy: 0.260417, mean_q: -1906.832113, mean_eps: 0.100000\n",
      " 15670/50000: episode: 2427, duration: 0.014s, episode steps:   3, steps per second: 219, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 485771.114583, mae: 2024.475830, accuracy: 0.187500, mean_q: -1908.034424, mean_eps: 0.100000\n",
      " 15673/50000: episode: 2428, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 469540.145833, mae: 2028.165975, accuracy: 0.239583, mean_q: -1891.562459, mean_eps: 0.100000\n",
      " 15676/50000: episode: 2429, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 251521.661458, mae: 1968.119995, accuracy: 0.177083, mean_q: -1902.760498, mean_eps: 0.100000\n",
      " 15679/50000: episode: 2430, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 249975.906250, mae: 1975.271200, accuracy: 0.239583, mean_q: -1908.332031, mean_eps: 0.100000\n",
      " 15682/50000: episode: 2431, duration: 0.012s, episode steps:   3, steps per second: 256, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 398912.906250, mae: 2013.704549, accuracy: 0.187500, mean_q: -1902.334676, mean_eps: 0.100000\n",
      " 15685/50000: episode: 2432, duration: 0.012s, episode steps:   3, steps per second: 254, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 438426.447917, mae: 2028.222493, accuracy: 0.166667, mean_q: -1894.438273, mean_eps: 0.100000\n",
      " 15689/50000: episode: 2433, duration: 0.015s, episode steps:   4, steps per second: 267, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 354629.457031, mae: 1997.378479, accuracy: 0.164062, mean_q: -1905.087646, mean_eps: 0.100000\n",
      " 15692/50000: episode: 2434, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 389854.244792, mae: 2016.501546, accuracy: 0.166667, mean_q: -1902.180461, mean_eps: 0.100000\n",
      " 15696/50000: episode: 2435, duration: 0.015s, episode steps:   4, steps per second: 264, episode reward: -1223.000, mean reward: -305.750 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 384058.445312, mae: 2021.762085, accuracy: 0.218750, mean_q: -1919.042999, mean_eps: 0.100000\n",
      " 15699/50000: episode: 2436, duration: 0.012s, episode steps:   3, steps per second: 255, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 469451.729167, mae: 2032.161255, accuracy: 0.239583, mean_q: -1915.965658, mean_eps: 0.100000\n",
      " 15702/50000: episode: 2437, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 250411.468750, mae: 1983.166829, accuracy: 0.197917, mean_q: -1893.652262, mean_eps: 0.100000\n",
      " 15705/50000: episode: 2438, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 423404.880208, mae: 2011.213704, accuracy: 0.177083, mean_q: -1889.056478, mean_eps: 0.100000\n",
      " 15708/50000: episode: 2439, duration: 0.012s, episode steps:   3, steps per second: 259, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 414900.281250, mae: 2004.510173, accuracy: 0.145833, mean_q: -1878.579142, mean_eps: 0.100000\n",
      " 15711/50000: episode: 2440, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 273458.395833, mae: 1928.789836, accuracy: 0.250000, mean_q: -1843.349976, mean_eps: 0.100000\n",
      " 15714/50000: episode: 2441, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.333 [0.000, 3.000],  loss: 382522.000000, mae: 1998.161133, accuracy: 0.177083, mean_q: -1886.454468, mean_eps: 0.100000\n",
      " 15717/50000: episode: 2442, duration: 0.015s, episode steps:   3, steps per second: 199, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 332342.802083, mae: 1962.567301, accuracy: 0.260417, mean_q: -1831.607788, mean_eps: 0.100000\n",
      " 15720/50000: episode: 2443, duration: 0.014s, episode steps:   3, steps per second: 218, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 306136.252604, mae: 1946.949504, accuracy: 0.218750, mean_q: -1837.468099, mean_eps: 0.100000\n",
      " 15723/50000: episode: 2444, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 318652.479167, mae: 1974.691366, accuracy: 0.208333, mean_q: -1858.843384, mean_eps: 0.100000\n",
      " 15726/50000: episode: 2445, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 401754.208333, mae: 2010.397949, accuracy: 0.197917, mean_q: -1893.029215, mean_eps: 0.100000\n",
      " 15729/50000: episode: 2446, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 298230.968750, mae: 1954.693563, accuracy: 0.239583, mean_q: -1871.101969, mean_eps: 0.100000\n",
      " 15732/50000: episode: 2447, duration: 0.012s, episode steps:   3, steps per second: 255, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 421801.208333, mae: 1996.267375, accuracy: 0.145833, mean_q: -1886.209595, mean_eps: 0.100000\n",
      " 15735/50000: episode: 2448, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 354004.645833, mae: 1968.029582, accuracy: 0.229167, mean_q: -1864.894613, mean_eps: 0.100000\n",
      " 15738/50000: episode: 2449, duration: 0.014s, episode steps:   3, steps per second: 221, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 393304.947917, mae: 1974.433919, accuracy: 0.208333, mean_q: -1840.912354, mean_eps: 0.100000\n",
      " 15741/50000: episode: 2450, duration: 0.014s, episode steps:   3, steps per second: 222, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 437894.614583, mae: 1986.959025, accuracy: 0.177083, mean_q: -1841.358683, mean_eps: 0.100000\n",
      " 15744/50000: episode: 2451, duration: 0.013s, episode steps:   3, steps per second: 227, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 410664.614583, mae: 1961.546712, accuracy: 0.145833, mean_q: -1849.239421, mean_eps: 0.100000\n",
      " 15748/50000: episode: 2452, duration: 0.018s, episode steps:   4, steps per second: 226, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 269767.871094, mae: 1929.292389, accuracy: 0.171875, mean_q: -1845.747650, mean_eps: 0.100000\n",
      " 15751/50000: episode: 2453, duration: 0.014s, episode steps:   3, steps per second: 221, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 308140.380208, mae: 1932.184652, accuracy: 0.208333, mean_q: -1837.516073, mean_eps: 0.100000\n",
      " 15754/50000: episode: 2454, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 363412.729167, mae: 1957.207764, accuracy: 0.156250, mean_q: -1838.518595, mean_eps: 0.100000\n",
      " 15757/50000: episode: 2455, duration: 0.014s, episode steps:   3, steps per second: 222, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 384471.395833, mae: 1983.411825, accuracy: 0.114583, mean_q: -1842.275960, mean_eps: 0.100000\n",
      " 15761/50000: episode: 2456, duration: 0.018s, episode steps:   4, steps per second: 224, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 304317.679688, mae: 1961.178375, accuracy: 0.140625, mean_q: -1855.670135, mean_eps: 0.100000\n",
      " 15765/50000: episode: 2457, duration: 0.019s, episode steps:   4, steps per second: 210, episode reward: -1238.000, mean reward: -309.500 [-999.000, -60.000], mean action: 1.500 [0.000, 3.000],  loss: 335838.867188, mae: 1963.764740, accuracy: 0.195312, mean_q: -1837.511139, mean_eps: 0.100000\n",
      " 15768/50000: episode: 2458, duration: 0.017s, episode steps:   3, steps per second: 180, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 375831.500000, mae: 1969.901449, accuracy: 0.166667, mean_q: -1865.845256, mean_eps: 0.100000\n",
      " 15771/50000: episode: 2459, duration: 0.015s, episode steps:   3, steps per second: 203, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 301776.239583, mae: 1939.152710, accuracy: 0.260417, mean_q: -1837.720459, mean_eps: 0.100000\n",
      " 15774/50000: episode: 2460, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 311398.348958, mae: 1971.036621, accuracy: 0.187500, mean_q: -1864.467163, mean_eps: 0.100000\n",
      " 15778/50000: episode: 2461, duration: 0.016s, episode steps:   4, steps per second: 249, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 286463.066406, mae: 1945.394379, accuracy: 0.210938, mean_q: -1861.331421, mean_eps: 0.100000\n",
      " 15781/50000: episode: 2462, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 341092.375000, mae: 1961.690918, accuracy: 0.229167, mean_q: -1832.604574, mean_eps: 0.100000\n",
      " 15784/50000: episode: 2463, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 347853.562500, mae: 1976.808472, accuracy: 0.229167, mean_q: -1855.386271, mean_eps: 0.100000\n",
      " 15787/50000: episode: 2464, duration: 0.013s, episode steps:   3, steps per second: 225, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 354788.500000, mae: 1970.050130, accuracy: 0.135417, mean_q: -1864.302653, mean_eps: 0.100000\n",
      " 15790/50000: episode: 2465, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 322923.531250, mae: 1966.372030, accuracy: 0.177083, mean_q: -1866.705648, mean_eps: 0.100000\n",
      " 15794/50000: episode: 2466, duration: 0.016s, episode steps:   4, steps per second: 242, episode reward: -1238.000, mean reward: -309.500 [-999.000, -60.000], mean action: 1.500 [0.000, 3.000],  loss: 319051.566406, mae: 1962.875366, accuracy: 0.257812, mean_q: -1878.441833, mean_eps: 0.100000\n",
      " 15797/50000: episode: 2467, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 307146.093750, mae: 1971.185832, accuracy: 0.218750, mean_q: -1860.079346, mean_eps: 0.100000\n",
      " 15800/50000: episode: 2468, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 256002.562500, mae: 1948.149577, accuracy: 0.208333, mean_q: -1865.677572, mean_eps: 0.100000\n",
      " 15803/50000: episode: 2469, duration: 0.014s, episode steps:   3, steps per second: 220, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 293714.072917, mae: 1933.677979, accuracy: 0.166667, mean_q: -1835.770711, mean_eps: 0.100000\n",
      " 15806/50000: episode: 2470, duration: 0.014s, episode steps:   3, steps per second: 208, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 339352.020833, mae: 1962.339315, accuracy: 0.197917, mean_q: -1843.599284, mean_eps: 0.100000\n",
      " 15809/50000: episode: 2471, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 322125.453125, mae: 1948.851237, accuracy: 0.177083, mean_q: -1858.555583, mean_eps: 0.100000\n",
      " 15812/50000: episode: 2472, duration: 0.018s, episode steps:   3, steps per second: 166, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 227763.895833, mae: 1940.129883, accuracy: 0.218750, mean_q: -1869.895874, mean_eps: 0.100000\n",
      " 15816/50000: episode: 2473, duration: 0.018s, episode steps:   4, steps per second: 223, episode reward: -1223.000, mean reward: -305.750 [-999.000, -58.000], mean action: 1.750 [0.000, 3.000],  loss: 323875.835938, mae: 1966.004730, accuracy: 0.250000, mean_q: -1854.922455, mean_eps: 0.100000\n",
      " 15819/50000: episode: 2474, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 343144.109375, mae: 1947.602132, accuracy: 0.156250, mean_q: -1832.633504, mean_eps: 0.100000\n",
      " 15822/50000: episode: 2475, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 412488.614583, mae: 1988.831258, accuracy: 0.177083, mean_q: -1843.802205, mean_eps: 0.100000\n",
      " 15825/50000: episode: 2476, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -224.000, mean reward: -74.667 [-116.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 321948.078125, mae: 1976.552938, accuracy: 0.145833, mean_q: -1858.838867, mean_eps: 0.100000\n",
      " 15828/50000: episode: 2477, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 277099.687500, mae: 1932.311035, accuracy: 0.187500, mean_q: -1823.168945, mean_eps: 0.100000\n",
      " 15831/50000: episode: 2478, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 395232.343750, mae: 1963.895467, accuracy: 0.177083, mean_q: -1802.675944, mean_eps: 0.100000\n",
      " 15834/50000: episode: 2479, duration: 0.012s, episode steps:   3, steps per second: 254, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 358216.312500, mae: 1963.322754, accuracy: 0.177083, mean_q: -1834.870117, mean_eps: 0.100000\n",
      " 15837/50000: episode: 2480, duration: 0.012s, episode steps:   3, steps per second: 254, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 269483.786458, mae: 1916.464315, accuracy: 0.208333, mean_q: -1834.786174, mean_eps: 0.100000\n",
      " 15840/50000: episode: 2481, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 261262.760417, mae: 1939.387817, accuracy: 0.177083, mean_q: -1835.089925, mean_eps: 0.100000\n",
      " 15843/50000: episode: 2482, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 314470.885417, mae: 1942.295817, accuracy: 0.093750, mean_q: -1828.601278, mean_eps: 0.100000\n",
      " 15846/50000: episode: 2483, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 309648.630208, mae: 1947.777791, accuracy: 0.218750, mean_q: -1839.236857, mean_eps: 0.100000\n",
      " 15849/50000: episode: 2484, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 410393.322917, mae: 2009.002279, accuracy: 0.083333, mean_q: -1832.655314, mean_eps: 0.100000\n",
      " 15852/50000: episode: 2485, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 300853.473958, mae: 1929.474202, accuracy: 0.250000, mean_q: -1803.120728, mean_eps: 0.100000\n",
      " 15855/50000: episode: 2486, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 331149.890625, mae: 1942.802450, accuracy: 0.166667, mean_q: -1809.190470, mean_eps: 0.100000\n",
      " 15859/50000: episode: 2487, duration: 0.015s, episode steps:   4, steps per second: 261, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 339741.332031, mae: 1935.338470, accuracy: 0.156250, mean_q: -1830.044220, mean_eps: 0.100000\n",
      " 15862/50000: episode: 2488, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 296729.802083, mae: 1948.547974, accuracy: 0.156250, mean_q: -1846.665934, mean_eps: 0.100000\n",
      " 15865/50000: episode: 2489, duration: 0.014s, episode steps:   3, steps per second: 213, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 318346.703125, mae: 1950.593099, accuracy: 0.218750, mean_q: -1822.165120, mean_eps: 0.100000\n",
      " 15868/50000: episode: 2490, duration: 0.014s, episode steps:   3, steps per second: 218, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 230306.125000, mae: 1905.232056, accuracy: 0.187500, mean_q: -1825.228597, mean_eps: 0.100000\n",
      " 15872/50000: episode: 2491, duration: 0.016s, episode steps:   4, steps per second: 258, episode reward: -1238.000, mean reward: -309.500 [-999.000, -60.000], mean action: 1.500 [0.000, 3.000],  loss: 286364.234375, mae: 1920.484039, accuracy: 0.250000, mean_q: -1788.664093, mean_eps: 0.100000\n",
      " 15877/50000: episode: 2492, duration: 0.019s, episode steps:   5, steps per second: 265, episode reward: -2193.000, mean reward: -438.600 [-999.000, -32.000], mean action: 1.400 [0.000, 3.000],  loss: 319147.043750, mae: 1962.145703, accuracy: 0.231250, mean_q: -1844.048413, mean_eps: 0.100000\n",
      " 15881/50000: episode: 2493, duration: 0.016s, episode steps:   4, steps per second: 244, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 363780.312500, mae: 1982.476685, accuracy: 0.164062, mean_q: -1843.972656, mean_eps: 0.100000\n",
      " 15884/50000: episode: 2494, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 270232.489583, mae: 1953.131877, accuracy: 0.197917, mean_q: -1846.937459, mean_eps: 0.100000\n",
      " 15887/50000: episode: 2495, duration: 0.013s, episode steps:   3, steps per second: 230, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 376510.010417, mae: 1992.758911, accuracy: 0.260417, mean_q: -1847.321859, mean_eps: 0.100000\n",
      " 15891/50000: episode: 2496, duration: 0.017s, episode steps:   4, steps per second: 242, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 340318.750000, mae: 1952.766693, accuracy: 0.218750, mean_q: -1828.385590, mean_eps: 0.100000\n",
      " 15894/50000: episode: 2497, duration: 0.014s, episode steps:   3, steps per second: 220, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 331084.802083, mae: 1953.618856, accuracy: 0.145833, mean_q: -1817.933919, mean_eps: 0.100000\n",
      " 15897/50000: episode: 2498, duration: 0.014s, episode steps:   3, steps per second: 209, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 361961.520833, mae: 1961.322917, accuracy: 0.260417, mean_q: -1801.707113, mean_eps: 0.100000\n",
      " 15900/50000: episode: 2499, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 279827.895833, mae: 1929.995605, accuracy: 0.166667, mean_q: -1837.623657, mean_eps: 0.100000\n",
      " 15903/50000: episode: 2500, duration: 0.013s, episode steps:   3, steps per second: 226, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 363179.635417, mae: 1956.036865, accuracy: 0.177083, mean_q: -1794.895955, mean_eps: 0.100000\n",
      " 15906/50000: episode: 2501, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 393711.208333, mae: 1967.219523, accuracy: 0.218750, mean_q: -1803.179606, mean_eps: 0.100000\n",
      " 15909/50000: episode: 2502, duration: 0.014s, episode steps:   3, steps per second: 219, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 307500.281250, mae: 1914.556925, accuracy: 0.197917, mean_q: -1826.643311, mean_eps: 0.100000\n",
      " 15912/50000: episode: 2503, duration: 0.015s, episode steps:   3, steps per second: 194, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 297443.072917, mae: 1906.335002, accuracy: 0.145833, mean_q: -1797.585042, mean_eps: 0.100000\n",
      " 15915/50000: episode: 2504, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 338647.104167, mae: 1897.938721, accuracy: 0.166667, mean_q: -1779.043701, mean_eps: 0.100000\n",
      " 15920/50000: episode: 2505, duration: 0.018s, episode steps:   5, steps per second: 273, episode reward: -2193.000, mean reward: -438.600 [-999.000, -45.000], mean action: 1.600 [0.000, 3.000],  loss: 374519.993750, mae: 1926.008203, accuracy: 0.187500, mean_q: -1772.903638, mean_eps: 0.100000\n",
      " 15923/50000: episode: 2506, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 260991.130208, mae: 1875.322021, accuracy: 0.239583, mean_q: -1768.874593, mean_eps: 0.100000\n",
      " 15927/50000: episode: 2507, duration: 0.015s, episode steps:   4, steps per second: 266, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 329449.601562, mae: 1906.200439, accuracy: 0.187500, mean_q: -1741.167328, mean_eps: 0.100000\n",
      " 15930/50000: episode: 2508, duration: 0.012s, episode steps:   3, steps per second: 255, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 371745.458333, mae: 1923.632568, accuracy: 0.187500, mean_q: -1765.568278, mean_eps: 0.100000\n",
      " 15933/50000: episode: 2509, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 316873.229167, mae: 1886.726603, accuracy: 0.197917, mean_q: -1748.398641, mean_eps: 0.100000\n",
      " 15936/50000: episode: 2510, duration: 0.012s, episode steps:   3, steps per second: 253, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 327955.520833, mae: 1883.763265, accuracy: 0.229167, mean_q: -1739.672241, mean_eps: 0.100000\n",
      " 15939/50000: episode: 2511, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 308793.875000, mae: 1873.465820, accuracy: 0.177083, mean_q: -1730.007487, mean_eps: 0.100000\n",
      " 15942/50000: episode: 2512, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 289693.406250, mae: 1856.909017, accuracy: 0.250000, mean_q: -1738.430176, mean_eps: 0.100000\n",
      " 15945/50000: episode: 2513, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 254059.026042, mae: 1863.039103, accuracy: 0.197917, mean_q: -1742.053304, mean_eps: 0.100000\n",
      " 15948/50000: episode: 2514, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 372129.677083, mae: 1891.474691, accuracy: 0.156250, mean_q: -1750.796712, mean_eps: 0.100000\n",
      " 15951/50000: episode: 2515, duration: 0.012s, episode steps:   3, steps per second: 255, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 341049.802083, mae: 1870.680420, accuracy: 0.208333, mean_q: -1721.242676, mean_eps: 0.100000\n",
      " 15955/50000: episode: 2516, duration: 0.016s, episode steps:   4, steps per second: 256, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 293451.085938, mae: 1887.014679, accuracy: 0.203125, mean_q: -1756.263489, mean_eps: 0.100000\n",
      " 15958/50000: episode: 2517, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 309180.062500, mae: 1860.344198, accuracy: 0.187500, mean_q: -1728.744629, mean_eps: 0.100000\n",
      " 15961/50000: episode: 2518, duration: 0.015s, episode steps:   3, steps per second: 204, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 381352.135417, mae: 1897.814779, accuracy: 0.187500, mean_q: -1735.832479, mean_eps: 0.100000\n",
      " 15964/50000: episode: 2519, duration: 0.014s, episode steps:   3, steps per second: 213, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 277635.052083, mae: 1856.312256, accuracy: 0.260417, mean_q: -1725.030843, mean_eps: 0.100000\n",
      " 15967/50000: episode: 2520, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 353829.000000, mae: 1874.548055, accuracy: 0.197917, mean_q: -1688.475993, mean_eps: 0.100000\n",
      " 15970/50000: episode: 2521, duration: 0.012s, episode steps:   3, steps per second: 254, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 322112.640625, mae: 1887.751994, accuracy: 0.156250, mean_q: -1730.649211, mean_eps: 0.100000\n",
      " 15973/50000: episode: 2522, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 256985.437500, mae: 1863.013672, accuracy: 0.208333, mean_q: -1719.864787, mean_eps: 0.100000\n",
      " 15976/50000: episode: 2523, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 335716.062500, mae: 1873.811320, accuracy: 0.197917, mean_q: -1723.131795, mean_eps: 0.100000\n",
      " 15979/50000: episode: 2524, duration: 0.012s, episode steps:   3, steps per second: 253, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 321854.791667, mae: 1877.467082, accuracy: 0.260417, mean_q: -1733.954590, mean_eps: 0.100000\n",
      " 15982/50000: episode: 2525, duration: 0.012s, episode steps:   3, steps per second: 255, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 326397.364583, mae: 1875.151367, accuracy: 0.218750, mean_q: -1702.607178, mean_eps: 0.100000\n",
      " 15985/50000: episode: 2526, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 321365.437500, mae: 1879.834839, accuracy: 0.177083, mean_q: -1714.346924, mean_eps: 0.100000\n",
      " 15988/50000: episode: 2527, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 296290.229167, mae: 1862.895386, accuracy: 0.156250, mean_q: -1689.522217, mean_eps: 0.100000\n",
      " 15991/50000: episode: 2528, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 2.000 [1.000, 3.000],  loss: 293783.604167, mae: 1851.063436, accuracy: 0.197917, mean_q: -1742.253866, mean_eps: 0.100000\n",
      " 15995/50000: episode: 2529, duration: 0.015s, episode steps:   4, steps per second: 270, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 364120.894531, mae: 1891.991486, accuracy: 0.187500, mean_q: -1710.624603, mean_eps: 0.100000\n",
      " 15998/50000: episode: 2530, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 339249.718750, mae: 1885.212036, accuracy: 0.156250, mean_q: -1727.363810, mean_eps: 0.100000\n",
      " 16001/50000: episode: 2531, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 242565.718750, mae: 1838.782308, accuracy: 0.177083, mean_q: -1716.219604, mean_eps: 0.100000\n",
      " 16005/50000: episode: 2532, duration: 0.016s, episode steps:   4, steps per second: 255, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 348271.921875, mae: 1860.926422, accuracy: 0.218750, mean_q: -1681.116699, mean_eps: 0.100000\n",
      " 16008/50000: episode: 2533, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 205044.161458, mae: 1816.171061, accuracy: 0.218750, mean_q: -1733.070394, mean_eps: 0.100000\n",
      " 16011/50000: episode: 2534, duration: 0.012s, episode steps:   3, steps per second: 254, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 373382.072917, mae: 1840.146525, accuracy: 0.239583, mean_q: -1658.350138, mean_eps: 0.100000\n",
      " 16014/50000: episode: 2535, duration: 0.015s, episode steps:   3, steps per second: 195, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 273606.098958, mae: 1826.695964, accuracy: 0.239583, mean_q: -1683.352905, mean_eps: 0.100000\n",
      " 16017/50000: episode: 2536, duration: 0.015s, episode steps:   3, steps per second: 194, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 292485.989583, mae: 1850.060262, accuracy: 0.218750, mean_q: -1684.935913, mean_eps: 0.100000\n",
      " 16020/50000: episode: 2537, duration: 0.015s, episode steps:   3, steps per second: 204, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 290871.552083, mae: 1841.265340, accuracy: 0.156250, mean_q: -1699.136719, mean_eps: 0.100000\n",
      " 16023/50000: episode: 2538, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 271114.666667, mae: 1834.799845, accuracy: 0.197917, mean_q: -1685.003337, mean_eps: 0.100000\n",
      " 16027/50000: episode: 2539, duration: 0.015s, episode steps:   4, steps per second: 259, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 2.250 [1.000, 3.000],  loss: 329737.382812, mae: 1866.392517, accuracy: 0.203125, mean_q: -1672.682098, mean_eps: 0.100000\n",
      " 16030/50000: episode: 2540, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 292788.156250, mae: 1857.296427, accuracy: 0.229167, mean_q: -1694.318278, mean_eps: 0.100000\n",
      " 16033/50000: episode: 2541, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 300286.166667, mae: 1856.372192, accuracy: 0.156250, mean_q: -1699.403564, mean_eps: 0.100000\n",
      " 16036/50000: episode: 2542, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 302261.395833, mae: 1855.518595, accuracy: 0.114583, mean_q: -1720.362223, mean_eps: 0.100000\n",
      " 16039/50000: episode: 2543, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 286882.994792, mae: 1880.884603, accuracy: 0.208333, mean_q: -1703.282145, mean_eps: 0.100000\n",
      " 16042/50000: episode: 2544, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 296548.645833, mae: 1859.336955, accuracy: 0.197917, mean_q: -1716.270833, mean_eps: 0.100000\n",
      " 16045/50000: episode: 2545, duration: 0.013s, episode steps:   3, steps per second: 225, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 326088.104167, mae: 1852.183797, accuracy: 0.218750, mean_q: -1696.872965, mean_eps: 0.100000\n",
      " 16048/50000: episode: 2546, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 392059.864583, mae: 1891.779785, accuracy: 0.187500, mean_q: -1689.998983, mean_eps: 0.100000\n",
      " 16051/50000: episode: 2547, duration: 0.014s, episode steps:   3, steps per second: 216, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 310496.281250, mae: 1835.052694, accuracy: 0.197917, mean_q: -1653.579793, mean_eps: 0.100000\n",
      " 16054/50000: episode: 2548, duration: 0.014s, episode steps:   3, steps per second: 208, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 262320.739583, mae: 1839.325399, accuracy: 0.187500, mean_q: -1696.044393, mean_eps: 0.100000\n",
      " 16057/50000: episode: 2549, duration: 0.015s, episode steps:   3, steps per second: 204, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 275958.406250, mae: 1838.410278, accuracy: 0.145833, mean_q: -1706.765340, mean_eps: 0.100000\n",
      " 16060/50000: episode: 2550, duration: 0.016s, episode steps:   3, steps per second: 184, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 299373.015625, mae: 1836.848470, accuracy: 0.208333, mean_q: -1671.066162, mean_eps: 0.100000\n",
      " 16063/50000: episode: 2551, duration: 0.015s, episode steps:   3, steps per second: 198, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 286892.510417, mae: 1818.058879, accuracy: 0.208333, mean_q: -1698.389445, mean_eps: 0.100000\n",
      " 16066/50000: episode: 2552, duration: 0.014s, episode steps:   3, steps per second: 208, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 328632.281250, mae: 1853.891520, accuracy: 0.156250, mean_q: -1676.516764, mean_eps: 0.100000\n",
      " 16069/50000: episode: 2553, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 295563.093750, mae: 1851.422648, accuracy: 0.187500, mean_q: -1697.223918, mean_eps: 0.100000\n",
      " 16072/50000: episode: 2554, duration: 0.014s, episode steps:   3, steps per second: 218, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 326177.791667, mae: 1846.720256, accuracy: 0.177083, mean_q: -1671.027466, mean_eps: 0.100000\n",
      " 16075/50000: episode: 2555, duration: 0.014s, episode steps:   3, steps per second: 221, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 301439.562500, mae: 1836.344116, accuracy: 0.187500, mean_q: -1671.762817, mean_eps: 0.100000\n",
      " 16078/50000: episode: 2556, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 229102.031250, mae: 1801.636515, accuracy: 0.125000, mean_q: -1681.074585, mean_eps: 0.100000\n",
      " 16081/50000: episode: 2557, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 328255.510417, mae: 1837.251343, accuracy: 0.166667, mean_q: -1660.438802, mean_eps: 0.100000\n",
      " 16084/50000: episode: 2558, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 321494.661458, mae: 1816.283122, accuracy: 0.250000, mean_q: -1661.653239, mean_eps: 0.100000\n",
      " 16087/50000: episode: 2559, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 316863.156250, mae: 1829.843872, accuracy: 0.156250, mean_q: -1646.789266, mean_eps: 0.100000\n",
      " 16090/50000: episode: 2560, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 304032.270833, mae: 1849.413289, accuracy: 0.166667, mean_q: -1654.790771, mean_eps: 0.100000\n",
      " 16094/50000: episode: 2561, duration: 0.015s, episode steps:   4, steps per second: 267, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 278837.312500, mae: 1827.623932, accuracy: 0.179688, mean_q: -1648.024658, mean_eps: 0.100000\n",
      " 16097/50000: episode: 2562, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 279888.854167, mae: 1818.115723, accuracy: 0.208333, mean_q: -1644.919067, mean_eps: 0.100000\n",
      " 16100/50000: episode: 2563, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 348785.750000, mae: 1858.693604, accuracy: 0.166667, mean_q: -1654.449748, mean_eps: 0.100000\n",
      " 16103/50000: episode: 2564, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 274201.281250, mae: 1822.431437, accuracy: 0.166667, mean_q: -1676.671875, mean_eps: 0.100000\n",
      " 16106/50000: episode: 2565, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 342356.447917, mae: 1829.835612, accuracy: 0.177083, mean_q: -1657.182739, mean_eps: 0.100000\n",
      " 16109/50000: episode: 2566, duration: 0.014s, episode steps:   3, steps per second: 214, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 312307.312500, mae: 1827.217082, accuracy: 0.145833, mean_q: -1621.007650, mean_eps: 0.100000\n",
      " 16112/50000: episode: 2567, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 290894.520833, mae: 1798.682251, accuracy: 0.177083, mean_q: -1659.234009, mean_eps: 0.100000\n",
      " 16115/50000: episode: 2568, duration: 0.014s, episode steps:   3, steps per second: 219, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 292672.963542, mae: 1777.894491, accuracy: 0.187500, mean_q: -1599.997518, mean_eps: 0.100000\n",
      " 16118/50000: episode: 2569, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 261399.348958, mae: 1784.616862, accuracy: 0.250000, mean_q: -1601.149902, mean_eps: 0.100000\n",
      " 16121/50000: episode: 2570, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 298038.734375, mae: 1787.431234, accuracy: 0.239583, mean_q: -1614.233927, mean_eps: 0.100000\n",
      " 16124/50000: episode: 2571, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 275190.807292, mae: 1777.412272, accuracy: 0.250000, mean_q: -1614.368652, mean_eps: 0.100000\n",
      " 16127/50000: episode: 2572, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 325001.187500, mae: 1810.735270, accuracy: 0.218750, mean_q: -1647.240519, mean_eps: 0.100000\n",
      " 16131/50000: episode: 2573, duration: 0.016s, episode steps:   4, steps per second: 255, episode reward: -1223.000, mean reward: -305.750 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 286452.011719, mae: 1803.304871, accuracy: 0.179688, mean_q: -1629.672485, mean_eps: 0.100000\n",
      " 16134/50000: episode: 2574, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 283083.145833, mae: 1793.861572, accuracy: 0.239583, mean_q: -1614.794963, mean_eps: 0.100000\n",
      " 16137/50000: episode: 2575, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 372889.281250, mae: 1826.297078, accuracy: 0.197917, mean_q: -1612.386759, mean_eps: 0.100000\n",
      " 16140/50000: episode: 2576, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 275991.541667, mae: 1790.307617, accuracy: 0.229167, mean_q: -1616.805827, mean_eps: 0.100000\n",
      " 16144/50000: episode: 2577, duration: 0.015s, episode steps:   4, steps per second: 268, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 264840.140625, mae: 1791.217163, accuracy: 0.257812, mean_q: -1603.745056, mean_eps: 0.100000\n",
      " 16147/50000: episode: 2578, duration: 0.012s, episode steps:   3, steps per second: 254, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 231636.203125, mae: 1795.041260, accuracy: 0.166667, mean_q: -1632.223470, mean_eps: 0.100000\n",
      " 16150/50000: episode: 2579, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 314920.802083, mae: 1821.465820, accuracy: 0.166667, mean_q: -1622.798543, mean_eps: 0.100000\n",
      " 16153/50000: episode: 2580, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 254688.494792, mae: 1804.593587, accuracy: 0.229167, mean_q: -1644.232503, mean_eps: 0.100000\n",
      " 16156/50000: episode: 2581, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 267488.838542, mae: 1782.845581, accuracy: 0.208333, mean_q: -1624.482056, mean_eps: 0.100000\n",
      " 16159/50000: episode: 2582, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 220567.500000, mae: 1801.795736, accuracy: 0.218750, mean_q: -1624.834473, mean_eps: 0.100000\n",
      " 16162/50000: episode: 2583, duration: 0.015s, episode steps:   3, steps per second: 205, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 356462.791667, mae: 1845.042887, accuracy: 0.135417, mean_q: -1629.119344, mean_eps: 0.100000\n",
      " 16165/50000: episode: 2584, duration: 0.014s, episode steps:   3, steps per second: 208, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 268581.760417, mae: 1798.140218, accuracy: 0.208333, mean_q: -1644.900553, mean_eps: 0.100000\n",
      " 16169/50000: episode: 2585, duration: 0.015s, episode steps:   4, steps per second: 258, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 289969.820312, mae: 1797.921997, accuracy: 0.148438, mean_q: -1600.864410, mean_eps: 0.100000\n",
      " 16172/50000: episode: 2586, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 321636.437500, mae: 1796.500732, accuracy: 0.229167, mean_q: -1610.523885, mean_eps: 0.100000\n",
      " 16175/50000: episode: 2587, duration: 0.013s, episode steps:   3, steps per second: 225, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 288317.833333, mae: 1764.556885, accuracy: 0.229167, mean_q: -1593.129272, mean_eps: 0.100000\n",
      " 16178/50000: episode: 2588, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 261081.062500, mae: 1756.075846, accuracy: 0.208333, mean_q: -1595.935303, mean_eps: 0.100000\n",
      " 16181/50000: episode: 2589, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 265067.692708, mae: 1739.052816, accuracy: 0.229167, mean_q: -1570.955811, mean_eps: 0.100000\n",
      " 16184/50000: episode: 2590, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 274213.046875, mae: 1761.097860, accuracy: 0.229167, mean_q: -1556.586466, mean_eps: 0.100000\n",
      " 16187/50000: episode: 2591, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 211396.989583, mae: 1742.276611, accuracy: 0.197917, mean_q: -1567.260417, mean_eps: 0.100000\n",
      " 16190/50000: episode: 2592, duration: 0.012s, episode steps:   3, steps per second: 254, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 256228.302083, mae: 1764.879395, accuracy: 0.250000, mean_q: -1603.267700, mean_eps: 0.100000\n",
      " 16193/50000: episode: 2593, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 230548.729167, mae: 1772.260661, accuracy: 0.229167, mean_q: -1581.414469, mean_eps: 0.100000\n",
      " 16196/50000: episode: 2594, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 274917.416667, mae: 1767.677327, accuracy: 0.270833, mean_q: -1595.421061, mean_eps: 0.100000\n",
      " 16199/50000: episode: 2595, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 245211.583333, mae: 1778.059408, accuracy: 0.208333, mean_q: -1620.786865, mean_eps: 0.100000\n",
      " 16202/50000: episode: 2596, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 270799.052083, mae: 1748.658407, accuracy: 0.187500, mean_q: -1593.555298, mean_eps: 0.100000\n",
      " 16205/50000: episode: 2597, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 304173.437500, mae: 1783.203573, accuracy: 0.177083, mean_q: -1605.761678, mean_eps: 0.100000\n",
      " 16208/50000: episode: 2598, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 266528.677083, mae: 1771.668416, accuracy: 0.218750, mean_q: -1610.835612, mean_eps: 0.100000\n",
      " 16211/50000: episode: 2599, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 268445.494792, mae: 1781.066813, accuracy: 0.166667, mean_q: -1602.453288, mean_eps: 0.100000\n",
      " 16214/50000: episode: 2600, duration: 0.018s, episode steps:   3, steps per second: 170, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 257867.062500, mae: 1782.482300, accuracy: 0.177083, mean_q: -1605.672892, mean_eps: 0.100000\n",
      " 16217/50000: episode: 2601, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 240607.541667, mae: 1753.058594, accuracy: 0.145833, mean_q: -1600.756592, mean_eps: 0.100000\n",
      " 16220/50000: episode: 2602, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 274512.979167, mae: 1772.280680, accuracy: 0.156250, mean_q: -1590.379517, mean_eps: 0.100000\n",
      " 16223/50000: episode: 2603, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 222151.453125, mae: 1761.017578, accuracy: 0.270833, mean_q: -1579.500081, mean_eps: 0.100000\n",
      " 16226/50000: episode: 2604, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 288503.656250, mae: 1790.834269, accuracy: 0.260417, mean_q: -1598.514526, mean_eps: 0.100000\n",
      " 16229/50000: episode: 2605, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 292690.062500, mae: 1772.872599, accuracy: 0.156250, mean_q: -1604.654012, mean_eps: 0.100000\n",
      " 16232/50000: episode: 2606, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 273356.625000, mae: 1793.324382, accuracy: 0.145833, mean_q: -1593.410807, mean_eps: 0.100000\n",
      " 16235/50000: episode: 2607, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 300296.468750, mae: 1779.893433, accuracy: 0.177083, mean_q: -1578.201416, mean_eps: 0.100000\n",
      " 16239/50000: episode: 2608, duration: 0.015s, episode steps:   4, steps per second: 272, episode reward: -1238.000, mean reward: -309.500 [-999.000, -45.000], mean action: 1.750 [0.000, 3.000],  loss: 291232.656250, mae: 1789.986206, accuracy: 0.156250, mean_q: -1594.945251, mean_eps: 0.100000\n",
      " 16242/50000: episode: 2609, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 299606.828125, mae: 1783.593791, accuracy: 0.187500, mean_q: -1595.115682, mean_eps: 0.100000\n",
      " 16245/50000: episode: 2610, duration: 0.012s, episode steps:   3, steps per second: 258, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 253384.885417, mae: 1745.336263, accuracy: 0.218750, mean_q: -1554.216268, mean_eps: 0.100000\n",
      " 16248/50000: episode: 2611, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 221561.265625, mae: 1733.797160, accuracy: 0.166667, mean_q: -1588.100016, mean_eps: 0.100000\n",
      " 16251/50000: episode: 2612, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 280196.875000, mae: 1729.517863, accuracy: 0.197917, mean_q: -1560.436239, mean_eps: 0.100000\n",
      " 16255/50000: episode: 2613, duration: 0.016s, episode steps:   4, steps per second: 252, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 285091.453125, mae: 1750.757538, accuracy: 0.156250, mean_q: -1537.719452, mean_eps: 0.100000\n",
      " 16258/50000: episode: 2614, duration: 0.014s, episode steps:   3, steps per second: 215, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 228943.421875, mae: 1709.412842, accuracy: 0.260417, mean_q: -1530.905924, mean_eps: 0.100000\n",
      " 16261/50000: episode: 2615, duration: 0.013s, episode steps:   3, steps per second: 227, episode reward: -224.000, mean reward: -74.667 [-116.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 215119.395833, mae: 1726.747192, accuracy: 0.187500, mean_q: -1548.816772, mean_eps: 0.100000\n",
      " 16264/50000: episode: 2616, duration: 0.015s, episode steps:   3, steps per second: 195, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 282642.041667, mae: 1725.213949, accuracy: 0.208333, mean_q: -1507.740763, mean_eps: 0.100000\n",
      " 16267/50000: episode: 2617, duration: 0.014s, episode steps:   3, steps per second: 215, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 258444.239583, mae: 1718.027100, accuracy: 0.197917, mean_q: -1525.877075, mean_eps: 0.100000\n",
      " 16270/50000: episode: 2618, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 277924.385417, mae: 1742.844157, accuracy: 0.187500, mean_q: -1550.388672, mean_eps: 0.100000\n",
      " 16273/50000: episode: 2619, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 255553.317708, mae: 1731.176921, accuracy: 0.197917, mean_q: -1523.691976, mean_eps: 0.100000\n",
      " 16276/50000: episode: 2620, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 257602.854167, mae: 1731.829712, accuracy: 0.187500, mean_q: -1520.989787, mean_eps: 0.100000\n",
      " 16279/50000: episode: 2621, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 244129.645833, mae: 1711.086629, accuracy: 0.187500, mean_q: -1539.500285, mean_eps: 0.100000\n",
      " 16282/50000: episode: 2622, duration: 0.013s, episode steps:   3, steps per second: 230, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 253536.593750, mae: 1682.167074, accuracy: 0.229167, mean_q: -1498.687256, mean_eps: 0.100000\n",
      " 16285/50000: episode: 2623, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 254267.692708, mae: 1722.602132, accuracy: 0.187500, mean_q: -1517.753092, mean_eps: 0.100000\n",
      " 16288/50000: episode: 2624, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 253545.468750, mae: 1737.663615, accuracy: 0.187500, mean_q: -1524.893311, mean_eps: 0.100000\n",
      " 16291/50000: episode: 2625, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 228760.713542, mae: 1715.244019, accuracy: 0.218750, mean_q: -1544.759237, mean_eps: 0.100000\n",
      " 16294/50000: episode: 2626, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 290166.885417, mae: 1768.847209, accuracy: 0.177083, mean_q: -1551.810181, mean_eps: 0.100000\n",
      " 16299/50000: episode: 2627, duration: 0.018s, episode steps:   5, steps per second: 284, episode reward: -2193.000, mean reward: -438.600 [-999.000, -58.000], mean action: 2.200 [1.000, 3.000],  loss: 196931.312500, mae: 1718.631104, accuracy: 0.243750, mean_q: -1557.115503, mean_eps: 0.100000\n",
      " 16302/50000: episode: 2628, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 189781.901042, mae: 1705.711304, accuracy: 0.197917, mean_q: -1532.871704, mean_eps: 0.100000\n",
      " 16305/50000: episode: 2629, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 298467.927083, mae: 1762.471639, accuracy: 0.197917, mean_q: -1533.226847, mean_eps: 0.100000\n",
      " 16308/50000: episode: 2630, duration: 0.012s, episode steps:   3, steps per second: 254, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.667 [0.000, 3.000],  loss: 323652.927083, mae: 1758.691935, accuracy: 0.197917, mean_q: -1531.217163, mean_eps: 0.100000\n",
      " 16311/50000: episode: 2631, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 254334.781250, mae: 1712.158203, accuracy: 0.156250, mean_q: -1524.960368, mean_eps: 0.100000\n",
      " 16314/50000: episode: 2632, duration: 0.015s, episode steps:   3, steps per second: 204, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 281228.692708, mae: 1728.766439, accuracy: 0.208333, mean_q: -1504.222941, mean_eps: 0.100000\n",
      " 16317/50000: episode: 2633, duration: 0.014s, episode steps:   3, steps per second: 216, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 249734.505208, mae: 1711.640015, accuracy: 0.197917, mean_q: -1506.054972, mean_eps: 0.100000\n",
      " 16320/50000: episode: 2634, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 261043.703125, mae: 1717.627035, accuracy: 0.260417, mean_q: -1475.169678, mean_eps: 0.100000\n",
      " 16323/50000: episode: 2635, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 271012.489583, mae: 1723.094523, accuracy: 0.156250, mean_q: -1492.727620, mean_eps: 0.100000\n",
      " 16326/50000: episode: 2636, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 196655.677083, mae: 1658.385091, accuracy: 0.229167, mean_q: -1495.899455, mean_eps: 0.100000\n",
      " 16329/50000: episode: 2637, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 276752.296875, mae: 1737.647502, accuracy: 0.145833, mean_q: -1528.841227, mean_eps: 0.100000\n",
      " 16332/50000: episode: 2638, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 245781.815104, mae: 1712.293050, accuracy: 0.187500, mean_q: -1501.088053, mean_eps: 0.100000\n",
      " 16336/50000: episode: 2639, duration: 0.015s, episode steps:   4, steps per second: 264, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 216844.792969, mae: 1713.809357, accuracy: 0.234375, mean_q: -1529.357880, mean_eps: 0.100000\n",
      " 16340/50000: episode: 2640, duration: 0.015s, episode steps:   4, steps per second: 262, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 289010.281250, mae: 1740.581909, accuracy: 0.148438, mean_q: -1504.430908, mean_eps: 0.100000\n",
      " 16343/50000: episode: 2641, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 289898.953125, mae: 1724.168416, accuracy: 0.239583, mean_q: -1481.546305, mean_eps: 0.100000\n",
      " 16346/50000: episode: 2642, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 278346.171875, mae: 1722.512777, accuracy: 0.197917, mean_q: -1485.477132, mean_eps: 0.100000\n",
      " 16350/50000: episode: 2643, duration: 0.015s, episode steps:   4, steps per second: 267, episode reward: -1238.000, mean reward: -309.500 [-999.000, -45.000], mean action: 1.750 [0.000, 3.000],  loss: 240566.089844, mae: 1694.437622, accuracy: 0.140625, mean_q: -1476.022736, mean_eps: 0.100000\n",
      " 16353/50000: episode: 2644, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 248417.052083, mae: 1703.674479, accuracy: 0.135417, mean_q: -1517.275920, mean_eps: 0.100000\n",
      " 16357/50000: episode: 2645, duration: 0.016s, episode steps:   4, steps per second: 251, episode reward: -1223.000, mean reward: -305.750 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 202673.546875, mae: 1677.030457, accuracy: 0.148438, mean_q: -1488.461090, mean_eps: 0.100000\n",
      " 16360/50000: episode: 2646, duration: 0.014s, episode steps:   3, steps per second: 221, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 263708.921875, mae: 1717.023315, accuracy: 0.166667, mean_q: -1515.676799, mean_eps: 0.100000\n",
      " 16363/50000: episode: 2647, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 272780.276042, mae: 1694.872030, accuracy: 0.197917, mean_q: -1487.126506, mean_eps: 0.100000\n",
      " 16366/50000: episode: 2648, duration: 0.015s, episode steps:   3, steps per second: 203, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 315282.270833, mae: 1727.977946, accuracy: 0.166667, mean_q: -1504.278117, mean_eps: 0.100000\n",
      " 16370/50000: episode: 2649, duration: 0.021s, episode steps:   4, steps per second: 194, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 243673.195312, mae: 1700.285309, accuracy: 0.210938, mean_q: -1471.447083, mean_eps: 0.100000\n",
      " 16373/50000: episode: 2650, duration: 0.014s, episode steps:   3, steps per second: 220, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 234607.838542, mae: 1691.001628, accuracy: 0.177083, mean_q: -1510.154867, mean_eps: 0.100000\n",
      " 16376/50000: episode: 2651, duration: 0.014s, episode steps:   3, steps per second: 222, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 246968.510417, mae: 1704.829956, accuracy: 0.125000, mean_q: -1499.765381, mean_eps: 0.100000\n",
      " 16379/50000: episode: 2652, duration: 0.013s, episode steps:   3, steps per second: 224, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 250453.437500, mae: 1689.533122, accuracy: 0.166667, mean_q: -1484.672160, mean_eps: 0.100000\n",
      " 16382/50000: episode: 2653, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 264014.552083, mae: 1676.321167, accuracy: 0.166667, mean_q: -1453.275594, mean_eps: 0.100000\n",
      " 16385/50000: episode: 2654, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -224.000, mean reward: -74.667 [-116.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 249625.510417, mae: 1680.809448, accuracy: 0.166667, mean_q: -1453.381510, mean_eps: 0.100000\n",
      " 16388/50000: episode: 2655, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 205835.937500, mae: 1641.997396, accuracy: 0.239583, mean_q: -1451.587077, mean_eps: 0.100000\n",
      " 16391/50000: episode: 2656, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 252744.625000, mae: 1681.473389, accuracy: 0.177083, mean_q: -1465.577596, mean_eps: 0.100000\n",
      " 16394/50000: episode: 2657, duration: 0.013s, episode steps:   3, steps per second: 227, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 304013.015625, mae: 1717.894613, accuracy: 0.166667, mean_q: -1488.753133, mean_eps: 0.100000\n",
      " 16397/50000: episode: 2658, duration: 0.014s, episode steps:   3, steps per second: 215, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 256628.270833, mae: 1693.551270, accuracy: 0.197917, mean_q: -1472.799723, mean_eps: 0.100000\n",
      " 16400/50000: episode: 2659, duration: 0.014s, episode steps:   3, steps per second: 211, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 237437.166667, mae: 1703.987020, accuracy: 0.197917, mean_q: -1476.148763, mean_eps: 0.100000\n",
      " 16403/50000: episode: 2660, duration: 0.014s, episode steps:   3, steps per second: 216, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 206995.442708, mae: 1667.226644, accuracy: 0.229167, mean_q: -1481.433594, mean_eps: 0.100000\n",
      " 16406/50000: episode: 2661, duration: 0.020s, episode steps:   3, steps per second: 149, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 258124.270833, mae: 1682.398966, accuracy: 0.187500, mean_q: -1445.890584, mean_eps: 0.100000\n",
      " 16409/50000: episode: 2662, duration: 0.015s, episode steps:   3, steps per second: 196, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 240405.187500, mae: 1685.573893, accuracy: 0.229167, mean_q: -1440.524170, mean_eps: 0.100000\n",
      " 16412/50000: episode: 2663, duration: 0.014s, episode steps:   3, steps per second: 208, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 210743.942708, mae: 1668.023560, accuracy: 0.250000, mean_q: -1451.124715, mean_eps: 0.100000\n",
      " 16415/50000: episode: 2664, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 246559.432292, mae: 1681.936442, accuracy: 0.187500, mean_q: -1460.435221, mean_eps: 0.100000\n",
      " 16419/50000: episode: 2665, duration: 0.016s, episode steps:   4, steps per second: 255, episode reward: -1238.000, mean reward: -309.500 [-999.000, -60.000], mean action: 1.750 [1.000, 3.000],  loss: 200392.796875, mae: 1660.009430, accuracy: 0.156250, mean_q: -1469.955353, mean_eps: 0.100000\n",
      " 16422/50000: episode: 2666, duration: 0.013s, episode steps:   3, steps per second: 225, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 212528.658854, mae: 1660.750895, accuracy: 0.218750, mean_q: -1460.392131, mean_eps: 0.100000\n",
      " 16425/50000: episode: 2667, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 181884.796875, mae: 1650.774658, accuracy: 0.177083, mean_q: -1442.418783, mean_eps: 0.100000\n",
      " 16428/50000: episode: 2668, duration: 0.013s, episode steps:   3, steps per second: 230, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 231768.401042, mae: 1678.889730, accuracy: 0.156250, mean_q: -1492.809652, mean_eps: 0.100000\n",
      " 16431/50000: episode: 2669, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 240973.593750, mae: 1695.923503, accuracy: 0.166667, mean_q: -1480.186849, mean_eps: 0.100000\n",
      " 16434/50000: episode: 2670, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 219354.109375, mae: 1675.500854, accuracy: 0.218750, mean_q: -1465.913981, mean_eps: 0.100000\n",
      " 16438/50000: episode: 2671, duration: 0.016s, episode steps:   4, steps per second: 245, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 222020.417969, mae: 1679.866364, accuracy: 0.132812, mean_q: -1463.914581, mean_eps: 0.100000\n",
      " 16441/50000: episode: 2672, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.667 [0.000, 3.000],  loss: 205857.041667, mae: 1682.896932, accuracy: 0.187500, mean_q: -1484.688477, mean_eps: 0.100000\n",
      " 16444/50000: episode: 2673, duration: 0.014s, episode steps:   3, steps per second: 212, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 245221.921875, mae: 1692.988892, accuracy: 0.187500, mean_q: -1481.276164, mean_eps: 0.100000\n",
      " 16447/50000: episode: 2674, duration: 0.014s, episode steps:   3, steps per second: 212, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 208268.364583, mae: 1694.666951, accuracy: 0.187500, mean_q: -1476.864095, mean_eps: 0.100000\n",
      " 16450/50000: episode: 2675, duration: 0.014s, episode steps:   3, steps per second: 220, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 259730.692708, mae: 1708.748820, accuracy: 0.218750, mean_q: -1479.380534, mean_eps: 0.100000\n",
      " 16453/50000: episode: 2676, duration: 0.016s, episode steps:   3, steps per second: 184, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 201372.953125, mae: 1666.769897, accuracy: 0.229167, mean_q: -1470.760173, mean_eps: 0.100000\n",
      " 16456/50000: episode: 2677, duration: 0.014s, episode steps:   3, steps per second: 208, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 242667.291667, mae: 1700.942342, accuracy: 0.208333, mean_q: -1487.829224, mean_eps: 0.100000\n",
      " 16459/50000: episode: 2678, duration: 0.013s, episode steps:   3, steps per second: 224, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 213899.041667, mae: 1695.764852, accuracy: 0.166667, mean_q: -1519.337646, mean_eps: 0.100000\n",
      " 16462/50000: episode: 2679, duration: 0.012s, episode steps:   3, steps per second: 253, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 225286.854167, mae: 1682.672404, accuracy: 0.197917, mean_q: -1499.920125, mean_eps: 0.100000\n",
      " 16466/50000: episode: 2680, duration: 0.015s, episode steps:   4, steps per second: 265, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 197777.546875, mae: 1672.782990, accuracy: 0.132812, mean_q: -1484.364410, mean_eps: 0.100000\n",
      " 16469/50000: episode: 2681, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 183966.078125, mae: 1668.885864, accuracy: 0.177083, mean_q: -1478.307943, mean_eps: 0.100000\n",
      " 16473/50000: episode: 2682, duration: 0.015s, episode steps:   4, steps per second: 269, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 220200.425781, mae: 1667.334503, accuracy: 0.179688, mean_q: -1461.766022, mean_eps: 0.100000\n",
      " 16476/50000: episode: 2683, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 202147.578125, mae: 1683.803955, accuracy: 0.187500, mean_q: -1474.642293, mean_eps: 0.100000\n",
      " 16479/50000: episode: 2684, duration: 0.012s, episode steps:   3, steps per second: 256, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 260034.203125, mae: 1692.823893, accuracy: 0.145833, mean_q: -1458.970052, mean_eps: 0.100000\n",
      " 16482/50000: episode: 2685, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 232062.406250, mae: 1662.694784, accuracy: 0.177083, mean_q: -1451.614380, mean_eps: 0.100000\n",
      " 16486/50000: episode: 2686, duration: 0.015s, episode steps:   4, steps per second: 270, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 213254.679688, mae: 1679.095093, accuracy: 0.140625, mean_q: -1475.680847, mean_eps: 0.100000\n",
      " 16489/50000: episode: 2687, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 214814.489583, mae: 1669.943278, accuracy: 0.239583, mean_q: -1469.031291, mean_eps: 0.100000\n",
      " 16492/50000: episode: 2688, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 186095.343750, mae: 1670.874674, accuracy: 0.187500, mean_q: -1468.187703, mean_eps: 0.100000\n",
      " 16495/50000: episode: 2689, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 258482.520833, mae: 1675.712687, accuracy: 0.218750, mean_q: -1441.310425, mean_eps: 0.100000\n",
      " 16498/50000: episode: 2690, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 186000.296875, mae: 1648.667277, accuracy: 0.208333, mean_q: -1462.741536, mean_eps: 0.100000\n",
      " 16501/50000: episode: 2691, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 259254.343750, mae: 1659.182495, accuracy: 0.270833, mean_q: -1415.682658, mean_eps: 0.100000\n",
      " 16504/50000: episode: 2692, duration: 0.012s, episode steps:   3, steps per second: 255, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 218334.333333, mae: 1667.068319, accuracy: 0.166667, mean_q: -1436.612793, mean_eps: 0.100000\n",
      " 16507/50000: episode: 2693, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 266906.994792, mae: 1675.081706, accuracy: 0.208333, mean_q: -1465.358968, mean_eps: 0.100000\n",
      " 16510/50000: episode: 2694, duration: 0.015s, episode steps:   3, steps per second: 196, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 236744.786458, mae: 1678.038005, accuracy: 0.145833, mean_q: -1443.192261, mean_eps: 0.100000\n",
      " 16513/50000: episode: 2695, duration: 0.014s, episode steps:   3, steps per second: 207, episode reward: -224.000, mean reward: -74.667 [-116.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 257575.041667, mae: 1668.184692, accuracy: 0.177083, mean_q: -1428.371582, mean_eps: 0.100000\n",
      " 16516/50000: episode: 2696, duration: 0.013s, episode steps:   3, steps per second: 227, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 217491.041667, mae: 1646.313639, accuracy: 0.156250, mean_q: -1412.153646, mean_eps: 0.100000\n",
      " 16520/50000: episode: 2697, duration: 0.016s, episode steps:   4, steps per second: 256, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 2.250 [1.000, 3.000],  loss: 226717.019531, mae: 1657.817139, accuracy: 0.195312, mean_q: -1419.001404, mean_eps: 0.100000\n",
      " 16524/50000: episode: 2698, duration: 0.015s, episode steps:   4, steps per second: 263, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 238073.433594, mae: 1620.534668, accuracy: 0.171875, mean_q: -1384.999451, mean_eps: 0.100000\n",
      " 16527/50000: episode: 2699, duration: 0.012s, episode steps:   3, steps per second: 255, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 211045.572917, mae: 1616.298787, accuracy: 0.197917, mean_q: -1384.143962, mean_eps: 0.100000\n",
      " 16531/50000: episode: 2700, duration: 0.015s, episode steps:   4, steps per second: 261, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 208924.015625, mae: 1628.621552, accuracy: 0.140625, mean_q: -1397.961639, mean_eps: 0.100000\n",
      " 16534/50000: episode: 2701, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 203264.312500, mae: 1599.232463, accuracy: 0.156250, mean_q: -1387.717448, mean_eps: 0.100000\n",
      " 16537/50000: episode: 2702, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 241148.046875, mae: 1632.844930, accuracy: 0.156250, mean_q: -1400.156047, mean_eps: 0.100000\n",
      " 16541/50000: episode: 2703, duration: 0.015s, episode steps:   4, steps per second: 266, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 224630.378906, mae: 1602.946289, accuracy: 0.187500, mean_q: -1381.408752, mean_eps: 0.100000\n",
      " 16545/50000: episode: 2704, duration: 0.015s, episode steps:   4, steps per second: 266, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 2.250 [1.000, 3.000],  loss: 194611.085938, mae: 1617.865936, accuracy: 0.203125, mean_q: -1383.284424, mean_eps: 0.100000\n",
      " 16548/50000: episode: 2705, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 198298.833333, mae: 1599.195719, accuracy: 0.260417, mean_q: -1371.425985, mean_eps: 0.100000\n",
      " 16552/50000: episode: 2706, duration: 0.015s, episode steps:   4, steps per second: 264, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 210664.800781, mae: 1628.768890, accuracy: 0.125000, mean_q: -1394.453400, mean_eps: 0.100000\n",
      " 16555/50000: episode: 2707, duration: 0.014s, episode steps:   3, steps per second: 207, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 225026.552083, mae: 1628.781006, accuracy: 0.156250, mean_q: -1364.319499, mean_eps: 0.100000\n",
      " 16558/50000: episode: 2708, duration: 0.014s, episode steps:   3, steps per second: 215, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 176372.755208, mae: 1622.258586, accuracy: 0.125000, mean_q: -1396.089193, mean_eps: 0.100000\n",
      " 16561/50000: episode: 2709, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 144731.489583, mae: 1593.860840, accuracy: 0.291667, mean_q: -1379.286051, mean_eps: 0.100000\n",
      " 16564/50000: episode: 2710, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 195594.697917, mae: 1604.001383, accuracy: 0.208333, mean_q: -1388.712565, mean_eps: 0.100000\n",
      " 16567/50000: episode: 2711, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 209500.500000, mae: 1620.405273, accuracy: 0.156250, mean_q: -1372.075236, mean_eps: 0.100000\n",
      " 16570/50000: episode: 2712, duration: 0.012s, episode steps:   3, steps per second: 257, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 254816.390625, mae: 1651.783081, accuracy: 0.166667, mean_q: -1394.442871, mean_eps: 0.100000\n",
      " 16574/50000: episode: 2713, duration: 0.015s, episode steps:   4, steps per second: 259, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.000 [0.000, 3.000],  loss: 178162.687500, mae: 1612.811188, accuracy: 0.242188, mean_q: -1384.915192, mean_eps: 0.100000\n",
      " 16577/50000: episode: 2714, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 198845.622396, mae: 1591.492350, accuracy: 0.166667, mean_q: -1381.068319, mean_eps: 0.100000\n",
      " 16580/50000: episode: 2715, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 241326.437500, mae: 1625.566162, accuracy: 0.156250, mean_q: -1382.228678, mean_eps: 0.100000\n",
      " 16583/50000: episode: 2716, duration: 0.012s, episode steps:   3, steps per second: 259, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 230548.890625, mae: 1626.236084, accuracy: 0.135417, mean_q: -1383.771566, mean_eps: 0.100000\n",
      " 16586/50000: episode: 2717, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 261594.833333, mae: 1646.499105, accuracy: 0.125000, mean_q: -1383.624186, mean_eps: 0.100000\n",
      " 16589/50000: episode: 2718, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 184459.317708, mae: 1581.110189, accuracy: 0.208333, mean_q: -1354.117716, mean_eps: 0.100000\n",
      " 16592/50000: episode: 2719, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 229655.062500, mae: 1605.880168, accuracy: 0.197917, mean_q: -1351.075358, mean_eps: 0.100000\n",
      " 16595/50000: episode: 2720, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 228032.041667, mae: 1616.991333, accuracy: 0.218750, mean_q: -1350.106689, mean_eps: 0.100000\n",
      " 16598/50000: episode: 2721, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 191581.796875, mae: 1601.748535, accuracy: 0.166667, mean_q: -1337.218384, mean_eps: 0.100000\n",
      " 16601/50000: episode: 2722, duration: 0.015s, episode steps:   3, steps per second: 202, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 193207.197917, mae: 1601.003377, accuracy: 0.166667, mean_q: -1359.528117, mean_eps: 0.100000\n",
      " 16604/50000: episode: 2723, duration: 0.017s, episode steps:   3, steps per second: 175, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 222703.781250, mae: 1593.702352, accuracy: 0.270833, mean_q: -1326.336344, mean_eps: 0.100000\n",
      " 16607/50000: episode: 2724, duration: 0.015s, episode steps:   3, steps per second: 195, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 196169.223958, mae: 1573.158122, accuracy: 0.156250, mean_q: -1338.094482, mean_eps: 0.100000\n",
      " 16610/50000: episode: 2725, duration: 0.018s, episode steps:   3, steps per second: 167, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 210174.250000, mae: 1604.366455, accuracy: 0.250000, mean_q: -1345.884277, mean_eps: 0.100000\n",
      " 16613/50000: episode: 2726, duration: 0.014s, episode steps:   3, steps per second: 210, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 233970.645833, mae: 1624.620239, accuracy: 0.218750, mean_q: -1341.740112, mean_eps: 0.100000\n",
      " 16616/50000: episode: 2727, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 228143.000000, mae: 1602.661255, accuracy: 0.166667, mean_q: -1350.639242, mean_eps: 0.100000\n",
      " 16619/50000: episode: 2728, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 169344.658854, mae: 1579.023153, accuracy: 0.239583, mean_q: -1377.119588, mean_eps: 0.100000\n",
      " 16622/50000: episode: 2729, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 176873.716146, mae: 1576.038778, accuracy: 0.281250, mean_q: -1327.693563, mean_eps: 0.100000\n",
      " 16625/50000: episode: 2730, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 202566.354167, mae: 1609.116984, accuracy: 0.114583, mean_q: -1374.628784, mean_eps: 0.100000\n",
      " 16628/50000: episode: 2731, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 151354.260417, mae: 1551.682536, accuracy: 0.197917, mean_q: -1324.332031, mean_eps: 0.100000\n",
      " 16631/50000: episode: 2732, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 241780.552083, mae: 1608.632324, accuracy: 0.156250, mean_q: -1344.668009, mean_eps: 0.100000\n",
      " 16634/50000: episode: 2733, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 193930.515625, mae: 1598.647990, accuracy: 0.135417, mean_q: -1344.798462, mean_eps: 0.100000\n",
      " 16637/50000: episode: 2734, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 157708.802083, mae: 1576.778158, accuracy: 0.177083, mean_q: -1346.800944, mean_eps: 0.100000\n",
      " 16640/50000: episode: 2735, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 223558.411458, mae: 1593.909668, accuracy: 0.260417, mean_q: -1335.078817, mean_eps: 0.100000\n",
      " 16643/50000: episode: 2736, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 220988.989583, mae: 1583.397624, accuracy: 0.208333, mean_q: -1326.679810, mean_eps: 0.100000\n",
      " 16646/50000: episode: 2737, duration: 0.012s, episode steps:   3, steps per second: 254, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 207144.627604, mae: 1570.566732, accuracy: 0.229167, mean_q: -1321.270223, mean_eps: 0.100000\n",
      " 16649/50000: episode: 2738, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 212845.567708, mae: 1590.478068, accuracy: 0.177083, mean_q: -1336.568644, mean_eps: 0.100000\n",
      " 16652/50000: episode: 2739, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 187297.041667, mae: 1597.906779, accuracy: 0.114583, mean_q: -1353.977336, mean_eps: 0.100000\n",
      " 16655/50000: episode: 2740, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 208378.208333, mae: 1612.219808, accuracy: 0.135417, mean_q: -1364.231405, mean_eps: 0.100000\n",
      " 16658/50000: episode: 2741, duration: 0.014s, episode steps:   3, steps per second: 219, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 195145.083333, mae: 1581.443359, accuracy: 0.229167, mean_q: -1326.685506, mean_eps: 0.100000\n",
      " 16661/50000: episode: 2742, duration: 0.014s, episode steps:   3, steps per second: 216, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 181726.635417, mae: 1573.105550, accuracy: 0.177083, mean_q: -1312.600871, mean_eps: 0.100000\n",
      " 16664/50000: episode: 2743, duration: 0.014s, episode steps:   3, steps per second: 215, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 204313.437500, mae: 1588.102702, accuracy: 0.270833, mean_q: -1339.169840, mean_eps: 0.100000\n",
      " 16667/50000: episode: 2744, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 192628.567708, mae: 1601.575114, accuracy: 0.125000, mean_q: -1342.289714, mean_eps: 0.100000\n",
      " 16671/50000: episode: 2745, duration: 0.016s, episode steps:   4, steps per second: 257, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 232337.125000, mae: 1608.804382, accuracy: 0.179688, mean_q: -1357.404938, mean_eps: 0.100000\n",
      " 16675/50000: episode: 2746, duration: 0.015s, episode steps:   4, steps per second: 264, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 197026.046875, mae: 1577.429138, accuracy: 0.210938, mean_q: -1331.975342, mean_eps: 0.100000\n",
      " 16678/50000: episode: 2747, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 221575.687500, mae: 1594.200236, accuracy: 0.218750, mean_q: -1321.303630, mean_eps: 0.100000\n",
      " 16681/50000: episode: 2748, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 253246.869792, mae: 1624.309570, accuracy: 0.135417, mean_q: -1335.641846, mean_eps: 0.100000\n",
      " 16684/50000: episode: 2749, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 211052.057292, mae: 1572.986898, accuracy: 0.187500, mean_q: -1294.924642, mean_eps: 0.100000\n",
      " 16687/50000: episode: 2750, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 236859.380208, mae: 1563.783040, accuracy: 0.218750, mean_q: -1260.374797, mean_eps: 0.100000\n",
      " 16690/50000: episode: 2751, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 205390.213542, mae: 1542.957153, accuracy: 0.125000, mean_q: -1245.315430, mean_eps: 0.100000\n",
      " 16693/50000: episode: 2752, duration: 0.012s, episode steps:   3, steps per second: 254, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 215198.916667, mae: 1559.434163, accuracy: 0.187500, mean_q: -1278.478434, mean_eps: 0.100000\n",
      " 16696/50000: episode: 2753, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 210674.755208, mae: 1535.718302, accuracy: 0.260417, mean_q: -1252.581665, mean_eps: 0.100000\n",
      " 16699/50000: episode: 2754, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 214352.953125, mae: 1554.909831, accuracy: 0.177083, mean_q: -1281.661011, mean_eps: 0.100000\n",
      " 16702/50000: episode: 2755, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 191117.848958, mae: 1533.181844, accuracy: 0.187500, mean_q: -1247.598185, mean_eps: 0.100000\n",
      " 16705/50000: episode: 2756, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 207707.260417, mae: 1542.176839, accuracy: 0.218750, mean_q: -1265.540080, mean_eps: 0.100000\n",
      " 16709/50000: episode: 2757, duration: 0.018s, episode steps:   4, steps per second: 223, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 204127.238281, mae: 1541.437836, accuracy: 0.226562, mean_q: -1281.387634, mean_eps: 0.100000\n",
      " 16712/50000: episode: 2758, duration: 0.014s, episode steps:   3, steps per second: 210, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 200122.447917, mae: 1559.428223, accuracy: 0.229167, mean_q: -1287.150146, mean_eps: 0.100000\n",
      " 16715/50000: episode: 2759, duration: 0.013s, episode steps:   3, steps per second: 226, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 232043.770833, mae: 1578.659180, accuracy: 0.177083, mean_q: -1271.111532, mean_eps: 0.100000\n",
      " 16719/50000: episode: 2760, duration: 0.016s, episode steps:   4, steps per second: 251, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 196165.769531, mae: 1530.137329, accuracy: 0.296875, mean_q: -1269.589081, mean_eps: 0.100000\n",
      " 16722/50000: episode: 2761, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 188285.156250, mae: 1539.292155, accuracy: 0.166667, mean_q: -1299.607788, mean_eps: 0.100000\n",
      " 16725/50000: episode: 2762, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.333 [0.000, 3.000],  loss: 181693.343750, mae: 1540.710897, accuracy: 0.166667, mean_q: -1313.728068, mean_eps: 0.100000\n",
      " 16728/50000: episode: 2763, duration: 0.012s, episode steps:   3, steps per second: 254, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 192872.020833, mae: 1537.313232, accuracy: 0.197917, mean_q: -1276.868774, mean_eps: 0.100000\n",
      " 16731/50000: episode: 2764, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 198478.390625, mae: 1541.738892, accuracy: 0.145833, mean_q: -1285.605428, mean_eps: 0.100000\n",
      " 16734/50000: episode: 2765, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 177056.802083, mae: 1552.840332, accuracy: 0.177083, mean_q: -1281.086548, mean_eps: 0.100000\n",
      " 16737/50000: episode: 2766, duration: 0.012s, episode steps:   3, steps per second: 256, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 183049.989583, mae: 1543.850708, accuracy: 0.156250, mean_q: -1279.985962, mean_eps: 0.100000\n",
      " 16740/50000: episode: 2767, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 150334.593750, mae: 1529.806112, accuracy: 0.145833, mean_q: -1289.682373, mean_eps: 0.100000\n",
      " 16743/50000: episode: 2768, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 206389.906250, mae: 1543.703084, accuracy: 0.166667, mean_q: -1282.685710, mean_eps: 0.100000\n",
      " 16746/50000: episode: 2769, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 195056.718750, mae: 1534.854248, accuracy: 0.197917, mean_q: -1248.693766, mean_eps: 0.100000\n",
      " 16749/50000: episode: 2770, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 184432.255208, mae: 1523.582438, accuracy: 0.156250, mean_q: -1276.663493, mean_eps: 0.100000\n",
      " 16752/50000: episode: 2771, duration: 0.014s, episode steps:   3, steps per second: 219, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 154004.770833, mae: 1520.199748, accuracy: 0.187500, mean_q: -1264.881226, mean_eps: 0.100000\n",
      " 16755/50000: episode: 2772, duration: 0.015s, episode steps:   3, steps per second: 199, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 162681.143229, mae: 1529.430013, accuracy: 0.250000, mean_q: -1270.462158, mean_eps: 0.100000\n",
      " 16758/50000: episode: 2773, duration: 0.016s, episode steps:   3, steps per second: 192, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 185743.140625, mae: 1533.055745, accuracy: 0.177083, mean_q: -1273.234049, mean_eps: 0.100000\n",
      " 16761/50000: episode: 2774, duration: 0.015s, episode steps:   3, steps per second: 195, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 182780.458333, mae: 1533.968872, accuracy: 0.250000, mean_q: -1261.207642, mean_eps: 0.100000\n",
      " 16764/50000: episode: 2775, duration: 0.014s, episode steps:   3, steps per second: 219, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 200109.270833, mae: 1550.755778, accuracy: 0.197917, mean_q: -1249.942505, mean_eps: 0.100000\n",
      " 16767/50000: episode: 2776, duration: 0.014s, episode steps:   3, steps per second: 220, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 190442.484375, mae: 1552.300537, accuracy: 0.145833, mean_q: -1293.366170, mean_eps: 0.100000\n",
      " 16770/50000: episode: 2777, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 137020.598958, mae: 1521.219930, accuracy: 0.218750, mean_q: -1281.133138, mean_eps: 0.100000\n",
      " 16773/50000: episode: 2778, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 136628.458333, mae: 1522.664307, accuracy: 0.156250, mean_q: -1279.687174, mean_eps: 0.100000\n",
      " 16776/50000: episode: 2779, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 157590.507812, mae: 1525.331909, accuracy: 0.208333, mean_q: -1290.046305, mean_eps: 0.100000\n",
      " 16779/50000: episode: 2780, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 192261.145833, mae: 1554.429525, accuracy: 0.218750, mean_q: -1274.464559, mean_eps: 0.100000\n",
      " 16782/50000: episode: 2781, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 182102.864583, mae: 1540.135986, accuracy: 0.166667, mean_q: -1283.232259, mean_eps: 0.100000\n",
      " 16785/50000: episode: 2782, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 186634.114583, mae: 1541.394653, accuracy: 0.145833, mean_q: -1304.558350, mean_eps: 0.100000\n",
      " 16788/50000: episode: 2783, duration: 0.012s, episode steps:   3, steps per second: 255, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 166114.606771, mae: 1531.668457, accuracy: 0.166667, mean_q: -1268.753377, mean_eps: 0.100000\n",
      " 16791/50000: episode: 2784, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 154224.270833, mae: 1495.406616, accuracy: 0.187500, mean_q: -1260.857300, mean_eps: 0.100000\n",
      " 16796/50000: episode: 2785, duration: 0.018s, episode steps:   5, steps per second: 276, episode reward: -2193.000, mean reward: -438.600 [-999.000, -45.000], mean action: 1.800 [0.000, 3.000],  loss: 193756.009375, mae: 1535.644775, accuracy: 0.187500, mean_q: -1264.771582, mean_eps: 0.100000\n",
      " 16799/50000: episode: 2786, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 154378.869792, mae: 1505.921712, accuracy: 0.208333, mean_q: -1237.869385, mean_eps: 0.100000\n",
      " 16802/50000: episode: 2787, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 184375.083333, mae: 1531.826294, accuracy: 0.156250, mean_q: -1253.541748, mean_eps: 0.100000\n",
      " 16805/50000: episode: 2788, duration: 0.018s, episode steps:   3, steps per second: 171, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 143018.981771, mae: 1492.747152, accuracy: 0.187500, mean_q: -1249.086670, mean_eps: 0.100000\n",
      " 16808/50000: episode: 2789, duration: 0.014s, episode steps:   3, steps per second: 212, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 198942.421875, mae: 1551.363118, accuracy: 0.125000, mean_q: -1269.279134, mean_eps: 0.100000\n",
      " 16811/50000: episode: 2790, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 164180.854167, mae: 1529.213298, accuracy: 0.229167, mean_q: -1280.394328, mean_eps: 0.100000\n",
      " 16814/50000: episode: 2791, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 168497.312500, mae: 1522.949626, accuracy: 0.197917, mean_q: -1276.696330, mean_eps: 0.100000\n",
      " 16817/50000: episode: 2792, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 177675.723958, mae: 1533.176025, accuracy: 0.229167, mean_q: -1267.993652, mean_eps: 0.100000\n",
      " 16820/50000: episode: 2793, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 204789.072917, mae: 1557.978312, accuracy: 0.208333, mean_q: -1280.088053, mean_eps: 0.100000\n",
      " 16823/50000: episode: 2794, duration: 0.012s, episode steps:   3, steps per second: 254, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 166862.302083, mae: 1520.437948, accuracy: 0.197917, mean_q: -1257.245361, mean_eps: 0.100000\n",
      " 16827/50000: episode: 2795, duration: 0.015s, episode steps:   4, steps per second: 263, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 174864.843750, mae: 1546.900909, accuracy: 0.171875, mean_q: -1264.475525, mean_eps: 0.100000\n",
      " 16830/50000: episode: 2796, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 165018.682292, mae: 1522.602783, accuracy: 0.197917, mean_q: -1256.565959, mean_eps: 0.100000\n",
      " 16833/50000: episode: 2797, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 206900.593750, mae: 1536.519735, accuracy: 0.218750, mean_q: -1248.731608, mean_eps: 0.100000\n",
      " 16836/50000: episode: 2798, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 195557.239583, mae: 1506.414347, accuracy: 0.197917, mean_q: -1220.367635, mean_eps: 0.100000\n",
      " 16839/50000: episode: 2799, duration: 0.012s, episode steps:   3, steps per second: 256, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 152877.890625, mae: 1493.022705, accuracy: 0.291667, mean_q: -1248.798381, mean_eps: 0.100000\n",
      " 16842/50000: episode: 2800, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 178106.979167, mae: 1511.947550, accuracy: 0.218750, mean_q: -1226.524333, mean_eps: 0.100000\n",
      " 16845/50000: episode: 2801, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 208240.854167, mae: 1520.006999, accuracy: 0.156250, mean_q: -1221.068115, mean_eps: 0.100000\n",
      " 16849/50000: episode: 2802, duration: 0.016s, episode steps:   4, steps per second: 250, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 2.250 [1.000, 3.000],  loss: 152755.226562, mae: 1482.967377, accuracy: 0.218750, mean_q: -1225.932251, mean_eps: 0.100000\n",
      " 16852/50000: episode: 2803, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 225125.958333, mae: 1533.206055, accuracy: 0.197917, mean_q: -1203.567790, mean_eps: 0.100000\n",
      " 16855/50000: episode: 2804, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 159904.062500, mae: 1491.847005, accuracy: 0.187500, mean_q: -1237.825358, mean_eps: 0.100000\n",
      " 16858/50000: episode: 2805, duration: 0.015s, episode steps:   3, steps per second: 200, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 201954.921875, mae: 1543.822835, accuracy: 0.072917, mean_q: -1232.127075, mean_eps: 0.100000\n",
      " 16861/50000: episode: 2806, duration: 0.014s, episode steps:   3, steps per second: 212, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 164379.442708, mae: 1499.002767, accuracy: 0.187500, mean_q: -1217.970703, mean_eps: 0.100000\n",
      " 16864/50000: episode: 2807, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 192247.213542, mae: 1498.511963, accuracy: 0.177083, mean_q: -1219.626506, mean_eps: 0.100000\n",
      " 16868/50000: episode: 2808, duration: 0.015s, episode steps:   4, steps per second: 266, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 204317.910156, mae: 1508.063782, accuracy: 0.140625, mean_q: -1212.481689, mean_eps: 0.100000\n",
      " 16871/50000: episode: 2809, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 169818.294271, mae: 1480.716919, accuracy: 0.208333, mean_q: -1207.548503, mean_eps: 0.100000\n",
      " 16874/50000: episode: 2810, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 193720.500000, mae: 1521.326009, accuracy: 0.135417, mean_q: -1244.194092, mean_eps: 0.100000\n",
      " 16877/50000: episode: 2811, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 168932.119792, mae: 1488.269368, accuracy: 0.145833, mean_q: -1215.405192, mean_eps: 0.100000\n",
      " 16880/50000: episode: 2812, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 180422.031250, mae: 1502.244954, accuracy: 0.187500, mean_q: -1194.279622, mean_eps: 0.100000\n",
      " 16883/50000: episode: 2813, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 153064.335938, mae: 1460.313354, accuracy: 0.239583, mean_q: -1179.756877, mean_eps: 0.100000\n",
      " 16886/50000: episode: 2814, duration: 0.012s, episode steps:   3, steps per second: 257, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 183118.119792, mae: 1486.243449, accuracy: 0.166667, mean_q: -1198.142456, mean_eps: 0.100000\n",
      " 16889/50000: episode: 2815, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 188834.755208, mae: 1498.388672, accuracy: 0.177083, mean_q: -1205.624471, mean_eps: 0.100000\n",
      " 16892/50000: episode: 2816, duration: 0.014s, episode steps:   3, steps per second: 221, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 184020.114583, mae: 1485.952718, accuracy: 0.156250, mean_q: -1178.896525, mean_eps: 0.100000\n",
      " 16895/50000: episode: 2817, duration: 0.014s, episode steps:   3, steps per second: 215, episode reward: -224.000, mean reward: -74.667 [-116.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 178398.234375, mae: 1495.270996, accuracy: 0.135417, mean_q: -1201.231567, mean_eps: 0.100000\n",
      " 16898/50000: episode: 2818, duration: 0.014s, episode steps:   3, steps per second: 220, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 199736.507812, mae: 1478.056478, accuracy: 0.218750, mean_q: -1186.081624, mean_eps: 0.100000\n",
      " 16901/50000: episode: 2819, duration: 0.015s, episode steps:   3, steps per second: 203, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 183228.531250, mae: 1484.714600, accuracy: 0.197917, mean_q: -1203.895508, mean_eps: 0.100000\n",
      " 16904/50000: episode: 2820, duration: 0.015s, episode steps:   3, steps per second: 195, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 145968.380208, mae: 1452.290690, accuracy: 0.125000, mean_q: -1202.773722, mean_eps: 0.100000\n",
      " 16908/50000: episode: 2821, duration: 0.018s, episode steps:   4, steps per second: 220, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 170598.878906, mae: 1468.516510, accuracy: 0.156250, mean_q: -1185.939758, mean_eps: 0.100000\n",
      " 16911/50000: episode: 2822, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 151945.921875, mae: 1476.881999, accuracy: 0.197917, mean_q: -1196.677653, mean_eps: 0.100000\n",
      " 16915/50000: episode: 2823, duration: 0.016s, episode steps:   4, steps per second: 244, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 139547.923828, mae: 1456.458282, accuracy: 0.179688, mean_q: -1203.810547, mean_eps: 0.100000\n",
      " 16918/50000: episode: 2824, duration: 0.014s, episode steps:   3, steps per second: 210, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 167940.843750, mae: 1478.010539, accuracy: 0.187500, mean_q: -1171.739583, mean_eps: 0.100000\n",
      " 16921/50000: episode: 2825, duration: 0.015s, episode steps:   3, steps per second: 205, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 162396.734375, mae: 1459.393962, accuracy: 0.208333, mean_q: -1189.056274, mean_eps: 0.100000\n",
      " 16924/50000: episode: 2826, duration: 0.014s, episode steps:   3, steps per second: 209, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 183304.255208, mae: 1502.096924, accuracy: 0.177083, mean_q: -1225.150960, mean_eps: 0.100000\n",
      " 16928/50000: episode: 2827, duration: 0.017s, episode steps:   4, steps per second: 240, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 131139.097656, mae: 1461.591827, accuracy: 0.203125, mean_q: -1203.878845, mean_eps: 0.100000\n",
      " 16931/50000: episode: 2828, duration: 0.013s, episode steps:   3, steps per second: 225, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 147359.895833, mae: 1466.657227, accuracy: 0.229167, mean_q: -1183.354736, mean_eps: 0.100000\n",
      " 16934/50000: episode: 2829, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 174031.166667, mae: 1493.615804, accuracy: 0.197917, mean_q: -1216.444987, mean_eps: 0.100000\n",
      " 16937/50000: episode: 2830, duration: 0.013s, episode steps:   3, steps per second: 230, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 187578.614583, mae: 1480.151937, accuracy: 0.156250, mean_q: -1196.654622, mean_eps: 0.100000\n",
      " 16940/50000: episode: 2831, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 185203.713542, mae: 1476.472738, accuracy: 0.197917, mean_q: -1181.050090, mean_eps: 0.100000\n",
      " 16943/50000: episode: 2832, duration: 0.013s, episode steps:   3, steps per second: 227, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 158102.651042, mae: 1447.690348, accuracy: 0.177083, mean_q: -1181.095744, mean_eps: 0.100000\n",
      " 16946/50000: episode: 2833, duration: 0.014s, episode steps:   3, steps per second: 208, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 127728.598958, mae: 1427.112386, accuracy: 0.218750, mean_q: -1161.068359, mean_eps: 0.100000\n",
      " 16949/50000: episode: 2834, duration: 0.016s, episode steps:   3, steps per second: 185, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 153682.609375, mae: 1473.333822, accuracy: 0.177083, mean_q: -1183.044515, mean_eps: 0.100000\n",
      " 16952/50000: episode: 2835, duration: 0.015s, episode steps:   3, steps per second: 197, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 212346.156250, mae: 1474.405436, accuracy: 0.145833, mean_q: -1169.540202, mean_eps: 0.100000\n",
      " 16955/50000: episode: 2836, duration: 0.014s, episode steps:   3, steps per second: 207, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 175245.494792, mae: 1476.594401, accuracy: 0.135417, mean_q: -1166.722249, mean_eps: 0.100000\n",
      " 16959/50000: episode: 2837, duration: 0.018s, episode steps:   4, steps per second: 217, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 133447.064453, mae: 1439.282471, accuracy: 0.218750, mean_q: -1181.950500, mean_eps: 0.100000\n",
      " 16962/50000: episode: 2838, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 148635.229167, mae: 1441.628988, accuracy: 0.218750, mean_q: -1165.631185, mean_eps: 0.100000\n",
      " 16965/50000: episode: 2839, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 168485.989583, mae: 1472.194946, accuracy: 0.145833, mean_q: -1184.013102, mean_eps: 0.100000\n",
      " 16968/50000: episode: 2840, duration: 0.013s, episode steps:   3, steps per second: 224, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 180501.473958, mae: 1487.830282, accuracy: 0.187500, mean_q: -1192.908285, mean_eps: 0.100000\n",
      " 16972/50000: episode: 2841, duration: 0.015s, episode steps:   4, steps per second: 267, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 145954.035156, mae: 1441.020111, accuracy: 0.148438, mean_q: -1164.671783, mean_eps: 0.100000\n",
      " 16975/50000: episode: 2842, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 179924.526042, mae: 1471.338501, accuracy: 0.166667, mean_q: -1178.529093, mean_eps: 0.100000\n",
      " 16978/50000: episode: 2843, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 170381.630208, mae: 1440.856974, accuracy: 0.197917, mean_q: -1140.227051, mean_eps: 0.100000\n",
      " 16981/50000: episode: 2844, duration: 0.012s, episode steps:   3, steps per second: 254, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 174461.036458, mae: 1467.914836, accuracy: 0.208333, mean_q: -1185.315592, mean_eps: 0.100000\n",
      " 16984/50000: episode: 2845, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 161966.578125, mae: 1466.028768, accuracy: 0.125000, mean_q: -1159.152954, mean_eps: 0.100000\n",
      " 16987/50000: episode: 2846, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 158577.619792, mae: 1444.211629, accuracy: 0.166667, mean_q: -1182.463460, mean_eps: 0.100000\n",
      " 16990/50000: episode: 2847, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 201691.182292, mae: 1466.662720, accuracy: 0.166667, mean_q: -1157.888509, mean_eps: 0.100000\n",
      " 16993/50000: episode: 2848, duration: 0.012s, episode steps:   3, steps per second: 257, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 172052.354167, mae: 1446.725220, accuracy: 0.166667, mean_q: -1141.388102, mean_eps: 0.100000\n",
      " 16996/50000: episode: 2849, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 158624.763021, mae: 1461.582642, accuracy: 0.197917, mean_q: -1152.258586, mean_eps: 0.100000\n",
      " 16999/50000: episode: 2850, duration: 0.019s, episode steps:   3, steps per second: 156, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 161818.953125, mae: 1434.743734, accuracy: 0.187500, mean_q: -1134.138631, mean_eps: 0.100000\n",
      " 17002/50000: episode: 2851, duration: 0.014s, episode steps:   3, steps per second: 209, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 168080.406250, mae: 1460.169637, accuracy: 0.114583, mean_q: -1159.739827, mean_eps: 0.100000\n",
      " 17006/50000: episode: 2852, duration: 0.016s, episode steps:   4, steps per second: 251, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 162453.105469, mae: 1448.937469, accuracy: 0.171875, mean_q: -1145.950165, mean_eps: 0.100000\n",
      " 17011/50000: episode: 2853, duration: 0.019s, episode steps:   5, steps per second: 267, episode reward: -2193.000, mean reward: -438.600 [-999.000, -32.000], mean action: 1.200 [0.000, 3.000],  loss: 157156.000000, mae: 1440.100854, accuracy: 0.187500, mean_q: -1130.271289, mean_eps: 0.100000\n",
      " 17014/50000: episode: 2854, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 152288.513021, mae: 1412.642944, accuracy: 0.177083, mean_q: -1132.402466, mean_eps: 0.100000\n",
      " 17018/50000: episode: 2855, duration: 0.016s, episode steps:   4, steps per second: 253, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 173625.595703, mae: 1431.190338, accuracy: 0.179688, mean_q: -1124.657990, mean_eps: 0.100000\n",
      " 17021/50000: episode: 2856, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 114253.750000, mae: 1403.826497, accuracy: 0.281250, mean_q: -1135.392415, mean_eps: 0.100000\n",
      " 17024/50000: episode: 2857, duration: 0.012s, episode steps:   3, steps per second: 253, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 162330.510417, mae: 1430.038493, accuracy: 0.156250, mean_q: -1131.622355, mean_eps: 0.100000\n",
      " 17027/50000: episode: 2858, duration: 0.012s, episode steps:   3, steps per second: 253, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 170411.880208, mae: 1446.906657, accuracy: 0.208333, mean_q: -1132.645793, mean_eps: 0.100000\n",
      " 17030/50000: episode: 2859, duration: 0.012s, episode steps:   3, steps per second: 255, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 167897.229167, mae: 1458.016439, accuracy: 0.177083, mean_q: -1147.940999, mean_eps: 0.100000\n",
      " 17034/50000: episode: 2860, duration: 0.015s, episode steps:   4, steps per second: 263, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 2.250 [1.000, 3.000],  loss: 163167.253906, mae: 1466.922546, accuracy: 0.140625, mean_q: -1171.177032, mean_eps: 0.100000\n",
      " 17037/50000: episode: 2861, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 171270.239583, mae: 1428.651001, accuracy: 0.135417, mean_q: -1123.462077, mean_eps: 0.100000\n",
      " 17040/50000: episode: 2862, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 144683.359375, mae: 1440.317749, accuracy: 0.197917, mean_q: -1158.446289, mean_eps: 0.100000\n",
      " 17043/50000: episode: 2863, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 176361.677083, mae: 1476.340007, accuracy: 0.177083, mean_q: -1132.039876, mean_eps: 0.100000\n",
      " 17046/50000: episode: 2864, duration: 0.014s, episode steps:   3, steps per second: 212, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 138048.572917, mae: 1417.835531, accuracy: 0.166667, mean_q: -1138.273356, mean_eps: 0.100000\n",
      " 17049/50000: episode: 2865, duration: 0.014s, episode steps:   3, steps per second: 218, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 133957.229167, mae: 1417.491048, accuracy: 0.208333, mean_q: -1124.103312, mean_eps: 0.100000\n",
      " 17052/50000: episode: 2866, duration: 0.013s, episode steps:   3, steps per second: 230, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 152921.666667, mae: 1411.295573, accuracy: 0.187500, mean_q: -1130.093953, mean_eps: 0.100000\n",
      " 17055/50000: episode: 2867, duration: 0.012s, episode steps:   3, steps per second: 258, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 139431.083333, mae: 1396.342041, accuracy: 0.239583, mean_q: -1103.841146, mean_eps: 0.100000\n",
      " 17058/50000: episode: 2868, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 161003.187500, mae: 1403.812541, accuracy: 0.281250, mean_q: -1112.945272, mean_eps: 0.100000\n",
      " 17061/50000: episode: 2869, duration: 0.013s, episode steps:   3, steps per second: 227, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 165335.791667, mae: 1424.640503, accuracy: 0.177083, mean_q: -1103.895671, mean_eps: 0.100000\n",
      " 17064/50000: episode: 2870, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 162592.911458, mae: 1415.010661, accuracy: 0.208333, mean_q: -1129.524943, mean_eps: 0.100000\n",
      " 17067/50000: episode: 2871, duration: 0.013s, episode steps:   3, steps per second: 222, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 161515.140625, mae: 1431.535441, accuracy: 0.177083, mean_q: -1100.946452, mean_eps: 0.100000\n",
      " 17070/50000: episode: 2872, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 143537.687500, mae: 1408.247233, accuracy: 0.239583, mean_q: -1080.237183, mean_eps: 0.100000\n",
      " 17073/50000: episode: 2873, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 143239.416667, mae: 1408.764404, accuracy: 0.229167, mean_q: -1095.882894, mean_eps: 0.100000\n",
      " 17076/50000: episode: 2874, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 164620.833333, mae: 1429.438151, accuracy: 0.177083, mean_q: -1112.875081, mean_eps: 0.100000\n",
      " 17079/50000: episode: 2875, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 143602.875000, mae: 1407.325968, accuracy: 0.208333, mean_q: -1079.057495, mean_eps: 0.100000\n",
      " 17082/50000: episode: 2876, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 146860.104167, mae: 1381.810710, accuracy: 0.218750, mean_q: -1070.201823, mean_eps: 0.100000\n",
      " 17085/50000: episode: 2877, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 142609.158854, mae: 1386.927490, accuracy: 0.229167, mean_q: -1095.534424, mean_eps: 0.100000\n",
      " 17088/50000: episode: 2878, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 129483.562500, mae: 1402.970093, accuracy: 0.239583, mean_q: -1106.102254, mean_eps: 0.100000\n",
      " 17091/50000: episode: 2879, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 134358.369792, mae: 1386.914225, accuracy: 0.208333, mean_q: -1096.867798, mean_eps: 0.100000\n",
      " 17094/50000: episode: 2880, duration: 0.014s, episode steps:   3, steps per second: 213, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 140720.690104, mae: 1420.458455, accuracy: 0.187500, mean_q: -1086.096395, mean_eps: 0.100000\n",
      " 17097/50000: episode: 2881, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 134250.078125, mae: 1409.673828, accuracy: 0.166667, mean_q: -1110.987061, mean_eps: 0.100000\n",
      " 17100/50000: episode: 2882, duration: 0.014s, episode steps:   3, steps per second: 212, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 150650.885417, mae: 1445.605143, accuracy: 0.177083, mean_q: -1127.072713, mean_eps: 0.100000\n",
      " 17103/50000: episode: 2883, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 121837.041667, mae: 1432.472249, accuracy: 0.177083, mean_q: -1125.984294, mean_eps: 0.100000\n",
      " 17106/50000: episode: 2884, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 154490.763021, mae: 1427.718872, accuracy: 0.197917, mean_q: -1130.081909, mean_eps: 0.100000\n",
      " 17109/50000: episode: 2885, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 117779.585938, mae: 1422.482096, accuracy: 0.197917, mean_q: -1135.811117, mean_eps: 0.100000\n",
      " 17112/50000: episode: 2886, duration: 0.012s, episode steps:   3, steps per second: 254, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 151464.104167, mae: 1419.043620, accuracy: 0.145833, mean_q: -1123.276530, mean_eps: 0.100000\n",
      " 17115/50000: episode: 2887, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 125671.419271, mae: 1389.067586, accuracy: 0.270833, mean_q: -1081.627686, mean_eps: 0.100000\n",
      " 17118/50000: episode: 2888, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 132087.046875, mae: 1432.298747, accuracy: 0.166667, mean_q: -1140.321330, mean_eps: 0.100000\n",
      " 17121/50000: episode: 2889, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 145391.473958, mae: 1406.809041, accuracy: 0.229167, mean_q: -1149.948201, mean_eps: 0.100000\n",
      " 17125/50000: episode: 2890, duration: 0.015s, episode steps:   4, steps per second: 269, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 2.000 [0.000, 3.000],  loss: 132435.525391, mae: 1408.365143, accuracy: 0.242188, mean_q: -1085.657288, mean_eps: 0.100000\n",
      " 17128/50000: episode: 2891, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 135762.348958, mae: 1387.464640, accuracy: 0.166667, mean_q: -1077.468384, mean_eps: 0.100000\n",
      " 17131/50000: episode: 2892, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 142936.807292, mae: 1397.852824, accuracy: 0.166667, mean_q: -1082.973999, mean_eps: 0.100000\n",
      " 17134/50000: episode: 2893, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 116380.947917, mae: 1386.844157, accuracy: 0.187500, mean_q: -1092.663737, mean_eps: 0.100000\n",
      " 17137/50000: episode: 2894, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 132599.447917, mae: 1398.800700, accuracy: 0.135417, mean_q: -1087.252401, mean_eps: 0.100000\n",
      " 17140/50000: episode: 2895, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 131254.500000, mae: 1389.800008, accuracy: 0.177083, mean_q: -1074.160360, mean_eps: 0.100000\n",
      " 17143/50000: episode: 2896, duration: 0.015s, episode steps:   3, steps per second: 201, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 118819.669271, mae: 1395.554443, accuracy: 0.166667, mean_q: -1067.322998, mean_eps: 0.100000\n",
      " 17147/50000: episode: 2897, duration: 0.021s, episode steps:   4, steps per second: 194, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 2.250 [1.000, 3.000],  loss: 128281.580078, mae: 1378.766235, accuracy: 0.210938, mean_q: -1093.892487, mean_eps: 0.100000\n",
      " 17150/50000: episode: 2898, duration: 0.014s, episode steps:   3, steps per second: 211, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 120040.351562, mae: 1361.938924, accuracy: 0.250000, mean_q: -1076.645589, mean_eps: 0.100000\n",
      " 17153/50000: episode: 2899, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 148119.484375, mae: 1424.320679, accuracy: 0.156250, mean_q: -1082.990072, mean_eps: 0.100000\n",
      " 17157/50000: episode: 2900, duration: 0.015s, episode steps:   4, steps per second: 259, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 159294.099609, mae: 1389.992401, accuracy: 0.234375, mean_q: -1083.837189, mean_eps: 0.100000\n",
      " 17160/50000: episode: 2901, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 143794.020833, mae: 1405.933512, accuracy: 0.125000, mean_q: -1085.233846, mean_eps: 0.100000\n",
      " 17163/50000: episode: 2902, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 118985.432292, mae: 1362.009806, accuracy: 0.187500, mean_q: -1057.639160, mean_eps: 0.100000\n",
      " 17166/50000: episode: 2903, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 118355.927083, mae: 1369.659058, accuracy: 0.187500, mean_q: -1086.942098, mean_eps: 0.100000\n",
      " 17169/50000: episode: 2904, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 155667.302083, mae: 1387.833049, accuracy: 0.177083, mean_q: -1053.339111, mean_eps: 0.100000\n",
      " 17172/50000: episode: 2905, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 135318.359375, mae: 1397.329875, accuracy: 0.145833, mean_q: -1075.452393, mean_eps: 0.100000\n",
      " 17175/50000: episode: 2906, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 176495.755208, mae: 1409.806966, accuracy: 0.166667, mean_q: -1035.422017, mean_eps: 0.100000\n",
      " 17178/50000: episode: 2907, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 127876.619792, mae: 1397.199544, accuracy: 0.166667, mean_q: -1081.498047, mean_eps: 0.100000\n",
      " 17181/50000: episode: 2908, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 2.000 [1.000, 3.000],  loss: 117048.283854, mae: 1355.449870, accuracy: 0.197917, mean_q: -1042.738647, mean_eps: 0.100000\n",
      " 17184/50000: episode: 2909, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 109226.632812, mae: 1376.207682, accuracy: 0.229167, mean_q: -1044.885864, mean_eps: 0.100000\n",
      " 17187/50000: episode: 2910, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 127312.804688, mae: 1367.645467, accuracy: 0.156250, mean_q: -1073.182129, mean_eps: 0.100000\n",
      " 17190/50000: episode: 2911, duration: 0.014s, episode steps:   3, steps per second: 220, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 137363.533854, mae: 1362.743734, accuracy: 0.197917, mean_q: -1040.634603, mean_eps: 0.100000\n",
      " 17193/50000: episode: 2912, duration: 0.013s, episode steps:   3, steps per second: 225, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 147314.255208, mae: 1387.115560, accuracy: 0.145833, mean_q: -1038.302979, mean_eps: 0.100000\n",
      " 17196/50000: episode: 2913, duration: 0.017s, episode steps:   3, steps per second: 178, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 142492.973958, mae: 1362.837199, accuracy: 0.156250, mean_q: -1047.179565, mean_eps: 0.100000\n",
      " 17199/50000: episode: 2914, duration: 0.018s, episode steps:   3, steps per second: 170, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 122028.088542, mae: 1364.054932, accuracy: 0.156250, mean_q: -1057.514974, mean_eps: 0.100000\n",
      " 17202/50000: episode: 2915, duration: 0.015s, episode steps:   3, steps per second: 202, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 125102.289062, mae: 1361.493652, accuracy: 0.114583, mean_q: -1045.432658, mean_eps: 0.100000\n",
      " 17205/50000: episode: 2916, duration: 0.016s, episode steps:   3, steps per second: 193, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 137604.432292, mae: 1384.325073, accuracy: 0.177083, mean_q: -1049.230672, mean_eps: 0.100000\n",
      " 17208/50000: episode: 2917, duration: 0.014s, episode steps:   3, steps per second: 212, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 149831.062500, mae: 1366.151449, accuracy: 0.187500, mean_q: -1044.210205, mean_eps: 0.100000\n",
      " 17212/50000: episode: 2918, duration: 0.017s, episode steps:   4, steps per second: 239, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 127050.679688, mae: 1365.096344, accuracy: 0.257812, mean_q: -1044.423523, mean_eps: 0.100000\n",
      " 17215/50000: episode: 2919, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 130434.963542, mae: 1351.846069, accuracy: 0.208333, mean_q: -1025.919556, mean_eps: 0.100000\n",
      " 17218/50000: episode: 2920, duration: 0.013s, episode steps:   3, steps per second: 223, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 114901.552083, mae: 1354.086507, accuracy: 0.239583, mean_q: -1040.613871, mean_eps: 0.100000\n",
      " 17221/50000: episode: 2921, duration: 0.014s, episode steps:   3, steps per second: 216, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 135314.213542, mae: 1359.806274, accuracy: 0.218750, mean_q: -1016.169373, mean_eps: 0.100000\n",
      " 17224/50000: episode: 2922, duration: 0.013s, episode steps:   3, steps per second: 223, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 137306.942708, mae: 1340.897624, accuracy: 0.197917, mean_q: -1022.375366, mean_eps: 0.100000\n",
      " 17227/50000: episode: 2923, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 151398.937500, mae: 1399.579508, accuracy: 0.177083, mean_q: -1034.225423, mean_eps: 0.100000\n",
      " 17230/50000: episode: 2924, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 132853.388021, mae: 1369.616048, accuracy: 0.239583, mean_q: -1036.549235, mean_eps: 0.100000\n",
      " 17234/50000: episode: 2925, duration: 0.016s, episode steps:   4, steps per second: 256, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 128232.509766, mae: 1352.672699, accuracy: 0.140625, mean_q: -1052.165375, mean_eps: 0.100000\n",
      " 17237/50000: episode: 2926, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 141770.635417, mae: 1350.751302, accuracy: 0.208333, mean_q: -1053.761597, mean_eps: 0.100000\n",
      " 17240/50000: episode: 2927, duration: 0.015s, episode steps:   3, steps per second: 204, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 124653.156250, mae: 1374.042847, accuracy: 0.156250, mean_q: -1035.967428, mean_eps: 0.100000\n",
      " 17243/50000: episode: 2928, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 118118.046875, mae: 1347.860026, accuracy: 0.187500, mean_q: -1026.874390, mean_eps: 0.100000\n",
      " 17246/50000: episode: 2929, duration: 0.013s, episode steps:   3, steps per second: 224, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 132683.658854, mae: 1367.128255, accuracy: 0.145833, mean_q: -1058.792114, mean_eps: 0.100000\n",
      " 17249/50000: episode: 2930, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 133921.848958, mae: 1372.470052, accuracy: 0.177083, mean_q: -1024.332397, mean_eps: 0.100000\n",
      " 17252/50000: episode: 2931, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 132660.617188, mae: 1383.259399, accuracy: 0.156250, mean_q: -1050.759013, mean_eps: 0.100000\n",
      " 17255/50000: episode: 2932, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 112937.963542, mae: 1372.094604, accuracy: 0.166667, mean_q: -1055.697673, mean_eps: 0.100000\n",
      " 17258/50000: episode: 2933, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 120242.838542, mae: 1358.986287, accuracy: 0.166667, mean_q: -1054.681193, mean_eps: 0.100000\n",
      " 17261/50000: episode: 2934, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 129495.864583, mae: 1357.293701, accuracy: 0.145833, mean_q: -1040.078939, mean_eps: 0.100000\n",
      " 17264/50000: episode: 2935, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 125561.789062, mae: 1333.106771, accuracy: 0.177083, mean_q: -1016.166809, mean_eps: 0.100000\n",
      " 17267/50000: episode: 2936, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 119390.052083, mae: 1351.112671, accuracy: 0.166667, mean_q: -1039.757955, mean_eps: 0.100000\n",
      " 17270/50000: episode: 2937, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 126937.450521, mae: 1343.895101, accuracy: 0.166667, mean_q: -1004.509379, mean_eps: 0.100000\n",
      " 17274/50000: episode: 2938, duration: 0.015s, episode steps:   4, steps per second: 268, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 104219.326172, mae: 1331.866974, accuracy: 0.203125, mean_q: -1013.171997, mean_eps: 0.100000\n",
      " 17277/50000: episode: 2939, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 121622.070312, mae: 1356.756307, accuracy: 0.177083, mean_q: -1030.470418, mean_eps: 0.100000\n",
      " 17281/50000: episode: 2940, duration: 0.015s, episode steps:   4, steps per second: 268, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 131237.191406, mae: 1356.358429, accuracy: 0.164062, mean_q: -1017.283707, mean_eps: 0.100000\n",
      " 17284/50000: episode: 2941, duration: 0.013s, episode steps:   3, steps per second: 224, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 112618.460938, mae: 1339.463949, accuracy: 0.177083, mean_q: -1029.719950, mean_eps: 0.100000\n",
      " 17287/50000: episode: 2942, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 2.000 [1.000, 3.000],  loss: 117134.421875, mae: 1346.247965, accuracy: 0.135417, mean_q: -1012.522380, mean_eps: 0.100000\n",
      " 17290/50000: episode: 2943, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 110065.466146, mae: 1333.662679, accuracy: 0.187500, mean_q: -1028.924845, mean_eps: 0.100000\n",
      " 17293/50000: episode: 2944, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 84674.723958, mae: 1347.566488, accuracy: 0.187500, mean_q: -1055.918091, mean_eps: 0.100000\n",
      " 17296/50000: episode: 2945, duration: 0.018s, episode steps:   3, steps per second: 170, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 107852.359375, mae: 1342.445719, accuracy: 0.218750, mean_q: -1013.698201, mean_eps: 0.100000\n",
      " 17299/50000: episode: 2946, duration: 0.015s, episode steps:   3, steps per second: 206, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 123219.960938, mae: 1340.867228, accuracy: 0.093750, mean_q: -1029.543335, mean_eps: 0.100000\n",
      " 17302/50000: episode: 2947, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 116855.755208, mae: 1344.830241, accuracy: 0.093750, mean_q: -1023.517700, mean_eps: 0.100000\n",
      " 17305/50000: episode: 2948, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 154134.135417, mae: 1343.606120, accuracy: 0.239583, mean_q: -995.321045, mean_eps: 0.100000\n",
      " 17308/50000: episode: 2949, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 121836.875000, mae: 1324.648519, accuracy: 0.156250, mean_q: -1001.162699, mean_eps: 0.100000\n",
      " 17311/50000: episode: 2950, duration: 0.012s, episode steps:   3, steps per second: 254, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 111039.406250, mae: 1316.020833, accuracy: 0.145833, mean_q: -1002.510417, mean_eps: 0.100000\n",
      " 17314/50000: episode: 2951, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 126047.716146, mae: 1350.835286, accuracy: 0.145833, mean_q: -1015.036377, mean_eps: 0.100000\n",
      " 17317/50000: episode: 2952, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 122574.015625, mae: 1334.360229, accuracy: 0.197917, mean_q: -999.560384, mean_eps: 0.100000\n",
      " 17320/50000: episode: 2953, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 108763.588542, mae: 1289.242432, accuracy: 0.270833, mean_q: -1003.354614, mean_eps: 0.100000\n",
      " 17324/50000: episode: 2954, duration: 0.015s, episode steps:   4, steps per second: 268, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.000 [0.000, 3.000],  loss: 108392.791016, mae: 1312.787781, accuracy: 0.179688, mean_q: -971.414215, mean_eps: 0.100000\n",
      " 17327/50000: episode: 2955, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 111982.927083, mae: 1305.199259, accuracy: 0.239583, mean_q: -990.763163, mean_eps: 0.100000\n",
      " 17330/50000: episode: 2956, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 122445.968750, mae: 1329.288249, accuracy: 0.104167, mean_q: -1014.371134, mean_eps: 0.100000\n",
      " 17333/50000: episode: 2957, duration: 0.013s, episode steps:   3, steps per second: 225, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 125865.671875, mae: 1319.834757, accuracy: 0.187500, mean_q: -992.670715, mean_eps: 0.100000\n",
      " 17336/50000: episode: 2958, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 121428.794271, mae: 1342.225342, accuracy: 0.166667, mean_q: -992.224162, mean_eps: 0.100000\n",
      " 17340/50000: episode: 2959, duration: 0.016s, episode steps:   4, steps per second: 243, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.000 [0.000, 3.000],  loss: 115068.468750, mae: 1308.867126, accuracy: 0.203125, mean_q: -984.232498, mean_eps: 0.100000\n",
      " 17343/50000: episode: 2960, duration: 0.014s, episode steps:   3, steps per second: 216, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 112409.523438, mae: 1306.124430, accuracy: 0.239583, mean_q: -957.987813, mean_eps: 0.100000\n",
      " 17346/50000: episode: 2961, duration: 0.014s, episode steps:   3, steps per second: 213, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 141866.276042, mae: 1337.635050, accuracy: 0.177083, mean_q: -965.777242, mean_eps: 0.100000\n",
      " 17349/50000: episode: 2962, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 127643.533854, mae: 1327.619792, accuracy: 0.145833, mean_q: -960.697978, mean_eps: 0.100000\n",
      " 17352/50000: episode: 2963, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 144275.526042, mae: 1338.966756, accuracy: 0.156250, mean_q: -973.423665, mean_eps: 0.100000\n",
      " 17355/50000: episode: 2964, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 115043.257812, mae: 1304.701823, accuracy: 0.187500, mean_q: -950.847473, mean_eps: 0.100000\n",
      " 17358/50000: episode: 2965, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 109632.700521, mae: 1304.737061, accuracy: 0.114583, mean_q: -970.039937, mean_eps: 0.100000\n",
      " 17361/50000: episode: 2966, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 108555.434896, mae: 1307.720215, accuracy: 0.156250, mean_q: -968.151245, mean_eps: 0.100000\n",
      " 17364/50000: episode: 2967, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 106159.875000, mae: 1295.638997, accuracy: 0.166667, mean_q: -958.991414, mean_eps: 0.100000\n",
      " 17367/50000: episode: 2968, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 119849.049479, mae: 1331.363810, accuracy: 0.114583, mean_q: -976.582194, mean_eps: 0.100000\n",
      " 17370/50000: episode: 2969, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 113132.161458, mae: 1284.890422, accuracy: 0.135417, mean_q: -961.445984, mean_eps: 0.100000\n",
      " 17373/50000: episode: 2970, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 122985.950521, mae: 1315.971436, accuracy: 0.197917, mean_q: -966.385478, mean_eps: 0.100000\n",
      " 17376/50000: episode: 2971, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 120692.140625, mae: 1325.520793, accuracy: 0.177083, mean_q: -963.856222, mean_eps: 0.100000\n",
      " 17379/50000: episode: 2972, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 107012.289062, mae: 1294.482869, accuracy: 0.239583, mean_q: -939.342082, mean_eps: 0.100000\n",
      " 17382/50000: episode: 2973, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 85645.601562, mae: 1268.985270, accuracy: 0.208333, mean_q: -959.358337, mean_eps: 0.100000\n",
      " 17386/50000: episode: 2974, duration: 0.016s, episode steps:   4, steps per second: 247, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.000 [0.000, 3.000],  loss: 108588.791016, mae: 1302.749084, accuracy: 0.195312, mean_q: -964.720581, mean_eps: 0.100000\n",
      " 17389/50000: episode: 2975, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 108544.750000, mae: 1295.375244, accuracy: 0.218750, mean_q: -940.525960, mean_eps: 0.100000\n",
      " 17392/50000: episode: 2976, duration: 0.014s, episode steps:   3, steps per second: 222, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 106425.976562, mae: 1297.949382, accuracy: 0.177083, mean_q: -963.146952, mean_eps: 0.100000\n",
      " 17395/50000: episode: 2977, duration: 0.014s, episode steps:   3, steps per second: 211, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 89766.424479, mae: 1271.417887, accuracy: 0.135417, mean_q: -920.298116, mean_eps: 0.100000\n",
      " 17398/50000: episode: 2978, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 115662.825521, mae: 1280.858683, accuracy: 0.125000, mean_q: -921.477519, mean_eps: 0.100000\n",
      " 17402/50000: episode: 2979, duration: 0.018s, episode steps:   4, steps per second: 224, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 119078.853516, mae: 1283.548401, accuracy: 0.164062, mean_q: -927.640869, mean_eps: 0.100000\n",
      " 17405/50000: episode: 2980, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 107515.421875, mae: 1278.778402, accuracy: 0.145833, mean_q: -933.302002, mean_eps: 0.100000\n",
      " 17408/50000: episode: 2981, duration: 0.014s, episode steps:   3, steps per second: 213, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 114489.283854, mae: 1311.259521, accuracy: 0.145833, mean_q: -901.491984, mean_eps: 0.100000\n",
      " 17411/50000: episode: 2982, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 116102.585938, mae: 1301.174032, accuracy: 0.114583, mean_q: -926.793986, mean_eps: 0.100000\n",
      " 17414/50000: episode: 2983, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 106780.877604, mae: 1285.704224, accuracy: 0.145833, mean_q: -956.916809, mean_eps: 0.100000\n",
      " 17417/50000: episode: 2984, duration: 0.012s, episode steps:   3, steps per second: 257, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 129131.500000, mae: 1309.872152, accuracy: 0.187500, mean_q: -937.681986, mean_eps: 0.100000\n",
      " 17420/50000: episode: 2985, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 108968.283854, mae: 1298.031331, accuracy: 0.166667, mean_q: -942.681722, mean_eps: 0.100000\n",
      " 17424/50000: episode: 2986, duration: 0.015s, episode steps:   4, steps per second: 261, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 92765.570312, mae: 1284.675201, accuracy: 0.156250, mean_q: -954.816010, mean_eps: 0.100000\n",
      " 17427/50000: episode: 2987, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 101133.468750, mae: 1299.264730, accuracy: 0.125000, mean_q: -938.064290, mean_eps: 0.100000\n",
      " 17430/50000: episode: 2988, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 123845.877604, mae: 1287.268799, accuracy: 0.166667, mean_q: -952.338318, mean_eps: 0.100000\n",
      " 17433/50000: episode: 2989, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.667 [0.000, 3.000],  loss: 96993.182292, mae: 1281.722656, accuracy: 0.208333, mean_q: -951.071248, mean_eps: 0.100000\n",
      " 17436/50000: episode: 2990, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 82808.783854, mae: 1255.393962, accuracy: 0.281250, mean_q: -935.425028, mean_eps: 0.100000\n",
      " 17439/50000: episode: 2991, duration: 0.014s, episode steps:   3, steps per second: 218, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 93786.726562, mae: 1293.020549, accuracy: 0.093750, mean_q: -938.319499, mean_eps: 0.100000\n",
      " 17442/50000: episode: 2992, duration: 0.018s, episode steps:   3, steps per second: 163, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 131603.718750, mae: 1308.478923, accuracy: 0.156250, mean_q: -932.954183, mean_eps: 0.100000\n",
      " 17445/50000: episode: 2993, duration: 0.015s, episode steps:   3, steps per second: 204, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 115297.648438, mae: 1291.066284, accuracy: 0.197917, mean_q: -946.471822, mean_eps: 0.100000\n",
      " 17448/50000: episode: 2994, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 85782.502604, mae: 1282.072347, accuracy: 0.197917, mean_q: -935.379049, mean_eps: 0.100000\n",
      " 17451/50000: episode: 2995, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 104604.929688, mae: 1291.594849, accuracy: 0.281250, mean_q: -936.332174, mean_eps: 0.100000\n",
      " 17454/50000: episode: 2996, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 112479.776042, mae: 1289.429443, accuracy: 0.208333, mean_q: -944.842794, mean_eps: 0.100000\n",
      " 17457/50000: episode: 2997, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 111904.994792, mae: 1267.240845, accuracy: 0.218750, mean_q: -927.323181, mean_eps: 0.100000\n",
      " 17460/50000: episode: 2998, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 99139.520833, mae: 1288.980835, accuracy: 0.166667, mean_q: -912.342529, mean_eps: 0.100000\n",
      " 17463/50000: episode: 2999, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 109726.033854, mae: 1273.548096, accuracy: 0.239583, mean_q: -928.201558, mean_eps: 0.100000\n",
      " 17467/50000: episode: 3000, duration: 0.015s, episode steps:   4, steps per second: 270, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 110377.830078, mae: 1293.681183, accuracy: 0.179688, mean_q: -912.813660, mean_eps: 0.100000\n",
      " 17470/50000: episode: 3001, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 97697.958333, mae: 1239.027588, accuracy: 0.187500, mean_q: -915.412170, mean_eps: 0.100000\n",
      " 17473/50000: episode: 3002, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 111130.835938, mae: 1264.501628, accuracy: 0.218750, mean_q: -911.243062, mean_eps: 0.100000\n",
      " 17476/50000: episode: 3003, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 105612.218750, mae: 1267.600830, accuracy: 0.156250, mean_q: -900.756185, mean_eps: 0.100000\n",
      " 17479/50000: episode: 3004, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 118898.281250, mae: 1283.065470, accuracy: 0.166667, mean_q: -927.610148, mean_eps: 0.100000\n",
      " 17482/50000: episode: 3005, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 83288.903646, mae: 1250.039591, accuracy: 0.187500, mean_q: -905.649129, mean_eps: 0.100000\n",
      " 17485/50000: episode: 3006, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 104296.942708, mae: 1265.494181, accuracy: 0.197917, mean_q: -911.058004, mean_eps: 0.100000\n",
      " 17488/50000: episode: 3007, duration: 0.015s, episode steps:   3, steps per second: 200, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 101451.346354, mae: 1266.377645, accuracy: 0.135417, mean_q: -917.022970, mean_eps: 0.100000\n",
      " 17491/50000: episode: 3008, duration: 0.014s, episode steps:   3, steps per second: 221, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 112071.731771, mae: 1266.332601, accuracy: 0.166667, mean_q: -910.678162, mean_eps: 0.100000\n",
      " 17494/50000: episode: 3009, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 100433.507812, mae: 1257.723796, accuracy: 0.187500, mean_q: -918.079753, mean_eps: 0.100000\n",
      " 17497/50000: episode: 3010, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 105589.317708, mae: 1272.692505, accuracy: 0.145833, mean_q: -934.875305, mean_eps: 0.100000\n",
      " 17500/50000: episode: 3011, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 88892.395833, mae: 1262.029582, accuracy: 0.177083, mean_q: -915.526225, mean_eps: 0.100000\n",
      " 17503/50000: episode: 3012, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 110321.486979, mae: 1258.380534, accuracy: 0.125000, mean_q: -903.416036, mean_eps: 0.100000\n",
      " 17506/50000: episode: 3013, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 100858.976562, mae: 1257.116740, accuracy: 0.156250, mean_q: -917.029236, mean_eps: 0.100000\n",
      " 17509/50000: episode: 3014, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 111187.127604, mae: 1269.840942, accuracy: 0.156250, mean_q: -882.167704, mean_eps: 0.100000\n",
      " 17512/50000: episode: 3015, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 85609.601562, mae: 1232.683634, accuracy: 0.218750, mean_q: -896.950989, mean_eps: 0.100000\n",
      " 17515/50000: episode: 3016, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 108643.052083, mae: 1278.691650, accuracy: 0.135417, mean_q: -922.053752, mean_eps: 0.100000\n",
      " 17519/50000: episode: 3017, duration: 0.015s, episode steps:   4, steps per second: 264, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 101595.001953, mae: 1256.107117, accuracy: 0.140625, mean_q: -908.201752, mean_eps: 0.100000\n",
      " 17522/50000: episode: 3018, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 92107.153646, mae: 1230.541423, accuracy: 0.197917, mean_q: -873.698181, mean_eps: 0.100000\n",
      " 17525/50000: episode: 3019, duration: 0.012s, episode steps:   3, steps per second: 253, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 97085.031250, mae: 1237.278646, accuracy: 0.145833, mean_q: -880.504252, mean_eps: 0.100000\n",
      " 17528/50000: episode: 3020, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 77288.080729, mae: 1244.294963, accuracy: 0.239583, mean_q: -901.359192, mean_eps: 0.100000\n",
      " 17532/50000: episode: 3021, duration: 0.015s, episode steps:   4, steps per second: 259, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 88805.675781, mae: 1247.478943, accuracy: 0.171875, mean_q: -898.668747, mean_eps: 0.100000\n",
      " 17535/50000: episode: 3022, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 92916.192708, mae: 1234.442057, accuracy: 0.250000, mean_q: -876.856608, mean_eps: 0.100000\n",
      " 17538/50000: episode: 3023, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 91396.750000, mae: 1231.761678, accuracy: 0.208333, mean_q: -889.217326, mean_eps: 0.100000\n",
      " 17541/50000: episode: 3024, duration: 0.015s, episode steps:   3, steps per second: 201, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 99734.127604, mae: 1266.544230, accuracy: 0.229167, mean_q: -910.331258, mean_eps: 0.100000\n",
      " 17544/50000: episode: 3025, duration: 0.014s, episode steps:   3, steps per second: 212, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 134814.630208, mae: 1294.702596, accuracy: 0.166667, mean_q: -923.503682, mean_eps: 0.100000\n",
      " 17547/50000: episode: 3026, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 78584.789062, mae: 1238.398153, accuracy: 0.260417, mean_q: -871.660339, mean_eps: 0.100000\n",
      " 17550/50000: episode: 3027, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 89694.929688, mae: 1240.934774, accuracy: 0.072917, mean_q: -906.477620, mean_eps: 0.100000\n",
      " 17553/50000: episode: 3028, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 88538.885417, mae: 1222.864746, accuracy: 0.197917, mean_q: -897.063477, mean_eps: 0.100000\n",
      " 17556/50000: episode: 3029, duration: 0.012s, episode steps:   3, steps per second: 257, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 81886.059896, mae: 1242.045492, accuracy: 0.187500, mean_q: -879.935710, mean_eps: 0.100000\n",
      " 17559/50000: episode: 3030, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 98129.020833, mae: 1234.612630, accuracy: 0.187500, mean_q: -877.116536, mean_eps: 0.100000\n",
      " 17562/50000: episode: 3031, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 95217.479167, mae: 1224.804525, accuracy: 0.114583, mean_q: -887.861064, mean_eps: 0.100000\n",
      " 17565/50000: episode: 3032, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 101459.445312, mae: 1218.959798, accuracy: 0.260417, mean_q: -884.289958, mean_eps: 0.100000\n",
      " 17568/50000: episode: 3033, duration: 0.012s, episode steps:   3, steps per second: 256, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 112613.369792, mae: 1233.579346, accuracy: 0.177083, mean_q: -851.293823, mean_eps: 0.100000\n",
      " 17571/50000: episode: 3034, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 94137.562500, mae: 1241.620280, accuracy: 0.177083, mean_q: -891.288839, mean_eps: 0.100000\n",
      " 17574/50000: episode: 3035, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 103048.445312, mae: 1242.925374, accuracy: 0.166667, mean_q: -865.783000, mean_eps: 0.100000\n",
      " 17577/50000: episode: 3036, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 95925.882812, mae: 1235.447388, accuracy: 0.177083, mean_q: -864.922343, mean_eps: 0.100000\n",
      " 17580/50000: episode: 3037, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 97531.226562, mae: 1217.795247, accuracy: 0.177083, mean_q: -865.744873, mean_eps: 0.100000\n",
      " 17584/50000: episode: 3038, duration: 0.015s, episode steps:   4, steps per second: 274, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 83191.433594, mae: 1207.494171, accuracy: 0.156250, mean_q: -853.919571, mean_eps: 0.100000\n",
      " 17587/50000: episode: 3039, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 90477.976562, mae: 1210.612508, accuracy: 0.145833, mean_q: -858.461060, mean_eps: 0.100000\n",
      " 17590/50000: episode: 3040, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 84478.765625, mae: 1198.764811, accuracy: 0.218750, mean_q: -838.843486, mean_eps: 0.100000\n",
      " 17593/50000: episode: 3041, duration: 0.017s, episode steps:   3, steps per second: 176, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 106638.250000, mae: 1210.394450, accuracy: 0.208333, mean_q: -832.922994, mean_eps: 0.100000\n",
      " 17596/50000: episode: 3042, duration: 0.014s, episode steps:   3, steps per second: 210, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 97670.518229, mae: 1205.145874, accuracy: 0.125000, mean_q: -816.691406, mean_eps: 0.100000\n",
      " 17599/50000: episode: 3043, duration: 0.013s, episode steps:   3, steps per second: 226, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 108663.281250, mae: 1213.990316, accuracy: 0.177083, mean_q: -850.664653, mean_eps: 0.100000\n",
      " 17602/50000: episode: 3044, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 109838.033854, mae: 1228.712158, accuracy: 0.166667, mean_q: -852.264608, mean_eps: 0.100000\n",
      " 17605/50000: episode: 3045, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 73543.798177, mae: 1209.485311, accuracy: 0.218750, mean_q: -829.823100, mean_eps: 0.100000\n",
      " 17608/50000: episode: 3046, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 101958.763021, mae: 1241.605265, accuracy: 0.125000, mean_q: -857.659444, mean_eps: 0.100000\n",
      " 17611/50000: episode: 3047, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 87800.619792, mae: 1214.533610, accuracy: 0.177083, mean_q: -863.865336, mean_eps: 0.100000\n",
      " 17614/50000: episode: 3048, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 98591.330729, mae: 1234.676147, accuracy: 0.166667, mean_q: -845.388407, mean_eps: 0.100000\n",
      " 17617/50000: episode: 3049, duration: 0.012s, episode steps:   3, steps per second: 253, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 90641.557292, mae: 1215.063965, accuracy: 0.218750, mean_q: -834.535421, mean_eps: 0.100000\n",
      " 17621/50000: episode: 3050, duration: 0.015s, episode steps:   4, steps per second: 263, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 102642.396484, mae: 1240.766602, accuracy: 0.179688, mean_q: -867.098755, mean_eps: 0.100000\n",
      " 17624/50000: episode: 3051, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 83613.252604, mae: 1207.390584, accuracy: 0.166667, mean_q: -835.002258, mean_eps: 0.100000\n",
      " 17628/50000: episode: 3052, duration: 0.015s, episode steps:   4, steps per second: 260, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 93182.996094, mae: 1214.096710, accuracy: 0.148438, mean_q: -821.250549, mean_eps: 0.100000\n",
      " 17631/50000: episode: 3053, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 98608.611979, mae: 1225.784017, accuracy: 0.177083, mean_q: -813.375610, mean_eps: 0.100000\n",
      " 17634/50000: episode: 3054, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 98468.114583, mae: 1225.418132, accuracy: 0.166667, mean_q: -832.230103, mean_eps: 0.100000\n",
      " 17638/50000: episode: 3055, duration: 0.016s, episode steps:   4, steps per second: 250, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 85799.285156, mae: 1215.754364, accuracy: 0.179688, mean_q: -860.210480, mean_eps: 0.100000\n",
      " 17641/50000: episode: 3056, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 82607.184896, mae: 1179.807780, accuracy: 0.197917, mean_q: -847.941203, mean_eps: 0.100000\n",
      " 17644/50000: episode: 3057, duration: 0.015s, episode steps:   3, steps per second: 207, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 101544.018229, mae: 1209.013916, accuracy: 0.083333, mean_q: -820.771342, mean_eps: 0.100000\n",
      " 17648/50000: episode: 3058, duration: 0.017s, episode steps:   4, steps per second: 231, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 81052.769531, mae: 1212.531067, accuracy: 0.109375, mean_q: -830.250397, mean_eps: 0.100000\n",
      " 17651/50000: episode: 3059, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 80087.151042, mae: 1197.260498, accuracy: 0.104167, mean_q: -824.112122, mean_eps: 0.100000\n",
      " 17654/50000: episode: 3060, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 107120.658854, mae: 1214.656982, accuracy: 0.135417, mean_q: -817.560954, mean_eps: 0.100000\n",
      " 17658/50000: episode: 3061, duration: 0.015s, episode steps:   4, steps per second: 262, episode reward: -1238.000, mean reward: -309.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 87913.410156, mae: 1183.411530, accuracy: 0.156250, mean_q: -815.772141, mean_eps: 0.100000\n",
      " 17661/50000: episode: 3062, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 81154.036458, mae: 1181.681641, accuracy: 0.125000, mean_q: -811.762960, mean_eps: 0.100000\n",
      " 17664/50000: episode: 3063, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 98749.843750, mae: 1235.485474, accuracy: 0.156250, mean_q: -813.889242, mean_eps: 0.100000\n",
      " 17667/50000: episode: 3064, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 70664.835938, mae: 1183.167318, accuracy: 0.125000, mean_q: -823.688741, mean_eps: 0.100000\n",
      " 17670/50000: episode: 3065, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 80451.151042, mae: 1195.205688, accuracy: 0.156250, mean_q: -812.816142, mean_eps: 0.100000\n",
      " 17673/50000: episode: 3066, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 91530.953125, mae: 1171.103190, accuracy: 0.239583, mean_q: -794.523132, mean_eps: 0.100000\n",
      " 17676/50000: episode: 3067, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 97009.471354, mae: 1209.042562, accuracy: 0.187500, mean_q: -805.284973, mean_eps: 0.100000\n",
      " 17679/50000: episode: 3068, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 84497.106771, mae: 1195.562703, accuracy: 0.156250, mean_q: -826.670776, mean_eps: 0.100000\n",
      " 17682/50000: episode: 3069, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 79103.611979, mae: 1162.016683, accuracy: 0.229167, mean_q: -797.002340, mean_eps: 0.100000\n",
      " 17685/50000: episode: 3070, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 70912.177083, mae: 1161.040161, accuracy: 0.166667, mean_q: -799.414612, mean_eps: 0.100000\n",
      " 17688/50000: episode: 3071, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 90907.104167, mae: 1183.362386, accuracy: 0.239583, mean_q: -783.606812, mean_eps: 0.100000\n",
      " 17691/50000: episode: 3072, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 89700.375000, mae: 1207.178630, accuracy: 0.197917, mean_q: -840.416178, mean_eps: 0.100000\n",
      " 17694/50000: episode: 3073, duration: 0.012s, episode steps:   3, steps per second: 253, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 66772.117188, mae: 1152.521891, accuracy: 0.197917, mean_q: -777.306213, mean_eps: 0.100000\n",
      " 17697/50000: episode: 3074, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 79865.166667, mae: 1167.859131, accuracy: 0.239583, mean_q: -787.610433, mean_eps: 0.100000\n",
      " 17700/50000: episode: 3075, duration: 0.015s, episode steps:   3, steps per second: 197, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 76462.398438, mae: 1173.827148, accuracy: 0.218750, mean_q: -803.213440, mean_eps: 0.100000\n",
      " 17703/50000: episode: 3076, duration: 0.014s, episode steps:   3, steps per second: 214, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 91780.979167, mae: 1180.388265, accuracy: 0.197917, mean_q: -793.718526, mean_eps: 0.100000\n",
      " 17707/50000: episode: 3077, duration: 0.016s, episode steps:   4, steps per second: 248, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 88413.210938, mae: 1177.329041, accuracy: 0.203125, mean_q: -774.398392, mean_eps: 0.100000\n",
      " 17710/50000: episode: 3078, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 88723.940104, mae: 1171.181274, accuracy: 0.197917, mean_q: -797.211100, mean_eps: 0.100000\n",
      " 17714/50000: episode: 3079, duration: 0.015s, episode steps:   4, steps per second: 265, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 2.250 [1.000, 3.000],  loss: 80513.119141, mae: 1150.823547, accuracy: 0.218750, mean_q: -763.069809, mean_eps: 0.100000\n",
      " 17717/50000: episode: 3080, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 79507.779948, mae: 1149.920736, accuracy: 0.125000, mean_q: -788.180339, mean_eps: 0.100000\n",
      " 17720/50000: episode: 3081, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 79094.781250, mae: 1171.757284, accuracy: 0.156250, mean_q: -772.123698, mean_eps: 0.100000\n",
      " 17723/50000: episode: 3082, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 78444.044271, mae: 1135.202271, accuracy: 0.302083, mean_q: -746.628927, mean_eps: 0.100000\n",
      " 17726/50000: episode: 3083, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 71879.348958, mae: 1128.460124, accuracy: 0.197917, mean_q: -771.397115, mean_eps: 0.100000\n",
      " 17729/50000: episode: 3084, duration: 0.012s, episode steps:   3, steps per second: 255, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 69071.661458, mae: 1160.825033, accuracy: 0.156250, mean_q: -772.291077, mean_eps: 0.100000\n",
      " 17732/50000: episode: 3085, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 77181.132812, mae: 1160.982544, accuracy: 0.229167, mean_q: -744.372030, mean_eps: 0.100000\n",
      " 17735/50000: episode: 3086, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 89324.653646, mae: 1176.273397, accuracy: 0.145833, mean_q: -771.041361, mean_eps: 0.100000\n",
      " 17738/50000: episode: 3087, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 86055.364583, mae: 1166.197876, accuracy: 0.145833, mean_q: -781.746724, mean_eps: 0.100000\n",
      " 17741/50000: episode: 3088, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 89840.848958, mae: 1151.070231, accuracy: 0.229167, mean_q: -752.122538, mean_eps: 0.100000\n",
      " 17744/50000: episode: 3089, duration: 0.017s, episode steps:   3, steps per second: 172, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 70613.631510, mae: 1148.208293, accuracy: 0.187500, mean_q: -769.457723, mean_eps: 0.100000\n",
      " 17747/50000: episode: 3090, duration: 0.015s, episode steps:   3, steps per second: 197, episode reward: -224.000, mean reward: -74.667 [-116.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 68810.419271, mae: 1125.840210, accuracy: 0.218750, mean_q: -761.156942, mean_eps: 0.100000\n",
      " 17750/50000: episode: 3091, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 78537.093750, mae: 1153.731160, accuracy: 0.187500, mean_q: -726.685160, mean_eps: 0.100000\n",
      " 17753/50000: episode: 3092, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 76420.778646, mae: 1136.618571, accuracy: 0.229167, mean_q: -762.073751, mean_eps: 0.100000\n",
      " 17756/50000: episode: 3093, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 89144.854167, mae: 1157.536825, accuracy: 0.260417, mean_q: -750.913167, mean_eps: 0.100000\n",
      " 17759/50000: episode: 3094, duration: 0.012s, episode steps:   3, steps per second: 255, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 65672.994792, mae: 1125.080241, accuracy: 0.218750, mean_q: -728.494425, mean_eps: 0.100000\n",
      " 17762/50000: episode: 3095, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 78406.677083, mae: 1129.281657, accuracy: 0.166667, mean_q: -749.101237, mean_eps: 0.100000\n",
      " 17765/50000: episode: 3096, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 67899.005208, mae: 1130.264364, accuracy: 0.145833, mean_q: -741.954427, mean_eps: 0.100000\n",
      " 17768/50000: episode: 3097, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 89336.648438, mae: 1128.485352, accuracy: 0.239583, mean_q: -719.966187, mean_eps: 0.100000\n",
      " 17771/50000: episode: 3098, duration: 0.012s, episode steps:   3, steps per second: 255, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 71400.203125, mae: 1128.590698, accuracy: 0.197917, mean_q: -747.446879, mean_eps: 0.100000\n",
      " 17774/50000: episode: 3099, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 72800.098958, mae: 1136.776489, accuracy: 0.166667, mean_q: -735.484151, mean_eps: 0.100000\n",
      " 17777/50000: episode: 3100, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 76608.625000, mae: 1152.173014, accuracy: 0.125000, mean_q: -728.746419, mean_eps: 0.100000\n",
      " 17780/50000: episode: 3101, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 81341.933594, mae: 1144.149251, accuracy: 0.177083, mean_q: -753.746623, mean_eps: 0.100000\n",
      " 17783/50000: episode: 3102, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 72740.279948, mae: 1127.442017, accuracy: 0.229167, mean_q: -704.391846, mean_eps: 0.100000\n",
      " 17786/50000: episode: 3103, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 83555.236979, mae: 1138.218262, accuracy: 0.197917, mean_q: -731.086589, mean_eps: 0.100000\n",
      " 17789/50000: episode: 3104, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 83490.888021, mae: 1137.742228, accuracy: 0.250000, mean_q: -735.758870, mean_eps: 0.100000\n",
      " 17792/50000: episode: 3105, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 64814.882812, mae: 1132.858805, accuracy: 0.156250, mean_q: -744.447428, mean_eps: 0.100000\n",
      " 17795/50000: episode: 3106, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 64933.973958, mae: 1113.280314, accuracy: 0.250000, mean_q: -693.327087, mean_eps: 0.100000\n",
      " 17799/50000: episode: 3107, duration: 0.016s, episode steps:   4, steps per second: 249, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 64591.802734, mae: 1110.081421, accuracy: 0.226562, mean_q: -744.540253, mean_eps: 0.100000\n",
      " 17802/50000: episode: 3108, duration: 0.014s, episode steps:   3, steps per second: 208, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 82531.947917, mae: 1115.232422, accuracy: 0.208333, mean_q: -729.873169, mean_eps: 0.100000\n",
      " 17805/50000: episode: 3109, duration: 0.015s, episode steps:   3, steps per second: 207, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 75534.682292, mae: 1100.540934, accuracy: 0.156250, mean_q: -693.731934, mean_eps: 0.100000\n",
      " 17808/50000: episode: 3110, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 78083.802083, mae: 1104.832926, accuracy: 0.166667, mean_q: -706.433472, mean_eps: 0.100000\n",
      " 17811/50000: episode: 3111, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 73733.019531, mae: 1130.657715, accuracy: 0.208333, mean_q: -710.517456, mean_eps: 0.100000\n",
      " 17814/50000: episode: 3112, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 72207.131510, mae: 1109.798381, accuracy: 0.218750, mean_q: -707.861003, mean_eps: 0.100000\n",
      " 17817/50000: episode: 3113, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 67195.871094, mae: 1098.901652, accuracy: 0.166667, mean_q: -702.009318, mean_eps: 0.100000\n",
      " 17820/50000: episode: 3114, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 59214.701823, mae: 1116.498413, accuracy: 0.135417, mean_q: -697.427368, mean_eps: 0.100000\n",
      " 17824/50000: episode: 3115, duration: 0.015s, episode steps:   4, steps per second: 261, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 69110.995117, mae: 1121.637482, accuracy: 0.156250, mean_q: -724.896149, mean_eps: 0.100000\n",
      " 17828/50000: episode: 3116, duration: 0.016s, episode steps:   4, steps per second: 248, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 74008.044922, mae: 1094.812500, accuracy: 0.195312, mean_q: -706.808350, mean_eps: 0.100000\n",
      " 17831/50000: episode: 3117, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 71133.542969, mae: 1117.532104, accuracy: 0.083333, mean_q: -726.239075, mean_eps: 0.100000\n",
      " 17834/50000: episode: 3118, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 67414.713542, mae: 1073.911051, accuracy: 0.125000, mean_q: -710.297302, mean_eps: 0.100000\n",
      " 17838/50000: episode: 3119, duration: 0.015s, episode steps:   4, steps per second: 262, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 59982.005859, mae: 1110.680573, accuracy: 0.148438, mean_q: -720.658417, mean_eps: 0.100000\n",
      " 17841/50000: episode: 3120, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 69531.458333, mae: 1111.759928, accuracy: 0.218750, mean_q: -705.376343, mean_eps: 0.100000\n",
      " 17844/50000: episode: 3121, duration: 0.015s, episode steps:   3, steps per second: 196, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 70311.251302, mae: 1126.468262, accuracy: 0.166667, mean_q: -743.102661, mean_eps: 0.100000\n",
      " 17847/50000: episode: 3122, duration: 0.015s, episode steps:   3, steps per second: 204, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 74063.908854, mae: 1127.282633, accuracy: 0.145833, mean_q: -707.309916, mean_eps: 0.100000\n",
      " 17850/50000: episode: 3123, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 66410.115885, mae: 1100.948242, accuracy: 0.281250, mean_q: -718.380188, mean_eps: 0.100000\n",
      " 17853/50000: episode: 3124, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 69840.484375, mae: 1115.159180, accuracy: 0.156250, mean_q: -710.111450, mean_eps: 0.100000\n",
      " 17856/50000: episode: 3125, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 73060.041667, mae: 1107.948934, accuracy: 0.145833, mean_q: -724.520447, mean_eps: 0.100000\n",
      " 17862/50000: episode: 3126, duration: 0.024s, episode steps:   6, steps per second: 253, episode reward: -3192.000, mean reward: -532.000 [-999.000, -45.000], mean action: 1.833 [0.000, 3.000],  loss: 68255.983073, mae: 1121.338949, accuracy: 0.177083, mean_q: -700.963633, mean_eps: 0.100000\n",
      " 17865/50000: episode: 3127, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 66261.466146, mae: 1106.232015, accuracy: 0.177083, mean_q: -705.783590, mean_eps: 0.100000\n",
      " 17868/50000: episode: 3128, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 62469.268229, mae: 1085.575033, accuracy: 0.229167, mean_q: -678.958272, mean_eps: 0.100000\n",
      " 17871/50000: episode: 3129, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 72177.601562, mae: 1105.935872, accuracy: 0.218750, mean_q: -692.055929, mean_eps: 0.100000\n",
      " 17874/50000: episode: 3130, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 60049.378906, mae: 1111.299764, accuracy: 0.156250, mean_q: -696.018595, mean_eps: 0.100000\n",
      " 17877/50000: episode: 3131, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 64445.804688, mae: 1094.256632, accuracy: 0.177083, mean_q: -686.088542, mean_eps: 0.100000\n",
      " 17880/50000: episode: 3132, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 49252.410156, mae: 1089.856649, accuracy: 0.229167, mean_q: -705.562887, mean_eps: 0.100000\n",
      " 17883/50000: episode: 3133, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 72713.964844, mae: 1106.485148, accuracy: 0.229167, mean_q: -686.157939, mean_eps: 0.100000\n",
      " 17886/50000: episode: 3134, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 62234.587240, mae: 1079.731527, accuracy: 0.145833, mean_q: -687.367106, mean_eps: 0.100000\n",
      " 17889/50000: episode: 3135, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 76684.049479, mae: 1102.750407, accuracy: 0.208333, mean_q: -666.672913, mean_eps: 0.100000\n",
      " 17892/50000: episode: 3136, duration: 0.019s, episode steps:   3, steps per second: 158, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 63036.425781, mae: 1086.338379, accuracy: 0.208333, mean_q: -699.356222, mean_eps: 0.100000\n",
      " 17895/50000: episode: 3137, duration: 0.015s, episode steps:   3, steps per second: 200, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 72246.430990, mae: 1099.600382, accuracy: 0.156250, mean_q: -686.396057, mean_eps: 0.100000\n",
      " 17898/50000: episode: 3138, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 59209.240885, mae: 1055.148763, accuracy: 0.177083, mean_q: -655.015849, mean_eps: 0.100000\n",
      " 17902/50000: episode: 3139, duration: 0.016s, episode steps:   4, steps per second: 254, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 60189.466797, mae: 1074.901031, accuracy: 0.218750, mean_q: -644.391693, mean_eps: 0.100000\n",
      " 17905/50000: episode: 3140, duration: 0.012s, episode steps:   3, steps per second: 255, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 82340.986979, mae: 1079.987590, accuracy: 0.166667, mean_q: -673.365194, mean_eps: 0.100000\n",
      " 17908/50000: episode: 3141, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 65538.255208, mae: 1076.067871, accuracy: 0.250000, mean_q: -644.979065, mean_eps: 0.100000\n",
      " 17911/50000: episode: 3142, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 72772.583333, mae: 1075.962850, accuracy: 0.197917, mean_q: -668.633667, mean_eps: 0.100000\n",
      " 17914/50000: episode: 3143, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 63349.117188, mae: 1063.361857, accuracy: 0.208333, mean_q: -653.350769, mean_eps: 0.100000\n",
      " 17917/50000: episode: 3144, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 68377.117188, mae: 1084.465535, accuracy: 0.093750, mean_q: -675.811686, mean_eps: 0.100000\n",
      " 17920/50000: episode: 3145, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 57355.089844, mae: 1061.729085, accuracy: 0.166667, mean_q: -654.908020, mean_eps: 0.100000\n",
      " 17923/50000: episode: 3146, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 68347.878906, mae: 1071.376668, accuracy: 0.093750, mean_q: -620.769165, mean_eps: 0.100000\n",
      " 17926/50000: episode: 3147, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 58846.584635, mae: 1059.698730, accuracy: 0.218750, mean_q: -647.988627, mean_eps: 0.100000\n",
      " 17930/50000: episode: 3148, duration: 0.015s, episode steps:   4, steps per second: 268, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 64727.013672, mae: 1066.347015, accuracy: 0.140625, mean_q: -646.784515, mean_eps: 0.100000\n",
      " 17934/50000: episode: 3149, duration: 0.016s, episode steps:   4, steps per second: 250, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 57286.256836, mae: 1082.605255, accuracy: 0.125000, mean_q: -683.419479, mean_eps: 0.100000\n",
      " 17937/50000: episode: 3150, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 64413.533854, mae: 1074.528809, accuracy: 0.114583, mean_q: -630.813090, mean_eps: 0.100000\n",
      " 17940/50000: episode: 3151, duration: 0.015s, episode steps:   3, steps per second: 197, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 72681.807292, mae: 1076.163534, accuracy: 0.218750, mean_q: -626.384094, mean_eps: 0.100000\n",
      " 17943/50000: episode: 3152, duration: 0.014s, episode steps:   3, steps per second: 208, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 63805.138021, mae: 1052.394450, accuracy: 0.177083, mean_q: -602.596720, mean_eps: 0.100000\n",
      " 17946/50000: episode: 3153, duration: 0.013s, episode steps:   3, steps per second: 230, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 62758.100260, mae: 1051.589478, accuracy: 0.187500, mean_q: -641.961263, mean_eps: 0.100000\n",
      " 17949/50000: episode: 3154, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 70871.750000, mae: 1076.398844, accuracy: 0.156250, mean_q: -644.957784, mean_eps: 0.100000\n",
      " 17952/50000: episode: 3155, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 71391.476562, mae: 1042.159342, accuracy: 0.135417, mean_q: -647.497091, mean_eps: 0.100000\n",
      " 17955/50000: episode: 3156, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 71232.714844, mae: 1065.031942, accuracy: 0.177083, mean_q: -638.370524, mean_eps: 0.100000\n",
      " 17958/50000: episode: 3157, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 60207.329427, mae: 1051.219727, accuracy: 0.197917, mean_q: -625.521016, mean_eps: 0.100000\n",
      " 17961/50000: episode: 3158, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 50540.726562, mae: 1027.906453, accuracy: 0.239583, mean_q: -632.536174, mean_eps: 0.100000\n",
      " 17964/50000: episode: 3159, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 67794.260417, mae: 1065.644572, accuracy: 0.187500, mean_q: -625.928019, mean_eps: 0.100000\n",
      " 17967/50000: episode: 3160, duration: 0.013s, episode steps:   3, steps per second: 227, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 53211.979167, mae: 1062.436442, accuracy: 0.187500, mean_q: -635.448853, mean_eps: 0.100000\n",
      " 17970/50000: episode: 3161, duration: 0.013s, episode steps:   3, steps per second: 227, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 52363.031250, mae: 1057.887533, accuracy: 0.229167, mean_q: -631.545776, mean_eps: 0.100000\n",
      " 17973/50000: episode: 3162, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 48337.360677, mae: 1045.969198, accuracy: 0.135417, mean_q: -655.573853, mean_eps: 0.100000\n",
      " 17976/50000: episode: 3163, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 66612.063802, mae: 1062.936157, accuracy: 0.239583, mean_q: -637.988281, mean_eps: 0.100000\n",
      " 17979/50000: episode: 3164, duration: 0.012s, episode steps:   3, steps per second: 254, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 51710.781250, mae: 1038.701904, accuracy: 0.166667, mean_q: -627.368449, mean_eps: 0.100000\n",
      " 17983/50000: episode: 3165, duration: 0.016s, episode steps:   4, steps per second: 251, episode reward: -1238.000, mean reward: -309.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 68169.945312, mae: 1068.427826, accuracy: 0.148438, mean_q: -636.601273, mean_eps: 0.100000\n",
      " 17986/50000: episode: 3166, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 65922.618490, mae: 1068.092855, accuracy: 0.114583, mean_q: -617.402873, mean_eps: 0.100000\n",
      " 17989/50000: episode: 3167, duration: 0.015s, episode steps:   3, steps per second: 204, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 55429.596354, mae: 1033.954041, accuracy: 0.197917, mean_q: -636.865967, mean_eps: 0.100000\n",
      " 17992/50000: episode: 3168, duration: 0.015s, episode steps:   3, steps per second: 204, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 65350.994792, mae: 1044.653402, accuracy: 0.104167, mean_q: -640.828634, mean_eps: 0.100000\n",
      " 17995/50000: episode: 3169, duration: 0.013s, episode steps:   3, steps per second: 227, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 47438.110677, mae: 1016.917582, accuracy: 0.197917, mean_q: -634.146647, mean_eps: 0.100000\n",
      " 17998/50000: episode: 3170, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 64070.815104, mae: 1061.770793, accuracy: 0.218750, mean_q: -611.964986, mean_eps: 0.100000\n",
      " 18001/50000: episode: 3171, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.667 [0.000, 3.000],  loss: 67039.091146, mae: 1040.186523, accuracy: 0.166667, mean_q: -605.769775, mean_eps: 0.100000\n",
      " 18005/50000: episode: 3172, duration: 0.015s, episode steps:   4, steps per second: 259, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 52505.959961, mae: 1028.724091, accuracy: 0.257812, mean_q: -614.958954, mean_eps: 0.100000\n",
      " 18008/50000: episode: 3173, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 57522.372396, mae: 1026.637451, accuracy: 0.156250, mean_q: -597.091573, mean_eps: 0.100000\n",
      " 18011/50000: episode: 3174, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 58399.473958, mae: 1037.809367, accuracy: 0.208333, mean_q: -626.356588, mean_eps: 0.100000\n",
      " 18014/50000: episode: 3175, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 54745.111979, mae: 1025.872437, accuracy: 0.208333, mean_q: -594.699829, mean_eps: 0.100000\n",
      " 18017/50000: episode: 3176, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.333 [0.000, 3.000],  loss: 49214.855469, mae: 1022.020732, accuracy: 0.208333, mean_q: -599.214966, mean_eps: 0.100000\n",
      " 18020/50000: episode: 3177, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 62034.148438, mae: 1025.619649, accuracy: 0.218750, mean_q: -616.549194, mean_eps: 0.100000\n",
      " 18023/50000: episode: 3178, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 53927.268229, mae: 1030.504822, accuracy: 0.177083, mean_q: -624.102824, mean_eps: 0.100000\n",
      " 18026/50000: episode: 3179, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 48944.394531, mae: 1026.531474, accuracy: 0.104167, mean_q: -601.550130, mean_eps: 0.100000\n",
      " 18030/50000: episode: 3180, duration: 0.015s, episode steps:   4, steps per second: 268, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 59611.994141, mae: 1044.410431, accuracy: 0.171875, mean_q: -633.117981, mean_eps: 0.100000\n",
      " 18034/50000: episode: 3181, duration: 0.016s, episode steps:   4, steps per second: 254, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 54722.003906, mae: 1016.403625, accuracy: 0.164062, mean_q: -611.618835, mean_eps: 0.100000\n",
      " 18037/50000: episode: 3182, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 46731.088542, mae: 1023.696411, accuracy: 0.218750, mean_q: -599.110921, mean_eps: 0.100000\n",
      " 18040/50000: episode: 3183, duration: 0.018s, episode steps:   3, steps per second: 165, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 61376.606771, mae: 1040.928874, accuracy: 0.145833, mean_q: -597.270772, mean_eps: 0.100000\n",
      " 18043/50000: episode: 3184, duration: 0.017s, episode steps:   3, steps per second: 177, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 53962.072917, mae: 1022.842021, accuracy: 0.208333, mean_q: -606.500020, mean_eps: 0.100000\n",
      " 18047/50000: episode: 3185, duration: 0.019s, episode steps:   4, steps per second: 211, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 55593.074219, mae: 1011.265747, accuracy: 0.234375, mean_q: -564.967560, mean_eps: 0.100000\n",
      " 18051/50000: episode: 3186, duration: 0.019s, episode steps:   4, steps per second: 206, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 44867.608398, mae: 1040.549545, accuracy: 0.210938, mean_q: -644.252457, mean_eps: 0.100000\n",
      " 18054/50000: episode: 3187, duration: 0.013s, episode steps:   3, steps per second: 225, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 51952.533854, mae: 1022.753357, accuracy: 0.229167, mean_q: -587.685893, mean_eps: 0.100000\n",
      " 18057/50000: episode: 3188, duration: 0.014s, episode steps:   3, steps per second: 222, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 48116.555990, mae: 1031.333130, accuracy: 0.135417, mean_q: -614.978495, mean_eps: 0.100000\n",
      " 18060/50000: episode: 3189, duration: 0.014s, episode steps:   3, steps per second: 221, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 58237.015625, mae: 1016.495646, accuracy: 0.218750, mean_q: -582.331828, mean_eps: 0.100000\n",
      " 18063/50000: episode: 3190, duration: 0.014s, episode steps:   3, steps per second: 217, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 48470.287760, mae: 1013.815023, accuracy: 0.166667, mean_q: -554.343547, mean_eps: 0.100000\n",
      " 18066/50000: episode: 3191, duration: 0.014s, episode steps:   3, steps per second: 216, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 62111.345052, mae: 1031.600098, accuracy: 0.177083, mean_q: -604.601095, mean_eps: 0.100000\n",
      " 18070/50000: episode: 3192, duration: 0.018s, episode steps:   4, steps per second: 226, episode reward: -1223.000, mean reward: -305.750 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 47606.842773, mae: 1001.456528, accuracy: 0.156250, mean_q: -608.727600, mean_eps: 0.100000\n",
      " 18073/50000: episode: 3193, duration: 0.014s, episode steps:   3, steps per second: 214, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 48278.575521, mae: 970.161092, accuracy: 0.229167, mean_q: -559.366659, mean_eps: 0.100000\n",
      " 18076/50000: episode: 3194, duration: 0.014s, episode steps:   3, steps per second: 220, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 57034.865885, mae: 1012.536540, accuracy: 0.177083, mean_q: -596.089437, mean_eps: 0.100000\n",
      " 18080/50000: episode: 3195, duration: 0.018s, episode steps:   4, steps per second: 226, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 56637.490234, mae: 999.021103, accuracy: 0.250000, mean_q: -560.558685, mean_eps: 0.100000\n",
      " 18083/50000: episode: 3196, duration: 0.014s, episode steps:   3, steps per second: 222, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 46815.113281, mae: 993.900940, accuracy: 0.156250, mean_q: -585.745789, mean_eps: 0.100000\n",
      " 18086/50000: episode: 3197, duration: 0.014s, episode steps:   3, steps per second: 219, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 50649.467448, mae: 1000.318522, accuracy: 0.260417, mean_q: -572.180766, mean_eps: 0.100000\n",
      " 18090/50000: episode: 3198, duration: 0.018s, episode steps:   4, steps per second: 218, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 47936.666016, mae: 1011.735016, accuracy: 0.164062, mean_q: -564.914780, mean_eps: 0.100000\n",
      " 18094/50000: episode: 3199, duration: 0.021s, episode steps:   4, steps per second: 195, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 51946.208008, mae: 1022.344681, accuracy: 0.164062, mean_q: -589.734512, mean_eps: 0.100000\n",
      " 18098/50000: episode: 3200, duration: 0.017s, episode steps:   4, steps per second: 238, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.000 [0.000, 3.000],  loss: 53711.610352, mae: 1011.566849, accuracy: 0.164062, mean_q: -564.115631, mean_eps: 0.100000\n",
      " 18102/50000: episode: 3201, duration: 0.017s, episode steps:   4, steps per second: 231, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 49625.209961, mae: 990.535706, accuracy: 0.273438, mean_q: -547.199295, mean_eps: 0.100000\n",
      " 18105/50000: episode: 3202, duration: 0.014s, episode steps:   3, steps per second: 210, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 45644.502604, mae: 994.609823, accuracy: 0.260417, mean_q: -569.602173, mean_eps: 0.100000\n",
      " 18108/50000: episode: 3203, duration: 0.014s, episode steps:   3, steps per second: 213, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 52478.662760, mae: 979.645081, accuracy: 0.156250, mean_q: -575.473328, mean_eps: 0.100000\n",
      " 18111/50000: episode: 3204, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 61743.140625, mae: 1002.924805, accuracy: 0.250000, mean_q: -575.620565, mean_eps: 0.100000\n",
      " 18114/50000: episode: 3205, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 47491.130208, mae: 1007.494425, accuracy: 0.166667, mean_q: -575.734680, mean_eps: 0.100000\n",
      " 18117/50000: episode: 3206, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 44757.420573, mae: 983.544352, accuracy: 0.239583, mean_q: -575.302999, mean_eps: 0.100000\n",
      " 18120/50000: episode: 3207, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 58049.713542, mae: 999.020549, accuracy: 0.166667, mean_q: -538.595561, mean_eps: 0.100000\n",
      " 18124/50000: episode: 3208, duration: 0.015s, episode steps:   4, steps per second: 259, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 53493.215820, mae: 990.345917, accuracy: 0.210938, mean_q: -556.807205, mean_eps: 0.100000\n",
      " 18128/50000: episode: 3209, duration: 0.016s, episode steps:   4, steps per second: 253, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 2.250 [1.000, 3.000],  loss: 38641.145508, mae: 993.982712, accuracy: 0.171875, mean_q: -571.790298, mean_eps: 0.100000\n",
      " 18131/50000: episode: 3210, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 55375.713542, mae: 993.789225, accuracy: 0.104167, mean_q: -580.534485, mean_eps: 0.100000\n",
      " 18134/50000: episode: 3211, duration: 0.013s, episode steps:   3, steps per second: 226, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 59015.638021, mae: 999.379740, accuracy: 0.156250, mean_q: -552.585449, mean_eps: 0.100000\n",
      " 18137/50000: episode: 3212, duration: 0.015s, episode steps:   3, steps per second: 201, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 49120.553385, mae: 974.384338, accuracy: 0.114583, mean_q: -579.968689, mean_eps: 0.100000\n",
      " 18140/50000: episode: 3213, duration: 0.015s, episode steps:   3, steps per second: 205, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 51185.654948, mae: 1002.028564, accuracy: 0.187500, mean_q: -553.508036, mean_eps: 0.100000\n",
      " 18143/50000: episode: 3214, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 48990.884115, mae: 980.559143, accuracy: 0.187500, mean_q: -563.701314, mean_eps: 0.100000\n",
      " 18147/50000: episode: 3215, duration: 0.016s, episode steps:   4, steps per second: 254, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 53330.499023, mae: 993.034698, accuracy: 0.164062, mean_q: -534.497284, mean_eps: 0.100000\n",
      " 18150/50000: episode: 3216, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 50776.532552, mae: 981.000346, accuracy: 0.250000, mean_q: -551.417501, mean_eps: 0.100000\n",
      " 18153/50000: episode: 3217, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 48266.666667, mae: 970.915141, accuracy: 0.218750, mean_q: -539.037292, mean_eps: 0.100000\n",
      " 18156/50000: episode: 3218, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 60169.893229, mae: 1003.769979, accuracy: 0.229167, mean_q: -568.858358, mean_eps: 0.100000\n",
      " 18159/50000: episode: 3219, duration: 0.012s, episode steps:   3, steps per second: 253, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 52405.173177, mae: 980.977885, accuracy: 0.166667, mean_q: -565.209513, mean_eps: 0.100000\n",
      " 18162/50000: episode: 3220, duration: 0.012s, episode steps:   3, steps per second: 257, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 45563.791667, mae: 985.459880, accuracy: 0.187500, mean_q: -607.394206, mean_eps: 0.100000\n",
      " 18166/50000: episode: 3221, duration: 0.015s, episode steps:   4, steps per second: 263, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 43617.784180, mae: 975.755264, accuracy: 0.203125, mean_q: -549.581100, mean_eps: 0.100000\n",
      " 18169/50000: episode: 3222, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 41806.355469, mae: 965.229350, accuracy: 0.208333, mean_q: -540.884155, mean_eps: 0.100000\n",
      " 18172/50000: episode: 3223, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 52040.554688, mae: 992.842326, accuracy: 0.229167, mean_q: -570.925537, mean_eps: 0.100000\n",
      " 18175/50000: episode: 3224, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 48239.621094, mae: 980.814779, accuracy: 0.104167, mean_q: -551.009989, mean_eps: 0.100000\n",
      " 18178/50000: episode: 3225, duration: 0.013s, episode steps:   3, steps per second: 226, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 45398.380208, mae: 969.925415, accuracy: 0.145833, mean_q: -543.555990, mean_eps: 0.100000\n",
      " 18181/50000: episode: 3226, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 49473.588542, mae: 983.612590, accuracy: 0.187500, mean_q: -552.238200, mean_eps: 0.100000\n",
      " 18184/50000: episode: 3227, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 54882.923177, mae: 972.042419, accuracy: 0.187500, mean_q: -560.104024, mean_eps: 0.100000\n",
      " 18187/50000: episode: 3228, duration: 0.018s, episode steps:   3, steps per second: 163, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 40113.927083, mae: 951.600688, accuracy: 0.197917, mean_q: -529.280070, mean_eps: 0.100000\n",
      " 18190/50000: episode: 3229, duration: 0.014s, episode steps:   3, steps per second: 214, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 51025.110677, mae: 966.177612, accuracy: 0.239583, mean_q: -540.991272, mean_eps: 0.100000\n",
      " 18193/50000: episode: 3230, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 46599.421875, mae: 976.440918, accuracy: 0.197917, mean_q: -548.188721, mean_eps: 0.100000\n",
      " 18196/50000: episode: 3231, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 42704.032552, mae: 943.608439, accuracy: 0.197917, mean_q: -549.644613, mean_eps: 0.100000\n",
      " 18199/50000: episode: 3232, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 47636.596354, mae: 952.650798, accuracy: 0.218750, mean_q: -504.821401, mean_eps: 0.100000\n",
      " 18202/50000: episode: 3233, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 48245.282552, mae: 970.143168, accuracy: 0.145833, mean_q: -545.129374, mean_eps: 0.100000\n",
      " 18205/50000: episode: 3234, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 50025.559896, mae: 957.980611, accuracy: 0.166667, mean_q: -548.945404, mean_eps: 0.100000\n",
      " 18208/50000: episode: 3235, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 39353.123698, mae: 955.405762, accuracy: 0.260417, mean_q: -503.700765, mean_eps: 0.100000\n",
      " 18212/50000: episode: 3236, duration: 0.015s, episode steps:   4, steps per second: 263, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 49968.365234, mae: 948.813644, accuracy: 0.187500, mean_q: -547.755493, mean_eps: 0.100000\n",
      " 18216/50000: episode: 3237, duration: 0.016s, episode steps:   4, steps per second: 250, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 38924.278320, mae: 944.578979, accuracy: 0.179688, mean_q: -538.516876, mean_eps: 0.100000\n",
      " 18219/50000: episode: 3238, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 45547.468750, mae: 969.292928, accuracy: 0.197917, mean_q: -526.828237, mean_eps: 0.100000\n",
      " 18222/50000: episode: 3239, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 43280.309896, mae: 941.467590, accuracy: 0.187500, mean_q: -515.594655, mean_eps: 0.100000\n",
      " 18226/50000: episode: 3240, duration: 0.015s, episode steps:   4, steps per second: 259, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 41124.754883, mae: 955.631332, accuracy: 0.148438, mean_q: -479.468681, mean_eps: 0.100000\n",
      " 18229/50000: episode: 3241, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 43158.295573, mae: 941.504435, accuracy: 0.187500, mean_q: -521.316182, mean_eps: 0.100000\n",
      " 18233/50000: episode: 3242, duration: 0.015s, episode steps:   4, steps per second: 266, episode reward: -1238.000, mean reward: -309.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 42031.054199, mae: 956.749695, accuracy: 0.203125, mean_q: -522.639786, mean_eps: 0.100000\n",
      " 18236/50000: episode: 3243, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 40909.218750, mae: 948.716146, accuracy: 0.239583, mean_q: -499.026489, mean_eps: 0.100000\n",
      " 18240/50000: episode: 3244, duration: 0.018s, episode steps:   4, steps per second: 216, episode reward: -1223.000, mean reward: -305.750 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 40952.094727, mae: 930.124176, accuracy: 0.226562, mean_q: -512.376930, mean_eps: 0.100000\n",
      " 18243/50000: episode: 3245, duration: 0.015s, episode steps:   3, steps per second: 198, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 60312.786458, mae: 943.831462, accuracy: 0.187500, mean_q: -502.941884, mean_eps: 0.100000\n",
      " 18246/50000: episode: 3246, duration: 0.013s, episode steps:   3, steps per second: 226, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 45773.966146, mae: 933.128540, accuracy: 0.302083, mean_q: -504.736745, mean_eps: 0.100000\n",
      " 18250/50000: episode: 3247, duration: 0.016s, episode steps:   4, steps per second: 245, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 50471.010742, mae: 932.926743, accuracy: 0.125000, mean_q: -500.161522, mean_eps: 0.100000\n",
      " 18253/50000: episode: 3248, duration: 0.013s, episode steps:   3, steps per second: 222, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 40252.359375, mae: 935.156820, accuracy: 0.239583, mean_q: -480.207662, mean_eps: 0.100000\n",
      " 18256/50000: episode: 3249, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 40205.253906, mae: 932.247172, accuracy: 0.208333, mean_q: -467.888062, mean_eps: 0.100000\n",
      " 18259/50000: episode: 3250, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 43859.085938, mae: 935.503560, accuracy: 0.208333, mean_q: -494.505341, mean_eps: 0.100000\n",
      " 18263/50000: episode: 3251, duration: 0.016s, episode steps:   4, steps per second: 250, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 42308.464844, mae: 935.128891, accuracy: 0.156250, mean_q: -508.257385, mean_eps: 0.100000\n",
      " 18268/50000: episode: 3252, duration: 0.020s, episode steps:   5, steps per second: 249, episode reward: -2193.000, mean reward: -438.600 [-999.000, -32.000], mean action: 1.200 [0.000, 3.000],  loss: 44894.038281, mae: 938.715967, accuracy: 0.150000, mean_q: -494.945660, mean_eps: 0.100000\n",
      " 18271/50000: episode: 3253, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 45121.117188, mae: 938.191162, accuracy: 0.208333, mean_q: -500.027608, mean_eps: 0.100000\n",
      " 18274/50000: episode: 3254, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 40704.367188, mae: 932.190633, accuracy: 0.177083, mean_q: -507.386251, mean_eps: 0.100000\n",
      " 18277/50000: episode: 3255, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 40651.605469, mae: 932.916361, accuracy: 0.156250, mean_q: -507.933014, mean_eps: 0.100000\n",
      " 18280/50000: episode: 3256, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 39004.942708, mae: 930.028381, accuracy: 0.239583, mean_q: -475.504242, mean_eps: 0.100000\n",
      " 18283/50000: episode: 3257, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 43382.412760, mae: 924.387756, accuracy: 0.218750, mean_q: -490.321859, mean_eps: 0.100000\n",
      " 18286/50000: episode: 3258, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 34593.209635, mae: 911.997660, accuracy: 0.114583, mean_q: -458.304972, mean_eps: 0.100000\n",
      " 18289/50000: episode: 3259, duration: 0.015s, episode steps:   3, steps per second: 206, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 37081.109375, mae: 890.050354, accuracy: 0.260417, mean_q: -450.746226, mean_eps: 0.100000\n",
      " 18292/50000: episode: 3260, duration: 0.015s, episode steps:   3, steps per second: 194, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 47157.420573, mae: 921.070760, accuracy: 0.114583, mean_q: -476.328379, mean_eps: 0.100000\n",
      " 18295/50000: episode: 3261, duration: 0.014s, episode steps:   3, steps per second: 216, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 43211.095703, mae: 940.256104, accuracy: 0.229167, mean_q: -488.780843, mean_eps: 0.100000\n",
      " 18298/50000: episode: 3262, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 46379.655599, mae: 923.952738, accuracy: 0.229167, mean_q: -474.021739, mean_eps: 0.100000\n",
      " 18301/50000: episode: 3263, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 36119.511068, mae: 937.313700, accuracy: 0.166667, mean_q: -476.485362, mean_eps: 0.100000\n",
      " 18304/50000: episode: 3264, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 29839.283203, mae: 922.305033, accuracy: 0.156250, mean_q: -471.145304, mean_eps: 0.100000\n",
      " 18307/50000: episode: 3265, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 37704.294271, mae: 918.751383, accuracy: 0.135417, mean_q: -472.499603, mean_eps: 0.100000\n",
      " 18310/50000: episode: 3266, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 34913.865885, mae: 917.219950, accuracy: 0.187500, mean_q: -479.812246, mean_eps: 0.100000\n",
      " 18313/50000: episode: 3267, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 51444.167969, mae: 914.692281, accuracy: 0.187500, mean_q: -455.788798, mean_eps: 0.100000\n",
      " 18316/50000: episode: 3268, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 42549.522135, mae: 892.440613, accuracy: 0.260417, mean_q: -431.980143, mean_eps: 0.100000\n",
      " 18319/50000: episode: 3269, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 42932.786458, mae: 917.014852, accuracy: 0.229167, mean_q: -466.020447, mean_eps: 0.100000\n",
      " 18322/50000: episode: 3270, duration: 0.012s, episode steps:   3, steps per second: 253, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 40591.863281, mae: 900.786906, accuracy: 0.218750, mean_q: -456.807251, mean_eps: 0.100000\n",
      " 18325/50000: episode: 3271, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 32907.138021, mae: 901.024984, accuracy: 0.187500, mean_q: -475.540334, mean_eps: 0.100000\n",
      " 18328/50000: episode: 3272, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 34140.570312, mae: 878.326436, accuracy: 0.239583, mean_q: -432.395243, mean_eps: 0.100000\n",
      " 18331/50000: episode: 3273, duration: 0.013s, episode steps:   3, steps per second: 226, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 39982.904948, mae: 870.896423, accuracy: 0.177083, mean_q: -440.175232, mean_eps: 0.100000\n",
      " 18334/50000: episode: 3274, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 31633.503906, mae: 901.820353, accuracy: 0.166667, mean_q: -467.785258, mean_eps: 0.100000\n",
      " 18337/50000: episode: 3275, duration: 0.013s, episode steps:   3, steps per second: 222, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 40460.695312, mae: 926.341634, accuracy: 0.145833, mean_q: -465.222616, mean_eps: 0.100000\n",
      " 18340/50000: episode: 3276, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 35711.431641, mae: 913.299947, accuracy: 0.156250, mean_q: -458.383413, mean_eps: 0.100000\n",
      " 18343/50000: episode: 3277, duration: 0.014s, episode steps:   3, steps per second: 216, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 32421.241536, mae: 908.316325, accuracy: 0.114583, mean_q: -462.879934, mean_eps: 0.100000\n",
      " 18348/50000: episode: 3278, duration: 0.023s, episode steps:   5, steps per second: 213, episode reward: -2193.000, mean reward: -438.600 [-999.000, -45.000], mean action: 1.400 [0.000, 3.000],  loss: 41873.189453, mae: 903.185278, accuracy: 0.137500, mean_q: -460.259088, mean_eps: 0.100000\n",
      " 18351/50000: episode: 3279, duration: 0.014s, episode steps:   3, steps per second: 209, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 30584.094401, mae: 875.008240, accuracy: 0.187500, mean_q: -472.983053, mean_eps: 0.100000\n",
      " 18354/50000: episode: 3280, duration: 0.013s, episode steps:   3, steps per second: 223, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 33766.101562, mae: 872.589152, accuracy: 0.260417, mean_q: -433.344950, mean_eps: 0.100000\n",
      " 18357/50000: episode: 3281, duration: 0.014s, episode steps:   3, steps per second: 216, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 44800.937500, mae: 895.422017, accuracy: 0.208333, mean_q: -447.535299, mean_eps: 0.100000\n",
      " 18360/50000: episode: 3282, duration: 0.014s, episode steps:   3, steps per second: 221, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 44844.946615, mae: 897.895142, accuracy: 0.145833, mean_q: -428.651194, mean_eps: 0.100000\n",
      " 18363/50000: episode: 3283, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 29962.445964, mae: 910.405538, accuracy: 0.114583, mean_q: -460.996236, mean_eps: 0.100000\n",
      " 18367/50000: episode: 3284, duration: 0.016s, episode steps:   4, steps per second: 252, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 35693.762695, mae: 905.777649, accuracy: 0.148438, mean_q: -468.570221, mean_eps: 0.100000\n",
      " 18370/50000: episode: 3285, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 36473.658854, mae: 882.700073, accuracy: 0.187500, mean_q: -431.683797, mean_eps: 0.100000\n",
      " 18373/50000: episode: 3286, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 41536.229167, mae: 903.222493, accuracy: 0.177083, mean_q: -469.773916, mean_eps: 0.100000\n",
      " 18376/50000: episode: 3287, duration: 0.014s, episode steps:   3, steps per second: 221, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 35236.661458, mae: 889.756714, accuracy: 0.208333, mean_q: -440.335154, mean_eps: 0.100000\n",
      " 18379/50000: episode: 3288, duration: 0.015s, episode steps:   3, steps per second: 202, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 44026.908854, mae: 911.051554, accuracy: 0.177083, mean_q: -460.431366, mean_eps: 0.100000\n",
      " 18382/50000: episode: 3289, duration: 0.013s, episode steps:   3, steps per second: 226, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 26948.535807, mae: 887.922750, accuracy: 0.187500, mean_q: -440.616191, mean_eps: 0.100000\n",
      " 18385/50000: episode: 3290, duration: 0.015s, episode steps:   3, steps per second: 201, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 43682.572917, mae: 871.558024, accuracy: 0.218750, mean_q: -416.239644, mean_eps: 0.100000\n",
      " 18388/50000: episode: 3291, duration: 0.014s, episode steps:   3, steps per second: 214, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 41456.731771, mae: 886.292135, accuracy: 0.166667, mean_q: -435.641978, mean_eps: 0.100000\n",
      " 18392/50000: episode: 3292, duration: 0.016s, episode steps:   4, steps per second: 245, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 39863.826172, mae: 899.931976, accuracy: 0.179688, mean_q: -460.086227, mean_eps: 0.100000\n",
      " 18395/50000: episode: 3293, duration: 0.014s, episode steps:   3, steps per second: 222, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 38931.580729, mae: 881.657186, accuracy: 0.208333, mean_q: -464.482402, mean_eps: 0.100000\n",
      " 18398/50000: episode: 3294, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 40206.066406, mae: 903.843628, accuracy: 0.093750, mean_q: -458.116516, mean_eps: 0.100000\n",
      " 18401/50000: episode: 3295, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 32494.284505, mae: 887.647868, accuracy: 0.177083, mean_q: -420.685649, mean_eps: 0.100000\n",
      " 18404/50000: episode: 3296, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 40079.919271, mae: 860.295247, accuracy: 0.177083, mean_q: -441.585114, mean_eps: 0.100000\n",
      " 18407/50000: episode: 3297, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 38369.298177, mae: 896.105184, accuracy: 0.156250, mean_q: -415.491801, mean_eps: 0.100000\n",
      " 18410/50000: episode: 3298, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 33571.654297, mae: 881.011108, accuracy: 0.208333, mean_q: -421.382151, mean_eps: 0.100000\n",
      " 18414/50000: episode: 3299, duration: 0.015s, episode steps:   4, steps per second: 258, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 2.250 [1.000, 3.000],  loss: 34847.233887, mae: 873.555283, accuracy: 0.250000, mean_q: -409.751198, mean_eps: 0.100000\n",
      " 18418/50000: episode: 3300, duration: 0.016s, episode steps:   4, steps per second: 258, episode reward: -1238.000, mean reward: -309.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 37054.002930, mae: 854.251266, accuracy: 0.195312, mean_q: -430.252708, mean_eps: 0.100000\n",
      " 18421/50000: episode: 3301, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 42900.621745, mae: 873.882772, accuracy: 0.187500, mean_q: -424.297150, mean_eps: 0.100000\n",
      " 18424/50000: episode: 3302, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 36425.923828, mae: 884.186462, accuracy: 0.166667, mean_q: -400.046855, mean_eps: 0.100000\n",
      " 18427/50000: episode: 3303, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 32694.894531, mae: 862.100057, accuracy: 0.218750, mean_q: -420.935923, mean_eps: 0.100000\n",
      " 18430/50000: episode: 3304, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 32589.388672, mae: 896.036560, accuracy: 0.145833, mean_q: -422.029744, mean_eps: 0.100000\n",
      " 18434/50000: episode: 3305, duration: 0.016s, episode steps:   4, steps per second: 251, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 28024.846924, mae: 867.616180, accuracy: 0.171875, mean_q: -431.930450, mean_eps: 0.100000\n",
      " 18437/50000: episode: 3306, duration: 0.017s, episode steps:   3, steps per second: 179, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 38943.186198, mae: 870.202372, accuracy: 0.145833, mean_q: -427.892263, mean_eps: 0.100000\n",
      " 18440/50000: episode: 3307, duration: 0.015s, episode steps:   3, steps per second: 205, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 30554.248047, mae: 868.221802, accuracy: 0.125000, mean_q: -413.589172, mean_eps: 0.100000\n",
      " 18443/50000: episode: 3308, duration: 0.013s, episode steps:   3, steps per second: 230, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 39667.789062, mae: 889.324178, accuracy: 0.177083, mean_q: -410.756765, mean_eps: 0.100000\n",
      " 18446/50000: episode: 3309, duration: 0.013s, episode steps:   3, steps per second: 226, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 34318.401042, mae: 849.967061, accuracy: 0.166667, mean_q: -434.334859, mean_eps: 0.100000\n",
      " 18449/50000: episode: 3310, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 31302.285807, mae: 869.137614, accuracy: 0.083333, mean_q: -446.324544, mean_eps: 0.100000\n",
      " 18453/50000: episode: 3311, duration: 0.015s, episode steps:   4, steps per second: 260, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 23991.095947, mae: 876.851044, accuracy: 0.101562, mean_q: -403.784897, mean_eps: 0.100000\n",
      " 18456/50000: episode: 3312, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 41909.608073, mae: 856.733785, accuracy: 0.166667, mean_q: -400.554738, mean_eps: 0.100000\n",
      " 18459/50000: episode: 3313, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 27727.736328, mae: 850.013428, accuracy: 0.197917, mean_q: -411.754995, mean_eps: 0.100000\n",
      " 18462/50000: episode: 3314, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 39976.187500, mae: 873.172384, accuracy: 0.166667, mean_q: -397.584188, mean_eps: 0.100000\n",
      " 18465/50000: episode: 3315, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 34309.839844, mae: 856.367045, accuracy: 0.135417, mean_q: -454.324473, mean_eps: 0.100000\n",
      " 18468/50000: episode: 3316, duration: 0.012s, episode steps:   3, steps per second: 253, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 38273.162109, mae: 858.752401, accuracy: 0.250000, mean_q: -408.235901, mean_eps: 0.100000\n",
      " 18471/50000: episode: 3317, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 37955.938802, mae: 874.981425, accuracy: 0.239583, mean_q: -426.388153, mean_eps: 0.100000\n",
      " 18474/50000: episode: 3318, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 46590.686198, mae: 865.716715, accuracy: 0.093750, mean_q: -455.838674, mean_eps: 0.100000\n",
      " 18477/50000: episode: 3319, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 29738.529297, mae: 834.302409, accuracy: 0.156250, mean_q: -401.065308, mean_eps: 0.100000\n",
      " 18480/50000: episode: 3320, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 42569.908203, mae: 863.333944, accuracy: 0.187500, mean_q: -433.866038, mean_eps: 0.100000\n",
      " 18484/50000: episode: 3321, duration: 0.015s, episode steps:   4, steps per second: 261, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 32848.784668, mae: 838.766937, accuracy: 0.156250, mean_q: -420.124512, mean_eps: 0.100000\n",
      " 18487/50000: episode: 3322, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 28660.392578, mae: 856.376587, accuracy: 0.208333, mean_q: -351.676656, mean_eps: 0.100000\n",
      " 18490/50000: episode: 3323, duration: 0.013s, episode steps:   3, steps per second: 227, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 33811.734375, mae: 837.954671, accuracy: 0.125000, mean_q: -419.470133, mean_eps: 0.100000\n",
      " 18494/50000: episode: 3324, duration: 0.020s, episode steps:   4, steps per second: 205, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 28556.779785, mae: 851.456833, accuracy: 0.179688, mean_q: -400.149651, mean_eps: 0.100000\n",
      " 18497/50000: episode: 3325, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 38246.829427, mae: 854.138102, accuracy: 0.166667, mean_q: -391.312531, mean_eps: 0.100000\n",
      " 18500/50000: episode: 3326, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 29465.856771, mae: 871.028056, accuracy: 0.114583, mean_q: -384.059926, mean_eps: 0.100000\n",
      " 18503/50000: episode: 3327, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 33521.693359, mae: 845.101278, accuracy: 0.166667, mean_q: -390.487366, mean_eps: 0.100000\n",
      " 18506/50000: episode: 3328, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 25138.077474, mae: 842.561279, accuracy: 0.177083, mean_q: -398.384104, mean_eps: 0.100000\n",
      " 18511/50000: episode: 3329, duration: 0.018s, episode steps:   5, steps per second: 274, episode reward: -2193.000, mean reward: -438.600 [-999.000, -32.000], mean action: 1.400 [0.000, 3.000],  loss: 31358.413281, mae: 846.568921, accuracy: 0.131250, mean_q: -415.082520, mean_eps: 0.100000\n",
      " 18514/50000: episode: 3330, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 38548.809896, mae: 862.220662, accuracy: 0.135417, mean_q: -414.798187, mean_eps: 0.100000\n",
      " 18517/50000: episode: 3331, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 29483.362630, mae: 848.792826, accuracy: 0.166667, mean_q: -391.396739, mean_eps: 0.100000\n",
      " 18520/50000: episode: 3332, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 34134.199219, mae: 838.487345, accuracy: 0.156250, mean_q: -393.786499, mean_eps: 0.100000\n",
      " 18523/50000: episode: 3333, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 28607.936198, mae: 837.975118, accuracy: 0.125000, mean_q: -367.718191, mean_eps: 0.100000\n",
      " 18527/50000: episode: 3334, duration: 0.015s, episode steps:   4, steps per second: 260, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 2.250 [1.000, 3.000],  loss: 32825.707031, mae: 840.509171, accuracy: 0.195312, mean_q: -394.226082, mean_eps: 0.100000\n",
      " 18531/50000: episode: 3335, duration: 0.016s, episode steps:   4, steps per second: 253, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 31254.316406, mae: 862.876251, accuracy: 0.101562, mean_q: -406.561989, mean_eps: 0.100000\n",
      " 18534/50000: episode: 3336, duration: 0.013s, episode steps:   3, steps per second: 226, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 31962.549479, mae: 851.442037, accuracy: 0.104167, mean_q: -370.829600, mean_eps: 0.100000\n",
      " 18537/50000: episode: 3337, duration: 0.018s, episode steps:   3, steps per second: 165, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 36055.190755, mae: 842.232483, accuracy: 0.135417, mean_q: -379.726888, mean_eps: 0.100000\n",
      " 18540/50000: episode: 3338, duration: 0.016s, episode steps:   3, steps per second: 189, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 29860.257812, mae: 833.563883, accuracy: 0.187500, mean_q: -383.944672, mean_eps: 0.100000\n",
      " 18543/50000: episode: 3339, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 30202.537109, mae: 852.386943, accuracy: 0.125000, mean_q: -393.906494, mean_eps: 0.100000\n",
      " 18546/50000: episode: 3340, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 29684.108073, mae: 821.096456, accuracy: 0.197917, mean_q: -362.707275, mean_eps: 0.100000\n",
      " 18549/50000: episode: 3341, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 38134.324219, mae: 839.873108, accuracy: 0.093750, mean_q: -371.376465, mean_eps: 0.100000\n",
      " 18552/50000: episode: 3342, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 28173.838542, mae: 829.805623, accuracy: 0.166667, mean_q: -395.976959, mean_eps: 0.100000\n",
      " 18555/50000: episode: 3343, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 24125.270833, mae: 808.577372, accuracy: 0.197917, mean_q: -365.799693, mean_eps: 0.100000\n",
      " 18558/50000: episode: 3344, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 26963.667318, mae: 811.044373, accuracy: 0.166667, mean_q: -360.458527, mean_eps: 0.100000\n",
      " 18561/50000: episode: 3345, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 31086.743490, mae: 827.629435, accuracy: 0.197917, mean_q: -356.623352, mean_eps: 0.100000\n",
      " 18564/50000: episode: 3346, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 31831.328125, mae: 809.129150, accuracy: 0.229167, mean_q: -339.738454, mean_eps: 0.100000\n",
      " 18567/50000: episode: 3347, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 32812.563802, mae: 831.045858, accuracy: 0.135417, mean_q: -414.750641, mean_eps: 0.100000\n",
      " 18570/50000: episode: 3348, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 35025.238281, mae: 833.080627, accuracy: 0.104167, mean_q: -351.152924, mean_eps: 0.100000\n",
      " 18573/50000: episode: 3349, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 31816.285807, mae: 817.498494, accuracy: 0.208333, mean_q: -359.194214, mean_eps: 0.100000\n",
      " 18576/50000: episode: 3350, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 28968.171875, mae: 827.862142, accuracy: 0.208333, mean_q: -330.978912, mean_eps: 0.100000\n",
      " 18579/50000: episode: 3351, duration: 0.014s, episode steps:   3, steps per second: 222, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 25786.190755, mae: 811.782837, accuracy: 0.187500, mean_q: -380.201742, mean_eps: 0.100000\n",
      " 18583/50000: episode: 3352, duration: 0.016s, episode steps:   4, steps per second: 255, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 36680.192383, mae: 828.540833, accuracy: 0.156250, mean_q: -384.046928, mean_eps: 0.100000\n",
      " 18587/50000: episode: 3353, duration: 0.016s, episode steps:   4, steps per second: 254, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.000 [0.000, 3.000],  loss: 24410.025391, mae: 833.960022, accuracy: 0.171875, mean_q: -377.130180, mean_eps: 0.100000\n",
      " 18590/50000: episode: 3354, duration: 0.014s, episode steps:   3, steps per second: 209, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 35963.550130, mae: 840.550496, accuracy: 0.114583, mean_q: -390.352844, mean_eps: 0.100000\n",
      " 18593/50000: episode: 3355, duration: 0.013s, episode steps:   3, steps per second: 227, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 25679.710286, mae: 822.040466, accuracy: 0.166667, mean_q: -365.727814, mean_eps: 0.100000\n",
      " 18596/50000: episode: 3356, duration: 0.013s, episode steps:   3, steps per second: 225, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 35989.938802, mae: 828.806112, accuracy: 0.145833, mean_q: -375.414998, mean_eps: 0.100000\n",
      " 18599/50000: episode: 3357, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 25672.831706, mae: 828.404236, accuracy: 0.104167, mean_q: -392.391510, mean_eps: 0.100000\n",
      " 18602/50000: episode: 3358, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 28938.193359, mae: 817.719727, accuracy: 0.135417, mean_q: -338.072042, mean_eps: 0.100000\n",
      " 18605/50000: episode: 3359, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 29152.762370, mae: 820.091858, accuracy: 0.156250, mean_q: -372.867310, mean_eps: 0.100000\n",
      " 18610/50000: episode: 3360, duration: 0.018s, episode steps:   5, steps per second: 281, episode reward: -2193.000, mean reward: -438.600 [-999.000, -45.000], mean action: 1.600 [0.000, 3.000],  loss: 30071.695703, mae: 833.506763, accuracy: 0.131250, mean_q: -368.460675, mean_eps: 0.100000\n",
      " 18614/50000: episode: 3361, duration: 0.015s, episode steps:   4, steps per second: 264, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 30223.925293, mae: 824.963257, accuracy: 0.132812, mean_q: -359.553047, mean_eps: 0.100000\n",
      " 18617/50000: episode: 3362, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 39907.787760, mae: 822.111206, accuracy: 0.166667, mean_q: -354.340535, mean_eps: 0.100000\n",
      " 18620/50000: episode: 3363, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 34791.271484, mae: 810.697978, accuracy: 0.104167, mean_q: -346.056498, mean_eps: 0.100000\n",
      " 18623/50000: episode: 3364, duration: 0.012s, episode steps:   3, steps per second: 254, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 33565.933594, mae: 804.041219, accuracy: 0.208333, mean_q: -337.801127, mean_eps: 0.100000\n",
      " 18626/50000: episode: 3365, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 23623.234375, mae: 809.952637, accuracy: 0.197917, mean_q: -303.133148, mean_eps: 0.100000\n",
      " 18629/50000: episode: 3366, duration: 0.012s, episode steps:   3, steps per second: 253, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 30425.106120, mae: 817.716268, accuracy: 0.145833, mean_q: -364.573181, mean_eps: 0.100000\n",
      " 18633/50000: episode: 3367, duration: 0.015s, episode steps:   4, steps per second: 258, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 25453.283691, mae: 813.419601, accuracy: 0.179688, mean_q: -347.155029, mean_eps: 0.100000\n",
      " 18638/50000: episode: 3368, duration: 0.018s, episode steps:   5, steps per second: 270, episode reward: -2193.000, mean reward: -438.600 [-999.000, -32.000], mean action: 1.400 [0.000, 3.000],  loss: 27056.090234, mae: 830.437451, accuracy: 0.125000, mean_q: -359.355408, mean_eps: 0.100000\n",
      " 18642/50000: episode: 3369, duration: 0.023s, episode steps:   4, steps per second: 175, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 33588.475586, mae: 805.762604, accuracy: 0.195312, mean_q: -353.570694, mean_eps: 0.100000\n",
      " 18645/50000: episode: 3370, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 37361.930990, mae: 848.450338, accuracy: 0.135417, mean_q: -367.886597, mean_eps: 0.100000\n",
      " 18648/50000: episode: 3371, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 30427.227214, mae: 836.478984, accuracy: 0.114583, mean_q: -345.128815, mean_eps: 0.100000\n",
      " 18651/50000: episode: 3372, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 33309.350911, mae: 819.688944, accuracy: 0.145833, mean_q: -353.407298, mean_eps: 0.100000\n",
      " 18654/50000: episode: 3373, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 35990.084635, mae: 838.372559, accuracy: 0.104167, mean_q: -357.968577, mean_eps: 0.100000\n",
      " 18657/50000: episode: 3374, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 31969.016927, mae: 811.959351, accuracy: 0.156250, mean_q: -354.488637, mean_eps: 0.100000\n",
      " 18660/50000: episode: 3375, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 30728.770833, mae: 804.753052, accuracy: 0.218750, mean_q: -326.993439, mean_eps: 0.100000\n",
      " 18663/50000: episode: 3376, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 33669.287760, mae: 811.553446, accuracy: 0.156250, mean_q: -349.140259, mean_eps: 0.100000\n",
      " 18666/50000: episode: 3377, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 23296.726562, mae: 814.535075, accuracy: 0.187500, mean_q: -341.498352, mean_eps: 0.100000\n",
      " 18669/50000: episode: 3378, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 35476.776042, mae: 791.103048, accuracy: 0.239583, mean_q: -355.011922, mean_eps: 0.100000\n",
      " 18672/50000: episode: 3379, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 29612.744141, mae: 817.125529, accuracy: 0.135417, mean_q: -360.339427, mean_eps: 0.100000\n",
      " 18675/50000: episode: 3380, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 28268.764974, mae: 796.967712, accuracy: 0.218750, mean_q: -338.670329, mean_eps: 0.100000\n",
      " 18678/50000: episode: 3381, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 28981.298177, mae: 805.149007, accuracy: 0.145833, mean_q: -332.291219, mean_eps: 0.100000\n",
      " 18681/50000: episode: 3382, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 30402.237630, mae: 792.766988, accuracy: 0.135417, mean_q: -324.628001, mean_eps: 0.100000\n",
      " 18684/50000: episode: 3383, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 25223.669271, mae: 796.836772, accuracy: 0.177083, mean_q: -320.880503, mean_eps: 0.100000\n",
      " 18687/50000: episode: 3384, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 29852.920573, mae: 789.220398, accuracy: 0.187500, mean_q: -328.033488, mean_eps: 0.100000\n",
      " 18690/50000: episode: 3385, duration: 0.014s, episode steps:   3, steps per second: 218, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 31423.148438, mae: 805.455587, accuracy: 0.114583, mean_q: -353.608602, mean_eps: 0.100000\n",
      " 18694/50000: episode: 3386, duration: 0.018s, episode steps:   4, steps per second: 218, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 32857.706055, mae: 795.988678, accuracy: 0.179688, mean_q: -324.385498, mean_eps: 0.100000\n",
      " 18697/50000: episode: 3387, duration: 0.014s, episode steps:   3, steps per second: 211, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 33709.879557, mae: 817.553935, accuracy: 0.156250, mean_q: -367.143911, mean_eps: 0.100000\n",
      " 18700/50000: episode: 3388, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 28426.335286, mae: 794.950155, accuracy: 0.125000, mean_q: -353.832926, mean_eps: 0.100000\n",
      " 18703/50000: episode: 3389, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 24623.072917, mae: 806.818766, accuracy: 0.156250, mean_q: -336.749044, mean_eps: 0.100000\n",
      " 18706/50000: episode: 3390, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 30955.945312, mae: 799.621073, accuracy: 0.125000, mean_q: -363.779022, mean_eps: 0.100000\n",
      " 18709/50000: episode: 3391, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 26174.677734, mae: 782.021647, accuracy: 0.156250, mean_q: -343.164185, mean_eps: 0.100000\n",
      " 18712/50000: episode: 3392, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 27241.852865, mae: 806.696493, accuracy: 0.145833, mean_q: -348.613027, mean_eps: 0.100000\n",
      " 18715/50000: episode: 3393, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 24761.113281, mae: 810.722656, accuracy: 0.125000, mean_q: -326.674815, mean_eps: 0.100000\n",
      " 18718/50000: episode: 3394, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 29832.578776, mae: 790.114461, accuracy: 0.166667, mean_q: -343.555735, mean_eps: 0.100000\n",
      " 18721/50000: episode: 3395, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 26238.769531, mae: 786.567627, accuracy: 0.156250, mean_q: -350.674561, mean_eps: 0.100000\n",
      " 18724/50000: episode: 3396, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 22866.742188, mae: 808.874715, accuracy: 0.083333, mean_q: -320.706024, mean_eps: 0.100000\n",
      " 18728/50000: episode: 3397, duration: 0.015s, episode steps:   4, steps per second: 264, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 25213.991699, mae: 793.386078, accuracy: 0.156250, mean_q: -301.305908, mean_eps: 0.100000\n",
      " 18731/50000: episode: 3398, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 26967.293620, mae: 788.864441, accuracy: 0.145833, mean_q: -308.667430, mean_eps: 0.100000\n",
      " 18734/50000: episode: 3399, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 34092.522135, mae: 775.429769, accuracy: 0.145833, mean_q: -318.403859, mean_eps: 0.100000\n",
      " 18737/50000: episode: 3400, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 28118.455078, mae: 798.656209, accuracy: 0.125000, mean_q: -309.629283, mean_eps: 0.100000\n",
      " 18740/50000: episode: 3401, duration: 0.014s, episode steps:   3, steps per second: 209, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 29157.854818, mae: 779.191508, accuracy: 0.145833, mean_q: -333.519887, mean_eps: 0.100000\n",
      " 18743/50000: episode: 3402, duration: 0.014s, episode steps:   3, steps per second: 214, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 23375.328125, mae: 752.580058, accuracy: 0.093750, mean_q: -303.684092, mean_eps: 0.100000\n",
      " 18747/50000: episode: 3403, duration: 0.017s, episode steps:   4, steps per second: 240, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 31193.827148, mae: 785.068207, accuracy: 0.132812, mean_q: -309.805916, mean_eps: 0.100000\n",
      " 18750/50000: episode: 3404, duration: 0.012s, episode steps:   3, steps per second: 253, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 25717.062500, mae: 776.793416, accuracy: 0.239583, mean_q: -321.266541, mean_eps: 0.100000\n",
      " 18753/50000: episode: 3405, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 29845.085286, mae: 793.537109, accuracy: 0.135417, mean_q: -336.349548, mean_eps: 0.100000\n",
      " 18756/50000: episode: 3406, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 23972.502604, mae: 774.694234, accuracy: 0.145833, mean_q: -330.492218, mean_eps: 0.100000\n",
      " 18759/50000: episode: 3407, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 26731.622396, mae: 773.968974, accuracy: 0.197917, mean_q: -313.067632, mean_eps: 0.100000\n",
      " 18762/50000: episode: 3408, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 27491.085286, mae: 783.325236, accuracy: 0.145833, mean_q: -317.040243, mean_eps: 0.100000\n",
      " 18765/50000: episode: 3409, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 24368.771484, mae: 773.212931, accuracy: 0.187500, mean_q: -302.530467, mean_eps: 0.100000\n",
      " 18768/50000: episode: 3410, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 24517.095052, mae: 769.919942, accuracy: 0.145833, mean_q: -317.962616, mean_eps: 0.100000\n",
      " 18771/50000: episode: 3411, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 22745.085938, mae: 784.129659, accuracy: 0.125000, mean_q: -311.495962, mean_eps: 0.100000\n",
      " 18774/50000: episode: 3412, duration: 0.012s, episode steps:   3, steps per second: 253, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 23266.942708, mae: 776.742920, accuracy: 0.145833, mean_q: -279.951274, mean_eps: 0.100000\n",
      " 18777/50000: episode: 3413, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 33273.174479, mae: 792.261312, accuracy: 0.125000, mean_q: -305.358103, mean_eps: 0.100000\n",
      " 18780/50000: episode: 3414, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 21607.473307, mae: 801.434224, accuracy: 0.093750, mean_q: -305.096303, mean_eps: 0.100000\n",
      " 18783/50000: episode: 3415, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 36716.871094, mae: 780.350077, accuracy: 0.125000, mean_q: -288.143799, mean_eps: 0.100000\n",
      " 18786/50000: episode: 3416, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 34022.195964, mae: 769.916870, accuracy: 0.156250, mean_q: -320.318075, mean_eps: 0.100000\n",
      " 18789/50000: episode: 3417, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 23902.049479, mae: 799.655192, accuracy: 0.135417, mean_q: -307.806000, mean_eps: 0.100000\n",
      " 18792/50000: episode: 3418, duration: 0.013s, episode steps:   3, steps per second: 227, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 18472.264323, mae: 754.104655, accuracy: 0.187500, mean_q: -266.432724, mean_eps: 0.100000\n",
      " 18795/50000: episode: 3419, duration: 0.018s, episode steps:   3, steps per second: 164, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 32156.144531, mae: 774.926575, accuracy: 0.104167, mean_q: -289.230245, mean_eps: 0.100000\n",
      " 18798/50000: episode: 3420, duration: 0.014s, episode steps:   3, steps per second: 213, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 24434.717448, mae: 767.932190, accuracy: 0.187500, mean_q: -254.474986, mean_eps: 0.100000\n",
      " 18801/50000: episode: 3421, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 24773.216146, mae: 753.644368, accuracy: 0.072917, mean_q: -284.608022, mean_eps: 0.100000\n",
      " 18804/50000: episode: 3422, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 24870.951823, mae: 769.294474, accuracy: 0.218750, mean_q: -285.773412, mean_eps: 0.100000\n",
      " 18807/50000: episode: 3423, duration: 0.012s, episode steps:   3, steps per second: 254, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 24780.765625, mae: 759.595439, accuracy: 0.135417, mean_q: -273.607168, mean_eps: 0.100000\n",
      " 18810/50000: episode: 3424, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 24049.189453, mae: 754.335327, accuracy: 0.135417, mean_q: -280.729258, mean_eps: 0.100000\n",
      " 18813/50000: episode: 3425, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 21345.125651, mae: 785.295451, accuracy: 0.156250, mean_q: -300.941518, mean_eps: 0.100000\n",
      " 18816/50000: episode: 3426, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 29676.354167, mae: 745.007975, accuracy: 0.135417, mean_q: -291.711405, mean_eps: 0.100000\n",
      " 18819/50000: episode: 3427, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -224.000, mean reward: -74.667 [-116.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 25430.063151, mae: 759.795614, accuracy: 0.125000, mean_q: -291.554555, mean_eps: 0.100000\n",
      " 18823/50000: episode: 3428, duration: 0.015s, episode steps:   4, steps per second: 260, episode reward: -1238.000, mean reward: -309.500 [-999.000, -60.000], mean action: 2.250 [1.000, 3.000],  loss: 29150.113281, mae: 776.472260, accuracy: 0.164062, mean_q: -295.985497, mean_eps: 0.100000\n",
      " 18826/50000: episode: 3429, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 24840.397786, mae: 782.959473, accuracy: 0.135417, mean_q: -308.086243, mean_eps: 0.100000\n",
      " 18830/50000: episode: 3430, duration: 0.015s, episode steps:   4, steps per second: 266, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 24333.264160, mae: 762.935120, accuracy: 0.164062, mean_q: -325.221443, mean_eps: 0.100000\n",
      " 18833/50000: episode: 3431, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 27430.610026, mae: 776.927002, accuracy: 0.093750, mean_q: -293.860708, mean_eps: 0.100000\n",
      " 18836/50000: episode: 3432, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 23937.178385, mae: 743.969564, accuracy: 0.197917, mean_q: -254.034068, mean_eps: 0.100000\n",
      " 18839/50000: episode: 3433, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 27871.059896, mae: 779.479431, accuracy: 0.125000, mean_q: -290.500997, mean_eps: 0.100000\n",
      " 18842/50000: episode: 3434, duration: 0.015s, episode steps:   3, steps per second: 198, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 26991.869792, mae: 765.757528, accuracy: 0.145833, mean_q: -293.162201, mean_eps: 0.100000\n",
      " 18846/50000: episode: 3435, duration: 0.018s, episode steps:   4, steps per second: 225, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 26590.995605, mae: 745.989182, accuracy: 0.218750, mean_q: -273.586979, mean_eps: 0.100000\n",
      " 18850/50000: episode: 3436, duration: 0.015s, episode steps:   4, steps per second: 260, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 23616.207520, mae: 741.698441, accuracy: 0.203125, mean_q: -288.393776, mean_eps: 0.100000\n",
      " 18853/50000: episode: 3437, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 23302.246094, mae: 733.199504, accuracy: 0.208333, mean_q: -251.880442, mean_eps: 0.100000\n",
      " 18856/50000: episode: 3438, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 24567.738281, mae: 743.160055, accuracy: 0.177083, mean_q: -260.044266, mean_eps: 0.100000\n",
      " 18860/50000: episode: 3439, duration: 0.016s, episode steps:   4, steps per second: 256, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 25902.623535, mae: 750.659363, accuracy: 0.148438, mean_q: -275.781235, mean_eps: 0.100000\n",
      " 18864/50000: episode: 3440, duration: 0.016s, episode steps:   4, steps per second: 251, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 28550.081055, mae: 750.150589, accuracy: 0.117188, mean_q: -270.594315, mean_eps: 0.100000\n",
      " 18867/50000: episode: 3441, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 20410.188151, mae: 729.082621, accuracy: 0.093750, mean_q: -291.064423, mean_eps: 0.100000\n",
      " 18870/50000: episode: 3442, duration: 0.012s, episode steps:   3, steps per second: 254, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 23148.940755, mae: 745.109619, accuracy: 0.187500, mean_q: -284.486120, mean_eps: 0.100000\n",
      " 18873/50000: episode: 3443, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 23215.811198, mae: 760.565633, accuracy: 0.177083, mean_q: -252.481389, mean_eps: 0.100000\n",
      " 18876/50000: episode: 3444, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 23577.151042, mae: 745.611857, accuracy: 0.135417, mean_q: -245.082326, mean_eps: 0.100000\n",
      " 18879/50000: episode: 3445, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 29936.093099, mae: 736.340169, accuracy: 0.156250, mean_q: -267.338531, mean_eps: 0.100000\n",
      " 18883/50000: episode: 3446, duration: 0.015s, episode steps:   4, steps per second: 272, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 24766.284180, mae: 748.216217, accuracy: 0.117188, mean_q: -263.187489, mean_eps: 0.100000\n",
      " 18886/50000: episode: 3447, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 19937.250977, mae: 732.721741, accuracy: 0.145833, mean_q: -267.732880, mean_eps: 0.100000\n",
      " 18889/50000: episode: 3448, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 24106.123698, mae: 754.925334, accuracy: 0.125000, mean_q: -256.112651, mean_eps: 0.100000\n",
      " 18892/50000: episode: 3449, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 25711.394531, mae: 744.983602, accuracy: 0.166667, mean_q: -274.011134, mean_eps: 0.100000\n",
      " 18895/50000: episode: 3450, duration: 0.012s, episode steps:   3, steps per second: 255, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 23249.351562, mae: 748.849080, accuracy: 0.145833, mean_q: -266.908335, mean_eps: 0.100000\n",
      " 18898/50000: episode: 3451, duration: 0.018s, episode steps:   3, steps per second: 171, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 28137.218099, mae: 730.995178, accuracy: 0.208333, mean_q: -266.448207, mean_eps: 0.100000\n",
      " 18901/50000: episode: 3452, duration: 0.015s, episode steps:   3, steps per second: 196, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 29772.897786, mae: 738.106445, accuracy: 0.218750, mean_q: -261.194509, mean_eps: 0.100000\n",
      " 18904/50000: episode: 3453, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 21953.184896, mae: 751.740112, accuracy: 0.177083, mean_q: -284.634939, mean_eps: 0.100000\n",
      " 18907/50000: episode: 3454, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 22847.725911, mae: 737.188416, accuracy: 0.177083, mean_q: -240.315760, mean_eps: 0.100000\n",
      " 18910/50000: episode: 3455, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 22808.363281, mae: 735.898722, accuracy: 0.125000, mean_q: -267.424154, mean_eps: 0.100000\n",
      " 18913/50000: episode: 3456, duration: 0.012s, episode steps:   3, steps per second: 257, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 23680.419271, mae: 753.407349, accuracy: 0.114583, mean_q: -285.773407, mean_eps: 0.100000\n",
      " 18916/50000: episode: 3457, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 20022.707031, mae: 738.728495, accuracy: 0.166667, mean_q: -256.528676, mean_eps: 0.100000\n",
      " 18919/50000: episode: 3458, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 21664.122396, mae: 747.095581, accuracy: 0.156250, mean_q: -230.808385, mean_eps: 0.100000\n",
      " 18922/50000: episode: 3459, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 21188.905599, mae: 708.148926, accuracy: 0.218750, mean_q: -252.500768, mean_eps: 0.100000\n",
      " 18926/50000: episode: 3460, duration: 0.015s, episode steps:   4, steps per second: 264, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 22938.433594, mae: 722.999634, accuracy: 0.148438, mean_q: -229.304260, mean_eps: 0.100000\n",
      " 18929/50000: episode: 3461, duration: 0.012s, episode steps:   3, steps per second: 255, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 22370.896484, mae: 741.843160, accuracy: 0.114583, mean_q: -261.241516, mean_eps: 0.100000\n",
      " 18932/50000: episode: 3462, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 22561.571615, mae: 730.828817, accuracy: 0.125000, mean_q: -265.609782, mean_eps: 0.100000\n",
      " 18935/50000: episode: 3463, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 24347.888672, mae: 733.015747, accuracy: 0.135417, mean_q: -253.559107, mean_eps: 0.100000\n",
      " 18938/50000: episode: 3464, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 23458.830729, mae: 739.474121, accuracy: 0.156250, mean_q: -237.064290, mean_eps: 0.100000\n",
      " 18941/50000: episode: 3465, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 18917.930664, mae: 733.869181, accuracy: 0.135417, mean_q: -285.913391, mean_eps: 0.100000\n",
      " 18944/50000: episode: 3466, duration: 0.013s, episode steps:   3, steps per second: 226, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 22206.856120, mae: 715.789225, accuracy: 0.166667, mean_q: -229.886739, mean_eps: 0.100000\n",
      " 18947/50000: episode: 3467, duration: 0.015s, episode steps:   3, steps per second: 201, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 27147.232422, mae: 729.700745, accuracy: 0.104167, mean_q: -237.104980, mean_eps: 0.100000\n",
      " 18950/50000: episode: 3468, duration: 0.015s, episode steps:   3, steps per second: 205, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 28572.803385, mae: 722.632772, accuracy: 0.166667, mean_q: -251.646805, mean_eps: 0.100000\n",
      " 18953/50000: episode: 3469, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 27557.900391, mae: 740.421305, accuracy: 0.145833, mean_q: -260.373657, mean_eps: 0.100000\n",
      " 18957/50000: episode: 3470, duration: 0.016s, episode steps:   4, steps per second: 254, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 22891.534180, mae: 718.864151, accuracy: 0.148438, mean_q: -255.116135, mean_eps: 0.100000\n",
      " 18960/50000: episode: 3471, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 21976.959635, mae: 728.153727, accuracy: 0.114583, mean_q: -217.923299, mean_eps: 0.100000\n",
      " 18963/50000: episode: 3472, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 25578.407552, mae: 711.436523, accuracy: 0.145833, mean_q: -219.180918, mean_eps: 0.100000\n",
      " 18966/50000: episode: 3473, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 20173.910807, mae: 701.836161, accuracy: 0.125000, mean_q: -249.284739, mean_eps: 0.100000\n",
      " 18969/50000: episode: 3474, duration: 0.012s, episode steps:   3, steps per second: 254, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 19012.895182, mae: 695.703328, accuracy: 0.197917, mean_q: -244.190425, mean_eps: 0.100000\n",
      " 18972/50000: episode: 3475, duration: 0.012s, episode steps:   3, steps per second: 253, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 24671.194661, mae: 710.688029, accuracy: 0.166667, mean_q: -210.255931, mean_eps: 0.100000\n",
      " 18975/50000: episode: 3476, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 24456.017578, mae: 721.047750, accuracy: 0.135417, mean_q: -214.599442, mean_eps: 0.100000\n",
      " 18978/50000: episode: 3477, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 24317.820964, mae: 710.754618, accuracy: 0.156250, mean_q: -217.192103, mean_eps: 0.100000\n",
      " 18983/50000: episode: 3478, duration: 0.018s, episode steps:   5, steps per second: 273, episode reward: -2193.000, mean reward: -438.600 [-999.000, -58.000], mean action: 1.800 [0.000, 3.000],  loss: 22665.169922, mae: 711.158008, accuracy: 0.175000, mean_q: -217.544687, mean_eps: 0.100000\n",
      " 18986/50000: episode: 3479, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 23635.103516, mae: 722.602478, accuracy: 0.083333, mean_q: -264.224986, mean_eps: 0.100000\n",
      " 18989/50000: episode: 3480, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 24322.994792, mae: 703.975159, accuracy: 0.229167, mean_q: -241.589101, mean_eps: 0.100000\n",
      " 18992/50000: episode: 3481, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 25850.994792, mae: 718.354350, accuracy: 0.114583, mean_q: -232.708227, mean_eps: 0.100000\n",
      " 18995/50000: episode: 3482, duration: 0.019s, episode steps:   3, steps per second: 157, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 22758.194010, mae: 695.578308, accuracy: 0.145833, mean_q: -230.195704, mean_eps: 0.100000\n",
      " 18999/50000: episode: 3483, duration: 0.018s, episode steps:   4, steps per second: 227, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 22307.545898, mae: 708.985062, accuracy: 0.187500, mean_q: -215.341190, mean_eps: 0.100000\n",
      " 19002/50000: episode: 3484, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 30440.163411, mae: 712.433187, accuracy: 0.093750, mean_q: -205.957876, mean_eps: 0.100000\n",
      " 19005/50000: episode: 3485, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 21329.489583, mae: 694.722616, accuracy: 0.187500, mean_q: -235.632680, mean_eps: 0.100000\n",
      " 19008/50000: episode: 3486, duration: 0.012s, episode steps:   3, steps per second: 253, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 25218.785156, mae: 709.737183, accuracy: 0.229167, mean_q: -192.800720, mean_eps: 0.100000\n",
      " 19011/50000: episode: 3487, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 24961.308594, mae: 700.365702, accuracy: 0.083333, mean_q: -211.759455, mean_eps: 0.100000\n",
      " 19014/50000: episode: 3488, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 24339.315755, mae: 705.626770, accuracy: 0.125000, mean_q: -245.353592, mean_eps: 0.100000\n",
      " 19017/50000: episode: 3489, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 22477.328125, mae: 719.317708, accuracy: 0.135417, mean_q: -230.601934, mean_eps: 0.100000\n",
      " 19020/50000: episode: 3490, duration: 0.012s, episode steps:   3, steps per second: 254, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 27426.229818, mae: 713.287374, accuracy: 0.166667, mean_q: -225.447723, mean_eps: 0.100000\n",
      " 19024/50000: episode: 3491, duration: 0.016s, episode steps:   4, steps per second: 247, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.750 [0.000, 3.000],  loss: 24002.334473, mae: 707.380814, accuracy: 0.140625, mean_q: -220.578812, mean_eps: 0.100000\n",
      " 19027/50000: episode: 3492, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 21620.101562, mae: 715.518453, accuracy: 0.135417, mean_q: -233.137756, mean_eps: 0.100000\n",
      " 19030/50000: episode: 3493, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 27031.698568, mae: 726.049520, accuracy: 0.125000, mean_q: -259.462229, mean_eps: 0.100000\n",
      " 19033/50000: episode: 3494, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 23726.896159, mae: 706.859721, accuracy: 0.197917, mean_q: -210.725978, mean_eps: 0.100000\n",
      " 19036/50000: episode: 3495, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 20743.741536, mae: 712.127116, accuracy: 0.187500, mean_q: -231.935847, mean_eps: 0.100000\n",
      " 19039/50000: episode: 3496, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 24012.173828, mae: 715.337199, accuracy: 0.208333, mean_q: -238.706985, mean_eps: 0.100000\n",
      " 19042/50000: episode: 3497, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 26130.023438, mae: 710.493083, accuracy: 0.145833, mean_q: -253.129644, mean_eps: 0.100000\n",
      " 19045/50000: episode: 3498, duration: 0.015s, episode steps:   3, steps per second: 201, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 26904.459635, mae: 709.018188, accuracy: 0.187500, mean_q: -234.257248, mean_eps: 0.100000\n",
      " 19048/50000: episode: 3499, duration: 0.015s, episode steps:   3, steps per second: 198, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 15951.978841, mae: 710.578105, accuracy: 0.114583, mean_q: -206.202189, mean_eps: 0.100000\n",
      " 19051/50000: episode: 3500, duration: 0.013s, episode steps:   3, steps per second: 222, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 20344.030599, mae: 716.412801, accuracy: 0.166667, mean_q: -238.385585, mean_eps: 0.100000\n",
      " 19054/50000: episode: 3501, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 29199.276042, mae: 709.572998, accuracy: 0.177083, mean_q: -242.523478, mean_eps: 0.100000\n",
      " 19057/50000: episode: 3502, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 20499.691406, mae: 692.822713, accuracy: 0.135417, mean_q: -227.325068, mean_eps: 0.100000\n",
      " 19061/50000: episode: 3503, duration: 0.015s, episode steps:   4, steps per second: 262, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 27714.771973, mae: 707.091461, accuracy: 0.195312, mean_q: -213.789288, mean_eps: 0.100000\n",
      " 19064/50000: episode: 3504, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 25401.848958, mae: 698.686523, accuracy: 0.145833, mean_q: -201.503815, mean_eps: 0.100000\n",
      " 19067/50000: episode: 3505, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 22435.141276, mae: 691.291687, accuracy: 0.166667, mean_q: -206.200185, mean_eps: 0.100000\n",
      " 19070/50000: episode: 3506, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 22466.783854, mae: 721.563619, accuracy: 0.135417, mean_q: -205.480799, mean_eps: 0.100000\n",
      " 19073/50000: episode: 3507, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 20625.384115, mae: 698.741069, accuracy: 0.135417, mean_q: -259.460826, mean_eps: 0.100000\n",
      " 19076/50000: episode: 3508, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 20562.255208, mae: 695.500407, accuracy: 0.187500, mean_q: -186.609685, mean_eps: 0.100000\n",
      " 19079/50000: episode: 3509, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 23224.064453, mae: 718.826884, accuracy: 0.166667, mean_q: -210.833064, mean_eps: 0.100000\n",
      " 19083/50000: episode: 3510, duration: 0.015s, episode steps:   4, steps per second: 268, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 28574.755859, mae: 716.841324, accuracy: 0.125000, mean_q: -254.641556, mean_eps: 0.100000\n",
      " 19087/50000: episode: 3511, duration: 0.016s, episode steps:   4, steps per second: 258, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.750 [0.000, 3.000],  loss: 27482.313477, mae: 693.708893, accuracy: 0.156250, mean_q: -203.036415, mean_eps: 0.100000\n",
      " 19090/50000: episode: 3512, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 21671.256510, mae: 696.032430, accuracy: 0.166667, mean_q: -207.672475, mean_eps: 0.100000\n",
      " 19094/50000: episode: 3513, duration: 0.015s, episode steps:   4, steps per second: 265, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 25209.701172, mae: 709.611969, accuracy: 0.171875, mean_q: -234.548988, mean_eps: 0.100000\n",
      " 19097/50000: episode: 3514, duration: 0.016s, episode steps:   3, steps per second: 188, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 27046.701823, mae: 696.193624, accuracy: 0.208333, mean_q: -210.732432, mean_eps: 0.100000\n",
      " 19100/50000: episode: 3515, duration: 0.015s, episode steps:   3, steps per second: 205, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 18417.676432, mae: 703.526754, accuracy: 0.197917, mean_q: -218.704163, mean_eps: 0.100000\n",
      " 19103/50000: episode: 3516, duration: 0.014s, episode steps:   3, steps per second: 213, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 18655.590495, mae: 692.817322, accuracy: 0.145833, mean_q: -215.207214, mean_eps: 0.100000\n",
      " 19106/50000: episode: 3517, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 22620.569661, mae: 697.183980, accuracy: 0.218750, mean_q: -196.885747, mean_eps: 0.100000\n",
      " 19109/50000: episode: 3518, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 23813.962240, mae: 692.808248, accuracy: 0.197917, mean_q: -226.999390, mean_eps: 0.100000\n",
      " 19112/50000: episode: 3519, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 23183.394531, mae: 702.319214, accuracy: 0.104167, mean_q: -195.401037, mean_eps: 0.100000\n",
      " 19115/50000: episode: 3520, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 19740.630859, mae: 712.740072, accuracy: 0.166667, mean_q: -194.394862, mean_eps: 0.100000\n",
      " 19119/50000: episode: 3521, duration: 0.016s, episode steps:   4, steps per second: 257, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 2.000 [0.000, 3.000],  loss: 22031.033691, mae: 710.318954, accuracy: 0.148438, mean_q: -212.611629, mean_eps: 0.100000\n",
      " 19122/50000: episode: 3522, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 22346.348307, mae: 676.492310, accuracy: 0.218750, mean_q: -220.089976, mean_eps: 0.100000\n",
      " 19125/50000: episode: 3523, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 19510.829427, mae: 686.656433, accuracy: 0.197917, mean_q: -221.800700, mean_eps: 0.100000\n",
      " 19128/50000: episode: 3524, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 20590.352865, mae: 674.236003, accuracy: 0.166667, mean_q: -219.370453, mean_eps: 0.100000\n",
      " 19131/50000: episode: 3525, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 28046.763997, mae: 692.843099, accuracy: 0.125000, mean_q: -210.603012, mean_eps: 0.100000\n",
      " 19134/50000: episode: 3526, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 24489.671875, mae: 712.198547, accuracy: 0.125000, mean_q: -216.678472, mean_eps: 0.100000\n",
      " 19137/50000: episode: 3527, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 25783.349609, mae: 695.724284, accuracy: 0.125000, mean_q: -211.469274, mean_eps: 0.100000\n",
      " 19141/50000: episode: 3528, duration: 0.018s, episode steps:   4, steps per second: 217, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 19289.953125, mae: 693.394867, accuracy: 0.132812, mean_q: -223.437180, mean_eps: 0.100000\n",
      " 19144/50000: episode: 3529, duration: 0.014s, episode steps:   3, steps per second: 207, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 20361.332682, mae: 699.217936, accuracy: 0.197917, mean_q: -192.328389, mean_eps: 0.100000\n",
      " 19147/50000: episode: 3530, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 22937.557292, mae: 678.326172, accuracy: 0.197917, mean_q: -207.053426, mean_eps: 0.100000\n",
      " 19150/50000: episode: 3531, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 19328.631510, mae: 680.565043, accuracy: 0.187500, mean_q: -205.016083, mean_eps: 0.100000\n",
      " 19153/50000: episode: 3532, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 20521.223958, mae: 690.175496, accuracy: 0.125000, mean_q: -222.043106, mean_eps: 0.100000\n",
      " 19156/50000: episode: 3533, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 24198.233073, mae: 698.686361, accuracy: 0.177083, mean_q: -223.844930, mean_eps: 0.100000\n",
      " 19159/50000: episode: 3534, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 16156.790690, mae: 665.977620, accuracy: 0.125000, mean_q: -203.280248, mean_eps: 0.100000\n",
      " 19162/50000: episode: 3535, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 16764.832682, mae: 678.218201, accuracy: 0.125000, mean_q: -208.995351, mean_eps: 0.100000\n",
      " 19165/50000: episode: 3536, duration: 0.012s, episode steps:   3, steps per second: 254, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 24417.342448, mae: 684.489644, accuracy: 0.114583, mean_q: -226.452433, mean_eps: 0.100000\n",
      " 19170/50000: episode: 3537, duration: 0.018s, episode steps:   5, steps per second: 276, episode reward: -2193.000, mean reward: -438.600 [-999.000, -58.000], mean action: 2.400 [1.000, 3.000],  loss: 23721.010742, mae: 675.066516, accuracy: 0.181250, mean_q: -194.900101, mean_eps: 0.100000\n",
      " 19173/50000: episode: 3538, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 20696.678385, mae: 672.833191, accuracy: 0.114583, mean_q: -219.437475, mean_eps: 0.100000\n",
      " 19177/50000: episode: 3539, duration: 0.015s, episode steps:   4, steps per second: 262, episode reward: -1238.000, mean reward: -309.500 [-999.000, -60.000], mean action: 1.500 [0.000, 3.000],  loss: 24877.787354, mae: 686.147568, accuracy: 0.156250, mean_q: -240.345047, mean_eps: 0.100000\n",
      " 19180/50000: episode: 3540, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 24402.273438, mae: 670.811137, accuracy: 0.208333, mean_q: -207.986872, mean_eps: 0.100000\n",
      " 19183/50000: episode: 3541, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 20071.278646, mae: 683.880778, accuracy: 0.125000, mean_q: -210.690608, mean_eps: 0.100000\n",
      " 19186/50000: episode: 3542, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 21483.692383, mae: 682.264140, accuracy: 0.125000, mean_q: -210.157064, mean_eps: 0.100000\n",
      " 19189/50000: episode: 3543, duration: 0.022s, episode steps:   3, steps per second: 139, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 26049.701823, mae: 705.073710, accuracy: 0.166667, mean_q: -199.256907, mean_eps: 0.100000\n",
      " 19192/50000: episode: 3544, duration: 0.019s, episode steps:   3, steps per second: 154, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 20951.073242, mae: 685.825033, accuracy: 0.156250, mean_q: -193.505753, mean_eps: 0.100000\n",
      " 19195/50000: episode: 3545, duration: 0.016s, episode steps:   3, steps per second: 185, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 21003.144531, mae: 670.210775, accuracy: 0.208333, mean_q: -160.462341, mean_eps: 0.100000\n",
      " 19198/50000: episode: 3546, duration: 0.014s, episode steps:   3, steps per second: 213, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 21331.377279, mae: 697.334147, accuracy: 0.125000, mean_q: -200.410817, mean_eps: 0.100000\n",
      " 19201/50000: episode: 3547, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 17751.648438, mae: 689.711466, accuracy: 0.156250, mean_q: -204.408076, mean_eps: 0.100000\n",
      " 19205/50000: episode: 3548, duration: 0.018s, episode steps:   4, steps per second: 229, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 24128.005371, mae: 676.746338, accuracy: 0.156250, mean_q: -181.632408, mean_eps: 0.100000\n",
      " 19208/50000: episode: 3549, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 20389.327474, mae: 672.722982, accuracy: 0.177083, mean_q: -155.641937, mean_eps: 0.100000\n",
      " 19211/50000: episode: 3550, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 26947.780599, mae: 666.150553, accuracy: 0.166667, mean_q: -194.841705, mean_eps: 0.100000\n",
      " 19214/50000: episode: 3551, duration: 0.014s, episode steps:   3, steps per second: 220, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 20852.881510, mae: 678.669067, accuracy: 0.239583, mean_q: -171.730886, mean_eps: 0.100000\n",
      " 19217/50000: episode: 3552, duration: 0.014s, episode steps:   3, steps per second: 212, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 20802.289714, mae: 657.451945, accuracy: 0.177083, mean_q: -168.473145, mean_eps: 0.100000\n",
      " 19220/50000: episode: 3553, duration: 0.013s, episode steps:   3, steps per second: 223, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 23085.688151, mae: 655.729614, accuracy: 0.208333, mean_q: -144.368423, mean_eps: 0.100000\n",
      " 19223/50000: episode: 3554, duration: 0.014s, episode steps:   3, steps per second: 219, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 28028.900391, mae: 680.969828, accuracy: 0.166667, mean_q: -201.375987, mean_eps: 0.100000\n",
      " 19227/50000: episode: 3555, duration: 0.018s, episode steps:   4, steps per second: 221, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 20618.190430, mae: 666.848175, accuracy: 0.179688, mean_q: -200.465549, mean_eps: 0.100000\n",
      " 19230/50000: episode: 3556, duration: 0.013s, episode steps:   3, steps per second: 224, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 18078.932292, mae: 670.947652, accuracy: 0.125000, mean_q: -179.717860, mean_eps: 0.100000\n",
      " 19233/50000: episode: 3557, duration: 0.019s, episode steps:   3, steps per second: 158, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 18213.991862, mae: 674.885050, accuracy: 0.208333, mean_q: -165.487396, mean_eps: 0.100000\n",
      " 19236/50000: episode: 3558, duration: 0.018s, episode steps:   3, steps per second: 168, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 19997.986979, mae: 683.420736, accuracy: 0.145833, mean_q: -193.329478, mean_eps: 0.100000\n",
      " 19239/50000: episode: 3559, duration: 0.015s, episode steps:   3, steps per second: 205, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 18906.425130, mae: 672.430400, accuracy: 0.166667, mean_q: -181.315140, mean_eps: 0.100000\n",
      " 19242/50000: episode: 3560, duration: 0.013s, episode steps:   3, steps per second: 223, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 21698.423177, mae: 668.025960, accuracy: 0.083333, mean_q: -160.282481, mean_eps: 0.100000\n",
      " 19245/50000: episode: 3561, duration: 0.014s, episode steps:   3, steps per second: 220, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 26634.968750, mae: 674.151021, accuracy: 0.125000, mean_q: -210.719157, mean_eps: 0.100000\n",
      " 19248/50000: episode: 3562, duration: 0.013s, episode steps:   3, steps per second: 226, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 18891.446615, mae: 667.808716, accuracy: 0.145833, mean_q: -179.089788, mean_eps: 0.100000\n",
      " 19251/50000: episode: 3563, duration: 0.014s, episode steps:   3, steps per second: 219, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 15122.079427, mae: 661.872050, accuracy: 0.156250, mean_q: -183.321467, mean_eps: 0.100000\n",
      " 19254/50000: episode: 3564, duration: 0.015s, episode steps:   3, steps per second: 203, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 19809.203125, mae: 666.169149, accuracy: 0.197917, mean_q: -192.761302, mean_eps: 0.100000\n",
      " 19257/50000: episode: 3565, duration: 0.014s, episode steps:   3, steps per second: 208, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 22807.732422, mae: 687.486735, accuracy: 0.104167, mean_q: -172.844177, mean_eps: 0.100000\n",
      " 19260/50000: episode: 3566, duration: 0.014s, episode steps:   3, steps per second: 212, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 22710.071615, mae: 672.608643, accuracy: 0.125000, mean_q: -210.307088, mean_eps: 0.100000\n",
      " 19263/50000: episode: 3567, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 19277.577148, mae: 684.851400, accuracy: 0.218750, mean_q: -189.534337, mean_eps: 0.100000\n",
      " 19266/50000: episode: 3568, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 20241.639323, mae: 684.935872, accuracy: 0.260417, mean_q: -174.141439, mean_eps: 0.100000\n",
      " 19269/50000: episode: 3569, duration: 0.013s, episode steps:   3, steps per second: 227, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 25960.882161, mae: 679.759766, accuracy: 0.197917, mean_q: -195.541117, mean_eps: 0.100000\n",
      " 19272/50000: episode: 3570, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 20393.377604, mae: 680.413778, accuracy: 0.156250, mean_q: -187.112595, mean_eps: 0.100000\n",
      " 19275/50000: episode: 3571, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 20345.328776, mae: 660.689616, accuracy: 0.208333, mean_q: -189.316961, mean_eps: 0.100000\n",
      " 19278/50000: episode: 3572, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 24202.512370, mae: 691.389303, accuracy: 0.156250, mean_q: -192.575679, mean_eps: 0.100000\n",
      " 19281/50000: episode: 3573, duration: 0.014s, episode steps:   3, steps per second: 207, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 23888.852214, mae: 680.649618, accuracy: 0.177083, mean_q: -188.286728, mean_eps: 0.100000\n",
      " 19284/50000: episode: 3574, duration: 0.015s, episode steps:   3, steps per second: 194, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 21429.718099, mae: 666.515808, accuracy: 0.135417, mean_q: -201.513514, mean_eps: 0.100000\n",
      " 19287/50000: episode: 3575, duration: 0.014s, episode steps:   3, steps per second: 208, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 23968.626302, mae: 681.159953, accuracy: 0.145833, mean_q: -178.681315, mean_eps: 0.100000\n",
      " 19290/50000: episode: 3576, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 17012.598307, mae: 670.581726, accuracy: 0.135417, mean_q: -163.164113, mean_eps: 0.100000\n",
      " 19293/50000: episode: 3577, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 18026.861328, mae: 652.198873, accuracy: 0.197917, mean_q: -155.224854, mean_eps: 0.100000\n",
      " 19296/50000: episode: 3578, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 19857.802734, mae: 664.415039, accuracy: 0.218750, mean_q: -182.320882, mean_eps: 0.100000\n",
      " 19299/50000: episode: 3579, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 22247.942708, mae: 659.163696, accuracy: 0.156250, mean_q: -174.176458, mean_eps: 0.100000\n",
      " 19302/50000: episode: 3580, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 17593.806315, mae: 667.516744, accuracy: 0.125000, mean_q: -202.724564, mean_eps: 0.100000\n",
      " 19305/50000: episode: 3581, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 27066.933594, mae: 677.219218, accuracy: 0.093750, mean_q: -194.812378, mean_eps: 0.100000\n",
      " 19308/50000: episode: 3582, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 21951.170247, mae: 668.455200, accuracy: 0.145833, mean_q: -173.305171, mean_eps: 0.100000\n",
      " 19311/50000: episode: 3583, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 16868.878255, mae: 672.857137, accuracy: 0.166667, mean_q: -166.646838, mean_eps: 0.100000\n",
      " 19314/50000: episode: 3584, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 20451.181966, mae: 652.337443, accuracy: 0.145833, mean_q: -191.368983, mean_eps: 0.100000\n",
      " 19317/50000: episode: 3585, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 19164.260417, mae: 637.773275, accuracy: 0.239583, mean_q: -162.387238, mean_eps: 0.100000\n",
      " 19320/50000: episode: 3586, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 22721.540365, mae: 659.379964, accuracy: 0.250000, mean_q: -162.046468, mean_eps: 0.100000\n",
      " 19323/50000: episode: 3587, duration: 0.013s, episode steps:   3, steps per second: 225, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 20869.136068, mae: 654.104492, accuracy: 0.197917, mean_q: -162.779338, mean_eps: 0.100000\n",
      " 19326/50000: episode: 3588, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 16064.600911, mae: 675.290527, accuracy: 0.177083, mean_q: -139.737976, mean_eps: 0.100000\n",
      " 19329/50000: episode: 3589, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 24335.738281, mae: 645.659281, accuracy: 0.291667, mean_q: -139.343755, mean_eps: 0.100000\n",
      " 19332/50000: episode: 3590, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 17914.813151, mae: 636.029256, accuracy: 0.187500, mean_q: -175.444158, mean_eps: 0.100000\n",
      " 19335/50000: episode: 3591, duration: 0.014s, episode steps:   3, steps per second: 210, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 14905.530599, mae: 655.460836, accuracy: 0.166667, mean_q: -182.827031, mean_eps: 0.100000\n",
      " 19338/50000: episode: 3592, duration: 0.014s, episode steps:   3, steps per second: 208, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 17734.445964, mae: 675.729818, accuracy: 0.093750, mean_q: -160.378421, mean_eps: 0.100000\n",
      " 19341/50000: episode: 3593, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 26551.278646, mae: 667.973185, accuracy: 0.156250, mean_q: -190.321508, mean_eps: 0.100000\n",
      " 19344/50000: episode: 3594, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 16817.151693, mae: 634.201355, accuracy: 0.135417, mean_q: -158.241084, mean_eps: 0.100000\n",
      " 19347/50000: episode: 3595, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 20660.370443, mae: 651.749715, accuracy: 0.177083, mean_q: -208.075073, mean_eps: 0.100000\n",
      " 19350/50000: episode: 3596, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 16504.582682, mae: 669.334167, accuracy: 0.208333, mean_q: -178.868637, mean_eps: 0.100000\n",
      " 19353/50000: episode: 3597, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 21258.699870, mae: 641.951457, accuracy: 0.187500, mean_q: -157.381358, mean_eps: 0.100000\n",
      " 19357/50000: episode: 3598, duration: 0.015s, episode steps:   4, steps per second: 263, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 16435.061523, mae: 648.143478, accuracy: 0.187500, mean_q: -145.334522, mean_eps: 0.100000\n",
      " 19360/50000: episode: 3599, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 20636.136068, mae: 645.604004, accuracy: 0.208333, mean_q: -198.286184, mean_eps: 0.100000\n",
      " 19363/50000: episode: 3600, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 21226.776042, mae: 651.424784, accuracy: 0.229167, mean_q: -161.003184, mean_eps: 0.100000\n",
      " 19366/50000: episode: 3601, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 20322.441732, mae: 638.821370, accuracy: 0.177083, mean_q: -159.407374, mean_eps: 0.100000\n",
      " 19369/50000: episode: 3602, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 20751.565104, mae: 652.701640, accuracy: 0.135417, mean_q: -162.966044, mean_eps: 0.100000\n",
      " 19372/50000: episode: 3603, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 20236.087240, mae: 670.883484, accuracy: 0.114583, mean_q: -165.605209, mean_eps: 0.100000\n",
      " 19375/50000: episode: 3604, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 22047.459635, mae: 644.963888, accuracy: 0.145833, mean_q: -172.693395, mean_eps: 0.100000\n",
      " 19378/50000: episode: 3605, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 21044.048503, mae: 678.215515, accuracy: 0.166667, mean_q: -180.993271, mean_eps: 0.100000\n",
      " 19382/50000: episode: 3606, duration: 0.028s, episode steps:   4, steps per second: 141, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 22654.556641, mae: 648.874908, accuracy: 0.218750, mean_q: -156.276756, mean_eps: 0.100000\n",
      " 19386/50000: episode: 3607, duration: 0.016s, episode steps:   4, steps per second: 251, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 21690.980225, mae: 679.397110, accuracy: 0.132812, mean_q: -194.938488, mean_eps: 0.100000\n",
      " 19389/50000: episode: 3608, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -224.000, mean reward: -74.667 [-116.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 22077.418620, mae: 637.274455, accuracy: 0.250000, mean_q: -182.968847, mean_eps: 0.100000\n",
      " 19392/50000: episode: 3609, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 20900.280599, mae: 652.370850, accuracy: 0.218750, mean_q: -177.262670, mean_eps: 0.100000\n",
      " 19395/50000: episode: 3610, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 18884.440755, mae: 631.919495, accuracy: 0.239583, mean_q: -153.813985, mean_eps: 0.100000\n",
      " 19398/50000: episode: 3611, duration: 0.013s, episode steps:   3, steps per second: 227, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 20876.538086, mae: 659.416687, accuracy: 0.177083, mean_q: -193.553904, mean_eps: 0.100000\n",
      " 19401/50000: episode: 3612, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 13806.275065, mae: 647.854614, accuracy: 0.229167, mean_q: -153.021957, mean_eps: 0.100000\n",
      " 19404/50000: episode: 3613, duration: 0.012s, episode steps:   3, steps per second: 254, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 16847.203451, mae: 640.545064, accuracy: 0.135417, mean_q: -171.486811, mean_eps: 0.100000\n",
      " 19407/50000: episode: 3614, duration: 0.012s, episode steps:   3, steps per second: 253, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 20311.466797, mae: 653.443990, accuracy: 0.177083, mean_q: -175.251775, mean_eps: 0.100000\n",
      " 19410/50000: episode: 3615, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 20904.270833, mae: 654.346883, accuracy: 0.208333, mean_q: -167.872950, mean_eps: 0.100000\n",
      " 19413/50000: episode: 3616, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 22184.908203, mae: 633.834473, accuracy: 0.166667, mean_q: -165.685857, mean_eps: 0.100000\n",
      " 19416/50000: episode: 3617, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 16762.818685, mae: 620.134094, accuracy: 0.197917, mean_q: -133.265862, mean_eps: 0.100000\n",
      " 19419/50000: episode: 3618, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 24606.040365, mae: 623.171916, accuracy: 0.187500, mean_q: -161.928975, mean_eps: 0.100000\n",
      " 19422/50000: episode: 3619, duration: 0.014s, episode steps:   3, steps per second: 212, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 20165.410807, mae: 656.923910, accuracy: 0.208333, mean_q: -139.496468, mean_eps: 0.100000\n",
      " 19425/50000: episode: 3620, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 21773.826172, mae: 667.785482, accuracy: 0.166667, mean_q: -160.629913, mean_eps: 0.100000\n",
      " 19428/50000: episode: 3621, duration: 0.014s, episode steps:   3, steps per second: 211, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 19759.607422, mae: 635.088582, accuracy: 0.208333, mean_q: -139.890455, mean_eps: 0.100000\n",
      " 19432/50000: episode: 3622, duration: 0.018s, episode steps:   4, steps per second: 220, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.750 [0.000, 3.000],  loss: 21235.829834, mae: 649.957397, accuracy: 0.117188, mean_q: -167.523506, mean_eps: 0.100000\n",
      " 19435/50000: episode: 3623, duration: 0.015s, episode steps:   3, steps per second: 200, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 24306.413411, mae: 645.738851, accuracy: 0.166667, mean_q: -151.613564, mean_eps: 0.100000\n",
      " 19439/50000: episode: 3624, duration: 0.018s, episode steps:   4, steps per second: 223, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 21114.005371, mae: 620.772827, accuracy: 0.132812, mean_q: -170.545727, mean_eps: 0.100000\n",
      " 19446/50000: episode: 3625, duration: 0.026s, episode steps:   7, steps per second: 273, episode reward: -4191.000, mean reward: -598.714 [-999.000, -45.000], mean action: 1.714 [0.000, 3.000],  loss: 23100.441406, mae: 652.616708, accuracy: 0.120536, mean_q: -163.015246, mean_eps: 0.100000\n",
      " 19449/50000: episode: 3626, duration: 0.013s, episode steps:   3, steps per second: 224, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 22959.102865, mae: 651.899394, accuracy: 0.239583, mean_q: -158.781982, mean_eps: 0.100000\n",
      " 19452/50000: episode: 3627, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 17466.870117, mae: 644.488078, accuracy: 0.104167, mean_q: -159.321396, mean_eps: 0.100000\n",
      " 19455/50000: episode: 3628, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 22089.957031, mae: 641.711629, accuracy: 0.281250, mean_q: -162.484655, mean_eps: 0.100000\n",
      " 19458/50000: episode: 3629, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 16788.697917, mae: 639.948059, accuracy: 0.166667, mean_q: -134.267230, mean_eps: 0.100000\n",
      " 19461/50000: episode: 3630, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 17627.688802, mae: 647.348348, accuracy: 0.135417, mean_q: -143.216980, mean_eps: 0.100000\n",
      " 19464/50000: episode: 3631, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 23946.634766, mae: 662.951172, accuracy: 0.166667, mean_q: -174.791397, mean_eps: 0.100000\n",
      " 19467/50000: episode: 3632, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 24384.117188, mae: 664.077901, accuracy: 0.218750, mean_q: -167.961955, mean_eps: 0.100000\n",
      " 19470/50000: episode: 3633, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 20822.245443, mae: 672.624064, accuracy: 0.104167, mean_q: -158.549492, mean_eps: 0.100000\n",
      " 19473/50000: episode: 3634, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 18996.256510, mae: 651.001811, accuracy: 0.229167, mean_q: -132.310013, mean_eps: 0.100000\n",
      " 19476/50000: episode: 3635, duration: 0.014s, episode steps:   3, steps per second: 221, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 28000.038411, mae: 660.404012, accuracy: 0.208333, mean_q: -209.996531, mean_eps: 0.100000\n",
      " 19479/50000: episode: 3636, duration: 0.014s, episode steps:   3, steps per second: 212, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 22445.559245, mae: 648.754618, accuracy: 0.197917, mean_q: -160.139971, mean_eps: 0.100000\n",
      " 19482/50000: episode: 3637, duration: 0.014s, episode steps:   3, steps per second: 208, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 24325.574870, mae: 639.518982, accuracy: 0.229167, mean_q: -150.728645, mean_eps: 0.100000\n",
      " 19485/50000: episode: 3638, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 20443.532552, mae: 647.564412, accuracy: 0.229167, mean_q: -163.623545, mean_eps: 0.100000\n",
      " 19488/50000: episode: 3639, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 21507.013021, mae: 644.639323, accuracy: 0.125000, mean_q: -163.606801, mean_eps: 0.100000\n",
      " 19491/50000: episode: 3640, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 22753.089193, mae: 635.732564, accuracy: 0.135417, mean_q: -174.219793, mean_eps: 0.100000\n",
      " 19494/50000: episode: 3641, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 21386.116536, mae: 652.297384, accuracy: 0.125000, mean_q: -178.732330, mean_eps: 0.100000\n",
      " 19498/50000: episode: 3642, duration: 0.018s, episode steps:   4, steps per second: 224, episode reward: -1238.000, mean reward: -309.500 [-999.000, -60.000], mean action: 1.500 [0.000, 3.000],  loss: 21281.000244, mae: 651.390823, accuracy: 0.132812, mean_q: -171.760979, mean_eps: 0.100000\n",
      " 19501/50000: episode: 3643, duration: 0.014s, episode steps:   3, steps per second: 219, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 18395.621419, mae: 634.348572, accuracy: 0.156250, mean_q: -150.813744, mean_eps: 0.100000\n",
      " 19504/50000: episode: 3644, duration: 0.015s, episode steps:   3, steps per second: 207, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 23586.841146, mae: 646.489482, accuracy: 0.177083, mean_q: -156.344584, mean_eps: 0.100000\n",
      " 19507/50000: episode: 3645, duration: 0.013s, episode steps:   3, steps per second: 224, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 21057.076823, mae: 632.025187, accuracy: 0.197917, mean_q: -113.367958, mean_eps: 0.100000\n",
      " 19510/50000: episode: 3646, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 15111.335938, mae: 623.035787, accuracy: 0.229167, mean_q: -126.880320, mean_eps: 0.100000\n",
      " 19513/50000: episode: 3647, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 19351.473958, mae: 630.336812, accuracy: 0.156250, mean_q: -144.207372, mean_eps: 0.100000\n",
      " 19516/50000: episode: 3648, duration: 0.014s, episode steps:   3, steps per second: 214, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 16626.524414, mae: 626.384277, accuracy: 0.177083, mean_q: -138.656013, mean_eps: 0.100000\n",
      " 19519/50000: episode: 3649, duration: 0.016s, episode steps:   3, steps per second: 188, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 19858.597656, mae: 640.097310, accuracy: 0.135417, mean_q: -152.319870, mean_eps: 0.100000\n",
      " 19523/50000: episode: 3650, duration: 0.023s, episode steps:   4, steps per second: 174, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 20372.705566, mae: 667.881317, accuracy: 0.218750, mean_q: -140.654980, mean_eps: 0.100000\n",
      " 19526/50000: episode: 3651, duration: 0.020s, episode steps:   3, steps per second: 151, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 18465.073568, mae: 628.250651, accuracy: 0.177083, mean_q: -151.901271, mean_eps: 0.100000\n",
      " 19530/50000: episode: 3652, duration: 0.016s, episode steps:   4, steps per second: 250, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.750 [0.000, 3.000],  loss: 21524.819336, mae: 641.066971, accuracy: 0.187500, mean_q: -169.175716, mean_eps: 0.100000\n",
      " 19533/50000: episode: 3653, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 17393.476562, mae: 652.686503, accuracy: 0.156250, mean_q: -147.163305, mean_eps: 0.100000\n",
      " 19537/50000: episode: 3654, duration: 0.016s, episode steps:   4, steps per second: 248, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 19596.139648, mae: 626.268326, accuracy: 0.179688, mean_q: -144.458780, mean_eps: 0.100000\n",
      " 19540/50000: episode: 3655, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 19138.417969, mae: 647.616435, accuracy: 0.125000, mean_q: -152.574890, mean_eps: 0.100000\n",
      " 19544/50000: episode: 3656, duration: 0.016s, episode steps:   4, steps per second: 254, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 13979.874512, mae: 599.003983, accuracy: 0.203125, mean_q: -109.729549, mean_eps: 0.100000\n",
      " 19547/50000: episode: 3657, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 16609.709635, mae: 642.695353, accuracy: 0.135417, mean_q: -141.624084, mean_eps: 0.100000\n",
      " 19550/50000: episode: 3658, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 17100.652344, mae: 640.128866, accuracy: 0.125000, mean_q: -138.259191, mean_eps: 0.100000\n",
      " 19553/50000: episode: 3659, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 18081.465169, mae: 627.092021, accuracy: 0.208333, mean_q: -146.244507, mean_eps: 0.100000\n",
      " 19556/50000: episode: 3660, duration: 0.012s, episode steps:   3, steps per second: 258, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 17240.952148, mae: 625.979980, accuracy: 0.229167, mean_q: -150.489080, mean_eps: 0.100000\n",
      " 19559/50000: episode: 3661, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 17218.222005, mae: 624.685547, accuracy: 0.187500, mean_q: -110.478155, mean_eps: 0.100000\n",
      " 19562/50000: episode: 3662, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 15633.684245, mae: 621.974935, accuracy: 0.104167, mean_q: -136.699992, mean_eps: 0.100000\n",
      " 19566/50000: episode: 3663, duration: 0.016s, episode steps:   4, steps per second: 253, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.750 [0.000, 3.000],  loss: 21158.813965, mae: 618.776611, accuracy: 0.195312, mean_q: -138.248550, mean_eps: 0.100000\n",
      " 19569/50000: episode: 3664, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 21880.837240, mae: 643.216573, accuracy: 0.197917, mean_q: -149.668966, mean_eps: 0.100000\n",
      " 19572/50000: episode: 3665, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 17263.708984, mae: 626.886251, accuracy: 0.177083, mean_q: -157.940826, mean_eps: 0.100000\n",
      " 19575/50000: episode: 3666, duration: 0.014s, episode steps:   3, steps per second: 211, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 15439.863932, mae: 612.790161, accuracy: 0.291667, mean_q: -125.912692, mean_eps: 0.100000\n",
      " 19578/50000: episode: 3667, duration: 0.015s, episode steps:   3, steps per second: 198, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 19363.772135, mae: 657.296265, accuracy: 0.166667, mean_q: -174.297841, mean_eps: 0.100000\n",
      " 19581/50000: episode: 3668, duration: 0.014s, episode steps:   3, steps per second: 211, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 18038.045898, mae: 632.044474, accuracy: 0.187500, mean_q: -138.602122, mean_eps: 0.100000\n",
      " 19584/50000: episode: 3669, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 17583.445312, mae: 617.189494, accuracy: 0.187500, mean_q: -124.689535, mean_eps: 0.100000\n",
      " 19587/50000: episode: 3670, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 24844.448568, mae: 644.152588, accuracy: 0.177083, mean_q: -144.150795, mean_eps: 0.100000\n",
      " 19590/50000: episode: 3671, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 20243.391276, mae: 636.088969, accuracy: 0.135417, mean_q: -114.254692, mean_eps: 0.100000\n",
      " 19593/50000: episode: 3672, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 15540.256185, mae: 625.802938, accuracy: 0.145833, mean_q: -142.745054, mean_eps: 0.100000\n",
      " 19596/50000: episode: 3673, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 25921.592122, mae: 647.113098, accuracy: 0.114583, mean_q: -164.310440, mean_eps: 0.100000\n",
      " 19599/50000: episode: 3674, duration: 0.012s, episode steps:   3, steps per second: 255, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 21998.299154, mae: 593.278605, accuracy: 0.239583, mean_q: -155.753393, mean_eps: 0.100000\n",
      " 19602/50000: episode: 3675, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 18909.313802, mae: 625.493917, accuracy: 0.166667, mean_q: -174.172516, mean_eps: 0.100000\n",
      " 19605/50000: episode: 3676, duration: 0.012s, episode steps:   3, steps per second: 255, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 16707.667643, mae: 618.039673, accuracy: 0.239583, mean_q: -154.969747, mean_eps: 0.100000\n",
      " 19608/50000: episode: 3677, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 15797.632487, mae: 631.840007, accuracy: 0.156250, mean_q: -128.346006, mean_eps: 0.100000\n",
      " 19611/50000: episode: 3678, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 21513.672201, mae: 625.971171, accuracy: 0.187500, mean_q: -117.576803, mean_eps: 0.100000\n",
      " 19614/50000: episode: 3679, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 16075.372396, mae: 621.522278, accuracy: 0.052083, mean_q: -130.689791, mean_eps: 0.100000\n",
      " 19617/50000: episode: 3680, duration: 0.013s, episode steps:   3, steps per second: 226, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 19790.511719, mae: 634.742350, accuracy: 0.177083, mean_q: -135.779137, mean_eps: 0.100000\n",
      " 19620/50000: episode: 3681, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 24357.679036, mae: 624.820943, accuracy: 0.135417, mean_q: -133.010925, mean_eps: 0.100000\n",
      " 19623/50000: episode: 3682, duration: 0.014s, episode steps:   3, steps per second: 211, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 21443.155599, mae: 621.904439, accuracy: 0.229167, mean_q: -122.803574, mean_eps: 0.100000\n",
      " 19626/50000: episode: 3683, duration: 0.014s, episode steps:   3, steps per second: 218, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 17826.977214, mae: 632.623901, accuracy: 0.166667, mean_q: -104.500853, mean_eps: 0.100000\n",
      " 19629/50000: episode: 3684, duration: 0.013s, episode steps:   3, steps per second: 223, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 20768.462240, mae: 592.010152, accuracy: 0.218750, mean_q: -149.775579, mean_eps: 0.100000\n",
      " 19632/50000: episode: 3685, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 18824.656250, mae: 617.704427, accuracy: 0.208333, mean_q: -130.564227, mean_eps: 0.100000\n",
      " 19635/50000: episode: 3686, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 19740.507161, mae: 618.802856, accuracy: 0.177083, mean_q: -126.260223, mean_eps: 0.100000\n",
      " 19638/50000: episode: 3687, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 19581.259115, mae: 631.633993, accuracy: 0.177083, mean_q: -129.725840, mean_eps: 0.100000\n",
      " 19642/50000: episode: 3688, duration: 0.015s, episode steps:   4, steps per second: 261, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 20183.213379, mae: 614.280869, accuracy: 0.156250, mean_q: -129.756800, mean_eps: 0.100000\n",
      " 19645/50000: episode: 3689, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 22719.000651, mae: 611.504476, accuracy: 0.229167, mean_q: -133.862831, mean_eps: 0.100000\n",
      " 19648/50000: episode: 3690, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 17015.151693, mae: 603.397156, accuracy: 0.197917, mean_q: -126.023974, mean_eps: 0.100000\n",
      " 19651/50000: episode: 3691, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 16673.005208, mae: 610.400106, accuracy: 0.145833, mean_q: -120.364812, mean_eps: 0.100000\n",
      " 19654/50000: episode: 3692, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 17997.734375, mae: 603.809509, accuracy: 0.208333, mean_q: -140.515060, mean_eps: 0.100000\n",
      " 19658/50000: episode: 3693, duration: 0.016s, episode steps:   4, steps per second: 258, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 18255.355957, mae: 623.450607, accuracy: 0.156250, mean_q: -133.830549, mean_eps: 0.100000\n",
      " 19661/50000: episode: 3694, duration: 0.012s, episode steps:   3, steps per second: 254, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 20495.127604, mae: 628.479818, accuracy: 0.135417, mean_q: -109.087817, mean_eps: 0.100000\n",
      " 19664/50000: episode: 3695, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 20482.242188, mae: 623.683167, accuracy: 0.197917, mean_q: -140.768717, mean_eps: 0.100000\n",
      " 19667/50000: episode: 3696, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 19488.542318, mae: 604.507894, accuracy: 0.145833, mean_q: -113.011121, mean_eps: 0.100000\n",
      " 19670/50000: episode: 3697, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 18453.246419, mae: 621.339315, accuracy: 0.187500, mean_q: -128.493807, mean_eps: 0.100000\n",
      " 19674/50000: episode: 3698, duration: 0.017s, episode steps:   4, steps per second: 240, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 19547.100830, mae: 614.207001, accuracy: 0.171875, mean_q: -126.350788, mean_eps: 0.100000\n",
      " 19677/50000: episode: 3699, duration: 0.019s, episode steps:   3, steps per second: 158, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 22042.873047, mae: 625.277751, accuracy: 0.197917, mean_q: -112.904177, mean_eps: 0.100000\n",
      " 19680/50000: episode: 3700, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 21821.632161, mae: 617.568339, accuracy: 0.145833, mean_q: -125.209948, mean_eps: 0.100000\n",
      " 19683/50000: episode: 3701, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 17005.577474, mae: 620.784465, accuracy: 0.260417, mean_q: -117.194893, mean_eps: 0.100000\n",
      " 19686/50000: episode: 3702, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 20515.003255, mae: 614.373698, accuracy: 0.302083, mean_q: -105.086870, mean_eps: 0.100000\n",
      " 19689/50000: episode: 3703, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 16754.834310, mae: 629.093363, accuracy: 0.166667, mean_q: -105.767357, mean_eps: 0.100000\n",
      " 19692/50000: episode: 3704, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 20228.965495, mae: 638.666890, accuracy: 0.156250, mean_q: -137.506699, mean_eps: 0.100000\n",
      " 19695/50000: episode: 3705, duration: 0.012s, episode steps:   3, steps per second: 259, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 20192.475911, mae: 634.873128, accuracy: 0.208333, mean_q: -122.472216, mean_eps: 0.100000\n",
      " 19698/50000: episode: 3706, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 15258.869466, mae: 608.220723, accuracy: 0.250000, mean_q: -115.390518, mean_eps: 0.100000\n",
      " 19701/50000: episode: 3707, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 21328.391276, mae: 640.570333, accuracy: 0.239583, mean_q: -141.911832, mean_eps: 0.100000\n",
      " 19704/50000: episode: 3708, duration: 0.013s, episode steps:   3, steps per second: 224, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 19653.847005, mae: 623.630127, accuracy: 0.218750, mean_q: -123.701836, mean_eps: 0.100000\n",
      " 19707/50000: episode: 3709, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 14424.489909, mae: 589.547323, accuracy: 0.156250, mean_q: -103.557203, mean_eps: 0.100000\n",
      " 19710/50000: episode: 3710, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 17800.054036, mae: 609.259521, accuracy: 0.197917, mean_q: -120.567510, mean_eps: 0.100000\n",
      " 19714/50000: episode: 3711, duration: 0.015s, episode steps:   4, steps per second: 264, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 24160.562012, mae: 607.559036, accuracy: 0.195312, mean_q: -128.419243, mean_eps: 0.100000\n",
      " 19717/50000: episode: 3712, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 17897.031901, mae: 623.159912, accuracy: 0.187500, mean_q: -115.334000, mean_eps: 0.100000\n",
      " 19720/50000: episode: 3713, duration: 0.013s, episode steps:   3, steps per second: 223, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 16556.351562, mae: 618.306356, accuracy: 0.208333, mean_q: -104.755333, mean_eps: 0.100000\n",
      " 19724/50000: episode: 3714, duration: 0.018s, episode steps:   4, steps per second: 223, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 20925.704590, mae: 607.587860, accuracy: 0.156250, mean_q: -115.647301, mean_eps: 0.100000\n",
      " 19727/50000: episode: 3715, duration: 0.014s, episode steps:   3, steps per second: 209, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 16585.463216, mae: 572.433187, accuracy: 0.208333, mean_q: -96.771924, mean_eps: 0.100000\n",
      " 19730/50000: episode: 3716, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 20332.843750, mae: 610.382141, accuracy: 0.187500, mean_q: -137.504105, mean_eps: 0.100000\n",
      " 19733/50000: episode: 3717, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 22026.834635, mae: 613.878764, accuracy: 0.187500, mean_q: -121.208771, mean_eps: 0.100000\n",
      " 19737/50000: episode: 3718, duration: 0.016s, episode steps:   4, steps per second: 245, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 22936.973877, mae: 616.413757, accuracy: 0.187500, mean_q: -96.662152, mean_eps: 0.100000\n",
      " 19740/50000: episode: 3719, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 19081.664714, mae: 592.759033, accuracy: 0.135417, mean_q: -142.988126, mean_eps: 0.100000\n",
      " 19743/50000: episode: 3720, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 22162.483724, mae: 615.451152, accuracy: 0.229167, mean_q: -106.891373, mean_eps: 0.100000\n",
      " 19746/50000: episode: 3721, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 16607.793945, mae: 595.508545, accuracy: 0.208333, mean_q: -89.319745, mean_eps: 0.100000\n",
      " 19749/50000: episode: 3722, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 21996.131510, mae: 612.603414, accuracy: 0.208333, mean_q: -134.966288, mean_eps: 0.100000\n",
      " 19752/50000: episode: 3723, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 22884.753906, mae: 625.850464, accuracy: 0.187500, mean_q: -146.628367, mean_eps: 0.100000\n",
      " 19755/50000: episode: 3724, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 18888.025391, mae: 606.378418, accuracy: 0.187500, mean_q: -133.755498, mean_eps: 0.100000\n",
      " 19758/50000: episode: 3725, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 18848.063802, mae: 609.807149, accuracy: 0.197917, mean_q: -114.381917, mean_eps: 0.100000\n",
      " 19761/50000: episode: 3726, duration: 0.012s, episode steps:   3, steps per second: 256, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 17199.030924, mae: 602.605937, accuracy: 0.270833, mean_q: -87.130591, mean_eps: 0.100000\n",
      " 19764/50000: episode: 3727, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.667 [0.000, 3.000],  loss: 24600.369792, mae: 634.400309, accuracy: 0.208333, mean_q: -134.081848, mean_eps: 0.100000\n",
      " 19767/50000: episode: 3728, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 21260.561198, mae: 613.241842, accuracy: 0.156250, mean_q: -130.848033, mean_eps: 0.100000\n",
      " 19770/50000: episode: 3729, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 22326.546875, mae: 621.192098, accuracy: 0.104167, mean_q: -119.557037, mean_eps: 0.100000\n",
      " 19774/50000: episode: 3730, duration: 0.017s, episode steps:   4, steps per second: 230, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 18952.449219, mae: 607.192078, accuracy: 0.140625, mean_q: -111.589752, mean_eps: 0.100000\n",
      " 19778/50000: episode: 3731, duration: 0.018s, episode steps:   4, steps per second: 226, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 18874.247803, mae: 602.251404, accuracy: 0.226562, mean_q: -123.059565, mean_eps: 0.100000\n",
      " 19781/50000: episode: 3732, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 22899.732422, mae: 598.814860, accuracy: 0.125000, mean_q: -119.190651, mean_eps: 0.100000\n",
      " 19784/50000: episode: 3733, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 16969.503255, mae: 601.230143, accuracy: 0.145833, mean_q: -107.650177, mean_eps: 0.100000\n",
      " 19788/50000: episode: 3734, duration: 0.016s, episode steps:   4, steps per second: 253, episode reward: -1223.000, mean reward: -305.750 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 16587.286621, mae: 595.918167, accuracy: 0.218750, mean_q: -93.938018, mean_eps: 0.100000\n",
      " 19791/50000: episode: 3735, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 17907.403646, mae: 597.083618, accuracy: 0.260417, mean_q: -80.048983, mean_eps: 0.100000\n",
      " 19794/50000: episode: 3736, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 13926.360677, mae: 606.091939, accuracy: 0.250000, mean_q: -108.179454, mean_eps: 0.100000\n",
      " 19797/50000: episode: 3737, duration: 0.012s, episode steps:   3, steps per second: 255, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 18174.865560, mae: 608.179159, accuracy: 0.197917, mean_q: -125.858597, mean_eps: 0.100000\n",
      " 19800/50000: episode: 3738, duration: 0.012s, episode steps:   3, steps per second: 257, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 19531.308268, mae: 617.192505, accuracy: 0.166667, mean_q: -134.561325, mean_eps: 0.100000\n",
      " 19803/50000: episode: 3739, duration: 0.012s, episode steps:   3, steps per second: 253, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 20010.792318, mae: 617.121216, accuracy: 0.187500, mean_q: -131.135302, mean_eps: 0.100000\n",
      " 19806/50000: episode: 3740, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 18157.019531, mae: 616.997152, accuracy: 0.218750, mean_q: -110.425278, mean_eps: 0.100000\n",
      " 19809/50000: episode: 3741, duration: 0.012s, episode steps:   3, steps per second: 253, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 16497.451497, mae: 603.548279, accuracy: 0.093750, mean_q: -94.654162, mean_eps: 0.100000\n",
      " 19812/50000: episode: 3742, duration: 0.012s, episode steps:   3, steps per second: 254, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 14055.787109, mae: 589.477234, accuracy: 0.145833, mean_q: -95.229341, mean_eps: 0.100000\n",
      " 19816/50000: episode: 3743, duration: 0.015s, episode steps:   4, steps per second: 261, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 2.250 [1.000, 3.000],  loss: 16873.781738, mae: 602.110855, accuracy: 0.203125, mean_q: -115.428568, mean_eps: 0.100000\n",
      " 19819/50000: episode: 3744, duration: 0.013s, episode steps:   3, steps per second: 230, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 19616.777344, mae: 599.914775, accuracy: 0.166667, mean_q: -114.657260, mean_eps: 0.100000\n",
      " 19822/50000: episode: 3745, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 20312.304688, mae: 592.638692, accuracy: 0.208333, mean_q: -105.881640, mean_eps: 0.100000\n",
      " 19825/50000: episode: 3746, duration: 0.017s, episode steps:   3, steps per second: 173, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 19654.625000, mae: 613.437154, accuracy: 0.177083, mean_q: -96.315189, mean_eps: 0.100000\n",
      " 19828/50000: episode: 3747, duration: 0.016s, episode steps:   3, steps per second: 193, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 21075.794271, mae: 600.740723, accuracy: 0.135417, mean_q: -129.169151, mean_eps: 0.100000\n",
      " 19831/50000: episode: 3748, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 16877.055664, mae: 603.468323, accuracy: 0.270833, mean_q: -119.561722, mean_eps: 0.100000\n",
      " 19834/50000: episode: 3749, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 19966.033854, mae: 590.252319, accuracy: 0.260417, mean_q: -124.482618, mean_eps: 0.100000\n",
      " 19837/50000: episode: 3750, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 19885.990885, mae: 619.493693, accuracy: 0.145833, mean_q: -124.006943, mean_eps: 0.100000\n",
      " 19840/50000: episode: 3751, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 14393.092448, mae: 589.057332, accuracy: 0.250000, mean_q: -77.234734, mean_eps: 0.100000\n",
      " 19843/50000: episode: 3752, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 18462.777344, mae: 593.876343, accuracy: 0.250000, mean_q: -123.092326, mean_eps: 0.100000\n",
      " 19846/50000: episode: 3753, duration: 0.012s, episode steps:   3, steps per second: 253, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 19339.292969, mae: 600.420492, accuracy: 0.166667, mean_q: -73.180032, mean_eps: 0.100000\n",
      " 19849/50000: episode: 3754, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.667 [0.000, 3.000],  loss: 16731.086589, mae: 587.981547, accuracy: 0.145833, mean_q: -90.833049, mean_eps: 0.100000\n",
      " 19852/50000: episode: 3755, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 18618.550130, mae: 633.642212, accuracy: 0.135417, mean_q: -136.309102, mean_eps: 0.100000\n",
      " 19855/50000: episode: 3756, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 22468.037109, mae: 590.082703, accuracy: 0.145833, mean_q: -115.233569, mean_eps: 0.100000\n",
      " 19858/50000: episode: 3757, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 14647.764974, mae: 603.099894, accuracy: 0.156250, mean_q: -88.707069, mean_eps: 0.100000\n",
      " 19861/50000: episode: 3758, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 20317.179688, mae: 614.608948, accuracy: 0.135417, mean_q: -88.783783, mean_eps: 0.100000\n",
      " 19864/50000: episode: 3759, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 16012.988932, mae: 595.232015, accuracy: 0.208333, mean_q: -82.558418, mean_eps: 0.100000\n",
      " 19867/50000: episode: 3760, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 16022.793294, mae: 605.106954, accuracy: 0.187500, mean_q: -116.418607, mean_eps: 0.100000\n",
      " 19870/50000: episode: 3761, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 21947.202474, mae: 607.236694, accuracy: 0.125000, mean_q: -145.643567, mean_eps: 0.100000\n",
      " 19873/50000: episode: 3762, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 19522.064453, mae: 606.191752, accuracy: 0.218750, mean_q: -81.387549, mean_eps: 0.100000\n",
      " 19876/50000: episode: 3763, duration: 0.015s, episode steps:   3, steps per second: 205, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 21327.481120, mae: 598.189189, accuracy: 0.145833, mean_q: -101.110502, mean_eps: 0.100000\n",
      " 19879/50000: episode: 3764, duration: 0.015s, episode steps:   3, steps per second: 195, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 21300.308268, mae: 573.524658, accuracy: 0.187500, mean_q: -78.926000, mean_eps: 0.100000\n",
      " 19882/50000: episode: 3765, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 21027.284505, mae: 604.575419, accuracy: 0.218750, mean_q: -88.611572, mean_eps: 0.100000\n",
      " 19886/50000: episode: 3766, duration: 0.016s, episode steps:   4, steps per second: 249, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 16800.834961, mae: 571.427399, accuracy: 0.164062, mean_q: -90.987055, mean_eps: 0.100000\n",
      " 19889/50000: episode: 3767, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 17973.161784, mae: 612.475789, accuracy: 0.125000, mean_q: -90.103526, mean_eps: 0.100000\n",
      " 19892/50000: episode: 3768, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 16339.992513, mae: 580.186808, accuracy: 0.197917, mean_q: -104.868248, mean_eps: 0.100000\n",
      " 19895/50000: episode: 3769, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 18492.657227, mae: 597.866801, accuracy: 0.156250, mean_q: -109.925494, mean_eps: 0.100000\n",
      " 19898/50000: episode: 3770, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 16830.517904, mae: 602.005269, accuracy: 0.104167, mean_q: -91.254064, mean_eps: 0.100000\n",
      " 19901/50000: episode: 3771, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 20705.077474, mae: 602.536092, accuracy: 0.260417, mean_q: -123.013357, mean_eps: 0.100000\n",
      " 19904/50000: episode: 3772, duration: 0.012s, episode steps:   3, steps per second: 256, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 18806.110026, mae: 594.037028, accuracy: 0.208333, mean_q: -123.752111, mean_eps: 0.100000\n",
      " 19907/50000: episode: 3773, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 17721.745117, mae: 604.113729, accuracy: 0.156250, mean_q: -123.450414, mean_eps: 0.100000\n",
      " 19910/50000: episode: 3774, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 16187.139323, mae: 588.028666, accuracy: 0.166667, mean_q: -77.504791, mean_eps: 0.100000\n",
      " 19913/50000: episode: 3775, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 19652.999674, mae: 583.983236, accuracy: 0.197917, mean_q: -121.009745, mean_eps: 0.100000\n",
      " 19916/50000: episode: 3776, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 19609.455729, mae: 590.704285, accuracy: 0.208333, mean_q: -111.093653, mean_eps: 0.100000\n",
      " 19919/50000: episode: 3777, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.333 [0.000, 3.000],  loss: 21920.773438, mae: 605.699117, accuracy: 0.125000, mean_q: -90.841373, mean_eps: 0.100000\n",
      " 19922/50000: episode: 3778, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 16833.846354, mae: 594.439006, accuracy: 0.187500, mean_q: -78.721709, mean_eps: 0.100000\n",
      " 19925/50000: episode: 3779, duration: 0.021s, episode steps:   3, steps per second: 141, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 18000.014323, mae: 600.692566, accuracy: 0.208333, mean_q: -87.945015, mean_eps: 0.100000\n",
      " 19928/50000: episode: 3780, duration: 0.019s, episode steps:   3, steps per second: 162, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 18068.235677, mae: 571.978638, accuracy: 0.187500, mean_q: -49.905396, mean_eps: 0.100000\n",
      " 19931/50000: episode: 3781, duration: 0.013s, episode steps:   3, steps per second: 223, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 16168.561849, mae: 588.575989, accuracy: 0.135417, mean_q: -92.909803, mean_eps: 0.100000\n",
      " 19934/50000: episode: 3782, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 18804.390299, mae: 584.752726, accuracy: 0.250000, mean_q: -86.212803, mean_eps: 0.100000\n",
      " 19937/50000: episode: 3783, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 17873.091471, mae: 586.345968, accuracy: 0.270833, mean_q: -101.846858, mean_eps: 0.100000\n",
      " 19940/50000: episode: 3784, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 19350.634766, mae: 580.960632, accuracy: 0.229167, mean_q: -101.065252, mean_eps: 0.100000\n",
      " 19943/50000: episode: 3785, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 17855.457682, mae: 602.269857, accuracy: 0.208333, mean_q: -114.541283, mean_eps: 0.100000\n",
      " 19946/50000: episode: 3786, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 15332.396159, mae: 601.132284, accuracy: 0.229167, mean_q: -84.136902, mean_eps: 0.100000\n",
      " 19949/50000: episode: 3787, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 19154.897135, mae: 583.474731, accuracy: 0.239583, mean_q: -95.001826, mean_eps: 0.100000\n",
      " 19952/50000: episode: 3788, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 19117.823568, mae: 586.339823, accuracy: 0.125000, mean_q: -104.214737, mean_eps: 0.100000\n",
      " 19955/50000: episode: 3789, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 18960.074544, mae: 591.748556, accuracy: 0.156250, mean_q: -108.129227, mean_eps: 0.100000\n",
      " 19958/50000: episode: 3790, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 19686.014323, mae: 595.812683, accuracy: 0.197917, mean_q: -111.266347, mean_eps: 0.100000\n",
      " 19961/50000: episode: 3791, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 18458.595378, mae: 605.228312, accuracy: 0.135417, mean_q: -104.437963, mean_eps: 0.100000\n",
      " 19965/50000: episode: 3792, duration: 0.016s, episode steps:   4, steps per second: 257, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 18898.388184, mae: 594.333221, accuracy: 0.164062, mean_q: -82.349644, mean_eps: 0.100000\n",
      " 19969/50000: episode: 3793, duration: 0.015s, episode steps:   4, steps per second: 260, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 20132.943848, mae: 578.266083, accuracy: 0.179688, mean_q: -87.675554, mean_eps: 0.100000\n",
      " 19972/50000: episode: 3794, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 18187.889974, mae: 562.481283, accuracy: 0.229167, mean_q: -109.666372, mean_eps: 0.100000\n",
      " 19975/50000: episode: 3795, duration: 0.015s, episode steps:   3, steps per second: 201, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 20347.339844, mae: 583.687968, accuracy: 0.187500, mean_q: -94.177043, mean_eps: 0.100000\n",
      " 19978/50000: episode: 3796, duration: 0.016s, episode steps:   3, steps per second: 193, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 14924.843099, mae: 566.112874, accuracy: 0.239583, mean_q: -78.326370, mean_eps: 0.100000\n",
      " 19981/50000: episode: 3797, duration: 0.015s, episode steps:   3, steps per second: 203, episode reward: -224.000, mean reward: -74.667 [-116.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 21735.227214, mae: 590.740946, accuracy: 0.239583, mean_q: -95.326052, mean_eps: 0.100000\n",
      " 19984/50000: episode: 3798, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 18073.763672, mae: 582.743530, accuracy: 0.177083, mean_q: -90.112945, mean_eps: 0.100000\n",
      " 19987/50000: episode: 3799, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 15059.977539, mae: 593.953532, accuracy: 0.218750, mean_q: -74.813568, mean_eps: 0.100000\n",
      " 19990/50000: episode: 3800, duration: 0.012s, episode steps:   3, steps per second: 256, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 20076.078776, mae: 589.355774, accuracy: 0.197917, mean_q: -93.934962, mean_eps: 0.100000\n",
      " 19994/50000: episode: 3801, duration: 0.015s, episode steps:   4, steps per second: 261, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.750 [0.000, 3.000],  loss: 18604.119629, mae: 595.546677, accuracy: 0.257812, mean_q: -93.460068, mean_eps: 0.100000\n",
      " 19997/50000: episode: 3802, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 21979.331380, mae: 590.954712, accuracy: 0.218750, mean_q: -88.562101, mean_eps: 0.100000\n",
      " 20000/50000: episode: 3803, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 15386.803385, mae: 576.429281, accuracy: 0.166667, mean_q: -74.948059, mean_eps: 0.100000\n",
      " 20003/50000: episode: 3804, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 16408.707357, mae: 595.254069, accuracy: 0.239583, mean_q: -95.774328, mean_eps: 0.100000\n",
      " 20006/50000: episode: 3805, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 21720.492188, mae: 602.560465, accuracy: 0.270833, mean_q: -85.299911, mean_eps: 0.100000\n",
      " 20009/50000: episode: 3806, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 17406.139323, mae: 593.433004, accuracy: 0.229167, mean_q: -104.159986, mean_eps: 0.100000\n",
      " 20012/50000: episode: 3807, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 15147.135742, mae: 584.169535, accuracy: 0.250000, mean_q: -103.244708, mean_eps: 0.100000\n",
      " 20015/50000: episode: 3808, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 17863.445638, mae: 584.368713, accuracy: 0.260417, mean_q: -92.618173, mean_eps: 0.100000\n",
      " 20018/50000: episode: 3809, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 18302.561198, mae: 584.800028, accuracy: 0.250000, mean_q: -87.896711, mean_eps: 0.100000\n",
      " 20021/50000: episode: 3810, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 18476.995768, mae: 579.599508, accuracy: 0.250000, mean_q: -124.260376, mean_eps: 0.100000\n",
      " 20024/50000: episode: 3811, duration: 0.014s, episode steps:   3, steps per second: 222, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 14808.125000, mae: 596.263692, accuracy: 0.197917, mean_q: -69.636669, mean_eps: 0.100000\n",
      " 20027/50000: episode: 3812, duration: 0.014s, episode steps:   3, steps per second: 208, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 17636.533854, mae: 583.133911, accuracy: 0.156250, mean_q: -87.370598, mean_eps: 0.100000\n",
      " 20030/50000: episode: 3813, duration: 0.013s, episode steps:   3, steps per second: 227, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 15596.667643, mae: 595.144043, accuracy: 0.197917, mean_q: -97.166702, mean_eps: 0.100000\n",
      " 20033/50000: episode: 3814, duration: 0.014s, episode steps:   3, steps per second: 220, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 17492.252604, mae: 579.358195, accuracy: 0.218750, mean_q: -83.774910, mean_eps: 0.100000\n",
      " 20036/50000: episode: 3815, duration: 0.013s, episode steps:   3, steps per second: 223, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 19778.393555, mae: 587.452698, accuracy: 0.260417, mean_q: -112.182022, mean_eps: 0.100000\n",
      " 20039/50000: episode: 3816, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 16128.996419, mae: 574.932170, accuracy: 0.218750, mean_q: -39.017873, mean_eps: 0.100000\n",
      " 20042/50000: episode: 3817, duration: 0.014s, episode steps:   3, steps per second: 219, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 22755.654297, mae: 573.069214, accuracy: 0.229167, mean_q: -61.407309, mean_eps: 0.100000\n",
      " 20045/50000: episode: 3818, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 15752.916016, mae: 566.332072, accuracy: 0.270833, mean_q: -52.727999, mean_eps: 0.100000\n",
      " 20048/50000: episode: 3819, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 19416.666992, mae: 588.450968, accuracy: 0.229167, mean_q: -101.508489, mean_eps: 0.100000\n",
      " 20051/50000: episode: 3820, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 22471.007812, mae: 589.438131, accuracy: 0.187500, mean_q: -93.164912, mean_eps: 0.100000\n",
      " 20054/50000: episode: 3821, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 18975.839844, mae: 584.383993, accuracy: 0.197917, mean_q: -80.700704, mean_eps: 0.100000\n",
      " 20057/50000: episode: 3822, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 16127.160156, mae: 570.268840, accuracy: 0.260417, mean_q: -78.839748, mean_eps: 0.100000\n",
      " 20060/50000: episode: 3823, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 19756.276693, mae: 585.118083, accuracy: 0.250000, mean_q: -126.074425, mean_eps: 0.100000\n",
      " 20063/50000: episode: 3824, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 17474.701823, mae: 602.885803, accuracy: 0.239583, mean_q: -67.241533, mean_eps: 0.100000\n",
      " 20066/50000: episode: 3825, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 18925.016927, mae: 595.663513, accuracy: 0.177083, mean_q: -131.601117, mean_eps: 0.100000\n",
      " 20071/50000: episode: 3826, duration: 0.019s, episode steps:   5, steps per second: 263, episode reward: -2193.000, mean reward: -438.600 [-999.000, -58.000], mean action: 1.800 [0.000, 3.000],  loss: 18332.272656, mae: 591.105750, accuracy: 0.256250, mean_q: -88.393700, mean_eps: 0.100000\n",
      " 20074/50000: episode: 3827, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 17957.501302, mae: 596.101217, accuracy: 0.187500, mean_q: -76.960271, mean_eps: 0.100000\n",
      " 20077/50000: episode: 3828, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 16241.369466, mae: 584.871297, accuracy: 0.270833, mean_q: -108.661003, mean_eps: 0.100000\n",
      " 20080/50000: episode: 3829, duration: 0.014s, episode steps:   3, steps per second: 221, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 22602.103516, mae: 571.585510, accuracy: 0.239583, mean_q: -101.218870, mean_eps: 0.100000\n",
      " 20083/50000: episode: 3830, duration: 0.014s, episode steps:   3, steps per second: 213, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 14408.490885, mae: 568.368998, accuracy: 0.187500, mean_q: -70.713545, mean_eps: 0.100000\n",
      " 20086/50000: episode: 3831, duration: 0.014s, episode steps:   3, steps per second: 217, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 16536.037435, mae: 581.071309, accuracy: 0.239583, mean_q: -74.918953, mean_eps: 0.100000\n",
      " 20089/50000: episode: 3832, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 20183.829753, mae: 578.468018, accuracy: 0.270833, mean_q: -99.124352, mean_eps: 0.100000\n",
      " 20092/50000: episode: 3833, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 19548.127930, mae: 592.189046, accuracy: 0.239583, mean_q: -76.928940, mean_eps: 0.100000\n",
      " 20096/50000: episode: 3834, duration: 0.016s, episode steps:   4, steps per second: 253, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 24689.930176, mae: 587.665543, accuracy: 0.250000, mean_q: -111.126179, mean_eps: 0.100000\n",
      " 20099/50000: episode: 3835, duration: 0.012s, episode steps:   3, steps per second: 257, episode reward: -224.000, mean reward: -74.667 [-116.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 15639.414714, mae: 589.709859, accuracy: 0.177083, mean_q: -56.613970, mean_eps: 0.100000\n",
      " 20102/50000: episode: 3836, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 19803.890625, mae: 581.763631, accuracy: 0.218750, mean_q: -126.044754, mean_eps: 0.100000\n",
      " 20105/50000: episode: 3837, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 17973.030924, mae: 589.231201, accuracy: 0.229167, mean_q: -96.174708, mean_eps: 0.100000\n",
      " 20109/50000: episode: 3838, duration: 0.015s, episode steps:   4, steps per second: 258, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 2.250 [1.000, 3.000],  loss: 18750.176270, mae: 577.143478, accuracy: 0.296875, mean_q: -94.316093, mean_eps: 0.100000\n",
      " 20112/50000: episode: 3839, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 18940.856120, mae: 580.395793, accuracy: 0.281250, mean_q: -86.022010, mean_eps: 0.100000\n",
      " 20115/50000: episode: 3840, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 22009.641927, mae: 578.260864, accuracy: 0.291667, mean_q: -98.581385, mean_eps: 0.100000\n",
      " 20119/50000: episode: 3841, duration: 0.015s, episode steps:   4, steps per second: 269, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 18700.826904, mae: 578.279831, accuracy: 0.234375, mean_q: -101.067671, mean_eps: 0.100000\n",
      " 20124/50000: episode: 3842, duration: 0.019s, episode steps:   5, steps per second: 268, episode reward: -2193.000, mean reward: -438.600 [-999.000, -45.000], mean action: 1.400 [0.000, 3.000],  loss: 20760.183984, mae: 576.080713, accuracy: 0.193750, mean_q: -60.481068, mean_eps: 0.100000\n",
      " 20127/50000: episode: 3843, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 16208.416667, mae: 589.283773, accuracy: 0.270833, mean_q: -61.278397, mean_eps: 0.100000\n",
      " 20130/50000: episode: 3844, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 17914.810872, mae: 584.111674, accuracy: 0.260417, mean_q: -65.308769, mean_eps: 0.100000\n",
      " 20133/50000: episode: 3845, duration: 0.017s, episode steps:   3, steps per second: 172, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.333 [0.000, 3.000],  loss: 16701.197266, mae: 563.458354, accuracy: 0.218750, mean_q: -84.948799, mean_eps: 0.100000\n",
      " 20136/50000: episode: 3846, duration: 0.016s, episode steps:   3, steps per second: 192, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 16630.820312, mae: 590.000529, accuracy: 0.281250, mean_q: -103.001358, mean_eps: 0.100000\n",
      " 20139/50000: episode: 3847, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 19095.816732, mae: 609.674438, accuracy: 0.322917, mean_q: -93.913559, mean_eps: 0.100000\n",
      " 20142/50000: episode: 3848, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 18683.138021, mae: 592.434448, accuracy: 0.218750, mean_q: -79.161743, mean_eps: 0.100000\n",
      " 20145/50000: episode: 3849, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 23392.619792, mae: 581.522970, accuracy: 0.208333, mean_q: -75.122583, mean_eps: 0.100000\n",
      " 20148/50000: episode: 3850, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 18851.169271, mae: 579.402547, accuracy: 0.187500, mean_q: -53.408291, mean_eps: 0.100000\n",
      " 20151/50000: episode: 3851, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 16923.978516, mae: 583.662984, accuracy: 0.177083, mean_q: -52.398189, mean_eps: 0.100000\n",
      " 20155/50000: episode: 3852, duration: 0.015s, episode steps:   4, steps per second: 264, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 17297.816895, mae: 565.444122, accuracy: 0.210938, mean_q: -36.430472, mean_eps: 0.100000\n",
      " 20158/50000: episode: 3853, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 18549.250977, mae: 574.165568, accuracy: 0.208333, mean_q: -63.410346, mean_eps: 0.100000\n",
      " 20162/50000: episode: 3854, duration: 0.016s, episode steps:   4, steps per second: 248, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 18196.744141, mae: 593.413406, accuracy: 0.210938, mean_q: -76.716109, mean_eps: 0.100000\n",
      " 20165/50000: episode: 3855, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.667 [0.000, 3.000],  loss: 20110.171224, mae: 574.880758, accuracy: 0.218750, mean_q: -81.478878, mean_eps: 0.100000\n",
      " 20168/50000: episode: 3856, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 18931.462891, mae: 584.181335, accuracy: 0.166667, mean_q: -88.833365, mean_eps: 0.100000\n",
      " 20171/50000: episode: 3857, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 15764.534831, mae: 573.449992, accuracy: 0.281250, mean_q: -75.439975, mean_eps: 0.100000\n",
      " 20174/50000: episode: 3858, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 19257.972656, mae: 582.901957, accuracy: 0.270833, mean_q: -74.366126, mean_eps: 0.100000\n",
      " 20178/50000: episode: 3859, duration: 0.016s, episode steps:   4, steps per second: 251, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 15855.684570, mae: 579.570618, accuracy: 0.210938, mean_q: -75.283967, mean_eps: 0.100000\n",
      " 20181/50000: episode: 3860, duration: 0.013s, episode steps:   3, steps per second: 230, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 17090.175781, mae: 588.054057, accuracy: 0.197917, mean_q: -60.162669, mean_eps: 0.100000\n",
      " 20185/50000: episode: 3861, duration: 0.018s, episode steps:   4, steps per second: 220, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.000 [0.000, 3.000],  loss: 18298.919434, mae: 608.186188, accuracy: 0.265625, mean_q: -69.582833, mean_eps: 0.100000\n",
      " 20188/50000: episode: 3862, duration: 0.015s, episode steps:   3, steps per second: 204, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.667 [0.000, 3.000],  loss: 24022.046875, mae: 569.834005, accuracy: 0.260417, mean_q: -79.022751, mean_eps: 0.100000\n",
      " 20191/50000: episode: 3863, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.667 [0.000, 3.000],  loss: 16179.217448, mae: 572.378662, accuracy: 0.250000, mean_q: -75.433858, mean_eps: 0.100000\n",
      " 20195/50000: episode: 3864, duration: 0.016s, episode steps:   4, steps per second: 255, episode reward: -1223.000, mean reward: -305.750 [-999.000, -58.000], mean action: 1.750 [0.000, 3.000],  loss: 17233.545166, mae: 571.376892, accuracy: 0.187500, mean_q: -72.517479, mean_eps: 0.100000\n",
      " 20198/50000: episode: 3865, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 17825.193685, mae: 582.797343, accuracy: 0.197917, mean_q: -89.614328, mean_eps: 0.100000\n",
      " 20201/50000: episode: 3866, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 17199.675781, mae: 571.052572, accuracy: 0.177083, mean_q: -70.443965, mean_eps: 0.100000\n",
      " 20205/50000: episode: 3867, duration: 0.016s, episode steps:   4, steps per second: 258, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 18959.753662, mae: 588.727203, accuracy: 0.203125, mean_q: -73.634440, mean_eps: 0.100000\n",
      " 20208/50000: episode: 3868, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 20859.695964, mae: 586.397400, accuracy: 0.197917, mean_q: -63.063478, mean_eps: 0.100000\n",
      " 20211/50000: episode: 3869, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 19343.531901, mae: 597.300598, accuracy: 0.166667, mean_q: -91.566004, mean_eps: 0.100000\n",
      " 20214/50000: episode: 3870, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 16005.063802, mae: 592.529277, accuracy: 0.281250, mean_q: -43.760738, mean_eps: 0.100000\n",
      " 20217/50000: episode: 3871, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 19552.634115, mae: 573.167277, accuracy: 0.239583, mean_q: -74.300716, mean_eps: 0.100000\n",
      " 20220/50000: episode: 3872, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 17786.204427, mae: 574.323588, accuracy: 0.333333, mean_q: -55.927675, mean_eps: 0.100000\n",
      " 20223/50000: episode: 3873, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 17226.658854, mae: 585.727539, accuracy: 0.291667, mean_q: -58.560408, mean_eps: 0.100000\n",
      " 20226/50000: episode: 3874, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 18376.098633, mae: 581.168152, accuracy: 0.239583, mean_q: -50.922933, mean_eps: 0.100000\n",
      " 20229/50000: episode: 3875, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 16310.152995, mae: 571.721517, accuracy: 0.239583, mean_q: -81.789937, mean_eps: 0.100000\n",
      " 20232/50000: episode: 3876, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.667 [0.000, 3.000],  loss: 17879.819336, mae: 567.983744, accuracy: 0.260417, mean_q: -42.990025, mean_eps: 0.100000\n",
      " 20235/50000: episode: 3877, duration: 0.015s, episode steps:   3, steps per second: 207, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 16618.336263, mae: 574.377991, accuracy: 0.218750, mean_q: -64.900607, mean_eps: 0.100000\n",
      " 20238/50000: episode: 3878, duration: 0.014s, episode steps:   3, steps per second: 212, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 19676.415365, mae: 595.178304, accuracy: 0.302083, mean_q: -97.214287, mean_eps: 0.100000\n",
      " 20241/50000: episode: 3879, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 17558.656250, mae: 573.869303, accuracy: 0.343750, mean_q: -86.858691, mean_eps: 0.100000\n",
      " 20245/50000: episode: 3880, duration: 0.015s, episode steps:   4, steps per second: 260, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 21960.584961, mae: 585.110977, accuracy: 0.234375, mean_q: -91.914513, mean_eps: 0.100000\n",
      " 20249/50000: episode: 3881, duration: 0.015s, episode steps:   4, steps per second: 261, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 19456.333496, mae: 591.086044, accuracy: 0.257812, mean_q: -104.455296, mean_eps: 0.100000\n",
      " 20252/50000: episode: 3882, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 16065.606445, mae: 596.617330, accuracy: 0.187500, mean_q: -73.154138, mean_eps: 0.100000\n",
      " 20255/50000: episode: 3883, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 13965.990885, mae: 572.173055, accuracy: 0.302083, mean_q: -69.321303, mean_eps: 0.100000\n",
      " 20258/50000: episode: 3884, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 18437.797526, mae: 558.528931, accuracy: 0.208333, mean_q: -83.031448, mean_eps: 0.100000\n",
      " 20261/50000: episode: 3885, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 19031.143880, mae: 595.841573, accuracy: 0.197917, mean_q: -76.002332, mean_eps: 0.100000\n",
      " 20264/50000: episode: 3886, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 21295.252604, mae: 590.263448, accuracy: 0.250000, mean_q: -102.524991, mean_eps: 0.100000\n",
      " 20267/50000: episode: 3887, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 21271.772135, mae: 554.869405, accuracy: 0.239583, mean_q: -91.741989, mean_eps: 0.100000\n",
      " 20270/50000: episode: 3888, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 21093.424154, mae: 585.144023, accuracy: 0.239583, mean_q: -58.245550, mean_eps: 0.100000\n",
      " 20273/50000: episode: 3889, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 17605.974609, mae: 582.490031, accuracy: 0.343750, mean_q: -58.561929, mean_eps: 0.100000\n",
      " 20276/50000: episode: 3890, duration: 0.013s, episode steps:   3, steps per second: 227, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 15439.453125, mae: 551.535278, accuracy: 0.239583, mean_q: -69.680402, mean_eps: 0.100000\n",
      " 20279/50000: episode: 3891, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 19923.816406, mae: 563.706787, accuracy: 0.208333, mean_q: -76.880775, mean_eps: 0.100000\n",
      " 20283/50000: episode: 3892, duration: 0.017s, episode steps:   4, steps per second: 241, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 19875.052246, mae: 571.917236, accuracy: 0.250000, mean_q: -58.123969, mean_eps: 0.100000\n",
      " 20286/50000: episode: 3893, duration: 0.020s, episode steps:   3, steps per second: 152, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 16181.154297, mae: 567.786845, accuracy: 0.312500, mean_q: -79.890195, mean_eps: 0.100000\n",
      " 20289/50000: episode: 3894, duration: 0.014s, episode steps:   3, steps per second: 212, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 19952.183919, mae: 576.534953, accuracy: 0.208333, mean_q: -85.792124, mean_eps: 0.100000\n",
      " 20292/50000: episode: 3895, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 17978.951823, mae: 593.241740, accuracy: 0.208333, mean_q: -66.914818, mean_eps: 0.100000\n",
      " 20295/50000: episode: 3896, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 18272.686849, mae: 577.201294, accuracy: 0.239583, mean_q: -71.908651, mean_eps: 0.100000\n",
      " 20298/50000: episode: 3897, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 18228.614583, mae: 572.913940, accuracy: 0.250000, mean_q: -80.382531, mean_eps: 0.100000\n",
      " 20301/50000: episode: 3898, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 18822.840495, mae: 588.347799, accuracy: 0.322917, mean_q: -85.404752, mean_eps: 0.100000\n",
      " 20304/50000: episode: 3899, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 21131.291992, mae: 586.374959, accuracy: 0.302083, mean_q: -76.091169, mean_eps: 0.100000\n",
      " 20307/50000: episode: 3900, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 17956.587891, mae: 560.652588, accuracy: 0.281250, mean_q: -112.826154, mean_eps: 0.100000\n",
      " 20310/50000: episode: 3901, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 19161.996745, mae: 592.164612, accuracy: 0.239583, mean_q: -94.337367, mean_eps: 0.100000\n",
      " 20313/50000: episode: 3902, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 19246.132161, mae: 600.310832, accuracy: 0.260417, mean_q: -54.044443, mean_eps: 0.100000\n",
      " 20316/50000: episode: 3903, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 18539.491211, mae: 585.585876, accuracy: 0.229167, mean_q: -97.974546, mean_eps: 0.100000\n",
      " 20319/50000: episode: 3904, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 18297.582682, mae: 588.203064, accuracy: 0.322917, mean_q: -74.700676, mean_eps: 0.100000\n",
      " 20322/50000: episode: 3905, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 16183.376628, mae: 589.333374, accuracy: 0.197917, mean_q: -52.109130, mean_eps: 0.100000\n",
      " 20325/50000: episode: 3906, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 21089.509115, mae: 590.335205, accuracy: 0.270833, mean_q: -76.982516, mean_eps: 0.100000\n",
      " 20328/50000: episode: 3907, duration: 0.018s, episode steps:   3, steps per second: 165, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 17187.307292, mae: 579.683451, accuracy: 0.312500, mean_q: -45.353703, mean_eps: 0.100000\n",
      " 20331/50000: episode: 3908, duration: 0.017s, episode steps:   3, steps per second: 181, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 17212.688477, mae: 586.890279, accuracy: 0.291667, mean_q: -46.772299, mean_eps: 0.100000\n",
      " 20334/50000: episode: 3909, duration: 0.014s, episode steps:   3, steps per second: 215, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 21681.597005, mae: 602.889608, accuracy: 0.156250, mean_q: -43.422264, mean_eps: 0.100000\n",
      " 20337/50000: episode: 3910, duration: 0.013s, episode steps:   3, steps per second: 227, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 18333.144531, mae: 567.688843, accuracy: 0.270833, mean_q: -61.869741, mean_eps: 0.100000\n",
      " 20340/50000: episode: 3911, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 19397.414714, mae: 592.196940, accuracy: 0.302083, mean_q: -101.659111, mean_eps: 0.100000\n",
      " 20343/50000: episode: 3912, duration: 0.014s, episode steps:   3, steps per second: 215, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 18207.580078, mae: 573.449137, accuracy: 0.250000, mean_q: -67.936958, mean_eps: 0.100000\n",
      " 20346/50000: episode: 3913, duration: 0.014s, episode steps:   3, steps per second: 208, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 17056.178385, mae: 577.809713, accuracy: 0.322917, mean_q: -54.960906, mean_eps: 0.100000\n",
      " 20349/50000: episode: 3914, duration: 0.014s, episode steps:   3, steps per second: 220, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 14955.863281, mae: 579.228455, accuracy: 0.187500, mean_q: -65.966672, mean_eps: 0.100000\n",
      " 20352/50000: episode: 3915, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 20705.604167, mae: 587.700236, accuracy: 0.260417, mean_q: -76.453562, mean_eps: 0.100000\n",
      " 20355/50000: episode: 3916, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 15383.863607, mae: 579.380229, accuracy: 0.250000, mean_q: -70.533714, mean_eps: 0.100000\n",
      " 20358/50000: episode: 3917, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 18717.731771, mae: 581.550069, accuracy: 0.156250, mean_q: -94.120430, mean_eps: 0.100000\n",
      " 20362/50000: episode: 3918, duration: 0.017s, episode steps:   4, steps per second: 242, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 18229.534180, mae: 579.658875, accuracy: 0.218750, mean_q: -69.941031, mean_eps: 0.100000\n",
      " 20365/50000: episode: 3919, duration: 0.014s, episode steps:   3, steps per second: 215, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 18658.253255, mae: 580.056132, accuracy: 0.218750, mean_q: -87.593976, mean_eps: 0.100000\n",
      " 20368/50000: episode: 3920, duration: 0.015s, episode steps:   3, steps per second: 203, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 16840.676432, mae: 575.121623, accuracy: 0.218750, mean_q: -84.376335, mean_eps: 0.100000\n",
      " 20371/50000: episode: 3921, duration: 0.016s, episode steps:   3, steps per second: 191, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 16846.273112, mae: 570.095805, accuracy: 0.208333, mean_q: -55.638027, mean_eps: 0.100000\n",
      " 20374/50000: episode: 3922, duration: 0.015s, episode steps:   3, steps per second: 197, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 17718.339193, mae: 569.515706, accuracy: 0.239583, mean_q: -79.935445, mean_eps: 0.100000\n",
      " 20377/50000: episode: 3923, duration: 0.015s, episode steps:   3, steps per second: 201, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 23102.874349, mae: 583.870300, accuracy: 0.270833, mean_q: -109.766777, mean_eps: 0.100000\n",
      " 20380/50000: episode: 3924, duration: 0.014s, episode steps:   3, steps per second: 212, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 19934.283203, mae: 558.997111, accuracy: 0.208333, mean_q: -56.018684, mean_eps: 0.100000\n",
      " 20383/50000: episode: 3925, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 18341.001302, mae: 570.867147, accuracy: 0.156250, mean_q: -68.663394, mean_eps: 0.100000\n",
      " 20386/50000: episode: 3926, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 15524.711263, mae: 555.042826, accuracy: 0.229167, mean_q: -47.094305, mean_eps: 0.100000\n",
      " 20389/50000: episode: 3927, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 19254.610677, mae: 575.708577, accuracy: 0.104167, mean_q: -87.423823, mean_eps: 0.100000\n",
      " 20392/50000: episode: 3928, duration: 0.014s, episode steps:   3, steps per second: 212, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 21163.635417, mae: 585.583415, accuracy: 0.187500, mean_q: -62.011971, mean_eps: 0.100000\n",
      " 20395/50000: episode: 3929, duration: 0.014s, episode steps:   3, steps per second: 217, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 17030.245117, mae: 590.783773, accuracy: 0.197917, mean_q: -75.921000, mean_eps: 0.100000\n",
      " 20398/50000: episode: 3930, duration: 0.015s, episode steps:   3, steps per second: 206, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 17609.183594, mae: 584.656148, accuracy: 0.260417, mean_q: -68.123744, mean_eps: 0.100000\n",
      " 20401/50000: episode: 3931, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 17196.451172, mae: 584.557576, accuracy: 0.343750, mean_q: -55.808453, mean_eps: 0.100000\n",
      " 20404/50000: episode: 3932, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 18501.141927, mae: 576.625651, accuracy: 0.250000, mean_q: -69.962400, mean_eps: 0.100000\n",
      " 20407/50000: episode: 3933, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 19439.458333, mae: 586.629395, accuracy: 0.364583, mean_q: -66.560115, mean_eps: 0.100000\n",
      " 20410/50000: episode: 3934, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 18576.654948, mae: 579.699504, accuracy: 0.364583, mean_q: -74.772133, mean_eps: 0.100000\n",
      " 20413/50000: episode: 3935, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 15512.765951, mae: 580.830119, accuracy: 0.333333, mean_q: -68.797160, mean_eps: 0.100000\n",
      " 20416/50000: episode: 3936, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 17706.458333, mae: 590.530680, accuracy: 0.385417, mean_q: -57.924301, mean_eps: 0.100000\n",
      " 20419/50000: episode: 3937, duration: 0.017s, episode steps:   3, steps per second: 180, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 21558.576172, mae: 558.900146, accuracy: 0.281250, mean_q: -69.646090, mean_eps: 0.100000\n",
      " 20423/50000: episode: 3938, duration: 0.019s, episode steps:   4, steps per second: 216, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 2.250 [1.000, 3.000],  loss: 19897.627441, mae: 567.733505, accuracy: 0.210938, mean_q: -78.304457, mean_eps: 0.100000\n",
      " 20426/50000: episode: 3939, duration: 0.015s, episode steps:   3, steps per second: 201, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 19771.780599, mae: 576.527568, accuracy: 0.208333, mean_q: -70.305588, mean_eps: 0.100000\n",
      " 20429/50000: episode: 3940, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 20479.498047, mae: 561.324402, accuracy: 0.218750, mean_q: -96.040021, mean_eps: 0.100000\n",
      " 20432/50000: episode: 3941, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.667 [0.000, 3.000],  loss: 17028.476237, mae: 572.738627, accuracy: 0.239583, mean_q: -41.278745, mean_eps: 0.100000\n",
      " 20435/50000: episode: 3942, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 18717.805339, mae: 572.654724, accuracy: 0.177083, mean_q: -76.798337, mean_eps: 0.100000\n",
      " 20438/50000: episode: 3943, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 14317.056315, mae: 563.742167, accuracy: 0.250000, mean_q: -40.967006, mean_eps: 0.100000\n",
      " 20441/50000: episode: 3944, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 18825.580078, mae: 561.732951, accuracy: 0.187500, mean_q: -87.453274, mean_eps: 0.100000\n",
      " 20444/50000: episode: 3945, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 20459.039062, mae: 577.565450, accuracy: 0.229167, mean_q: -64.772834, mean_eps: 0.100000\n",
      " 20447/50000: episode: 3946, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 21047.720703, mae: 574.301331, accuracy: 0.177083, mean_q: -85.471359, mean_eps: 0.100000\n",
      " 20450/50000: episode: 3947, duration: 0.012s, episode steps:   3, steps per second: 256, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 15967.249349, mae: 579.575623, accuracy: 0.218750, mean_q: -77.324235, mean_eps: 0.100000\n",
      " 20455/50000: episode: 3948, duration: 0.018s, episode steps:   5, steps per second: 274, episode reward: -2193.000, mean reward: -438.600 [-999.000, -45.000], mean action: 1.800 [0.000, 3.000],  loss: 18223.361133, mae: 568.556165, accuracy: 0.306250, mean_q: -70.273666, mean_eps: 0.100000\n",
      " 20458/50000: episode: 3949, duration: 0.012s, episode steps:   3, steps per second: 253, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.667 [0.000, 3.000],  loss: 16961.876628, mae: 575.771708, accuracy: 0.250000, mean_q: -69.747763, mean_eps: 0.100000\n",
      " 20461/50000: episode: 3950, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 19122.130208, mae: 595.099345, accuracy: 0.166667, mean_q: -31.947073, mean_eps: 0.100000\n",
      " 20465/50000: episode: 3951, duration: 0.016s, episode steps:   4, steps per second: 244, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 2.250 [1.000, 3.000],  loss: 16179.560791, mae: 555.858093, accuracy: 0.273438, mean_q: -55.131934, mean_eps: 0.100000\n",
      " 20468/50000: episode: 3952, duration: 0.014s, episode steps:   3, steps per second: 213, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 16461.851562, mae: 567.263265, accuracy: 0.239583, mean_q: -79.443464, mean_eps: 0.100000\n",
      " 20471/50000: episode: 3953, duration: 0.015s, episode steps:   3, steps per second: 201, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 19649.147135, mae: 579.632446, accuracy: 0.187500, mean_q: -83.577169, mean_eps: 0.100000\n",
      " 20474/50000: episode: 3954, duration: 0.014s, episode steps:   3, steps per second: 213, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 20118.146810, mae: 598.456807, accuracy: 0.364583, mean_q: -62.208537, mean_eps: 0.100000\n",
      " 20477/50000: episode: 3955, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 18275.934896, mae: 580.344279, accuracy: 0.229167, mean_q: -62.929110, mean_eps: 0.100000\n",
      " 20480/50000: episode: 3956, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 20906.608073, mae: 582.292379, accuracy: 0.239583, mean_q: -52.922202, mean_eps: 0.100000\n",
      " 20483/50000: episode: 3957, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 15640.815755, mae: 568.332804, accuracy: 0.322917, mean_q: -52.583400, mean_eps: 0.100000\n",
      " 20486/50000: episode: 3958, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 17727.544922, mae: 567.980916, accuracy: 0.218750, mean_q: -86.872683, mean_eps: 0.100000\n",
      " 20490/50000: episode: 3959, duration: 0.016s, episode steps:   4, steps per second: 256, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 18656.475098, mae: 561.641922, accuracy: 0.226562, mean_q: -82.060148, mean_eps: 0.100000\n",
      " 20493/50000: episode: 3960, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 17885.217122, mae: 568.634969, accuracy: 0.197917, mean_q: -52.144644, mean_eps: 0.100000\n",
      " 20496/50000: episode: 3961, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 17323.257161, mae: 576.454285, accuracy: 0.270833, mean_q: -91.192551, mean_eps: 0.100000\n",
      " 20499/50000: episode: 3962, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 19730.830078, mae: 555.857727, accuracy: 0.291667, mean_q: -78.327374, mean_eps: 0.100000\n",
      " 20502/50000: episode: 3963, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 19874.939453, mae: 584.427043, accuracy: 0.250000, mean_q: -94.259163, mean_eps: 0.100000\n",
      " 20506/50000: episode: 3964, duration: 0.015s, episode steps:   4, steps per second: 259, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 21518.563477, mae: 573.994705, accuracy: 0.242188, mean_q: -101.630423, mean_eps: 0.100000\n",
      " 20509/50000: episode: 3965, duration: 0.012s, episode steps:   3, steps per second: 254, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 20029.064453, mae: 579.393229, accuracy: 0.208333, mean_q: -43.793087, mean_eps: 0.100000\n",
      " 20512/50000: episode: 3966, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.667 [0.000, 3.000],  loss: 17446.766276, mae: 566.812378, accuracy: 0.229167, mean_q: -38.297018, mean_eps: 0.100000\n",
      " 20515/50000: episode: 3967, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 17543.672526, mae: 556.349691, accuracy: 0.239583, mean_q: -51.142487, mean_eps: 0.100000\n",
      " 20518/50000: episode: 3968, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 16868.871094, mae: 569.985453, accuracy: 0.239583, mean_q: -51.642848, mean_eps: 0.100000\n",
      " 20521/50000: episode: 3969, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 23824.677083, mae: 563.695312, accuracy: 0.291667, mean_q: -59.355866, mean_eps: 0.100000\n",
      " 20524/50000: episode: 3970, duration: 0.015s, episode steps:   3, steps per second: 199, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 17915.146484, mae: 563.868408, accuracy: 0.197917, mean_q: -56.843362, mean_eps: 0.100000\n",
      " 20527/50000: episode: 3971, duration: 0.014s, episode steps:   3, steps per second: 209, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 14611.230469, mae: 562.530680, accuracy: 0.208333, mean_q: -37.154807, mean_eps: 0.100000\n",
      " 20530/50000: episode: 3972, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 19971.342122, mae: 586.308655, accuracy: 0.333333, mean_q: -56.957432, mean_eps: 0.100000\n",
      " 20533/50000: episode: 3973, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 16843.957682, mae: 573.232585, accuracy: 0.218750, mean_q: -60.061958, mean_eps: 0.100000\n",
      " 20536/50000: episode: 3974, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 19793.774740, mae: 558.868734, accuracy: 0.177083, mean_q: -43.629082, mean_eps: 0.100000\n",
      " 20539/50000: episode: 3975, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 16582.577148, mae: 547.971008, accuracy: 0.197917, mean_q: -46.787005, mean_eps: 0.100000\n",
      " 20542/50000: episode: 3976, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 16909.852539, mae: 577.006877, accuracy: 0.187500, mean_q: -31.475541, mean_eps: 0.100000\n",
      " 20545/50000: episode: 3977, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 16169.701497, mae: 560.170898, accuracy: 0.354167, mean_q: -40.620803, mean_eps: 0.100000\n",
      " 20548/50000: episode: 3978, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 15693.567708, mae: 556.466329, accuracy: 0.312500, mean_q: -71.937510, mean_eps: 0.100000\n",
      " 20551/50000: episode: 3979, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 17372.425130, mae: 552.726522, accuracy: 0.260417, mean_q: -63.647638, mean_eps: 0.100000\n",
      " 20554/50000: episode: 3980, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 16763.357422, mae: 559.196370, accuracy: 0.239583, mean_q: -62.415250, mean_eps: 0.100000\n",
      " 20558/50000: episode: 3981, duration: 0.015s, episode steps:   4, steps per second: 264, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 14078.745605, mae: 555.911026, accuracy: 0.179688, mean_q: -40.080537, mean_eps: 0.100000\n",
      " 20562/50000: episode: 3982, duration: 0.015s, episode steps:   4, steps per second: 262, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 19524.277832, mae: 572.102417, accuracy: 0.203125, mean_q: -51.299379, mean_eps: 0.100000\n",
      " 20565/50000: episode: 3983, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 1.667 [0.000, 3.000],  loss: 23512.661458, mae: 578.807739, accuracy: 0.218750, mean_q: -71.640320, mean_eps: 0.100000\n",
      " 20568/50000: episode: 3984, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 16849.998047, mae: 544.759745, accuracy: 0.291667, mean_q: -66.810567, mean_eps: 0.100000\n",
      " 20571/50000: episode: 3985, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 23248.425130, mae: 547.583211, accuracy: 0.239583, mean_q: -69.186605, mean_eps: 0.100000\n",
      " 20574/50000: episode: 3986, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 18547.201497, mae: 554.455933, accuracy: 0.281250, mean_q: -39.265850, mean_eps: 0.100000\n",
      " 20577/50000: episode: 3987, duration: 0.018s, episode steps:   3, steps per second: 167, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 16613.395508, mae: 570.882589, accuracy: 0.239583, mean_q: -30.303263, mean_eps: 0.100000\n",
      " 20581/50000: episode: 3988, duration: 0.018s, episode steps:   4, steps per second: 227, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 2.000 [0.000, 3.000],  loss: 20878.042480, mae: 578.915421, accuracy: 0.210938, mean_q: -47.901753, mean_eps: 0.100000\n",
      " 20584/50000: episode: 3989, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 20995.365234, mae: 566.929565, accuracy: 0.281250, mean_q: -52.341136, mean_eps: 0.100000\n",
      " 20588/50000: episode: 3990, duration: 0.016s, episode steps:   4, steps per second: 256, episode reward: -1223.000, mean reward: -305.750 [-999.000, -58.000], mean action: 1.750 [0.000, 3.000],  loss: 16971.895020, mae: 551.622650, accuracy: 0.343750, mean_q: -52.574691, mean_eps: 0.100000\n",
      " 20592/50000: episode: 3991, duration: 0.015s, episode steps:   4, steps per second: 260, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 20269.691895, mae: 559.386276, accuracy: 0.226562, mean_q: -86.504066, mean_eps: 0.100000\n",
      " 20595/50000: episode: 3992, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 17551.718750, mae: 552.231771, accuracy: 0.260417, mean_q: -43.369076, mean_eps: 0.100000\n",
      " 20598/50000: episode: 3993, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 20294.519531, mae: 566.401306, accuracy: 0.333333, mean_q: -28.761188, mean_eps: 0.100000\n",
      " 20601/50000: episode: 3994, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 15946.048177, mae: 562.706502, accuracy: 0.260417, mean_q: -26.703330, mean_eps: 0.100000\n",
      " 20604/50000: episode: 3995, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 16219.348307, mae: 566.031820, accuracy: 0.260417, mean_q: -52.242589, mean_eps: 0.100000\n",
      " 20607/50000: episode: 3996, duration: 0.013s, episode steps:   3, steps per second: 224, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 15713.974935, mae: 563.960144, accuracy: 0.218750, mean_q: -56.125912, mean_eps: 0.100000\n",
      " 20610/50000: episode: 3997, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 16985.742513, mae: 560.214905, accuracy: 0.250000, mean_q: -44.079418, mean_eps: 0.100000\n",
      " 20613/50000: episode: 3998, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 16511.547526, mae: 563.048238, accuracy: 0.218750, mean_q: -53.159074, mean_eps: 0.100000\n",
      " 20616/50000: episode: 3999, duration: 0.014s, episode steps:   3, steps per second: 219, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 18243.373372, mae: 569.495483, accuracy: 0.208333, mean_q: -16.745979, mean_eps: 0.100000\n",
      " 20619/50000: episode: 4000, duration: 0.016s, episode steps:   3, steps per second: 188, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 17430.763021, mae: 552.078552, accuracy: 0.260417, mean_q: -44.037961, mean_eps: 0.100000\n",
      " 20622/50000: episode: 4001, duration: 0.015s, episode steps:   3, steps per second: 200, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 19390.141276, mae: 561.332194, accuracy: 0.260417, mean_q: -94.228663, mean_eps: 0.100000\n",
      " 20626/50000: episode: 4002, duration: 0.017s, episode steps:   4, steps per second: 238, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 18060.402344, mae: 548.080643, accuracy: 0.210938, mean_q: -27.540305, mean_eps: 0.100000\n",
      " 20629/50000: episode: 4003, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 16446.981120, mae: 569.353231, accuracy: 0.302083, mean_q: -37.631056, mean_eps: 0.100000\n",
      " 20632/50000: episode: 4004, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 20054.723958, mae: 574.784444, accuracy: 0.218750, mean_q: -45.829878, mean_eps: 0.100000\n",
      " 20636/50000: episode: 4005, duration: 0.016s, episode steps:   4, steps per second: 255, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 17372.103027, mae: 560.383606, accuracy: 0.250000, mean_q: -36.160548, mean_eps: 0.100000\n",
      " 20639/50000: episode: 4006, duration: 0.023s, episode steps:   3, steps per second: 133, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 15428.253581, mae: 576.426921, accuracy: 0.322917, mean_q: -19.420923, mean_eps: 0.100000\n",
      " 20642/50000: episode: 4007, duration: 0.018s, episode steps:   3, steps per second: 166, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 17341.108398, mae: 573.847127, accuracy: 0.177083, mean_q: -49.734528, mean_eps: 0.100000\n",
      " 20645/50000: episode: 4008, duration: 0.015s, episode steps:   3, steps per second: 203, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 14934.276042, mae: 567.535482, accuracy: 0.239583, mean_q: -40.917582, mean_eps: 0.100000\n",
      " 20648/50000: episode: 4009, duration: 0.013s, episode steps:   3, steps per second: 226, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 22234.263672, mae: 570.732076, accuracy: 0.239583, mean_q: -59.366734, mean_eps: 0.100000\n",
      " 20651/50000: episode: 4010, duration: 0.013s, episode steps:   3, steps per second: 230, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 15884.253906, mae: 558.572591, accuracy: 0.333333, mean_q: -36.713784, mean_eps: 0.100000\n",
      " 20654/50000: episode: 4011, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 18456.201172, mae: 556.132243, accuracy: 0.208333, mean_q: -47.249020, mean_eps: 0.100000\n",
      " 20657/50000: episode: 4012, duration: 0.014s, episode steps:   3, steps per second: 208, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 18083.888997, mae: 552.462443, accuracy: 0.229167, mean_q: -59.266003, mean_eps: 0.100000\n",
      " 20660/50000: episode: 4013, duration: 0.015s, episode steps:   3, steps per second: 195, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 19836.766927, mae: 570.640645, accuracy: 0.270833, mean_q: -51.165232, mean_eps: 0.100000\n",
      " 20663/50000: episode: 4014, duration: 0.014s, episode steps:   3, steps per second: 221, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 17451.421549, mae: 563.552226, accuracy: 0.177083, mean_q: -49.507931, mean_eps: 0.100000\n",
      " 20666/50000: episode: 4015, duration: 0.015s, episode steps:   3, steps per second: 195, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 18869.691406, mae: 565.992839, accuracy: 0.250000, mean_q: -51.734367, mean_eps: 0.100000\n",
      " 20670/50000: episode: 4016, duration: 0.018s, episode steps:   4, steps per second: 217, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 18340.215332, mae: 571.227112, accuracy: 0.171875, mean_q: -66.581882, mean_eps: 0.100000\n",
      " 20674/50000: episode: 4017, duration: 0.017s, episode steps:   4, steps per second: 236, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 15749.211426, mae: 552.436935, accuracy: 0.289062, mean_q: -36.643410, mean_eps: 0.100000\n",
      " 20678/50000: episode: 4018, duration: 0.016s, episode steps:   4, steps per second: 248, episode reward: -1223.000, mean reward: -305.750 [-999.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 18191.122070, mae: 561.106140, accuracy: 0.179688, mean_q: -87.725386, mean_eps: 0.100000\n",
      " 20681/50000: episode: 4019, duration: 0.013s, episode steps:   3, steps per second: 224, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 16251.857747, mae: 563.672729, accuracy: 0.166667, mean_q: -61.752143, mean_eps: 0.100000\n",
      " 20684/50000: episode: 4020, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 18958.981120, mae: 566.904175, accuracy: 0.250000, mean_q: -59.628106, mean_eps: 0.100000\n",
      " 20687/50000: episode: 4021, duration: 0.012s, episode steps:   3, steps per second: 258, episode reward: -224.000, mean reward: -74.667 [-116.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 16446.229492, mae: 552.596415, accuracy: 0.322917, mean_q: -50.424314, mean_eps: 0.100000\n",
      " 20690/50000: episode: 4022, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -224.000, mean reward: -74.667 [-116.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 21645.711589, mae: 570.955404, accuracy: 0.343750, mean_q: -54.591407, mean_eps: 0.100000\n",
      " 20693/50000: episode: 4023, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 20358.119141, mae: 556.186666, accuracy: 0.385417, mean_q: -55.829317, mean_eps: 0.100000\n",
      " 20697/50000: episode: 4024, duration: 0.016s, episode steps:   4, steps per second: 256, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 17873.208008, mae: 572.859680, accuracy: 0.234375, mean_q: -69.500510, mean_eps: 0.100000\n",
      " 20700/50000: episode: 4025, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 20514.586589, mae: 567.999898, accuracy: 0.239583, mean_q: -83.440239, mean_eps: 0.100000\n",
      " 20703/50000: episode: 4026, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 15609.049805, mae: 560.661011, accuracy: 0.302083, mean_q: -26.962966, mean_eps: 0.100000\n",
      " 20706/50000: episode: 4027, duration: 0.013s, episode steps:   3, steps per second: 225, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 15411.038737, mae: 565.560282, accuracy: 0.229167, mean_q: -53.230474, mean_eps: 0.100000\n",
      " 20709/50000: episode: 4028, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 16470.982096, mae: 563.468282, accuracy: 0.312500, mean_q: -78.987536, mean_eps: 0.100000\n",
      " 20712/50000: episode: 4029, duration: 0.018s, episode steps:   3, steps per second: 165, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 18841.940755, mae: 552.165568, accuracy: 0.218750, mean_q: -74.701047, mean_eps: 0.100000\n",
      " 20716/50000: episode: 4030, duration: 0.018s, episode steps:   4, steps per second: 224, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 19800.871826, mae: 567.337311, accuracy: 0.242188, mean_q: -45.342865, mean_eps: 0.100000\n",
      " 20719/50000: episode: 4031, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 15518.503255, mae: 556.969076, accuracy: 0.302083, mean_q: -6.129107, mean_eps: 0.100000\n",
      " 20722/50000: episode: 4032, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 14414.009766, mae: 561.280924, accuracy: 0.291667, mean_q: -58.842135, mean_eps: 0.100000\n",
      " 20725/50000: episode: 4033, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 21786.373047, mae: 570.305196, accuracy: 0.177083, mean_q: -68.914061, mean_eps: 0.100000\n",
      " 20728/50000: episode: 4034, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 15068.084635, mae: 546.473531, accuracy: 0.270833, mean_q: -55.218545, mean_eps: 0.100000\n",
      " 20731/50000: episode: 4035, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 20428.552734, mae: 564.105184, accuracy: 0.145833, mean_q: -27.850448, mean_eps: 0.100000\n",
      " 20734/50000: episode: 4036, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 19126.809896, mae: 573.863180, accuracy: 0.197917, mean_q: -53.453893, mean_eps: 0.100000\n",
      " 20737/50000: episode: 4037, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.667 [0.000, 3.000],  loss: 16838.463867, mae: 564.610128, accuracy: 0.291667, mean_q: -37.131274, mean_eps: 0.100000\n",
      " 20740/50000: episode: 4038, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 18871.242188, mae: 566.637410, accuracy: 0.229167, mean_q: -71.856946, mean_eps: 0.100000\n",
      " 20744/50000: episode: 4039, duration: 0.015s, episode steps:   4, steps per second: 263, episode reward: -1238.000, mean reward: -309.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 15830.462158, mae: 559.403793, accuracy: 0.273438, mean_q: -30.777991, mean_eps: 0.100000\n",
      " 20747/50000: episode: 4040, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 22031.541341, mae: 585.690104, accuracy: 0.302083, mean_q: -53.837588, mean_eps: 0.100000\n",
      " 20750/50000: episode: 4041, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 18931.260742, mae: 563.190450, accuracy: 0.229167, mean_q: -69.548258, mean_eps: 0.100000\n",
      " 20753/50000: episode: 4042, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 19071.787109, mae: 560.315592, accuracy: 0.208333, mean_q: -53.141944, mean_eps: 0.100000\n",
      " 20757/50000: episode: 4043, duration: 0.016s, episode steps:   4, steps per second: 249, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 17236.552246, mae: 560.120804, accuracy: 0.203125, mean_q: -20.464998, mean_eps: 0.100000\n",
      " 20760/50000: episode: 4044, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 18580.864583, mae: 576.670308, accuracy: 0.229167, mean_q: -24.958052, mean_eps: 0.100000\n",
      " 20763/50000: episode: 4045, duration: 0.014s, episode steps:   3, steps per second: 210, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 16481.341146, mae: 576.899455, accuracy: 0.135417, mean_q: -52.702869, mean_eps: 0.100000\n",
      " 20766/50000: episode: 4046, duration: 0.015s, episode steps:   3, steps per second: 205, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 19435.935547, mae: 556.611287, accuracy: 0.281250, mean_q: -44.251394, mean_eps: 0.100000\n",
      " 20769/50000: episode: 4047, duration: 0.013s, episode steps:   3, steps per second: 230, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 19894.883464, mae: 538.094828, accuracy: 0.302083, mean_q: -58.068985, mean_eps: 0.100000\n",
      " 20772/50000: episode: 4048, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 21379.520182, mae: 549.397685, accuracy: 0.343750, mean_q: -61.070415, mean_eps: 0.100000\n",
      " 20775/50000: episode: 4049, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 19455.735026, mae: 567.532532, accuracy: 0.229167, mean_q: -32.136226, mean_eps: 0.100000\n",
      " 20779/50000: episode: 4050, duration: 0.016s, episode steps:   4, steps per second: 253, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 18241.247559, mae: 556.072647, accuracy: 0.242188, mean_q: -57.208538, mean_eps: 0.100000\n",
      " 20782/50000: episode: 4051, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 18413.839844, mae: 569.909383, accuracy: 0.166667, mean_q: -53.409167, mean_eps: 0.100000\n",
      " 20785/50000: episode: 4052, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 18789.535156, mae: 552.829712, accuracy: 0.177083, mean_q: -49.424624, mean_eps: 0.100000\n",
      " 20789/50000: episode: 4053, duration: 0.016s, episode steps:   4, steps per second: 257, episode reward: -1223.000, mean reward: -305.750 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 20500.404785, mae: 552.674057, accuracy: 0.218750, mean_q: -86.176436, mean_eps: 0.100000\n",
      " 20793/50000: episode: 4054, duration: 0.015s, episode steps:   4, steps per second: 259, episode reward: -1223.000, mean reward: -305.750 [-999.000, -58.000], mean action: 2.000 [0.000, 3.000],  loss: 17541.696777, mae: 561.099579, accuracy: 0.234375, mean_q: -35.482733, mean_eps: 0.100000\n",
      " 20796/50000: episode: 4055, duration: 0.012s, episode steps:   3, steps per second: 254, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 12661.147135, mae: 535.116984, accuracy: 0.218750, mean_q: -30.286496, mean_eps: 0.100000\n",
      " 20799/50000: episode: 4056, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 21416.042969, mae: 557.809387, accuracy: 0.187500, mean_q: -49.567175, mean_eps: 0.100000\n",
      " 20803/50000: episode: 4057, duration: 0.016s, episode steps:   4, steps per second: 257, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 18545.083984, mae: 549.748688, accuracy: 0.179688, mean_q: -59.293636, mean_eps: 0.100000\n",
      " 20806/50000: episode: 4058, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 18155.920573, mae: 567.161967, accuracy: 0.208333, mean_q: -48.784889, mean_eps: 0.100000\n",
      " 20809/50000: episode: 4059, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 16793.619466, mae: 563.505595, accuracy: 0.291667, mean_q: -41.683286, mean_eps: 0.100000\n",
      " 20812/50000: episode: 4060, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 17313.257812, mae: 557.505697, accuracy: 0.187500, mean_q: -19.508656, mean_eps: 0.100000\n",
      " 20815/50000: episode: 4061, duration: 0.013s, episode steps:   3, steps per second: 230, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 19940.282552, mae: 554.941101, accuracy: 0.229167, mean_q: -31.475411, mean_eps: 0.100000\n",
      " 20819/50000: episode: 4062, duration: 0.020s, episode steps:   4, steps per second: 196, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 19063.942871, mae: 560.824905, accuracy: 0.289062, mean_q: -59.575930, mean_eps: 0.100000\n",
      " 20822/50000: episode: 4063, duration: 0.016s, episode steps:   3, steps per second: 189, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 15676.180339, mae: 557.731201, accuracy: 0.291667, mean_q: -27.315595, mean_eps: 0.100000\n",
      " 20826/50000: episode: 4064, duration: 0.017s, episode steps:   4, steps per second: 241, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 19027.799805, mae: 550.607788, accuracy: 0.250000, mean_q: -32.115214, mean_eps: 0.100000\n",
      " 20829/50000: episode: 4065, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 20429.711589, mae: 570.692383, accuracy: 0.302083, mean_q: -60.306687, mean_eps: 0.100000\n",
      " 20832/50000: episode: 4066, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 15823.240560, mae: 554.700195, accuracy: 0.229167, mean_q: -63.234584, mean_eps: 0.100000\n",
      " 20835/50000: episode: 4067, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 16092.343099, mae: 571.026062, accuracy: 0.250000, mean_q: -40.797164, mean_eps: 0.100000\n",
      " 20839/50000: episode: 4068, duration: 0.015s, episode steps:   4, steps per second: 264, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 17602.897949, mae: 562.634628, accuracy: 0.242188, mean_q: -57.608125, mean_eps: 0.100000\n",
      " 20842/50000: episode: 4069, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 17967.116862, mae: 566.209676, accuracy: 0.312500, mean_q: -66.061080, mean_eps: 0.100000\n",
      " 20846/50000: episode: 4070, duration: 0.016s, episode steps:   4, steps per second: 246, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 18398.879883, mae: 554.092499, accuracy: 0.304688, mean_q: -50.517054, mean_eps: 0.100000\n",
      " 20849/50000: episode: 4071, duration: 0.014s, episode steps:   3, steps per second: 213, episode reward: -224.000, mean reward: -74.667 [-116.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 16359.934245, mae: 543.511088, accuracy: 0.260417, mean_q: -60.442245, mean_eps: 0.100000\n",
      " 20852/50000: episode: 4072, duration: 0.013s, episode steps:   3, steps per second: 223, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 16910.845378, mae: 548.634277, accuracy: 0.302083, mean_q: -81.503535, mean_eps: 0.100000\n",
      " 20855/50000: episode: 4073, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 17085.188802, mae: 568.557699, accuracy: 0.270833, mean_q: -35.134088, mean_eps: 0.100000\n",
      " 20858/50000: episode: 4074, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 22979.592448, mae: 558.651225, accuracy: 0.312500, mean_q: -53.122612, mean_eps: 0.100000\n",
      " 20861/50000: episode: 4075, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 20061.571615, mae: 560.956645, accuracy: 0.250000, mean_q: -61.826894, mean_eps: 0.100000\n",
      " 20864/50000: episode: 4076, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 17060.800456, mae: 570.129069, accuracy: 0.218750, mean_q: -17.764590, mean_eps: 0.100000\n",
      " 20867/50000: episode: 4077, duration: 0.014s, episode steps:   3, steps per second: 210, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 19398.650391, mae: 554.414347, accuracy: 0.145833, mean_q: -29.129824, mean_eps: 0.100000\n",
      " 20870/50000: episode: 4078, duration: 0.015s, episode steps:   3, steps per second: 203, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 16918.949219, mae: 549.323873, accuracy: 0.270833, mean_q: -50.037704, mean_eps: 0.100000\n",
      " 20874/50000: episode: 4079, duration: 0.018s, episode steps:   4, steps per second: 221, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 17721.960205, mae: 543.901657, accuracy: 0.304688, mean_q: -34.461512, mean_eps: 0.100000\n",
      " 20877/50000: episode: 4080, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 15871.290365, mae: 527.971029, accuracy: 0.229167, mean_q: -76.402281, mean_eps: 0.100000\n",
      " 20880/50000: episode: 4081, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 19970.389974, mae: 539.832174, accuracy: 0.166667, mean_q: -38.220729, mean_eps: 0.100000\n",
      " 20883/50000: episode: 4082, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 18139.488281, mae: 546.902222, accuracy: 0.197917, mean_q: -25.724659, mean_eps: 0.100000\n",
      " 20887/50000: episode: 4083, duration: 0.015s, episode steps:   4, steps per second: 261, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 18003.138184, mae: 560.266464, accuracy: 0.187500, mean_q: -62.775932, mean_eps: 0.100000\n",
      " 20890/50000: episode: 4084, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.667 [0.000, 3.000],  loss: 17747.693034, mae: 554.267761, accuracy: 0.187500, mean_q: -36.749007, mean_eps: 0.100000\n",
      " 20893/50000: episode: 4085, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 15508.700521, mae: 567.444722, accuracy: 0.229167, mean_q: -33.325415, mean_eps: 0.100000\n",
      " 20896/50000: episode: 4086, duration: 0.012s, episode steps:   3, steps per second: 257, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 16123.126953, mae: 548.735718, accuracy: 0.322917, mean_q: -51.425555, mean_eps: 0.100000\n",
      " 20899/50000: episode: 4087, duration: 0.013s, episode steps:   3, steps per second: 227, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 17940.894531, mae: 557.261536, accuracy: 0.239583, mean_q: -93.804848, mean_eps: 0.100000\n",
      " 20902/50000: episode: 4088, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 18105.650391, mae: 554.572184, accuracy: 0.281250, mean_q: -50.600339, mean_eps: 0.100000\n",
      " 20905/50000: episode: 4089, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 19017.117839, mae: 564.858236, accuracy: 0.239583, mean_q: -31.183261, mean_eps: 0.100000\n",
      " 20909/50000: episode: 4090, duration: 0.016s, episode steps:   4, steps per second: 254, episode reward: -1223.000, mean reward: -305.750 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 16743.489746, mae: 556.564087, accuracy: 0.257812, mean_q: -53.577407, mean_eps: 0.100000\n",
      " 20912/50000: episode: 4091, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 15852.202799, mae: 544.554850, accuracy: 0.239583, mean_q: -56.211599, mean_eps: 0.100000\n",
      " 20915/50000: episode: 4092, duration: 0.022s, episode steps:   3, steps per second: 135, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 15631.666667, mae: 550.114136, accuracy: 0.250000, mean_q: -26.461979, mean_eps: 0.100000\n",
      " 20918/50000: episode: 4093, duration: 0.016s, episode steps:   3, steps per second: 183, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 16916.091471, mae: 559.677470, accuracy: 0.291667, mean_q: -54.817139, mean_eps: 0.100000\n",
      " 20921/50000: episode: 4094, duration: 0.014s, episode steps:   3, steps per second: 216, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 16838.168945, mae: 557.223775, accuracy: 0.291667, mean_q: -50.759102, mean_eps: 0.100000\n",
      " 20925/50000: episode: 4095, duration: 0.016s, episode steps:   4, steps per second: 257, episode reward: -1238.000, mean reward: -309.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 18914.519531, mae: 564.707886, accuracy: 0.242188, mean_q: -42.376143, mean_eps: 0.100000\n",
      " 20928/50000: episode: 4096, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 14637.392904, mae: 550.206502, accuracy: 0.343750, mean_q: -46.405154, mean_eps: 0.100000\n",
      " 20931/50000: episode: 4097, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 19670.420573, mae: 531.608663, accuracy: 0.302083, mean_q: -54.632478, mean_eps: 0.100000\n",
      " 20934/50000: episode: 4098, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 15894.919922, mae: 572.624084, accuracy: 0.364583, mean_q: -49.210711, mean_eps: 0.100000\n",
      " 20937/50000: episode: 4099, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 17989.383464, mae: 556.637309, accuracy: 0.354167, mean_q: -62.451772, mean_eps: 0.100000\n",
      " 20940/50000: episode: 4100, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 17840.289714, mae: 570.887573, accuracy: 0.218750, mean_q: -50.718049, mean_eps: 0.100000\n",
      " 20943/50000: episode: 4101, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 19670.711589, mae: 542.111552, accuracy: 0.260417, mean_q: -47.600850, mean_eps: 0.100000\n",
      " 20946/50000: episode: 4102, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 16066.822591, mae: 556.442871, accuracy: 0.281250, mean_q: -48.906169, mean_eps: 0.100000\n",
      " 20949/50000: episode: 4103, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 15329.445964, mae: 549.968201, accuracy: 0.260417, mean_q: -39.768777, mean_eps: 0.100000\n",
      " 20953/50000: episode: 4104, duration: 0.015s, episode steps:   4, steps per second: 263, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 15642.865967, mae: 536.448563, accuracy: 0.367188, mean_q: -30.308457, mean_eps: 0.100000\n",
      " 20957/50000: episode: 4105, duration: 0.016s, episode steps:   4, steps per second: 256, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 18031.520020, mae: 534.496552, accuracy: 0.273438, mean_q: -39.996608, mean_eps: 0.100000\n",
      " 20960/50000: episode: 4106, duration: 0.014s, episode steps:   3, steps per second: 211, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 17103.611654, mae: 543.801900, accuracy: 0.250000, mean_q: 0.886783, mean_eps: 0.100000\n",
      " 20963/50000: episode: 4107, duration: 0.016s, episode steps:   3, steps per second: 190, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 17370.700195, mae: 544.035319, accuracy: 0.250000, mean_q: -53.978280, mean_eps: 0.100000\n",
      " 20966/50000: episode: 4108, duration: 0.020s, episode steps:   3, steps per second: 150, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 20181.175781, mae: 564.741903, accuracy: 0.208333, mean_q: -56.241507, mean_eps: 0.100000\n",
      " 20969/50000: episode: 4109, duration: 0.015s, episode steps:   3, steps per second: 205, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 17236.113607, mae: 575.471334, accuracy: 0.281250, mean_q: -47.057785, mean_eps: 0.100000\n",
      " 20972/50000: episode: 4110, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 19008.794922, mae: 574.632080, accuracy: 0.208333, mean_q: -58.212547, mean_eps: 0.100000\n",
      " 20975/50000: episode: 4111, duration: 0.013s, episode steps:   3, steps per second: 230, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 14308.737956, mae: 544.592051, accuracy: 0.177083, mean_q: -45.186074, mean_eps: 0.100000\n",
      " 20980/50000: episode: 4112, duration: 0.018s, episode steps:   5, steps per second: 274, episode reward: -2222.000, mean reward: -444.400 [-999.000, -58.000], mean action: 1.600 [0.000, 3.000],  loss: 14965.027539, mae: 535.377515, accuracy: 0.368750, mean_q: -46.044220, mean_eps: 0.100000\n",
      " 20983/50000: episode: 4113, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 18453.586914, mae: 558.947998, accuracy: 0.270833, mean_q: -62.414274, mean_eps: 0.100000\n",
      " 20986/50000: episode: 4114, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 18601.745443, mae: 546.776937, accuracy: 0.187500, mean_q: -54.821142, mean_eps: 0.100000\n",
      " 20989/50000: episode: 4115, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 15751.078776, mae: 562.624207, accuracy: 0.229167, mean_q: -14.141564, mean_eps: 0.100000\n",
      " 20992/50000: episode: 4116, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 18032.749674, mae: 557.955973, accuracy: 0.187500, mean_q: -48.494700, mean_eps: 0.100000\n",
      " 20995/50000: episode: 4117, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 19753.563151, mae: 567.609253, accuracy: 0.250000, mean_q: -76.462975, mean_eps: 0.100000\n",
      " 20998/50000: episode: 4118, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 19662.553711, mae: 553.877604, accuracy: 0.250000, mean_q: -78.884697, mean_eps: 0.100000\n",
      " 21001/50000: episode: 4119, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 16764.744141, mae: 539.515503, accuracy: 0.281250, mean_q: -56.139161, mean_eps: 0.100000\n",
      " 21004/50000: episode: 4120, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 19315.497396, mae: 566.205383, accuracy: 0.260417, mean_q: -50.414101, mean_eps: 0.100000\n",
      " 21007/50000: episode: 4121, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 19397.301432, mae: 554.480448, accuracy: 0.166667, mean_q: -50.424730, mean_eps: 0.100000\n",
      " 21011/50000: episode: 4122, duration: 0.017s, episode steps:   4, steps per second: 238, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 17003.087402, mae: 536.034363, accuracy: 0.218750, mean_q: -33.996414, mean_eps: 0.100000\n",
      " 21014/50000: episode: 4123, duration: 0.016s, episode steps:   3, steps per second: 183, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 17738.185872, mae: 538.246053, accuracy: 0.218750, mean_q: -63.687758, mean_eps: 0.100000\n",
      " 21017/50000: episode: 4124, duration: 0.014s, episode steps:   3, steps per second: 215, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 18364.251953, mae: 557.115885, accuracy: 0.281250, mean_q: -95.692736, mean_eps: 0.100000\n",
      " 21020/50000: episode: 4125, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 15412.717122, mae: 553.576050, accuracy: 0.260417, mean_q: -63.130994, mean_eps: 0.100000\n",
      " 21023/50000: episode: 4126, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 19524.820312, mae: 568.727091, accuracy: 0.302083, mean_q: -28.754319, mean_eps: 0.100000\n",
      " 21026/50000: episode: 4127, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 18556.850260, mae: 544.300069, accuracy: 0.218750, mean_q: -61.392492, mean_eps: 0.100000\n",
      " 21029/50000: episode: 4128, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 17429.924479, mae: 555.611410, accuracy: 0.239583, mean_q: -50.719574, mean_eps: 0.100000\n",
      " 21032/50000: episode: 4129, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 14856.965495, mae: 551.797343, accuracy: 0.260417, mean_q: -53.749922, mean_eps: 0.100000\n",
      " 21035/50000: episode: 4130, duration: 0.012s, episode steps:   3, steps per second: 253, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 20784.922526, mae: 543.695455, accuracy: 0.270833, mean_q: -30.959318, mean_eps: 0.100000\n",
      " 21038/50000: episode: 4131, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 18362.531250, mae: 554.647013, accuracy: 0.218750, mean_q: -10.949737, mean_eps: 0.100000\n",
      " 21041/50000: episode: 4132, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 19885.591797, mae: 575.442952, accuracy: 0.270833, mean_q: -36.801388, mean_eps: 0.100000\n",
      " 21044/50000: episode: 4133, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 16468.065430, mae: 562.543274, accuracy: 0.250000, mean_q: -45.791685, mean_eps: 0.100000\n",
      " 21047/50000: episode: 4134, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 14774.079102, mae: 548.349569, accuracy: 0.312500, mean_q: -75.525772, mean_eps: 0.100000\n",
      " 21050/50000: episode: 4135, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 16706.584635, mae: 544.745483, accuracy: 0.197917, mean_q: -61.421658, mean_eps: 0.100000\n",
      " 21053/50000: episode: 4136, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 15098.718750, mae: 582.811361, accuracy: 0.281250, mean_q: -42.811892, mean_eps: 0.100000\n",
      " 21056/50000: episode: 4137, duration: 0.014s, episode steps:   3, steps per second: 218, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 15857.124023, mae: 547.355957, accuracy: 0.197917, mean_q: -41.956061, mean_eps: 0.100000\n",
      " 21059/50000: episode: 4138, duration: 0.015s, episode steps:   3, steps per second: 200, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 15904.570638, mae: 557.055562, accuracy: 0.239583, mean_q: -34.949876, mean_eps: 0.100000\n",
      " 21062/50000: episode: 4139, duration: 0.014s, episode steps:   3, steps per second: 210, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 15588.393229, mae: 550.676249, accuracy: 0.302083, mean_q: -32.953676, mean_eps: 0.100000\n",
      " 21065/50000: episode: 4140, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 18509.421875, mae: 562.388835, accuracy: 0.302083, mean_q: -71.398262, mean_eps: 0.100000\n",
      " 21068/50000: episode: 4141, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 20581.500651, mae: 571.725240, accuracy: 0.343750, mean_q: -73.783000, mean_eps: 0.100000\n",
      " 21071/50000: episode: 4142, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 14546.819661, mae: 571.107829, accuracy: 0.229167, mean_q: -58.984165, mean_eps: 0.100000\n",
      " 21074/50000: episode: 4143, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 15845.339844, mae: 554.560527, accuracy: 0.177083, mean_q: -23.702756, mean_eps: 0.100000\n",
      " 21078/50000: episode: 4144, duration: 0.015s, episode steps:   4, steps per second: 261, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 15433.416748, mae: 570.591827, accuracy: 0.289062, mean_q: -43.925230, mean_eps: 0.100000\n",
      " 21082/50000: episode: 4145, duration: 0.015s, episode steps:   4, steps per second: 269, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 16001.778076, mae: 559.005676, accuracy: 0.281250, mean_q: -45.429279, mean_eps: 0.100000\n",
      " 21085/50000: episode: 4146, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 20035.950195, mae: 556.622396, accuracy: 0.229167, mean_q: -53.733859, mean_eps: 0.100000\n",
      " 21088/50000: episode: 4147, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 16711.225911, mae: 568.036926, accuracy: 0.333333, mean_q: -76.444733, mean_eps: 0.100000\n",
      " 21091/50000: episode: 4148, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 18938.238932, mae: 550.337240, accuracy: 0.239583, mean_q: -47.934232, mean_eps: 0.100000\n",
      " 21094/50000: episode: 4149, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 17743.337891, mae: 553.556559, accuracy: 0.197917, mean_q: -59.608686, mean_eps: 0.100000\n",
      " 21097/50000: episode: 4150, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 19231.076823, mae: 567.594055, accuracy: 0.250000, mean_q: -81.685755, mean_eps: 0.100000\n",
      " 21100/50000: episode: 4151, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 17159.435872, mae: 544.899556, accuracy: 0.208333, mean_q: -53.315961, mean_eps: 0.100000\n",
      " 21103/50000: episode: 4152, duration: 0.013s, episode steps:   3, steps per second: 227, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 18040.542318, mae: 542.698405, accuracy: 0.250000, mean_q: -64.170839, mean_eps: 0.100000\n",
      " 21106/50000: episode: 4153, duration: 0.020s, episode steps:   3, steps per second: 149, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 16738.476562, mae: 550.125488, accuracy: 0.218750, mean_q: -44.758662, mean_eps: 0.100000\n",
      " 21109/50000: episode: 4154, duration: 0.016s, episode steps:   3, steps per second: 190, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 17847.370117, mae: 559.695903, accuracy: 0.260417, mean_q: -46.978091, mean_eps: 0.100000\n",
      " 21112/50000: episode: 4155, duration: 0.013s, episode steps:   3, steps per second: 225, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 17094.346029, mae: 568.561178, accuracy: 0.302083, mean_q: -57.575816, mean_eps: 0.100000\n",
      " 21115/50000: episode: 4156, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 11775.634766, mae: 523.858663, accuracy: 0.312500, mean_q: -44.798411, mean_eps: 0.100000\n",
      " 21118/50000: episode: 4157, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 17951.526693, mae: 559.718384, accuracy: 0.218750, mean_q: -72.941500, mean_eps: 0.100000\n",
      " 21121/50000: episode: 4158, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 18651.662109, mae: 561.228007, accuracy: 0.208333, mean_q: -40.211556, mean_eps: 0.100000\n",
      " 21127/50000: episode: 4159, duration: 0.021s, episode steps:   6, steps per second: 285, episode reward: -3192.000, mean reward: -532.000 [-999.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 17718.117025, mae: 551.980235, accuracy: 0.244792, mean_q: -39.408874, mean_eps: 0.100000\n",
      " 21130/50000: episode: 4160, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 16324.419922, mae: 549.576050, accuracy: 0.281250, mean_q: -52.124254, mean_eps: 0.100000\n",
      " 21133/50000: episode: 4161, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 14598.004883, mae: 568.327759, accuracy: 0.291667, mean_q: -49.390731, mean_eps: 0.100000\n",
      " 21137/50000: episode: 4162, duration: 0.015s, episode steps:   4, steps per second: 264, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 2.250 [1.000, 3.000],  loss: 15913.836426, mae: 563.122498, accuracy: 0.234375, mean_q: -49.865259, mean_eps: 0.100000\n",
      " 21140/50000: episode: 4163, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 18282.703125, mae: 557.090393, accuracy: 0.229167, mean_q: -92.709773, mean_eps: 0.100000\n",
      " 21143/50000: episode: 4164, duration: 0.012s, episode steps:   3, steps per second: 258, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 16300.711263, mae: 547.403280, accuracy: 0.208333, mean_q: -67.021347, mean_eps: 0.100000\n",
      " 21147/50000: episode: 4165, duration: 0.015s, episode steps:   4, steps per second: 265, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 18162.104004, mae: 560.226303, accuracy: 0.289062, mean_q: -52.016476, mean_eps: 0.100000\n",
      " 21150/50000: episode: 4166, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 16629.536458, mae: 551.833232, accuracy: 0.239583, mean_q: -34.760565, mean_eps: 0.100000\n",
      " 21153/50000: episode: 4167, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 16720.994141, mae: 563.639119, accuracy: 0.239583, mean_q: -46.425578, mean_eps: 0.100000\n",
      " 21156/50000: episode: 4168, duration: 0.014s, episode steps:   3, steps per second: 214, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 17837.199544, mae: 566.337992, accuracy: 0.187500, mean_q: -36.288492, mean_eps: 0.100000\n",
      " 21159/50000: episode: 4169, duration: 0.014s, episode steps:   3, steps per second: 221, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 19635.916667, mae: 560.462077, accuracy: 0.135417, mean_q: -53.638863, mean_eps: 0.100000\n",
      " 21162/50000: episode: 4170, duration: 0.014s, episode steps:   3, steps per second: 220, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 13782.208984, mae: 524.762594, accuracy: 0.197917, mean_q: -60.306349, mean_eps: 0.100000\n",
      " 21165/50000: episode: 4171, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 16894.436198, mae: 561.726990, accuracy: 0.197917, mean_q: -30.180150, mean_eps: 0.100000\n",
      " 21168/50000: episode: 4172, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 17507.265625, mae: 567.687887, accuracy: 0.218750, mean_q: -65.996751, mean_eps: 0.100000\n",
      " 21171/50000: episode: 4173, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 18374.948568, mae: 551.093709, accuracy: 0.218750, mean_q: -73.708265, mean_eps: 0.100000\n",
      " 21174/50000: episode: 4174, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 17633.090495, mae: 560.804077, accuracy: 0.229167, mean_q: -47.434807, mean_eps: 0.100000\n",
      " 21177/50000: episode: 4175, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 16702.602865, mae: 581.503296, accuracy: 0.229167, mean_q: -63.300608, mean_eps: 0.100000\n",
      " 21180/50000: episode: 4176, duration: 0.016s, episode steps:   3, steps per second: 193, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 16766.343099, mae: 557.252686, accuracy: 0.281250, mean_q: -70.299222, mean_eps: 0.100000\n",
      " 21183/50000: episode: 4177, duration: 0.016s, episode steps:   3, steps per second: 187, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 14512.306641, mae: 545.608988, accuracy: 0.229167, mean_q: -35.191180, mean_eps: 0.100000\n",
      " 21187/50000: episode: 4178, duration: 0.016s, episode steps:   4, steps per second: 245, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 19470.803223, mae: 557.738693, accuracy: 0.234375, mean_q: -78.137146, mean_eps: 0.100000\n",
      " 21190/50000: episode: 4179, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 16829.491211, mae: 545.412516, accuracy: 0.197917, mean_q: -76.945808, mean_eps: 0.100000\n",
      " 21193/50000: episode: 4180, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 15434.267904, mae: 544.783997, accuracy: 0.229167, mean_q: -38.975733, mean_eps: 0.100000\n",
      " 21196/50000: episode: 4181, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 16058.860026, mae: 556.232992, accuracy: 0.229167, mean_q: -41.219696, mean_eps: 0.100000\n",
      " 21199/50000: episode: 4182, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 16960.027344, mae: 543.984172, accuracy: 0.260417, mean_q: -71.649951, mean_eps: 0.100000\n",
      " 21202/50000: episode: 4183, duration: 0.014s, episode steps:   3, steps per second: 222, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 17279.569987, mae: 543.494263, accuracy: 0.208333, mean_q: -50.787074, mean_eps: 0.100000\n",
      " 21205/50000: episode: 4184, duration: 0.017s, episode steps:   3, steps per second: 176, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 17799.147135, mae: 553.509908, accuracy: 0.218750, mean_q: -49.188488, mean_eps: 0.100000\n",
      " 21208/50000: episode: 4185, duration: 0.015s, episode steps:   3, steps per second: 195, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 16202.081706, mae: 564.756592, accuracy: 0.218750, mean_q: -65.495135, mean_eps: 0.100000\n",
      " 21212/50000: episode: 4186, duration: 0.016s, episode steps:   4, steps per second: 254, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 17637.386963, mae: 548.534149, accuracy: 0.210938, mean_q: -74.034455, mean_eps: 0.100000\n",
      " 21215/50000: episode: 4187, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 16903.682943, mae: 554.480815, accuracy: 0.208333, mean_q: -33.078044, mean_eps: 0.100000\n",
      " 21218/50000: episode: 4188, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 15087.826009, mae: 538.973246, accuracy: 0.281250, mean_q: -35.064988, mean_eps: 0.100000\n",
      " 21222/50000: episode: 4189, duration: 0.015s, episode steps:   4, steps per second: 269, episode reward: -1238.000, mean reward: -309.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 16056.524414, mae: 540.213257, accuracy: 0.343750, mean_q: -55.761934, mean_eps: 0.100000\n",
      " 21225/50000: episode: 4190, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 14726.216146, mae: 535.490214, accuracy: 0.250000, mean_q: -44.333974, mean_eps: 0.100000\n",
      " 21229/50000: episode: 4191, duration: 0.016s, episode steps:   4, steps per second: 256, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 2.000 [0.000, 3.000],  loss: 16040.982666, mae: 553.796738, accuracy: 0.203125, mean_q: -1.624692, mean_eps: 0.100000\n",
      " 21232/50000: episode: 4192, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 15220.569010, mae: 556.109639, accuracy: 0.156250, mean_q: -14.611215, mean_eps: 0.100000\n",
      " 21235/50000: episode: 4193, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 16667.133138, mae: 561.686971, accuracy: 0.302083, mean_q: -42.140540, mean_eps: 0.100000\n",
      " 21238/50000: episode: 4194, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 16910.566081, mae: 577.976054, accuracy: 0.270833, mean_q: -39.857646, mean_eps: 0.100000\n",
      " 21241/50000: episode: 4195, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 18027.916341, mae: 538.463379, accuracy: 0.260417, mean_q: -68.352356, mean_eps: 0.100000\n",
      " 21244/50000: episode: 4196, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 18066.427083, mae: 570.691711, accuracy: 0.260417, mean_q: -64.814443, mean_eps: 0.100000\n",
      " 21247/50000: episode: 4197, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 14900.787760, mae: 547.886963, accuracy: 0.250000, mean_q: -80.376948, mean_eps: 0.100000\n",
      " 21250/50000: episode: 4198, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 16251.169271, mae: 540.979228, accuracy: 0.250000, mean_q: -49.520552, mean_eps: 0.100000\n",
      " 21253/50000: episode: 4199, duration: 0.013s, episode steps:   3, steps per second: 227, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 17839.132487, mae: 559.833313, accuracy: 0.270833, mean_q: -16.826836, mean_eps: 0.100000\n",
      " 21256/50000: episode: 4200, duration: 0.015s, episode steps:   3, steps per second: 201, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 19586.414062, mae: 558.923625, accuracy: 0.270833, mean_q: -47.698199, mean_eps: 0.100000\n",
      " 21259/50000: episode: 4201, duration: 0.015s, episode steps:   3, steps per second: 206, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 15638.222005, mae: 562.085999, accuracy: 0.270833, mean_q: -42.279819, mean_eps: 0.100000\n",
      " 21262/50000: episode: 4202, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 15963.824870, mae: 551.734904, accuracy: 0.312500, mean_q: -50.809166, mean_eps: 0.100000\n",
      " 21265/50000: episode: 4203, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 11948.539388, mae: 553.465251, accuracy: 0.250000, mean_q: -12.270480, mean_eps: 0.100000\n",
      " 21268/50000: episode: 4204, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 1.667 [0.000, 3.000],  loss: 18003.766927, mae: 572.476929, accuracy: 0.302083, mean_q: -96.528908, mean_eps: 0.100000\n",
      " 21272/50000: episode: 4205, duration: 0.015s, episode steps:   4, steps per second: 264, episode reward: -1223.000, mean reward: -305.750 [-999.000, -58.000], mean action: 1.250 [0.000, 3.000],  loss: 15784.489502, mae: 559.104416, accuracy: 0.242188, mean_q: -56.473075, mean_eps: 0.100000\n",
      " 21275/50000: episode: 4206, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 17267.291667, mae: 551.370951, accuracy: 0.218750, mean_q: -60.172588, mean_eps: 0.100000\n",
      " 21278/50000: episode: 4207, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 15193.972331, mae: 547.843119, accuracy: 0.302083, mean_q: -40.917576, mean_eps: 0.100000\n",
      " 21281/50000: episode: 4208, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 16068.291341, mae: 534.163066, accuracy: 0.208333, mean_q: -42.952361, mean_eps: 0.100000\n",
      " 21285/50000: episode: 4209, duration: 0.016s, episode steps:   4, steps per second: 255, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 2.250 [1.000, 3.000],  loss: 16941.566162, mae: 561.624802, accuracy: 0.265625, mean_q: -62.749861, mean_eps: 0.100000\n",
      " 21288/50000: episode: 4210, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 17946.587240, mae: 548.305054, accuracy: 0.229167, mean_q: -57.891441, mean_eps: 0.100000\n",
      " 21291/50000: episode: 4211, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 16581.324870, mae: 565.537923, accuracy: 0.239583, mean_q: -24.085711, mean_eps: 0.100000\n",
      " 21294/50000: episode: 4212, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 14866.467122, mae: 572.164714, accuracy: 0.270833, mean_q: -21.034925, mean_eps: 0.100000\n",
      " 21297/50000: episode: 4213, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 14391.704427, mae: 566.347026, accuracy: 0.427083, mean_q: -28.173910, mean_eps: 0.100000\n",
      " 21300/50000: episode: 4214, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 18785.836589, mae: 557.929036, accuracy: 0.260417, mean_q: -62.683895, mean_eps: 0.100000\n",
      " 21303/50000: episode: 4215, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 17744.681641, mae: 567.449565, accuracy: 0.364583, mean_q: -53.930984, mean_eps: 0.100000\n",
      " 21306/50000: episode: 4216, duration: 0.015s, episode steps:   3, steps per second: 198, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 18763.451497, mae: 558.631694, accuracy: 0.260417, mean_q: -45.803643, mean_eps: 0.100000\n",
      " 21309/50000: episode: 4217, duration: 0.016s, episode steps:   3, steps per second: 186, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 15986.783854, mae: 552.213664, accuracy: 0.302083, mean_q: -33.462906, mean_eps: 0.100000\n",
      " 21312/50000: episode: 4218, duration: 0.015s, episode steps:   3, steps per second: 206, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 15646.293294, mae: 564.253296, accuracy: 0.208333, mean_q: -50.100720, mean_eps: 0.100000\n",
      " 21315/50000: episode: 4219, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 17210.396484, mae: 557.933309, accuracy: 0.250000, mean_q: -79.964048, mean_eps: 0.100000\n",
      " 21318/50000: episode: 4220, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 17365.621745, mae: 543.540446, accuracy: 0.270833, mean_q: -62.126137, mean_eps: 0.100000\n",
      " 21322/50000: episode: 4221, duration: 0.015s, episode steps:   4, steps per second: 260, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 16102.747803, mae: 561.070099, accuracy: 0.226562, mean_q: -56.033169, mean_eps: 0.100000\n",
      " 21326/50000: episode: 4222, duration: 0.016s, episode steps:   4, steps per second: 248, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 15743.896973, mae: 552.748383, accuracy: 0.226562, mean_q: -38.152355, mean_eps: 0.100000\n",
      " 21329/50000: episode: 4223, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 17261.054688, mae: 555.647095, accuracy: 0.270833, mean_q: -53.525323, mean_eps: 0.100000\n",
      " 21332/50000: episode: 4224, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 15939.830078, mae: 557.422892, accuracy: 0.364583, mean_q: -39.853657, mean_eps: 0.100000\n",
      " 21335/50000: episode: 4225, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 16749.455078, mae: 566.350627, accuracy: 0.260417, mean_q: -83.436780, mean_eps: 0.100000\n",
      " 21338/50000: episode: 4226, duration: 0.015s, episode steps:   3, steps per second: 199, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 15981.825521, mae: 552.604167, accuracy: 0.260417, mean_q: -51.631008, mean_eps: 0.100000\n",
      " 21341/50000: episode: 4227, duration: 0.022s, episode steps:   3, steps per second: 136, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 14612.328125, mae: 543.958496, accuracy: 0.250000, mean_q: -61.186019, mean_eps: 0.100000\n",
      " 21344/50000: episode: 4228, duration: 0.017s, episode steps:   3, steps per second: 173, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 18402.147786, mae: 556.742310, accuracy: 0.208333, mean_q: -42.653365, mean_eps: 0.100000\n",
      " 21347/50000: episode: 4229, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 15742.498047, mae: 573.583577, accuracy: 0.208333, mean_q: -49.082795, mean_eps: 0.100000\n",
      " 21350/50000: episode: 4230, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 16832.168620, mae: 571.959249, accuracy: 0.333333, mean_q: -52.854442, mean_eps: 0.100000\n",
      " 21353/50000: episode: 4231, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 15784.931315, mae: 550.534302, accuracy: 0.385417, mean_q: -31.418428, mean_eps: 0.100000\n",
      " 21356/50000: episode: 4232, duration: 0.014s, episode steps:   3, steps per second: 216, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 17304.244792, mae: 557.779419, accuracy: 0.197917, mean_q: -64.361731, mean_eps: 0.100000\n",
      " 21359/50000: episode: 4233, duration: 0.014s, episode steps:   3, steps per second: 217, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 15971.928711, mae: 546.210876, accuracy: 0.250000, mean_q: -65.537261, mean_eps: 0.100000\n",
      " 21362/50000: episode: 4234, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 18002.102865, mae: 577.619242, accuracy: 0.302083, mean_q: -61.970684, mean_eps: 0.100000\n",
      " 21365/50000: episode: 4235, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 15820.313477, mae: 558.410787, accuracy: 0.156250, mean_q: -16.606869, mean_eps: 0.100000\n",
      " 21368/50000: episode: 4236, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 16599.889323, mae: 536.441620, accuracy: 0.218750, mean_q: -30.768164, mean_eps: 0.100000\n",
      " 21371/50000: episode: 4237, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 16042.556641, mae: 540.527629, accuracy: 0.239583, mean_q: -58.899344, mean_eps: 0.100000\n",
      " 21374/50000: episode: 4238, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 15945.930339, mae: 557.048055, accuracy: 0.239583, mean_q: -47.320611, mean_eps: 0.100000\n",
      " 21377/50000: episode: 4239, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 16089.347656, mae: 551.880656, accuracy: 0.229167, mean_q: -56.061072, mean_eps: 0.100000\n",
      " 21380/50000: episode: 4240, duration: 0.012s, episode steps:   3, steps per second: 253, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 17056.526042, mae: 555.598857, accuracy: 0.187500, mean_q: -93.258471, mean_eps: 0.100000\n",
      " 21383/50000: episode: 4241, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 14349.942708, mae: 548.519694, accuracy: 0.250000, mean_q: -60.447044, mean_eps: 0.100000\n",
      " 21387/50000: episode: 4242, duration: 0.015s, episode steps:   4, steps per second: 266, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 15698.462402, mae: 572.245804, accuracy: 0.265625, mean_q: -32.564071, mean_eps: 0.100000\n",
      " 21390/50000: episode: 4243, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 20083.082031, mae: 548.392802, accuracy: 0.291667, mean_q: -67.178852, mean_eps: 0.100000\n",
      " 21393/50000: episode: 4244, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 16917.277995, mae: 549.582031, accuracy: 0.281250, mean_q: -64.461631, mean_eps: 0.100000\n",
      " 21396/50000: episode: 4245, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 15538.664714, mae: 549.687093, accuracy: 0.208333, mean_q: -63.736240, mean_eps: 0.100000\n",
      " 21399/50000: episode: 4246, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 18034.594727, mae: 553.466675, accuracy: 0.270833, mean_q: -32.025049, mean_eps: 0.100000\n",
      " 21402/50000: episode: 4247, duration: 0.013s, episode steps:   3, steps per second: 222, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 16653.386719, mae: 550.556966, accuracy: 0.197917, mean_q: -19.420803, mean_eps: 0.100000\n",
      " 21405/50000: episode: 4248, duration: 0.019s, episode steps:   3, steps per second: 158, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.667 [0.000, 3.000],  loss: 16967.000326, mae: 556.032389, accuracy: 0.177083, mean_q: -52.477510, mean_eps: 0.100000\n",
      " 21410/50000: episode: 4249, duration: 0.021s, episode steps:   5, steps per second: 238, episode reward: -2222.000, mean reward: -444.400 [-999.000, -58.000], mean action: 1.400 [0.000, 3.000],  loss: 16691.786914, mae: 546.217822, accuracy: 0.218750, mean_q: -39.904980, mean_eps: 0.100000\n",
      " 21413/50000: episode: 4250, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.667 [0.000, 3.000],  loss: 13749.312174, mae: 544.623779, accuracy: 0.197917, mean_q: -17.884633, mean_eps: 0.100000\n",
      " 21416/50000: episode: 4251, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 16197.951497, mae: 567.317932, accuracy: 0.260417, mean_q: -44.486242, mean_eps: 0.100000\n",
      " 21419/50000: episode: 4252, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 14315.542643, mae: 542.203349, accuracy: 0.270833, mean_q: -59.542799, mean_eps: 0.100000\n",
      " 21424/50000: episode: 4253, duration: 0.019s, episode steps:   5, steps per second: 270, episode reward: -2193.000, mean reward: -438.600 [-999.000, -32.000], mean action: 1.400 [0.000, 3.000],  loss: 16538.264258, mae: 558.880408, accuracy: 0.218750, mean_q: -66.438554, mean_eps: 0.100000\n",
      " 21427/50000: episode: 4254, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 14711.097656, mae: 550.966288, accuracy: 0.197917, mean_q: -30.446233, mean_eps: 0.100000\n",
      " 21430/50000: episode: 4255, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 17681.597982, mae: 571.728699, accuracy: 0.239583, mean_q: -52.573184, mean_eps: 0.100000\n",
      " 21433/50000: episode: 4256, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 18913.220378, mae: 542.876750, accuracy: 0.218750, mean_q: -45.708963, mean_eps: 0.100000\n",
      " 21436/50000: episode: 4257, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 17279.723958, mae: 563.642883, accuracy: 0.270833, mean_q: -63.837983, mean_eps: 0.100000\n",
      " 21439/50000: episode: 4258, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 14997.093750, mae: 560.132365, accuracy: 0.333333, mean_q: -55.851434, mean_eps: 0.100000\n",
      " 21442/50000: episode: 4259, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 16766.048828, mae: 576.666117, accuracy: 0.354167, mean_q: -68.215368, mean_eps: 0.100000\n",
      " 21446/50000: episode: 4260, duration: 0.016s, episode steps:   4, steps per second: 256, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 17548.066895, mae: 565.903183, accuracy: 0.250000, mean_q: -55.381134, mean_eps: 0.100000\n",
      " 21450/50000: episode: 4261, duration: 0.017s, episode steps:   4, steps per second: 241, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 14675.359375, mae: 557.956390, accuracy: 0.250000, mean_q: -45.675146, mean_eps: 0.100000\n",
      " 21453/50000: episode: 4262, duration: 0.015s, episode steps:   3, steps per second: 205, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 15587.489258, mae: 535.696431, accuracy: 0.302083, mean_q: -51.983261, mean_eps: 0.100000\n",
      " 21456/50000: episode: 4263, duration: 0.015s, episode steps:   3, steps per second: 200, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 18223.193685, mae: 555.017212, accuracy: 0.260417, mean_q: -68.211478, mean_eps: 0.100000\n",
      " 21459/50000: episode: 4264, duration: 0.013s, episode steps:   3, steps per second: 225, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 15162.166341, mae: 578.054647, accuracy: 0.208333, mean_q: -54.364106, mean_eps: 0.100000\n",
      " 21462/50000: episode: 4265, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 15021.743815, mae: 545.678121, accuracy: 0.281250, mean_q: -64.197454, mean_eps: 0.100000\n",
      " 21465/50000: episode: 4266, duration: 0.012s, episode steps:   3, steps per second: 253, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 14626.150065, mae: 561.504089, accuracy: 0.166667, mean_q: -29.058565, mean_eps: 0.100000\n",
      " 21468/50000: episode: 4267, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 15631.131836, mae: 557.072306, accuracy: 0.322917, mean_q: -44.618179, mean_eps: 0.100000\n",
      " 21471/50000: episode: 4268, duration: 0.015s, episode steps:   3, steps per second: 202, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 18456.787109, mae: 561.727091, accuracy: 0.260417, mean_q: -81.290555, mean_eps: 0.100000\n",
      " 21474/50000: episode: 4269, duration: 0.013s, episode steps:   3, steps per second: 224, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 16982.548503, mae: 552.918986, accuracy: 0.312500, mean_q: -59.310450, mean_eps: 0.100000\n",
      " 21477/50000: episode: 4270, duration: 0.014s, episode steps:   3, steps per second: 213, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 16830.021159, mae: 554.366394, accuracy: 0.229167, mean_q: -40.469584, mean_eps: 0.100000\n",
      " 21480/50000: episode: 4271, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 13139.198893, mae: 575.072998, accuracy: 0.208333, mean_q: -27.861012, mean_eps: 0.100000\n",
      " 21483/50000: episode: 4272, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 16090.383464, mae: 550.946025, accuracy: 0.291667, mean_q: -59.710154, mean_eps: 0.100000\n",
      " 21486/50000: episode: 4273, duration: 0.013s, episode steps:   3, steps per second: 226, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 15398.666016, mae: 553.115804, accuracy: 0.250000, mean_q: -72.860196, mean_eps: 0.100000\n",
      " 21489/50000: episode: 4274, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 15123.856771, mae: 560.372538, accuracy: 0.260417, mean_q: -37.086485, mean_eps: 0.100000\n",
      " 21494/50000: episode: 4275, duration: 0.020s, episode steps:   5, steps per second: 245, episode reward: -2193.000, mean reward: -438.600 [-999.000, -32.000], mean action: 1.200 [0.000, 3.000],  loss: 16058.358984, mae: 567.293018, accuracy: 0.243750, mean_q: -48.193067, mean_eps: 0.100000\n",
      " 21497/50000: episode: 4276, duration: 0.014s, episode steps:   3, steps per second: 211, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 14142.317383, mae: 541.641541, accuracy: 0.322917, mean_q: -30.758475, mean_eps: 0.100000\n",
      " 21500/50000: episode: 4277, duration: 0.015s, episode steps:   3, steps per second: 196, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 16140.854818, mae: 576.923971, accuracy: 0.177083, mean_q: -37.574215, mean_eps: 0.100000\n",
      " 21504/50000: episode: 4278, duration: 0.026s, episode steps:   4, steps per second: 157, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 16424.828369, mae: 566.393631, accuracy: 0.218750, mean_q: -67.447659, mean_eps: 0.100000\n",
      " 21507/50000: episode: 4279, duration: 0.015s, episode steps:   3, steps per second: 205, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 16466.583984, mae: 551.286214, accuracy: 0.322917, mean_q: -70.301329, mean_eps: 0.100000\n",
      " 21510/50000: episode: 4280, duration: 0.015s, episode steps:   3, steps per second: 206, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 15537.574219, mae: 547.145833, accuracy: 0.218750, mean_q: -102.261303, mean_eps: 0.100000\n",
      " 21513/50000: episode: 4281, duration: 0.014s, episode steps:   3, steps per second: 217, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 16112.533203, mae: 544.501343, accuracy: 0.281250, mean_q: -42.800219, mean_eps: 0.100000\n",
      " 21517/50000: episode: 4282, duration: 0.017s, episode steps:   4, steps per second: 235, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 15849.528809, mae: 554.313416, accuracy: 0.281250, mean_q: -60.600113, mean_eps: 0.100000\n",
      " 21520/50000: episode: 4283, duration: 0.014s, episode steps:   3, steps per second: 214, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 14596.482096, mae: 555.531738, accuracy: 0.312500, mean_q: -77.508095, mean_eps: 0.100000\n",
      " 21523/50000: episode: 4284, duration: 0.013s, episode steps:   3, steps per second: 227, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 17979.869792, mae: 576.185506, accuracy: 0.218750, mean_q: -80.103354, mean_eps: 0.100000\n",
      " 21526/50000: episode: 4285, duration: 0.014s, episode steps:   3, steps per second: 216, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 12858.030599, mae: 544.475911, accuracy: 0.250000, mean_q: -60.390727, mean_eps: 0.100000\n",
      " 21529/50000: episode: 4286, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 16523.588542, mae: 543.070557, accuracy: 0.239583, mean_q: -59.825498, mean_eps: 0.100000\n",
      " 21532/50000: episode: 4287, duration: 0.014s, episode steps:   3, steps per second: 221, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 16789.247396, mae: 547.044698, accuracy: 0.197917, mean_q: -49.893992, mean_eps: 0.100000\n",
      " 21536/50000: episode: 4288, duration: 0.018s, episode steps:   4, steps per second: 225, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 14837.674805, mae: 539.376770, accuracy: 0.289062, mean_q: -14.930061, mean_eps: 0.100000\n",
      " 21541/50000: episode: 4289, duration: 0.019s, episode steps:   5, steps per second: 262, episode reward: -2193.000, mean reward: -438.600 [-999.000, -32.000], mean action: 1.200 [0.000, 3.000],  loss: 16384.857422, mae: 556.095239, accuracy: 0.193750, mean_q: -72.064960, mean_eps: 0.100000\n",
      " 21544/50000: episode: 4290, duration: 0.015s, episode steps:   3, steps per second: 195, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 17767.176107, mae: 557.652364, accuracy: 0.281250, mean_q: -63.813962, mean_eps: 0.100000\n",
      " 21547/50000: episode: 4291, duration: 0.019s, episode steps:   3, steps per second: 155, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 14061.463216, mae: 567.292786, accuracy: 0.197917, mean_q: -40.013693, mean_eps: 0.100000\n",
      " 21550/50000: episode: 4292, duration: 0.014s, episode steps:   3, steps per second: 219, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 17639.024740, mae: 576.025024, accuracy: 0.291667, mean_q: -90.880152, mean_eps: 0.100000\n",
      " 21553/50000: episode: 4293, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 12435.958333, mae: 534.436544, accuracy: 0.302083, mean_q: -48.247017, mean_eps: 0.100000\n",
      " 21557/50000: episode: 4294, duration: 0.015s, episode steps:   4, steps per second: 270, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 14362.353760, mae: 551.743500, accuracy: 0.351562, mean_q: -73.330926, mean_eps: 0.100000\n",
      " 21562/50000: episode: 4295, duration: 0.019s, episode steps:   5, steps per second: 268, episode reward: -2193.000, mean reward: -438.600 [-999.000, -32.000], mean action: 1.200 [0.000, 3.000],  loss: 17046.114063, mae: 572.687646, accuracy: 0.212500, mean_q: -44.168611, mean_eps: 0.100000\n",
      " 21565/50000: episode: 4296, duration: 0.013s, episode steps:   3, steps per second: 226, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 17200.210938, mae: 554.375183, accuracy: 0.197917, mean_q: -47.549648, mean_eps: 0.100000\n",
      " 21568/50000: episode: 4297, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 16247.465820, mae: 555.718831, accuracy: 0.187500, mean_q: -66.981360, mean_eps: 0.100000\n",
      " 21572/50000: episode: 4298, duration: 0.015s, episode steps:   4, steps per second: 260, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 14274.129883, mae: 563.081177, accuracy: 0.289062, mean_q: -41.168826, mean_eps: 0.100000\n",
      " 21575/50000: episode: 4299, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 15607.100260, mae: 557.402100, accuracy: 0.270833, mean_q: -50.178047, mean_eps: 0.100000\n",
      " 21579/50000: episode: 4300, duration: 0.015s, episode steps:   4, steps per second: 265, episode reward: -1223.000, mean reward: -305.750 [-999.000, -58.000], mean action: 0.750 [0.000, 2.000],  loss: 16757.637207, mae: 551.696869, accuracy: 0.203125, mean_q: -33.523922, mean_eps: 0.100000\n",
      " 21582/50000: episode: 4301, duration: 0.012s, episode steps:   3, steps per second: 255, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 15408.984375, mae: 572.675822, accuracy: 0.197917, mean_q: -34.327418, mean_eps: 0.100000\n",
      " 21585/50000: episode: 4302, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 13401.051758, mae: 559.070658, accuracy: 0.302083, mean_q: -48.266620, mean_eps: 0.100000\n",
      " 21588/50000: episode: 4303, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 16198.547201, mae: 586.374451, accuracy: 0.208333, mean_q: -49.270737, mean_eps: 0.100000\n",
      " 21591/50000: episode: 4304, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 17356.636719, mae: 553.356689, accuracy: 0.291667, mean_q: -53.057205, mean_eps: 0.100000\n",
      " 21594/50000: episode: 4305, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 15985.882812, mae: 557.967326, accuracy: 0.260417, mean_q: -60.213785, mean_eps: 0.100000\n",
      " 21597/50000: episode: 4306, duration: 0.021s, episode steps:   3, steps per second: 143, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 16315.136068, mae: 557.672424, accuracy: 0.197917, mean_q: -71.529757, mean_eps: 0.100000\n",
      " 21601/50000: episode: 4307, duration: 0.022s, episode steps:   4, steps per second: 185, episode reward: -1223.000, mean reward: -305.750 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 16409.986816, mae: 549.048996, accuracy: 0.320312, mean_q: -38.595338, mean_eps: 0.100000\n",
      " 21604/50000: episode: 4308, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 18250.165690, mae: 554.638611, accuracy: 0.208333, mean_q: -61.332384, mean_eps: 0.100000\n",
      " 21607/50000: episode: 4309, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 16642.222982, mae: 557.073385, accuracy: 0.333333, mean_q: -38.214685, mean_eps: 0.100000\n",
      " 21611/50000: episode: 4310, duration: 0.015s, episode steps:   4, steps per second: 263, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 14436.897217, mae: 542.400894, accuracy: 0.281250, mean_q: -50.870617, mean_eps: 0.100000\n",
      " 21615/50000: episode: 4311, duration: 0.016s, episode steps:   4, steps per second: 250, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 17195.980225, mae: 550.832565, accuracy: 0.257812, mean_q: -49.462942, mean_eps: 0.100000\n",
      " 21618/50000: episode: 4312, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 18053.870443, mae: 546.122457, accuracy: 0.270833, mean_q: -35.710894, mean_eps: 0.100000\n",
      " 21621/50000: episode: 4313, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 15867.632161, mae: 568.144552, accuracy: 0.291667, mean_q: -51.126431, mean_eps: 0.100000\n",
      " 21624/50000: episode: 4314, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 15107.378255, mae: 553.159424, accuracy: 0.156250, mean_q: -44.801475, mean_eps: 0.100000\n",
      " 21627/50000: episode: 4315, duration: 0.012s, episode steps:   3, steps per second: 255, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 14285.930664, mae: 553.075033, accuracy: 0.260417, mean_q: -25.550699, mean_eps: 0.100000\n",
      " 21630/50000: episode: 4316, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 16262.291667, mae: 565.439799, accuracy: 0.218750, mean_q: -55.596647, mean_eps: 0.100000\n",
      " 21633/50000: episode: 4317, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 17858.267578, mae: 564.811218, accuracy: 0.239583, mean_q: -50.933144, mean_eps: 0.100000\n",
      " 21636/50000: episode: 4318, duration: 0.013s, episode steps:   3, steps per second: 230, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 17494.639323, mae: 559.293091, accuracy: 0.218750, mean_q: -52.100034, mean_eps: 0.100000\n",
      " 21639/50000: episode: 4319, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 15911.104818, mae: 552.691589, accuracy: 0.229167, mean_q: -46.491155, mean_eps: 0.100000\n",
      " 21642/50000: episode: 4320, duration: 0.017s, episode steps:   3, steps per second: 173, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 15455.652669, mae: 561.207418, accuracy: 0.302083, mean_q: -69.687979, mean_eps: 0.100000\n",
      " 21645/50000: episode: 4321, duration: 0.014s, episode steps:   3, steps per second: 211, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 15042.192708, mae: 538.169342, accuracy: 0.270833, mean_q: -48.358780, mean_eps: 0.100000\n",
      " 21648/50000: episode: 4322, duration: 0.014s, episode steps:   3, steps per second: 207, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 17220.603190, mae: 564.081889, accuracy: 0.229167, mean_q: -70.420924, mean_eps: 0.100000\n",
      " 21651/50000: episode: 4323, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 15919.624349, mae: 559.240153, accuracy: 0.218750, mean_q: -30.993505, mean_eps: 0.100000\n",
      " 21654/50000: episode: 4324, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 16370.212240, mae: 559.767904, accuracy: 0.260417, mean_q: -55.344859, mean_eps: 0.100000\n",
      " 21658/50000: episode: 4325, duration: 0.015s, episode steps:   4, steps per second: 258, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 2.250 [1.000, 3.000],  loss: 14553.544678, mae: 543.919357, accuracy: 0.242188, mean_q: -53.103831, mean_eps: 0.100000\n",
      " 21661/50000: episode: 4326, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 15454.065430, mae: 557.510396, accuracy: 0.281250, mean_q: -9.199985, mean_eps: 0.100000\n",
      " 21664/50000: episode: 4327, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 14055.688477, mae: 540.586304, accuracy: 0.291667, mean_q: -51.930302, mean_eps: 0.100000\n",
      " 21667/50000: episode: 4328, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 14764.199544, mae: 557.081685, accuracy: 0.281250, mean_q: -11.316033, mean_eps: 0.100000\n",
      " 21670/50000: episode: 4329, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 15994.897135, mae: 561.953288, accuracy: 0.270833, mean_q: -55.136922, mean_eps: 0.100000\n",
      " 21673/50000: episode: 4330, duration: 0.012s, episode steps:   3, steps per second: 257, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 18686.310547, mae: 567.125448, accuracy: 0.208333, mean_q: -70.572205, mean_eps: 0.100000\n",
      " 21677/50000: episode: 4331, duration: 0.015s, episode steps:   4, steps per second: 266, episode reward: -1238.000, mean reward: -309.500 [-999.000, -58.000], mean action: 1.000 [0.000, 3.000],  loss: 15248.753906, mae: 546.445007, accuracy: 0.234375, mean_q: -55.894273, mean_eps: 0.100000\n",
      " 21681/50000: episode: 4332, duration: 0.015s, episode steps:   4, steps per second: 273, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 15719.735840, mae: 566.879700, accuracy: 0.289062, mean_q: -34.572052, mean_eps: 0.100000\n",
      " 21684/50000: episode: 4333, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 14823.484049, mae: 561.421977, accuracy: 0.322917, mean_q: -54.244998, mean_eps: 0.100000\n",
      " 21687/50000: episode: 4334, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 17134.633464, mae: 552.959900, accuracy: 0.281250, mean_q: -59.936356, mean_eps: 0.100000\n",
      " 21690/50000: episode: 4335, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 16517.373372, mae: 541.061259, accuracy: 0.250000, mean_q: -68.644507, mean_eps: 0.100000\n",
      " 21693/50000: episode: 4336, duration: 0.014s, episode steps:   3, steps per second: 217, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 14835.847656, mae: 560.271322, accuracy: 0.312500, mean_q: -67.834961, mean_eps: 0.100000\n",
      " 21696/50000: episode: 4337, duration: 0.014s, episode steps:   3, steps per second: 208, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 16926.003255, mae: 546.473470, accuracy: 0.333333, mean_q: -60.267011, mean_eps: 0.100000\n",
      " 21699/50000: episode: 4338, duration: 0.015s, episode steps:   3, steps per second: 206, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 15020.738607, mae: 553.985535, accuracy: 0.354167, mean_q: -51.316940, mean_eps: 0.100000\n",
      " 21702/50000: episode: 4339, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 18760.824219, mae: 572.626221, accuracy: 0.354167, mean_q: -51.486729, mean_eps: 0.100000\n",
      " 21705/50000: episode: 4340, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 16481.163411, mae: 548.928324, accuracy: 0.333333, mean_q: -66.058655, mean_eps: 0.100000\n",
      " 21708/50000: episode: 4341, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 16686.479818, mae: 546.347880, accuracy: 0.260417, mean_q: -19.567027, mean_eps: 0.100000\n",
      " 21711/50000: episode: 4342, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 16238.135417, mae: 559.272786, accuracy: 0.197917, mean_q: -29.707968, mean_eps: 0.100000\n",
      " 21714/50000: episode: 4343, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 17750.074219, mae: 548.585266, accuracy: 0.322917, mean_q: -41.604763, mean_eps: 0.100000\n",
      " 21718/50000: episode: 4344, duration: 0.015s, episode steps:   4, steps per second: 263, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 13933.682129, mae: 547.831894, accuracy: 0.257812, mean_q: -47.775200, mean_eps: 0.100000\n",
      " 21721/50000: episode: 4345, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 17185.811849, mae: 558.102376, accuracy: 0.187500, mean_q: -42.057037, mean_eps: 0.100000\n",
      " 21724/50000: episode: 4346, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 14998.186849, mae: 558.668335, accuracy: 0.250000, mean_q: -39.962352, mean_eps: 0.100000\n",
      " 21727/50000: episode: 4347, duration: 0.012s, episode steps:   3, steps per second: 254, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 16532.759766, mae: 544.594381, accuracy: 0.218750, mean_q: -93.681366, mean_eps: 0.100000\n",
      " 21730/50000: episode: 4348, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 16267.850260, mae: 558.394104, accuracy: 0.197917, mean_q: -55.508921, mean_eps: 0.100000\n",
      " 21733/50000: episode: 4349, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 14169.770508, mae: 566.578593, accuracy: 0.281250, mean_q: -33.735137, mean_eps: 0.100000\n",
      " 21736/50000: episode: 4350, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 15830.576823, mae: 573.751567, accuracy: 0.250000, mean_q: -50.954769, mean_eps: 0.100000\n",
      " 21739/50000: episode: 4351, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 14689.108724, mae: 560.016947, accuracy: 0.177083, mean_q: -34.670352, mean_eps: 0.100000\n",
      " 21742/50000: episode: 4352, duration: 0.014s, episode steps:   3, steps per second: 214, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 13909.040365, mae: 546.129435, accuracy: 0.218750, mean_q: -30.244068, mean_eps: 0.100000\n",
      " 21745/50000: episode: 4353, duration: 0.019s, episode steps:   3, steps per second: 159, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 17821.917318, mae: 550.332804, accuracy: 0.322917, mean_q: -46.439362, mean_eps: 0.100000\n",
      " 21748/50000: episode: 4354, duration: 0.014s, episode steps:   3, steps per second: 215, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 15966.923177, mae: 554.823629, accuracy: 0.333333, mean_q: -65.586491, mean_eps: 0.100000\n",
      " 21752/50000: episode: 4355, duration: 0.016s, episode steps:   4, steps per second: 253, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 14141.343262, mae: 538.563736, accuracy: 0.296875, mean_q: -61.966190, mean_eps: 0.100000\n",
      " 21755/50000: episode: 4356, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 17550.844727, mae: 558.569397, accuracy: 0.322917, mean_q: -43.763614, mean_eps: 0.100000\n",
      " 21758/50000: episode: 4357, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 15605.760091, mae: 557.410807, accuracy: 0.302083, mean_q: -47.893506, mean_eps: 0.100000\n",
      " 21761/50000: episode: 4358, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 15332.983724, mae: 572.739889, accuracy: 0.291667, mean_q: -54.010357, mean_eps: 0.100000\n",
      " 21764/50000: episode: 4359, duration: 0.013s, episode steps:   3, steps per second: 230, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 18592.657878, mae: 549.734436, accuracy: 0.260417, mean_q: -82.101931, mean_eps: 0.100000\n",
      " 21767/50000: episode: 4360, duration: 0.015s, episode steps:   3, steps per second: 196, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 15868.267578, mae: 546.158040, accuracy: 0.302083, mean_q: -50.715591, mean_eps: 0.100000\n",
      " 21770/50000: episode: 4361, duration: 0.015s, episode steps:   3, steps per second: 206, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 13808.162760, mae: 561.318095, accuracy: 0.218750, mean_q: -30.479185, mean_eps: 0.100000\n",
      " 21774/50000: episode: 4362, duration: 0.019s, episode steps:   4, steps per second: 206, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 15209.260742, mae: 551.261078, accuracy: 0.234375, mean_q: -46.631655, mean_eps: 0.100000\n",
      " 21777/50000: episode: 4363, duration: 0.015s, episode steps:   3, steps per second: 205, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 13931.075195, mae: 537.902608, accuracy: 0.218750, mean_q: -34.380296, mean_eps: 0.100000\n",
      " 21780/50000: episode: 4364, duration: 0.015s, episode steps:   3, steps per second: 203, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 14282.428060, mae: 550.112101, accuracy: 0.239583, mean_q: -22.772455, mean_eps: 0.100000\n",
      " 21784/50000: episode: 4365, duration: 0.018s, episode steps:   4, steps per second: 218, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 16489.971680, mae: 546.811447, accuracy: 0.265625, mean_q: -51.556367, mean_eps: 0.100000\n",
      " 21787/50000: episode: 4366, duration: 0.014s, episode steps:   3, steps per second: 210, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 14972.468750, mae: 563.780965, accuracy: 0.218750, mean_q: -62.091897, mean_eps: 0.100000\n",
      " 21790/50000: episode: 4367, duration: 0.017s, episode steps:   3, steps per second: 178, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 13580.895182, mae: 538.121501, accuracy: 0.302083, mean_q: -47.096606, mean_eps: 0.100000\n",
      " 21794/50000: episode: 4368, duration: 0.022s, episode steps:   4, steps per second: 186, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 15313.388428, mae: 558.057236, accuracy: 0.218750, mean_q: -45.554790, mean_eps: 0.100000\n",
      " 21797/50000: episode: 4369, duration: 0.017s, episode steps:   3, steps per second: 181, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 17379.146484, mae: 565.889303, accuracy: 0.239583, mean_q: -45.845552, mean_eps: 0.100000\n",
      " 21800/50000: episode: 4370, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 13756.326823, mae: 563.081746, accuracy: 0.302083, mean_q: -45.301035, mean_eps: 0.100000\n",
      " 21803/50000: episode: 4371, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 11714.808268, mae: 530.200063, accuracy: 0.260417, mean_q: -15.392924, mean_eps: 0.100000\n",
      " 21806/50000: episode: 4372, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 15784.467122, mae: 552.553813, accuracy: 0.197917, mean_q: -58.131496, mean_eps: 0.100000\n",
      " 21810/50000: episode: 4373, duration: 0.015s, episode steps:   4, steps per second: 266, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 16085.616211, mae: 553.861053, accuracy: 0.289062, mean_q: -34.329946, mean_eps: 0.100000\n",
      " 21813/50000: episode: 4374, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 17338.970052, mae: 560.843526, accuracy: 0.239583, mean_q: -67.293749, mean_eps: 0.100000\n",
      " 21816/50000: episode: 4375, duration: 0.012s, episode steps:   3, steps per second: 255, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 16803.098307, mae: 546.355794, accuracy: 0.281250, mean_q: -58.178780, mean_eps: 0.100000\n",
      " 21819/50000: episode: 4376, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 15881.767253, mae: 561.600260, accuracy: 0.291667, mean_q: -28.088134, mean_eps: 0.100000\n",
      " 21822/50000: episode: 4377, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 16905.185547, mae: 554.917094, accuracy: 0.302083, mean_q: -58.860489, mean_eps: 0.100000\n",
      " 21825/50000: episode: 4378, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 15057.171875, mae: 548.420085, accuracy: 0.260417, mean_q: -45.458347, mean_eps: 0.100000\n",
      " 21828/50000: episode: 4379, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 15902.891927, mae: 583.171753, accuracy: 0.229167, mean_q: -25.800017, mean_eps: 0.100000\n",
      " 21831/50000: episode: 4380, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 16024.461589, mae: 564.332987, accuracy: 0.229167, mean_q: -43.625677, mean_eps: 0.100000\n",
      " 21834/50000: episode: 4381, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 16928.672852, mae: 553.989095, accuracy: 0.229167, mean_q: -48.704475, mean_eps: 0.100000\n",
      " 21837/50000: episode: 4382, duration: 0.014s, episode steps:   3, steps per second: 221, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 13424.764974, mae: 557.944092, accuracy: 0.208333, mean_q: -49.843856, mean_eps: 0.100000\n",
      " 21840/50000: episode: 4383, duration: 0.014s, episode steps:   3, steps per second: 209, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 15629.277344, mae: 555.568990, accuracy: 0.270833, mean_q: -47.857530, mean_eps: 0.100000\n",
      " 21843/50000: episode: 4384, duration: 0.014s, episode steps:   3, steps per second: 209, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 16810.467122, mae: 556.553223, accuracy: 0.229167, mean_q: -46.391146, mean_eps: 0.100000\n",
      " 21846/50000: episode: 4385, duration: 0.015s, episode steps:   3, steps per second: 197, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 16836.511719, mae: 556.332560, accuracy: 0.343750, mean_q: -19.588957, mean_eps: 0.100000\n",
      " 21849/50000: episode: 4386, duration: 0.014s, episode steps:   3, steps per second: 221, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 12800.612630, mae: 533.212341, accuracy: 0.354167, mean_q: -27.261409, mean_eps: 0.100000\n",
      " 21852/50000: episode: 4387, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 17921.649740, mae: 558.168152, accuracy: 0.250000, mean_q: -36.014595, mean_eps: 0.100000\n",
      " 21855/50000: episode: 4388, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 13324.702148, mae: 566.798584, accuracy: 0.343750, mean_q: -37.311801, mean_eps: 0.100000\n",
      " 21858/50000: episode: 4389, duration: 0.013s, episode steps:   3, steps per second: 227, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 15245.455729, mae: 566.390157, accuracy: 0.302083, mean_q: -54.623867, mean_eps: 0.100000\n",
      " 21861/50000: episode: 4390, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 16405.705404, mae: 553.747192, accuracy: 0.229167, mean_q: -66.496203, mean_eps: 0.100000\n",
      " 21864/50000: episode: 4391, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 16045.831706, mae: 554.079305, accuracy: 0.302083, mean_q: -58.128194, mean_eps: 0.100000\n",
      " 21867/50000: episode: 4392, duration: 0.013s, episode steps:   3, steps per second: 225, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 16684.918294, mae: 546.458252, accuracy: 0.218750, mean_q: -51.896497, mean_eps: 0.100000\n",
      " 21870/50000: episode: 4393, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 15374.526693, mae: 533.705526, accuracy: 0.302083, mean_q: -44.863157, mean_eps: 0.100000\n",
      " 21873/50000: episode: 4394, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.667 [0.000, 3.000],  loss: 16643.449870, mae: 528.749064, accuracy: 0.333333, mean_q: -42.337360, mean_eps: 0.100000\n",
      " 21877/50000: episode: 4395, duration: 0.015s, episode steps:   4, steps per second: 260, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 14274.259521, mae: 536.899643, accuracy: 0.234375, mean_q: -39.498084, mean_eps: 0.100000\n",
      " 21880/50000: episode: 4396, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 14624.427409, mae: 529.170227, accuracy: 0.260417, mean_q: -18.146322, mean_eps: 0.100000\n",
      " 21883/50000: episode: 4397, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 16085.325521, mae: 537.120036, accuracy: 0.312500, mean_q: -47.602083, mean_eps: 0.100000\n",
      " 21886/50000: episode: 4398, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 14831.749023, mae: 562.789022, accuracy: 0.333333, mean_q: -30.888503, mean_eps: 0.100000\n",
      " 21889/50000: episode: 4399, duration: 0.018s, episode steps:   3, steps per second: 170, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 15536.679036, mae: 565.789998, accuracy: 0.229167, mean_q: -26.518461, mean_eps: 0.100000\n",
      " 21892/50000: episode: 4400, duration: 0.014s, episode steps:   3, steps per second: 209, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 19485.927083, mae: 567.963603, accuracy: 0.218750, mean_q: -85.077507, mean_eps: 0.100000\n",
      " 21895/50000: episode: 4401, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 14341.692057, mae: 546.395894, accuracy: 0.291667, mean_q: -36.611382, mean_eps: 0.100000\n",
      " 21898/50000: episode: 4402, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 17523.069987, mae: 543.624186, accuracy: 0.302083, mean_q: -47.501786, mean_eps: 0.100000\n",
      " 21901/50000: episode: 4403, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 14841.044922, mae: 552.637655, accuracy: 0.229167, mean_q: -27.866339, mean_eps: 0.100000\n",
      " 21904/50000: episode: 4404, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 13889.670898, mae: 566.400228, accuracy: 0.395833, mean_q: -67.316326, mean_eps: 0.100000\n",
      " 21907/50000: episode: 4405, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 16560.602865, mae: 555.095398, accuracy: 0.250000, mean_q: -15.406364, mean_eps: 0.100000\n",
      " 21910/50000: episode: 4406, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 16813.496745, mae: 560.558329, accuracy: 0.270833, mean_q: -32.415325, mean_eps: 0.100000\n",
      " 21913/50000: episode: 4407, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 13341.777344, mae: 538.985372, accuracy: 0.322917, mean_q: -25.648161, mean_eps: 0.100000\n",
      " 21916/50000: episode: 4408, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 17102.785156, mae: 556.677144, accuracy: 0.302083, mean_q: -28.220996, mean_eps: 0.100000\n",
      " 21919/50000: episode: 4409, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 16413.968750, mae: 557.480652, accuracy: 0.239583, mean_q: -51.079648, mean_eps: 0.100000\n",
      " 21922/50000: episode: 4410, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 13148.768229, mae: 533.289510, accuracy: 0.270833, mean_q: -47.872787, mean_eps: 0.100000\n",
      " 21925/50000: episode: 4411, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 17159.386068, mae: 571.593343, accuracy: 0.416667, mean_q: -77.168551, mean_eps: 0.100000\n",
      " 21928/50000: episode: 4412, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 14941.282878, mae: 550.079386, accuracy: 0.302083, mean_q: -54.519742, mean_eps: 0.100000\n",
      " 21931/50000: episode: 4413, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 15921.674154, mae: 555.594543, accuracy: 0.291667, mean_q: -34.017450, mean_eps: 0.100000\n",
      " 21934/50000: episode: 4414, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 17488.380208, mae: 542.479248, accuracy: 0.270833, mean_q: -35.545593, mean_eps: 0.100000\n",
      " 21938/50000: episode: 4415, duration: 0.019s, episode steps:   4, steps per second: 209, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 17391.632324, mae: 551.155853, accuracy: 0.273438, mean_q: -57.711545, mean_eps: 0.100000\n",
      " 21941/50000: episode: 4416, duration: 0.013s, episode steps:   3, steps per second: 224, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 13547.578451, mae: 546.298055, accuracy: 0.312500, mean_q: -34.162318, mean_eps: 0.100000\n",
      " 21944/50000: episode: 4417, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 14563.787435, mae: 536.730509, accuracy: 0.312500, mean_q: -65.746995, mean_eps: 0.100000\n",
      " 21947/50000: episode: 4418, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 14760.077474, mae: 548.491231, accuracy: 0.354167, mean_q: -55.834213, mean_eps: 0.100000\n",
      " 21950/50000: episode: 4419, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 14546.183594, mae: 541.897237, accuracy: 0.333333, mean_q: -39.885143, mean_eps: 0.100000\n",
      " 21954/50000: episode: 4420, duration: 0.015s, episode steps:   4, steps per second: 260, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 13583.589600, mae: 534.717621, accuracy: 0.304688, mean_q: -50.497962, mean_eps: 0.100000\n",
      " 21957/50000: episode: 4421, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 13198.742839, mae: 537.401042, accuracy: 0.250000, mean_q: -17.181311, mean_eps: 0.100000\n",
      " 21960/50000: episode: 4422, duration: 0.020s, episode steps:   3, steps per second: 149, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 17337.379557, mae: 543.915100, accuracy: 0.322917, mean_q: -49.528775, mean_eps: 0.100000\n",
      " 21963/50000: episode: 4423, duration: 0.015s, episode steps:   3, steps per second: 196, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 17528.624349, mae: 567.216512, accuracy: 0.229167, mean_q: -45.043516, mean_eps: 0.100000\n",
      " 21966/50000: episode: 4424, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 15186.652995, mae: 534.097412, accuracy: 0.322917, mean_q: -51.175934, mean_eps: 0.100000\n",
      " 21969/50000: episode: 4425, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 16046.179362, mae: 546.309143, accuracy: 0.281250, mean_q: -27.658791, mean_eps: 0.100000\n",
      " 21972/50000: episode: 4426, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 17054.772461, mae: 562.360026, accuracy: 0.250000, mean_q: -36.907450, mean_eps: 0.100000\n",
      " 21975/50000: episode: 4427, duration: 0.013s, episode steps:   3, steps per second: 225, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 15492.954753, mae: 536.627543, accuracy: 0.375000, mean_q: -24.363471, mean_eps: 0.100000\n",
      " 21978/50000: episode: 4428, duration: 0.014s, episode steps:   3, steps per second: 209, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 14430.450195, mae: 542.874552, accuracy: 0.385417, mean_q: -24.549791, mean_eps: 0.100000\n",
      " 21981/50000: episode: 4429, duration: 0.017s, episode steps:   3, steps per second: 175, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 14089.094076, mae: 529.109182, accuracy: 0.364583, mean_q: -46.792900, mean_eps: 0.100000\n",
      " 21984/50000: episode: 4430, duration: 0.014s, episode steps:   3, steps per second: 216, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 15858.911133, mae: 549.075623, accuracy: 0.354167, mean_q: -54.662469, mean_eps: 0.100000\n",
      " 21987/50000: episode: 4431, duration: 0.014s, episode steps:   3, steps per second: 216, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 15465.311198, mae: 538.520081, accuracy: 0.177083, mean_q: -48.439353, mean_eps: 0.100000\n",
      " 21990/50000: episode: 4432, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 16993.690755, mae: 572.667887, accuracy: 0.239583, mean_q: -40.018285, mean_eps: 0.100000\n",
      " 21993/50000: episode: 4433, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 15602.447917, mae: 556.691243, accuracy: 0.291667, mean_q: -40.257266, mean_eps: 0.100000\n",
      " 21996/50000: episode: 4434, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 15984.446615, mae: 550.419556, accuracy: 0.239583, mean_q: -30.965379, mean_eps: 0.100000\n",
      " 21999/50000: episode: 4435, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 17904.703776, mae: 568.992554, accuracy: 0.260417, mean_q: -54.300166, mean_eps: 0.100000\n",
      " 22002/50000: episode: 4436, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 16971.669271, mae: 569.521891, accuracy: 0.156250, mean_q: -57.257601, mean_eps: 0.100000\n",
      " 22005/50000: episode: 4437, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 16858.479167, mae: 551.544495, accuracy: 0.260417, mean_q: -68.030571, mean_eps: 0.100000\n",
      " 22008/50000: episode: 4438, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 14380.938802, mae: 547.963623, accuracy: 0.333333, mean_q: -52.694675, mean_eps: 0.100000\n",
      " 22011/50000: episode: 4439, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 14656.059896, mae: 566.476461, accuracy: 0.385417, mean_q: -46.040160, mean_eps: 0.100000\n",
      " 22014/50000: episode: 4440, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 14642.882812, mae: 566.379374, accuracy: 0.343750, mean_q: -68.232853, mean_eps: 0.100000\n",
      " 22017/50000: episode: 4441, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 16477.236328, mae: 562.903056, accuracy: 0.250000, mean_q: -71.171819, mean_eps: 0.100000\n",
      " 22020/50000: episode: 4442, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 16791.481771, mae: 565.914551, accuracy: 0.197917, mean_q: -42.097565, mean_eps: 0.100000\n",
      " 22023/50000: episode: 4443, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 15915.347982, mae: 555.189962, accuracy: 0.302083, mean_q: -26.887279, mean_eps: 0.100000\n",
      " 22026/50000: episode: 4444, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 17516.514648, mae: 559.747375, accuracy: 0.197917, mean_q: -74.370276, mean_eps: 0.100000\n",
      " 22029/50000: episode: 4445, duration: 0.015s, episode steps:   3, steps per second: 202, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 17195.113281, mae: 555.942627, accuracy: 0.208333, mean_q: -70.354368, mean_eps: 0.100000\n",
      " 22032/50000: episode: 4446, duration: 0.015s, episode steps:   3, steps per second: 198, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 16487.151693, mae: 560.497396, accuracy: 0.218750, mean_q: -25.452222, mean_eps: 0.100000\n",
      " 22035/50000: episode: 4447, duration: 0.014s, episode steps:   3, steps per second: 210, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 15520.281576, mae: 528.384908, accuracy: 0.229167, mean_q: -44.249730, mean_eps: 0.100000\n",
      " 22038/50000: episode: 4448, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 15228.057617, mae: 549.496053, accuracy: 0.322917, mean_q: -45.707600, mean_eps: 0.100000\n",
      " 22041/50000: episode: 4449, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 17801.515625, mae: 580.961670, accuracy: 0.229167, mean_q: -61.596021, mean_eps: 0.100000\n",
      " 22044/50000: episode: 4450, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 14387.966146, mae: 538.836873, accuracy: 0.218750, mean_q: -68.342046, mean_eps: 0.100000\n",
      " 22047/50000: episode: 4451, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 17169.585286, mae: 565.745199, accuracy: 0.229167, mean_q: -73.177330, mean_eps: 0.100000\n",
      " 22050/50000: episode: 4452, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 13253.401693, mae: 564.545024, accuracy: 0.270833, mean_q: -28.109950, mean_eps: 0.100000\n",
      " 22053/50000: episode: 4453, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 16264.957031, mae: 568.772929, accuracy: 0.218750, mean_q: -68.903742, mean_eps: 0.100000\n",
      " 22056/50000: episode: 4454, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 13902.304362, mae: 539.972758, accuracy: 0.270833, mean_q: -25.659342, mean_eps: 0.100000\n",
      " 22059/50000: episode: 4455, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 12503.099284, mae: 542.412130, accuracy: 0.375000, mean_q: -40.347293, mean_eps: 0.100000\n",
      " 22063/50000: episode: 4456, duration: 0.015s, episode steps:   4, steps per second: 263, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 14654.586914, mae: 568.252213, accuracy: 0.312500, mean_q: -37.097140, mean_eps: 0.100000\n",
      " 22066/50000: episode: 4457, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 14311.217448, mae: 561.857524, accuracy: 0.270833, mean_q: -61.002186, mean_eps: 0.100000\n",
      " 22069/50000: episode: 4458, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 17487.615885, mae: 567.073425, accuracy: 0.218750, mean_q: -58.214147, mean_eps: 0.100000\n",
      " 22072/50000: episode: 4459, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 14629.663737, mae: 534.757222, accuracy: 0.270833, mean_q: -65.846278, mean_eps: 0.100000\n",
      " 22075/50000: episode: 4460, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 14670.371094, mae: 550.827515, accuracy: 0.208333, mean_q: -46.787226, mean_eps: 0.100000\n",
      " 22079/50000: episode: 4461, duration: 0.016s, episode steps:   4, steps per second: 247, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 2.000 [0.000, 3.000],  loss: 14403.507812, mae: 559.065430, accuracy: 0.210938, mean_q: -20.030859, mean_eps: 0.100000\n",
      " 22082/50000: episode: 4462, duration: 0.019s, episode steps:   3, steps per second: 158, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 15502.354492, mae: 551.807882, accuracy: 0.208333, mean_q: -69.234791, mean_eps: 0.100000\n",
      " 22085/50000: episode: 4463, duration: 0.014s, episode steps:   3, steps per second: 211, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 16018.179688, mae: 547.894633, accuracy: 0.239583, mean_q: -28.798470, mean_eps: 0.100000\n",
      " 22088/50000: episode: 4464, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 17068.225911, mae: 565.313375, accuracy: 0.229167, mean_q: -66.351625, mean_eps: 0.100000\n",
      " 22091/50000: episode: 4465, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 13502.125977, mae: 553.688538, accuracy: 0.229167, mean_q: -58.488724, mean_eps: 0.100000\n",
      " 22094/50000: episode: 4466, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 16127.243815, mae: 563.094198, accuracy: 0.250000, mean_q: -50.338109, mean_eps: 0.100000\n",
      " 22097/50000: episode: 4467, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 15130.479167, mae: 553.606262, accuracy: 0.177083, mean_q: -41.881072, mean_eps: 0.100000\n",
      " 22100/50000: episode: 4468, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 14223.911784, mae: 557.905172, accuracy: 0.322917, mean_q: -64.633713, mean_eps: 0.100000\n",
      " 22103/50000: episode: 4469, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 17763.444010, mae: 571.736145, accuracy: 0.260417, mean_q: -74.407159, mean_eps: 0.100000\n",
      " 22106/50000: episode: 4470, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 14476.709635, mae: 538.667318, accuracy: 0.250000, mean_q: -52.974716, mean_eps: 0.100000\n",
      " 22109/50000: episode: 4471, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 15710.005534, mae: 549.055908, accuracy: 0.208333, mean_q: -45.522115, mean_eps: 0.100000\n",
      " 22112/50000: episode: 4472, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 15224.206380, mae: 544.944163, accuracy: 0.229167, mean_q: -62.704495, mean_eps: 0.100000\n",
      " 22115/50000: episode: 4473, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 14417.478190, mae: 547.658630, accuracy: 0.291667, mean_q: -50.667743, mean_eps: 0.100000\n",
      " 22118/50000: episode: 4474, duration: 0.012s, episode steps:   3, steps per second: 256, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 13677.744466, mae: 538.540507, accuracy: 0.343750, mean_q: -35.116095, mean_eps: 0.100000\n",
      " 22121/50000: episode: 4475, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 14281.458333, mae: 540.614136, accuracy: 0.322917, mean_q: -39.092969, mean_eps: 0.100000\n",
      " 22125/50000: episode: 4476, duration: 0.017s, episode steps:   4, steps per second: 242, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 15309.392090, mae: 549.355331, accuracy: 0.218750, mean_q: -45.351536, mean_eps: 0.100000\n",
      " 22128/50000: episode: 4477, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 12166.448568, mae: 533.296183, accuracy: 0.395833, mean_q: -64.472033, mean_eps: 0.100000\n",
      " 22131/50000: episode: 4478, duration: 0.014s, episode steps:   3, steps per second: 215, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 15990.805664, mae: 563.112752, accuracy: 0.343750, mean_q: -61.808858, mean_eps: 0.100000\n",
      " 22134/50000: episode: 4479, duration: 0.018s, episode steps:   3, steps per second: 164, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 14318.627604, mae: 547.836121, accuracy: 0.260417, mean_q: -70.072941, mean_eps: 0.100000\n",
      " 22138/50000: episode: 4480, duration: 0.018s, episode steps:   4, steps per second: 219, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 14789.047363, mae: 553.991669, accuracy: 0.203125, mean_q: -52.081905, mean_eps: 0.100000\n",
      " 22141/50000: episode: 4481, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 12216.139974, mae: 543.307963, accuracy: 0.322917, mean_q: -56.173302, mean_eps: 0.100000\n",
      " 22144/50000: episode: 4482, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 14137.632812, mae: 546.837179, accuracy: 0.156250, mean_q: -31.500611, mean_eps: 0.100000\n",
      " 22148/50000: episode: 4483, duration: 0.016s, episode steps:   4, steps per second: 255, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 16013.138428, mae: 561.117249, accuracy: 0.187500, mean_q: -17.459259, mean_eps: 0.100000\n",
      " 22151/50000: episode: 4484, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 14130.977865, mae: 533.438822, accuracy: 0.229167, mean_q: -13.629957, mean_eps: 0.100000\n",
      " 22154/50000: episode: 4485, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 13777.102539, mae: 547.959880, accuracy: 0.229167, mean_q: -43.913839, mean_eps: 0.100000\n",
      " 22157/50000: episode: 4486, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 15793.048177, mae: 564.710347, accuracy: 0.197917, mean_q: -78.595676, mean_eps: 0.100000\n",
      " 22160/50000: episode: 4487, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 15572.058919, mae: 571.118083, accuracy: 0.218750, mean_q: -55.713415, mean_eps: 0.100000\n",
      " 22163/50000: episode: 4488, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 17001.329427, mae: 579.012329, accuracy: 0.291667, mean_q: -73.595993, mean_eps: 0.100000\n",
      " 22166/50000: episode: 4489, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.333 [0.000, 3.000],  loss: 13606.631836, mae: 558.055908, accuracy: 0.281250, mean_q: -53.625689, mean_eps: 0.100000\n",
      " 22169/50000: episode: 4490, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 15098.149740, mae: 555.677124, accuracy: 0.229167, mean_q: -52.067725, mean_eps: 0.100000\n",
      " 22172/50000: episode: 4491, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 15460.018880, mae: 559.006490, accuracy: 0.281250, mean_q: -68.846525, mean_eps: 0.100000\n",
      " 22175/50000: episode: 4492, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 13264.728190, mae: 558.674622, accuracy: 0.333333, mean_q: -35.844009, mean_eps: 0.100000\n",
      " 22178/50000: episode: 4493, duration: 0.013s, episode steps:   3, steps per second: 227, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 13881.379557, mae: 533.687683, accuracy: 0.197917, mean_q: -49.952546, mean_eps: 0.100000\n",
      " 22181/50000: episode: 4494, duration: 0.015s, episode steps:   3, steps per second: 206, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 14862.537435, mae: 567.026998, accuracy: 0.166667, mean_q: -25.686342, mean_eps: 0.100000\n",
      " 22186/50000: episode: 4495, duration: 0.021s, episode steps:   5, steps per second: 239, episode reward: -2193.000, mean reward: -438.600 [-999.000, -32.000], mean action: 1.200 [0.000, 3.000],  loss: 13817.213281, mae: 561.001587, accuracy: 0.231250, mean_q: -19.527372, mean_eps: 0.100000\n",
      " 22189/50000: episode: 4496, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 13496.254557, mae: 538.606079, accuracy: 0.302083, mean_q: -32.809650, mean_eps: 0.100000\n",
      " 22192/50000: episode: 4497, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 14108.133138, mae: 558.067973, accuracy: 0.250000, mean_q: -58.373725, mean_eps: 0.100000\n",
      " 22196/50000: episode: 4498, duration: 0.015s, episode steps:   4, steps per second: 263, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 15217.790771, mae: 552.793045, accuracy: 0.281250, mean_q: -38.944895, mean_eps: 0.100000\n",
      " 22199/50000: episode: 4499, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 13676.748372, mae: 538.549255, accuracy: 0.312500, mean_q: -25.726592, mean_eps: 0.100000\n",
      " 22202/50000: episode: 4500, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 13117.399414, mae: 526.008759, accuracy: 0.291667, mean_q: -6.016038, mean_eps: 0.100000\n",
      " 22205/50000: episode: 4501, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 17685.807943, mae: 556.630127, accuracy: 0.239583, mean_q: -25.834930, mean_eps: 0.100000\n",
      " 22208/50000: episode: 4502, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 16464.515951, mae: 553.444519, accuracy: 0.208333, mean_q: -45.004921, mean_eps: 0.100000\n",
      " 22211/50000: episode: 4503, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 14369.833333, mae: 552.481710, accuracy: 0.291667, mean_q: -27.303388, mean_eps: 0.100000\n",
      " 22215/50000: episode: 4504, duration: 0.015s, episode steps:   4, steps per second: 267, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 2.000 [0.000, 3.000],  loss: 14548.051025, mae: 548.460220, accuracy: 0.226562, mean_q: -34.334137, mean_eps: 0.100000\n",
      " 22218/50000: episode: 4505, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 15549.851562, mae: 545.247030, accuracy: 0.312500, mean_q: -54.934736, mean_eps: 0.100000\n",
      " 22221/50000: episode: 4506, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 14319.453125, mae: 553.389058, accuracy: 0.322917, mean_q: -19.010331, mean_eps: 0.100000\n",
      " 22224/50000: episode: 4507, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 16459.617513, mae: 567.730794, accuracy: 0.312500, mean_q: -64.696619, mean_eps: 0.100000\n",
      " 22227/50000: episode: 4508, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 14448.124023, mae: 528.674683, accuracy: 0.250000, mean_q: -36.192423, mean_eps: 0.100000\n",
      " 22230/50000: episode: 4509, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 14466.657552, mae: 564.813578, accuracy: 0.333333, mean_q: -20.339383, mean_eps: 0.100000\n",
      " 22233/50000: episode: 4510, duration: 0.019s, episode steps:   3, steps per second: 157, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 15514.704427, mae: 572.841125, accuracy: 0.281250, mean_q: -42.978484, mean_eps: 0.100000\n",
      " 22236/50000: episode: 4511, duration: 0.014s, episode steps:   3, steps per second: 212, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 14941.888997, mae: 547.874247, accuracy: 0.208333, mean_q: -45.333692, mean_eps: 0.100000\n",
      " 22240/50000: episode: 4512, duration: 0.016s, episode steps:   4, steps per second: 256, episode reward: -1238.000, mean reward: -309.500 [-999.000, -58.000], mean action: 1.750 [0.000, 3.000],  loss: 12694.146240, mae: 536.023087, accuracy: 0.304688, mean_q: -41.059518, mean_eps: 0.100000\n",
      " 22243/50000: episode: 4513, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 13842.456380, mae: 534.305990, accuracy: 0.250000, mean_q: -57.356533, mean_eps: 0.100000\n",
      " 22246/50000: episode: 4514, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 14439.956706, mae: 549.681458, accuracy: 0.281250, mean_q: -63.030757, mean_eps: 0.100000\n",
      " 22249/50000: episode: 4515, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 13842.625651, mae: 536.702616, accuracy: 0.322917, mean_q: -57.009561, mean_eps: 0.100000\n",
      " 22252/50000: episode: 4516, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 13816.100260, mae: 549.359660, accuracy: 0.229167, mean_q: -36.555232, mean_eps: 0.100000\n",
      " 22255/50000: episode: 4517, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 13742.690104, mae: 522.363780, accuracy: 0.177083, mean_q: -61.593772, mean_eps: 0.100000\n",
      " 22259/50000: episode: 4518, duration: 0.015s, episode steps:   4, steps per second: 265, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 14848.955322, mae: 553.704193, accuracy: 0.218750, mean_q: -33.694743, mean_eps: 0.100000\n",
      " 22262/50000: episode: 4519, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 14328.634440, mae: 552.516378, accuracy: 0.250000, mean_q: -32.079136, mean_eps: 0.100000\n",
      " 22265/50000: episode: 4520, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 15777.466797, mae: 544.450826, accuracy: 0.270833, mean_q: -24.487499, mean_eps: 0.100000\n",
      " 22268/50000: episode: 4521, duration: 0.012s, episode steps:   3, steps per second: 257, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.667 [0.000, 3.000],  loss: 15882.739583, mae: 547.834574, accuracy: 0.187500, mean_q: -48.721203, mean_eps: 0.100000\n",
      " 22271/50000: episode: 4522, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 15601.977865, mae: 563.907654, accuracy: 0.250000, mean_q: -76.832812, mean_eps: 0.100000\n",
      " 22274/50000: episode: 4523, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 14877.177083, mae: 540.789001, accuracy: 0.218750, mean_q: -33.595439, mean_eps: 0.100000\n",
      " 22277/50000: episode: 4524, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 12681.105469, mae: 538.634705, accuracy: 0.260417, mean_q: -23.488906, mean_eps: 0.100000\n",
      " 22281/50000: episode: 4525, duration: 0.016s, episode steps:   4, steps per second: 244, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 13905.004883, mae: 539.901001, accuracy: 0.242188, mean_q: -42.671106, mean_eps: 0.100000\n",
      " 22284/50000: episode: 4526, duration: 0.014s, episode steps:   3, steps per second: 208, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 15501.008464, mae: 555.225932, accuracy: 0.166667, mean_q: -60.154702, mean_eps: 0.100000\n",
      " 22287/50000: episode: 4527, duration: 0.014s, episode steps:   3, steps per second: 208, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 13467.269531, mae: 546.825643, accuracy: 0.291667, mean_q: -33.157144, mean_eps: 0.100000\n",
      " 22291/50000: episode: 4528, duration: 0.016s, episode steps:   4, steps per second: 250, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 2.250 [1.000, 3.000],  loss: 13515.229980, mae: 541.078644, accuracy: 0.367188, mean_q: -34.247672, mean_eps: 0.100000\n",
      " 22294/50000: episode: 4529, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 13591.652995, mae: 543.485555, accuracy: 0.354167, mean_q: -38.984729, mean_eps: 0.100000\n",
      " 22298/50000: episode: 4530, duration: 0.015s, episode steps:   4, steps per second: 261, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 13673.964600, mae: 543.004272, accuracy: 0.304688, mean_q: -31.882149, mean_eps: 0.100000\n",
      " 22301/50000: episode: 4531, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 17192.123047, mae: 547.634766, accuracy: 0.260417, mean_q: -67.732437, mean_eps: 0.100000\n",
      " 22304/50000: episode: 4532, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 13139.539062, mae: 548.300130, accuracy: 0.229167, mean_q: -16.876299, mean_eps: 0.100000\n",
      " 22307/50000: episode: 4533, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 12691.291992, mae: 549.933899, accuracy: 0.270833, mean_q: -49.820201, mean_eps: 0.100000\n",
      " 22310/50000: episode: 4534, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 15978.820638, mae: 538.601156, accuracy: 0.312500, mean_q: -46.710757, mean_eps: 0.100000\n",
      " 22314/50000: episode: 4535, duration: 0.015s, episode steps:   4, steps per second: 262, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 15511.672852, mae: 535.360001, accuracy: 0.273438, mean_q: -69.756305, mean_eps: 0.100000\n",
      " 22317/50000: episode: 4536, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 13344.185547, mae: 543.706156, accuracy: 0.291667, mean_q: -31.655509, mean_eps: 0.100000\n",
      " 22320/50000: episode: 4537, duration: 0.012s, episode steps:   3, steps per second: 255, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 14251.285156, mae: 551.796366, accuracy: 0.239583, mean_q: -37.688747, mean_eps: 0.100000\n",
      " 22323/50000: episode: 4538, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 14250.606120, mae: 533.219482, accuracy: 0.250000, mean_q: -58.939816, mean_eps: 0.100000\n",
      " 22327/50000: episode: 4539, duration: 0.016s, episode steps:   4, steps per second: 254, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 14112.875488, mae: 532.496246, accuracy: 0.296875, mean_q: -48.692392, mean_eps: 0.100000\n",
      " 22330/50000: episode: 4540, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 14843.498698, mae: 555.566833, accuracy: 0.187500, mean_q: -57.261096, mean_eps: 0.100000\n",
      " 22333/50000: episode: 4541, duration: 0.014s, episode steps:   3, steps per second: 220, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 14992.240234, mae: 555.724935, accuracy: 0.260417, mean_q: -66.284602, mean_eps: 0.100000\n",
      " 22336/50000: episode: 4542, duration: 0.019s, episode steps:   3, steps per second: 161, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 16586.850586, mae: 565.641032, accuracy: 0.197917, mean_q: -59.116275, mean_eps: 0.100000\n",
      " 22339/50000: episode: 4543, duration: 0.013s, episode steps:   3, steps per second: 223, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 15195.998047, mae: 565.109294, accuracy: 0.229167, mean_q: -41.232686, mean_eps: 0.100000\n",
      " 22342/50000: episode: 4544, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 12747.184570, mae: 534.751282, accuracy: 0.312500, mean_q: -54.269479, mean_eps: 0.100000\n",
      " 22345/50000: episode: 4545, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 13674.239258, mae: 553.182699, accuracy: 0.125000, mean_q: -61.521568, mean_eps: 0.100000\n",
      " 22348/50000: episode: 4546, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 15514.319661, mae: 538.785583, accuracy: 0.229167, mean_q: -79.700198, mean_eps: 0.100000\n",
      " 22351/50000: episode: 4547, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 16092.502279, mae: 533.083069, accuracy: 0.270833, mean_q: -94.103350, mean_eps: 0.100000\n",
      " 22354/50000: episode: 4548, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 13801.656576, mae: 537.158936, accuracy: 0.166667, mean_q: -34.639392, mean_eps: 0.100000\n",
      " 22357/50000: episode: 4549, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 13364.805339, mae: 552.229268, accuracy: 0.270833, mean_q: -32.019718, mean_eps: 0.100000\n",
      " 22360/50000: episode: 4550, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 15888.714193, mae: 551.985168, accuracy: 0.291667, mean_q: -57.283714, mean_eps: 0.100000\n",
      " 22363/50000: episode: 4551, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 13155.686198, mae: 547.703857, accuracy: 0.281250, mean_q: -35.647110, mean_eps: 0.100000\n",
      " 22367/50000: episode: 4552, duration: 0.015s, episode steps:   4, steps per second: 260, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 14139.641357, mae: 537.155273, accuracy: 0.210938, mean_q: -37.794716, mean_eps: 0.100000\n",
      " 22370/50000: episode: 4553, duration: 0.012s, episode steps:   3, steps per second: 253, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 14466.114909, mae: 537.583354, accuracy: 0.333333, mean_q: -35.314372, mean_eps: 0.100000\n",
      " 22373/50000: episode: 4554, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 14134.128906, mae: 550.769592, accuracy: 0.187500, mean_q: -40.135033, mean_eps: 0.100000\n",
      " 22376/50000: episode: 4555, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 16590.310547, mae: 556.739421, accuracy: 0.239583, mean_q: -86.002116, mean_eps: 0.100000\n",
      " 22379/50000: episode: 4556, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 13762.054362, mae: 540.311015, accuracy: 0.229167, mean_q: -35.372613, mean_eps: 0.100000\n",
      " 22382/50000: episode: 4557, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 13250.218424, mae: 545.536804, accuracy: 0.197917, mean_q: -25.870036, mean_eps: 0.100000\n",
      " 22385/50000: episode: 4558, duration: 0.015s, episode steps:   3, steps per second: 204, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 14110.155273, mae: 544.434814, accuracy: 0.333333, mean_q: -68.475164, mean_eps: 0.100000\n",
      " 22388/50000: episode: 4559, duration: 0.014s, episode steps:   3, steps per second: 215, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 13178.000000, mae: 556.590007, accuracy: 0.395833, mean_q: -24.861984, mean_eps: 0.100000\n",
      " 22391/50000: episode: 4560, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 13796.062174, mae: 549.954549, accuracy: 0.291667, mean_q: -52.574556, mean_eps: 0.100000\n",
      " 22394/50000: episode: 4561, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 12818.864909, mae: 521.505819, accuracy: 0.197917, mean_q: -4.086537, mean_eps: 0.100000\n",
      " 22397/50000: episode: 4562, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 14846.199870, mae: 538.077738, accuracy: 0.229167, mean_q: -56.073613, mean_eps: 0.100000\n",
      " 22400/50000: episode: 4563, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 15776.736979, mae: 539.487488, accuracy: 0.197917, mean_q: -35.259114, mean_eps: 0.100000\n",
      " 22403/50000: episode: 4564, duration: 0.022s, episode steps:   3, steps per second: 137, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 14402.736328, mae: 532.039591, accuracy: 0.250000, mean_q: -51.675672, mean_eps: 0.100000\n",
      " 22406/50000: episode: 4565, duration: 0.015s, episode steps:   3, steps per second: 195, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 12373.024740, mae: 520.762105, accuracy: 0.281250, mean_q: -14.440408, mean_eps: 0.100000\n",
      " 22409/50000: episode: 4566, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 14871.256185, mae: 534.707418, accuracy: 0.239583, mean_q: -12.929583, mean_eps: 0.100000\n",
      " 22412/50000: episode: 4567, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 13656.934896, mae: 555.214132, accuracy: 0.250000, mean_q: -25.472113, mean_eps: 0.100000\n",
      " 22415/50000: episode: 4568, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 14728.929688, mae: 539.052327, accuracy: 0.281250, mean_q: -55.672490, mean_eps: 0.100000\n",
      " 22418/50000: episode: 4569, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 12997.403646, mae: 535.678101, accuracy: 0.312500, mean_q: -31.816175, mean_eps: 0.100000\n",
      " 22421/50000: episode: 4570, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 11286.986979, mae: 512.169515, accuracy: 0.260417, mean_q: -14.464604, mean_eps: 0.100000\n",
      " 22424/50000: episode: 4571, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 14934.068685, mae: 554.715129, accuracy: 0.312500, mean_q: -35.385066, mean_eps: 0.100000\n",
      " 22427/50000: episode: 4572, duration: 0.015s, episode steps:   3, steps per second: 197, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 15607.208008, mae: 555.296773, accuracy: 0.197917, mean_q: -54.187327, mean_eps: 0.100000\n",
      " 22430/50000: episode: 4573, duration: 0.017s, episode steps:   3, steps per second: 173, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 13101.374023, mae: 531.283081, accuracy: 0.250000, mean_q: -20.212713, mean_eps: 0.100000\n",
      " 22433/50000: episode: 4574, duration: 0.015s, episode steps:   3, steps per second: 207, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 13677.598958, mae: 556.881226, accuracy: 0.270833, mean_q: -47.803244, mean_eps: 0.100000\n",
      " 22436/50000: episode: 4575, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 16610.144857, mae: 550.776326, accuracy: 0.229167, mean_q: -34.392491, mean_eps: 0.100000\n",
      " 22439/50000: episode: 4576, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 13842.901367, mae: 549.638804, accuracy: 0.343750, mean_q: -44.398687, mean_eps: 0.100000\n",
      " 22442/50000: episode: 4577, duration: 0.012s, episode steps:   3, steps per second: 253, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 13807.058919, mae: 561.739339, accuracy: 0.333333, mean_q: -25.313046, mean_eps: 0.100000\n",
      " 22445/50000: episode: 4578, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 13977.559245, mae: 536.420614, accuracy: 0.250000, mean_q: -39.583237, mean_eps: 0.100000\n",
      " 22448/50000: episode: 4579, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 14557.535482, mae: 547.353638, accuracy: 0.229167, mean_q: -63.707094, mean_eps: 0.100000\n",
      " 22451/50000: episode: 4580, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 14271.435547, mae: 529.970083, accuracy: 0.333333, mean_q: -48.474760, mean_eps: 0.100000\n",
      " 22454/50000: episode: 4581, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 14966.749349, mae: 542.232808, accuracy: 0.208333, mean_q: -13.212327, mean_eps: 0.100000\n",
      " 22457/50000: episode: 4582, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 13974.532878, mae: 534.763814, accuracy: 0.187500, mean_q: -42.922046, mean_eps: 0.100000\n",
      " 22461/50000: episode: 4583, duration: 0.015s, episode steps:   4, steps per second: 262, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.000 [0.000, 3.000],  loss: 16312.281494, mae: 568.041199, accuracy: 0.218750, mean_q: -60.877958, mean_eps: 0.100000\n",
      " 22464/50000: episode: 4584, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 14554.229167, mae: 546.061259, accuracy: 0.302083, mean_q: -49.036275, mean_eps: 0.100000\n",
      " 22467/50000: episode: 4585, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 15889.044922, mae: 568.932495, accuracy: 0.281250, mean_q: -35.193574, mean_eps: 0.100000\n",
      " 22470/50000: episode: 4586, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 17176.246745, mae: 566.846313, accuracy: 0.177083, mean_q: -77.820389, mean_eps: 0.100000\n",
      " 22474/50000: episode: 4587, duration: 0.021s, episode steps:   4, steps per second: 190, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 12885.703857, mae: 541.118912, accuracy: 0.250000, mean_q: -26.028797, mean_eps: 0.100000\n",
      " 22477/50000: episode: 4588, duration: 0.015s, episode steps:   3, steps per second: 196, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 14137.070638, mae: 548.770162, accuracy: 0.218750, mean_q: -29.112116, mean_eps: 0.100000\n",
      " 22480/50000: episode: 4589, duration: 0.014s, episode steps:   3, steps per second: 222, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 13000.968424, mae: 560.393107, accuracy: 0.270833, mean_q: -42.111591, mean_eps: 0.100000\n",
      " 22483/50000: episode: 4590, duration: 0.014s, episode steps:   3, steps per second: 213, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 14213.031901, mae: 543.694417, accuracy: 0.218750, mean_q: -34.972933, mean_eps: 0.100000\n",
      " 22486/50000: episode: 4591, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 17110.240560, mae: 571.416423, accuracy: 0.270833, mean_q: -50.132020, mean_eps: 0.100000\n",
      " 22489/50000: episode: 4592, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 14575.627930, mae: 545.947856, accuracy: 0.291667, mean_q: -55.399897, mean_eps: 0.100000\n",
      " 22492/50000: episode: 4593, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 13705.717122, mae: 550.887288, accuracy: 0.354167, mean_q: -50.553439, mean_eps: 0.100000\n",
      " 22497/50000: episode: 4594, duration: 0.019s, episode steps:   5, steps per second: 266, episode reward: -2193.000, mean reward: -438.600 [-999.000, -32.000], mean action: 1.600 [0.000, 3.000],  loss: 14281.330078, mae: 552.534265, accuracy: 0.387500, mean_q: -33.449935, mean_eps: 0.100000\n",
      " 22500/50000: episode: 4595, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 14476.552734, mae: 538.461141, accuracy: 0.343750, mean_q: -58.086324, mean_eps: 0.100000\n",
      " 22503/50000: episode: 4596, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 15126.499349, mae: 546.850159, accuracy: 0.343750, mean_q: -44.853214, mean_eps: 0.100000\n",
      " 22506/50000: episode: 4597, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 15180.604492, mae: 558.638346, accuracy: 0.281250, mean_q: -33.146395, mean_eps: 0.100000\n",
      " 22509/50000: episode: 4598, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 13811.354167, mae: 541.365336, accuracy: 0.354167, mean_q: -28.444441, mean_eps: 0.100000\n",
      " 22512/50000: episode: 4599, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 15567.382487, mae: 551.082214, accuracy: 0.270833, mean_q: -19.029092, mean_eps: 0.100000\n",
      " 22515/50000: episode: 4600, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 16503.593099, mae: 550.604919, accuracy: 0.343750, mean_q: -68.080671, mean_eps: 0.100000\n",
      " 22519/50000: episode: 4601, duration: 0.016s, episode steps:   4, steps per second: 254, episode reward: -1223.000, mean reward: -305.750 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 15892.865967, mae: 548.278519, accuracy: 0.257812, mean_q: -50.516998, mean_eps: 0.100000\n",
      " 22522/50000: episode: 4602, duration: 0.014s, episode steps:   3, steps per second: 218, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 16370.121745, mae: 539.334757, accuracy: 0.229167, mean_q: -34.385508, mean_eps: 0.100000\n",
      " 22525/50000: episode: 4603, duration: 0.014s, episode steps:   3, steps per second: 219, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 11867.161458, mae: 539.789856, accuracy: 0.312500, mean_q: -12.147952, mean_eps: 0.100000\n",
      " 22528/50000: episode: 4604, duration: 0.014s, episode steps:   3, steps per second: 216, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 14307.360026, mae: 545.230998, accuracy: 0.270833, mean_q: -45.979720, mean_eps: 0.100000\n",
      " 22531/50000: episode: 4605, duration: 0.014s, episode steps:   3, steps per second: 210, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 15484.863932, mae: 548.828552, accuracy: 0.291667, mean_q: -71.893196, mean_eps: 0.100000\n",
      " 22534/50000: episode: 4606, duration: 0.013s, episode steps:   3, steps per second: 227, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 15525.680664, mae: 550.207418, accuracy: 0.333333, mean_q: -7.494553, mean_eps: 0.100000\n",
      " 22537/50000: episode: 4607, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 15556.497721, mae: 572.406148, accuracy: 0.229167, mean_q: -39.608960, mean_eps: 0.100000\n",
      " 22540/50000: episode: 4608, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 12583.266602, mae: 546.830526, accuracy: 0.270833, mean_q: -4.764987, mean_eps: 0.100000\n",
      " 22543/50000: episode: 4609, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 13913.397786, mae: 557.230713, accuracy: 0.250000, mean_q: -44.454864, mean_eps: 0.100000\n",
      " 22546/50000: episode: 4610, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 14314.956055, mae: 557.380249, accuracy: 0.250000, mean_q: -48.998399, mean_eps: 0.100000\n",
      " 22549/50000: episode: 4611, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 15940.575195, mae: 534.217285, accuracy: 0.270833, mean_q: -60.856674, mean_eps: 0.100000\n",
      " 22552/50000: episode: 4612, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 13333.654622, mae: 541.374817, accuracy: 0.343750, mean_q: -25.813985, mean_eps: 0.100000\n",
      " 22556/50000: episode: 4613, duration: 0.015s, episode steps:   4, steps per second: 259, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 15922.514648, mae: 546.790741, accuracy: 0.257812, mean_q: -51.593146, mean_eps: 0.100000\n",
      " 22559/50000: episode: 4614, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 15319.450195, mae: 537.298726, accuracy: 0.270833, mean_q: -47.080519, mean_eps: 0.100000\n",
      " 22562/50000: episode: 4615, duration: 0.012s, episode steps:   3, steps per second: 255, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 18204.601888, mae: 551.298319, accuracy: 0.260417, mean_q: -88.435883, mean_eps: 0.100000\n",
      " 22565/50000: episode: 4616, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 14107.228516, mae: 527.600321, accuracy: 0.302083, mean_q: -50.066628, mean_eps: 0.100000\n",
      " 22569/50000: episode: 4617, duration: 0.016s, episode steps:   4, steps per second: 255, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 13914.061523, mae: 539.673248, accuracy: 0.203125, mean_q: -39.004278, mean_eps: 0.100000\n",
      " 22572/50000: episode: 4618, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 15360.927083, mae: 538.568471, accuracy: 0.250000, mean_q: -44.735950, mean_eps: 0.100000\n",
      " 22575/50000: episode: 4619, duration: 0.019s, episode steps:   3, steps per second: 161, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 15533.980469, mae: 539.708354, accuracy: 0.135417, mean_q: -57.222747, mean_eps: 0.100000\n",
      " 22578/50000: episode: 4620, duration: 0.016s, episode steps:   3, steps per second: 191, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.667 [0.000, 3.000],  loss: 12803.934245, mae: 546.082804, accuracy: 0.291667, mean_q: -39.013812, mean_eps: 0.100000\n",
      " 22581/50000: episode: 4621, duration: 0.013s, episode steps:   3, steps per second: 225, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 16166.809896, mae: 554.578735, accuracy: 0.218750, mean_q: -28.000667, mean_eps: 0.100000\n",
      " 22584/50000: episode: 4622, duration: 0.013s, episode steps:   3, steps per second: 225, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 16018.161784, mae: 543.144796, accuracy: 0.177083, mean_q: -39.321038, mean_eps: 0.100000\n",
      " 22587/50000: episode: 4623, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 15318.658854, mae: 548.824361, accuracy: 0.145833, mean_q: -20.317260, mean_eps: 0.100000\n",
      " 22591/50000: episode: 4624, duration: 0.015s, episode steps:   4, steps per second: 262, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 15210.407471, mae: 545.199356, accuracy: 0.289062, mean_q: -53.873366, mean_eps: 0.100000\n",
      " 22594/50000: episode: 4625, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 15186.985352, mae: 552.298645, accuracy: 0.250000, mean_q: -38.408591, mean_eps: 0.100000\n",
      " 22598/50000: episode: 4626, duration: 0.016s, episode steps:   4, steps per second: 244, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 14136.775146, mae: 551.394409, accuracy: 0.328125, mean_q: -44.033063, mean_eps: 0.100000\n",
      " 22601/50000: episode: 4627, duration: 0.014s, episode steps:   3, steps per second: 218, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 14333.515299, mae: 547.146484, accuracy: 0.208333, mean_q: -32.903254, mean_eps: 0.100000\n",
      " 22605/50000: episode: 4628, duration: 0.017s, episode steps:   4, steps per second: 236, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 15841.194336, mae: 550.723846, accuracy: 0.187500, mean_q: -56.132538, mean_eps: 0.100000\n",
      " 22608/50000: episode: 4629, duration: 0.014s, episode steps:   3, steps per second: 212, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 16547.043620, mae: 542.481364, accuracy: 0.239583, mean_q: -49.793691, mean_eps: 0.100000\n",
      " 22611/50000: episode: 4630, duration: 0.013s, episode steps:   3, steps per second: 226, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 13533.915365, mae: 554.836121, accuracy: 0.291667, mean_q: -26.732533, mean_eps: 0.100000\n",
      " 22614/50000: episode: 4631, duration: 0.013s, episode steps:   3, steps per second: 226, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 15482.652344, mae: 546.987732, accuracy: 0.343750, mean_q: -38.993649, mean_eps: 0.100000\n",
      " 22617/50000: episode: 4632, duration: 0.014s, episode steps:   3, steps per second: 221, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 18530.846354, mae: 559.729594, accuracy: 0.406250, mean_q: -68.676278, mean_eps: 0.100000\n",
      " 22621/50000: episode: 4633, duration: 0.018s, episode steps:   4, steps per second: 224, episode reward: -1238.000, mean reward: -309.500 [-999.000, -58.000], mean action: 1.000 [0.000, 3.000],  loss: 14430.259277, mae: 551.706940, accuracy: 0.265625, mean_q: -16.501917, mean_eps: 0.100000\n",
      " 22624/50000: episode: 4634, duration: 0.017s, episode steps:   3, steps per second: 175, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 15177.576172, mae: 558.928569, accuracy: 0.270833, mean_q: -48.809795, mean_eps: 0.100000\n",
      " 22627/50000: episode: 4635, duration: 0.016s, episode steps:   3, steps per second: 185, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 15843.421549, mae: 548.107300, accuracy: 0.218750, mean_q: -34.253360, mean_eps: 0.100000\n",
      " 22630/50000: episode: 4636, duration: 0.014s, episode steps:   3, steps per second: 210, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 15038.247721, mae: 552.379740, accuracy: 0.260417, mean_q: -61.807213, mean_eps: 0.100000\n",
      " 22633/50000: episode: 4637, duration: 0.014s, episode steps:   3, steps per second: 218, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 16414.957031, mae: 545.651123, accuracy: 0.291667, mean_q: -45.132620, mean_eps: 0.100000\n",
      " 22636/50000: episode: 4638, duration: 0.014s, episode steps:   3, steps per second: 209, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 14008.923177, mae: 537.431295, accuracy: 0.291667, mean_q: -64.647086, mean_eps: 0.100000\n",
      " 22639/50000: episode: 4639, duration: 0.014s, episode steps:   3, steps per second: 214, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 16239.384115, mae: 558.459188, accuracy: 0.229167, mean_q: -43.278790, mean_eps: 0.100000\n",
      " 22642/50000: episode: 4640, duration: 0.013s, episode steps:   3, steps per second: 224, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 14744.998047, mae: 551.554667, accuracy: 0.239583, mean_q: -40.014270, mean_eps: 0.100000\n",
      " 22645/50000: episode: 4641, duration: 0.014s, episode steps:   3, steps per second: 219, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 12963.706055, mae: 550.604675, accuracy: 0.239583, mean_q: -20.793210, mean_eps: 0.100000\n",
      " 22648/50000: episode: 4642, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 14828.933594, mae: 563.619568, accuracy: 0.229167, mean_q: -38.188359, mean_eps: 0.100000\n",
      " 22651/50000: episode: 4643, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 14026.241211, mae: 564.366720, accuracy: 0.229167, mean_q: -50.360413, mean_eps: 0.100000\n",
      " 22654/50000: episode: 4644, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 13582.786458, mae: 553.183431, accuracy: 0.312500, mean_q: -75.524729, mean_eps: 0.100000\n",
      " 22657/50000: episode: 4645, duration: 0.014s, episode steps:   3, steps per second: 213, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 15438.876302, mae: 562.735962, accuracy: 0.208333, mean_q: -58.031056, mean_eps: 0.100000\n",
      " 22660/50000: episode: 4646, duration: 0.014s, episode steps:   3, steps per second: 215, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 15019.253906, mae: 560.635396, accuracy: 0.239583, mean_q: -19.500281, mean_eps: 0.100000\n",
      " 22663/50000: episode: 4647, duration: 0.015s, episode steps:   3, steps per second: 203, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 15197.657552, mae: 561.215617, accuracy: 0.260417, mean_q: -28.582067, mean_eps: 0.100000\n",
      " 22666/50000: episode: 4648, duration: 0.015s, episode steps:   3, steps per second: 205, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 12230.877930, mae: 541.602926, accuracy: 0.291667, mean_q: -31.989631, mean_eps: 0.100000\n",
      " 22669/50000: episode: 4649, duration: 0.020s, episode steps:   3, steps per second: 150, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 14636.766602, mae: 560.275899, accuracy: 0.250000, mean_q: -23.080710, mean_eps: 0.100000\n",
      " 22672/50000: episode: 4650, duration: 0.014s, episode steps:   3, steps per second: 213, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 16404.074219, mae: 558.295125, accuracy: 0.208333, mean_q: -36.798887, mean_eps: 0.100000\n",
      " 22675/50000: episode: 4651, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 14789.625651, mae: 557.951782, accuracy: 0.333333, mean_q: -73.968795, mean_eps: 0.100000\n",
      " 22678/50000: episode: 4652, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 14934.638346, mae: 540.313802, accuracy: 0.312500, mean_q: -64.397352, mean_eps: 0.100000\n",
      " 22681/50000: episode: 4653, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 15227.138021, mae: 548.224304, accuracy: 0.229167, mean_q: -24.648438, mean_eps: 0.100000\n",
      " 22684/50000: episode: 4654, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 13810.058919, mae: 555.692220, accuracy: 0.281250, mean_q: -43.675952, mean_eps: 0.100000\n",
      " 22687/50000: episode: 4655, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 15942.059245, mae: 556.587667, accuracy: 0.302083, mean_q: -70.243729, mean_eps: 0.100000\n",
      " 22690/50000: episode: 4656, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 14444.564453, mae: 551.491597, accuracy: 0.250000, mean_q: -53.663670, mean_eps: 0.100000\n",
      " 22693/50000: episode: 4657, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 13304.373372, mae: 548.232442, accuracy: 0.281250, mean_q: -32.337601, mean_eps: 0.100000\n",
      " 22696/50000: episode: 4658, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 13439.787435, mae: 552.567342, accuracy: 0.354167, mean_q: -34.998322, mean_eps: 0.100000\n",
      " 22699/50000: episode: 4659, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 13644.131510, mae: 553.433431, accuracy: 0.302083, mean_q: -44.740752, mean_eps: 0.100000\n",
      " 22702/50000: episode: 4660, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 13640.343424, mae: 554.170654, accuracy: 0.333333, mean_q: -62.786402, mean_eps: 0.100000\n",
      " 22705/50000: episode: 4661, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 14356.872396, mae: 552.648682, accuracy: 0.260417, mean_q: -60.739597, mean_eps: 0.100000\n",
      " 22708/50000: episode: 4662, duration: 0.014s, episode steps:   3, steps per second: 221, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 15392.373047, mae: 544.922872, accuracy: 0.260417, mean_q: -83.758530, mean_eps: 0.100000\n",
      " 22711/50000: episode: 4663, duration: 0.013s, episode steps:   3, steps per second: 230, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 15504.010742, mae: 535.806478, accuracy: 0.239583, mean_q: -60.083632, mean_eps: 0.100000\n",
      " 22714/50000: episode: 4664, duration: 0.016s, episode steps:   3, steps per second: 188, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 15412.267904, mae: 566.115397, accuracy: 0.270833, mean_q: -42.067924, mean_eps: 0.100000\n",
      " 22717/50000: episode: 4665, duration: 0.013s, episode steps:   3, steps per second: 230, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 13363.859701, mae: 546.805929, accuracy: 0.239583, mean_q: -49.586862, mean_eps: 0.100000\n",
      " 22720/50000: episode: 4666, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 15933.100260, mae: 556.367981, accuracy: 0.260417, mean_q: -34.180665, mean_eps: 0.100000\n",
      " 22723/50000: episode: 4667, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 14474.225260, mae: 558.752848, accuracy: 0.270833, mean_q: -26.279287, mean_eps: 0.100000\n",
      " 22726/50000: episode: 4668, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 15422.219727, mae: 562.756042, accuracy: 0.197917, mean_q: -51.555539, mean_eps: 0.100000\n",
      " 22729/50000: episode: 4669, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 13580.845378, mae: 546.926025, accuracy: 0.229167, mean_q: -59.905860, mean_eps: 0.100000\n",
      " 22733/50000: episode: 4670, duration: 0.015s, episode steps:   4, steps per second: 259, episode reward: -1223.000, mean reward: -305.750 [-999.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 15055.083740, mae: 555.728653, accuracy: 0.250000, mean_q: -49.862695, mean_eps: 0.100000\n",
      " 22737/50000: episode: 4671, duration: 0.015s, episode steps:   4, steps per second: 267, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 2.000 [0.000, 3.000],  loss: 15082.468262, mae: 544.912689, accuracy: 0.328125, mean_q: -58.521291, mean_eps: 0.100000\n",
      " 22740/50000: episode: 4672, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 12293.649740, mae: 541.577515, accuracy: 0.260417, mean_q: -18.710965, mean_eps: 0.100000\n",
      " 22743/50000: episode: 4673, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 15323.167643, mae: 561.926554, accuracy: 0.302083, mean_q: -47.615785, mean_eps: 0.100000\n",
      " 22746/50000: episode: 4674, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 14671.320638, mae: 562.511658, accuracy: 0.260417, mean_q: -70.222440, mean_eps: 0.100000\n",
      " 22749/50000: episode: 4675, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 15264.881510, mae: 563.317769, accuracy: 0.239583, mean_q: -69.403074, mean_eps: 0.100000\n",
      " 22752/50000: episode: 4676, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 14054.346680, mae: 549.942973, accuracy: 0.281250, mean_q: -62.118212, mean_eps: 0.100000\n",
      " 22755/50000: episode: 4677, duration: 0.013s, episode steps:   3, steps per second: 222, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 15925.169271, mae: 554.581238, accuracy: 0.291667, mean_q: -76.205544, mean_eps: 0.100000\n",
      " 22758/50000: episode: 4678, duration: 0.015s, episode steps:   3, steps per second: 201, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 12748.421875, mae: 533.666901, accuracy: 0.270833, mean_q: -27.976582, mean_eps: 0.100000\n",
      " 22761/50000: episode: 4679, duration: 0.022s, episode steps:   3, steps per second: 136, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 13300.597656, mae: 538.978455, accuracy: 0.281250, mean_q: -52.947583, mean_eps: 0.100000\n",
      " 22764/50000: episode: 4680, duration: 0.015s, episode steps:   3, steps per second: 204, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 13549.349935, mae: 535.613993, accuracy: 0.218750, mean_q: -54.805286, mean_eps: 0.100000\n",
      " 22768/50000: episode: 4681, duration: 0.016s, episode steps:   4, steps per second: 256, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 2.000 [0.000, 3.000],  loss: 14309.997070, mae: 562.910751, accuracy: 0.242188, mean_q: -65.715306, mean_eps: 0.100000\n",
      " 22771/50000: episode: 4682, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 14090.704427, mae: 540.482076, accuracy: 0.239583, mean_q: -58.358861, mean_eps: 0.100000\n",
      " 22774/50000: episode: 4683, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 15153.160482, mae: 562.789734, accuracy: 0.250000, mean_q: -43.358579, mean_eps: 0.100000\n",
      " 22777/50000: episode: 4684, duration: 0.012s, episode steps:   3, steps per second: 254, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 11478.630859, mae: 541.050293, accuracy: 0.322917, mean_q: -45.150697, mean_eps: 0.100000\n",
      " 22780/50000: episode: 4685, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 13373.800456, mae: 560.723450, accuracy: 0.187500, mean_q: -49.371884, mean_eps: 0.100000\n",
      " 22783/50000: episode: 4686, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 15460.291992, mae: 556.229553, accuracy: 0.239583, mean_q: -62.655167, mean_eps: 0.100000\n",
      " 22786/50000: episode: 4687, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 14090.417969, mae: 554.570109, accuracy: 0.250000, mean_q: -74.080744, mean_eps: 0.100000\n",
      " 22789/50000: episode: 4688, duration: 0.012s, episode steps:   3, steps per second: 255, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 13841.982747, mae: 540.091227, accuracy: 0.250000, mean_q: -44.811242, mean_eps: 0.100000\n",
      " 22792/50000: episode: 4689, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 13207.828451, mae: 532.699259, accuracy: 0.250000, mean_q: -21.636168, mean_eps: 0.100000\n",
      " 22795/50000: episode: 4690, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 14547.005859, mae: 541.100993, accuracy: 0.229167, mean_q: -49.639668, mean_eps: 0.100000\n",
      " 22798/50000: episode: 4691, duration: 0.014s, episode steps:   3, steps per second: 214, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 13980.628255, mae: 543.130880, accuracy: 0.260417, mean_q: -57.993268, mean_eps: 0.100000\n",
      " 22801/50000: episode: 4692, duration: 0.019s, episode steps:   3, steps per second: 162, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 13515.625326, mae: 547.164347, accuracy: 0.281250, mean_q: -17.378089, mean_eps: 0.100000\n",
      " 22804/50000: episode: 4693, duration: 0.016s, episode steps:   3, steps per second: 189, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 15240.923503, mae: 555.011637, accuracy: 0.260417, mean_q: -19.496192, mean_eps: 0.100000\n",
      " 22809/50000: episode: 4694, duration: 0.021s, episode steps:   5, steps per second: 235, episode reward: -2193.000, mean reward: -438.600 [-999.000, -58.000], mean action: 1.800 [0.000, 3.000],  loss: 14085.139258, mae: 569.410156, accuracy: 0.231250, mean_q: -50.854879, mean_eps: 0.100000\n",
      " 22812/50000: episode: 4695, duration: 0.014s, episode steps:   3, steps per second: 214, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 16417.439453, mae: 559.488159, accuracy: 0.260417, mean_q: -47.497134, mean_eps: 0.100000\n",
      " 22815/50000: episode: 4696, duration: 0.013s, episode steps:   3, steps per second: 230, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 12909.297526, mae: 554.975850, accuracy: 0.281250, mean_q: -35.312571, mean_eps: 0.100000\n",
      " 22818/50000: episode: 4697, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 13872.765299, mae: 550.689514, accuracy: 0.312500, mean_q: -36.409074, mean_eps: 0.100000\n",
      " 22822/50000: episode: 4698, duration: 0.015s, episode steps:   4, steps per second: 261, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 13571.620117, mae: 561.854996, accuracy: 0.273438, mean_q: -61.060160, mean_eps: 0.100000\n",
      " 22825/50000: episode: 4699, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 14460.342448, mae: 553.714559, accuracy: 0.229167, mean_q: -47.650552, mean_eps: 0.100000\n",
      " 22828/50000: episode: 4700, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 16101.851562, mae: 560.137492, accuracy: 0.218750, mean_q: -56.882264, mean_eps: 0.100000\n",
      " 22831/50000: episode: 4701, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 13776.674805, mae: 563.619202, accuracy: 0.187500, mean_q: -46.637922, mean_eps: 0.100000\n",
      " 22834/50000: episode: 4702, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 12823.357747, mae: 551.929240, accuracy: 0.291667, mean_q: -46.717992, mean_eps: 0.100000\n",
      " 22837/50000: episode: 4703, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 14314.470052, mae: 570.983378, accuracy: 0.229167, mean_q: -25.799744, mean_eps: 0.100000\n",
      " 22840/50000: episode: 4704, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 13490.215820, mae: 542.813416, accuracy: 0.208333, mean_q: -74.201810, mean_eps: 0.100000\n",
      " 22843/50000: episode: 4705, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 13399.745768, mae: 557.562846, accuracy: 0.260417, mean_q: -29.321545, mean_eps: 0.100000\n",
      " 22846/50000: episode: 4706, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 13094.105143, mae: 546.531413, accuracy: 0.291667, mean_q: -65.241784, mean_eps: 0.100000\n",
      " 22849/50000: episode: 4707, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 15165.970703, mae: 564.103638, accuracy: 0.166667, mean_q: -33.555811, mean_eps: 0.100000\n",
      " 22852/50000: episode: 4708, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 11406.899089, mae: 536.369486, accuracy: 0.291667, mean_q: -60.228181, mean_eps: 0.100000\n",
      " 22855/50000: episode: 4709, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 14592.479818, mae: 558.695272, accuracy: 0.218750, mean_q: -24.393540, mean_eps: 0.100000\n",
      " 22858/50000: episode: 4710, duration: 0.017s, episode steps:   3, steps per second: 175, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 11760.462240, mae: 533.816366, accuracy: 0.333333, mean_q: -36.022483, mean_eps: 0.100000\n",
      " 22861/50000: episode: 4711, duration: 0.016s, episode steps:   3, steps per second: 186, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 12443.946940, mae: 544.339437, accuracy: 0.343750, mean_q: -43.846457, mean_eps: 0.100000\n",
      " 22864/50000: episode: 4712, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 11447.511393, mae: 545.412496, accuracy: 0.427083, mean_q: -31.808606, mean_eps: 0.100000\n",
      " 22867/50000: episode: 4713, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.667 [0.000, 3.000],  loss: 12828.931641, mae: 542.752482, accuracy: 0.281250, mean_q: -38.960867, mean_eps: 0.100000\n",
      " 22870/50000: episode: 4714, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 14110.021484, mae: 564.372396, accuracy: 0.187500, mean_q: -43.441083, mean_eps: 0.100000\n",
      " 22873/50000: episode: 4715, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 13204.690755, mae: 559.973145, accuracy: 0.260417, mean_q: -38.659655, mean_eps: 0.100000\n",
      " 22876/50000: episode: 4716, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 15418.447266, mae: 567.482463, accuracy: 0.250000, mean_q: -65.910551, mean_eps: 0.100000\n",
      " 22879/50000: episode: 4717, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 12931.605794, mae: 549.858480, accuracy: 0.229167, mean_q: -50.225826, mean_eps: 0.100000\n",
      " 22882/50000: episode: 4718, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 12376.571940, mae: 536.674805, accuracy: 0.229167, mean_q: -25.823380, mean_eps: 0.100000\n",
      " 22885/50000: episode: 4719, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 16154.104492, mae: 550.884888, accuracy: 0.208333, mean_q: -45.664168, mean_eps: 0.100000\n",
      " 22888/50000: episode: 4720, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 13937.050456, mae: 556.736206, accuracy: 0.218750, mean_q: -36.261073, mean_eps: 0.100000\n",
      " 22891/50000: episode: 4721, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 13114.745768, mae: 553.898499, accuracy: 0.281250, mean_q: -49.819163, mean_eps: 0.100000\n",
      " 22894/50000: episode: 4722, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 13985.476562, mae: 541.031311, accuracy: 0.229167, mean_q: -85.155093, mean_eps: 0.100000\n",
      " 22898/50000: episode: 4723, duration: 0.018s, episode steps:   4, steps per second: 220, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 15168.916748, mae: 568.517365, accuracy: 0.250000, mean_q: -44.589686, mean_eps: 0.100000\n",
      " 22901/50000: episode: 4724, duration: 0.014s, episode steps:   3, steps per second: 221, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 14762.786133, mae: 534.794108, accuracy: 0.260417, mean_q: -65.312870, mean_eps: 0.100000\n",
      " 22905/50000: episode: 4725, duration: 0.018s, episode steps:   4, steps per second: 227, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 14513.612793, mae: 559.589020, accuracy: 0.312500, mean_q: -53.173335, mean_eps: 0.100000\n",
      " 22908/50000: episode: 4726, duration: 0.015s, episode steps:   3, steps per second: 201, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 12468.841146, mae: 550.947489, accuracy: 0.312500, mean_q: -53.996095, mean_eps: 0.100000\n",
      " 22911/50000: episode: 4727, duration: 0.015s, episode steps:   3, steps per second: 195, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 13475.928711, mae: 560.988770, accuracy: 0.260417, mean_q: -57.307584, mean_eps: 0.100000\n",
      " 22914/50000: episode: 4728, duration: 0.015s, episode steps:   3, steps per second: 197, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 16179.542969, mae: 573.743123, accuracy: 0.312500, mean_q: -60.420110, mean_eps: 0.100000\n",
      " 22917/50000: episode: 4729, duration: 0.014s, episode steps:   3, steps per second: 208, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 14052.448568, mae: 549.102234, accuracy: 0.187500, mean_q: -57.920387, mean_eps: 0.100000\n",
      " 22920/50000: episode: 4730, duration: 0.014s, episode steps:   3, steps per second: 209, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 15216.032552, mae: 545.696309, accuracy: 0.260417, mean_q: -53.303088, mean_eps: 0.100000\n",
      " 22924/50000: episode: 4731, duration: 0.018s, episode steps:   4, steps per second: 219, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 13613.735107, mae: 552.197769, accuracy: 0.234375, mean_q: -23.233033, mean_eps: 0.100000\n",
      " 22927/50000: episode: 4732, duration: 0.015s, episode steps:   3, steps per second: 198, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 12021.347982, mae: 530.873922, accuracy: 0.385417, mean_q: -65.123805, mean_eps: 0.100000\n",
      " 22930/50000: episode: 4733, duration: 0.014s, episode steps:   3, steps per second: 220, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 14073.028320, mae: 535.959920, accuracy: 0.291667, mean_q: -26.486982, mean_eps: 0.100000\n",
      " 22934/50000: episode: 4734, duration: 0.017s, episode steps:   4, steps per second: 242, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 2.000 [0.000, 3.000],  loss: 14733.203369, mae: 551.451874, accuracy: 0.242188, mean_q: -50.431708, mean_eps: 0.100000\n",
      " 22938/50000: episode: 4735, duration: 0.016s, episode steps:   4, steps per second: 247, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 2.250 [1.000, 3.000],  loss: 13045.501221, mae: 558.097534, accuracy: 0.320312, mean_q: -63.608591, mean_eps: 0.100000\n",
      " 22941/50000: episode: 4736, duration: 0.013s, episode steps:   3, steps per second: 227, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 16043.663737, mae: 572.129720, accuracy: 0.229167, mean_q: -100.513390, mean_eps: 0.100000\n",
      " 22944/50000: episode: 4737, duration: 0.013s, episode steps:   3, steps per second: 226, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 15259.795573, mae: 561.366943, accuracy: 0.197917, mean_q: -40.739146, mean_eps: 0.100000\n",
      " 22947/50000: episode: 4738, duration: 0.013s, episode steps:   3, steps per second: 227, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 12918.308268, mae: 550.482666, accuracy: 0.250000, mean_q: -45.778429, mean_eps: 0.100000\n",
      " 22950/50000: episode: 4739, duration: 0.021s, episode steps:   3, steps per second: 144, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 14397.317057, mae: 575.087952, accuracy: 0.354167, mean_q: -83.903735, mean_eps: 0.100000\n",
      " 22953/50000: episode: 4740, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 13559.030599, mae: 551.148092, accuracy: 0.281250, mean_q: -81.690323, mean_eps: 0.100000\n",
      " 22956/50000: episode: 4741, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 14499.760417, mae: 563.369609, accuracy: 0.250000, mean_q: -52.650719, mean_eps: 0.100000\n",
      " 22959/50000: episode: 4742, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 13799.713542, mae: 556.790507, accuracy: 0.218750, mean_q: -33.074601, mean_eps: 0.100000\n",
      " 22963/50000: episode: 4743, duration: 0.015s, episode steps:   4, steps per second: 264, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 2.000 [0.000, 3.000],  loss: 13448.671875, mae: 535.664337, accuracy: 0.242188, mean_q: -32.695753, mean_eps: 0.100000\n",
      " 22966/50000: episode: 4744, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 13373.991536, mae: 543.838908, accuracy: 0.281250, mean_q: -60.209487, mean_eps: 0.100000\n",
      " 22970/50000: episode: 4745, duration: 0.015s, episode steps:   4, steps per second: 263, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 16143.800537, mae: 554.061615, accuracy: 0.226562, mean_q: -52.429616, mean_eps: 0.100000\n",
      " 22973/50000: episode: 4746, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 13193.133464, mae: 524.626689, accuracy: 0.333333, mean_q: -48.720222, mean_eps: 0.100000\n",
      " 22976/50000: episode: 4747, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 14260.117513, mae: 568.598836, accuracy: 0.281250, mean_q: -71.267759, mean_eps: 0.100000\n",
      " 22979/50000: episode: 4748, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 13039.443359, mae: 557.206116, accuracy: 0.250000, mean_q: -22.291951, mean_eps: 0.100000\n",
      " 22982/50000: episode: 4749, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 13242.583659, mae: 539.788595, accuracy: 0.218750, mean_q: -25.997721, mean_eps: 0.100000\n",
      " 22985/50000: episode: 4750, duration: 0.012s, episode steps:   3, steps per second: 258, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 13623.016276, mae: 564.008667, accuracy: 0.229167, mean_q: -76.702563, mean_eps: 0.100000\n",
      " 22988/50000: episode: 4751, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 14562.472656, mae: 536.836060, accuracy: 0.302083, mean_q: -30.260217, mean_eps: 0.100000\n",
      " 22991/50000: episode: 4752, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 12872.251953, mae: 537.658427, accuracy: 0.260417, mean_q: -48.097166, mean_eps: 0.100000\n",
      " 22994/50000: episode: 4753, duration: 0.013s, episode steps:   3, steps per second: 226, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 13623.093750, mae: 539.709513, accuracy: 0.197917, mean_q: -15.261816, mean_eps: 0.100000\n",
      " 22997/50000: episode: 4754, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 13821.479818, mae: 532.668152, accuracy: 0.302083, mean_q: -50.507418, mean_eps: 0.100000\n",
      " 23000/50000: episode: 4755, duration: 0.014s, episode steps:   3, steps per second: 213, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 12294.499349, mae: 545.301249, accuracy: 0.270833, mean_q: -65.279065, mean_eps: 0.100000\n",
      " 23003/50000: episode: 4756, duration: 0.014s, episode steps:   3, steps per second: 212, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 13422.300456, mae: 554.977824, accuracy: 0.312500, mean_q: -48.068895, mean_eps: 0.100000\n",
      " 23006/50000: episode: 4757, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 14036.328776, mae: 557.668884, accuracy: 0.354167, mean_q: -44.040763, mean_eps: 0.100000\n",
      " 23010/50000: episode: 4758, duration: 0.015s, episode steps:   4, steps per second: 260, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 13066.746094, mae: 540.659805, accuracy: 0.250000, mean_q: -36.545476, mean_eps: 0.100000\n",
      " 23013/50000: episode: 4759, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 14244.790039, mae: 548.214884, accuracy: 0.239583, mean_q: -40.776526, mean_eps: 0.100000\n",
      " 23016/50000: episode: 4760, duration: 0.012s, episode steps:   3, steps per second: 253, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 11570.079102, mae: 548.126119, accuracy: 0.239583, mean_q: -3.728485, mean_eps: 0.100000\n",
      " 23019/50000: episode: 4761, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 16411.671224, mae: 556.459269, accuracy: 0.208333, mean_q: -56.258367, mean_eps: 0.100000\n",
      " 23022/50000: episode: 4762, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 12304.584961, mae: 537.855042, accuracy: 0.260417, mean_q: -56.660570, mean_eps: 0.100000\n",
      " 23025/50000: episode: 4763, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 12860.754232, mae: 540.654032, accuracy: 0.250000, mean_q: -41.254643, mean_eps: 0.100000\n",
      " 23028/50000: episode: 4764, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 12056.992839, mae: 552.887817, accuracy: 0.312500, mean_q: -62.783629, mean_eps: 0.100000\n",
      " 23032/50000: episode: 4765, duration: 0.015s, episode steps:   4, steps per second: 261, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 13283.229980, mae: 554.171906, accuracy: 0.226562, mean_q: -45.693945, mean_eps: 0.100000\n",
      " 23036/50000: episode: 4766, duration: 0.016s, episode steps:   4, steps per second: 254, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.000 [0.000, 3.000],  loss: 13042.784912, mae: 558.638657, accuracy: 0.257812, mean_q: -23.340585, mean_eps: 0.100000\n",
      " 23039/50000: episode: 4767, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 13959.671875, mae: 548.053996, accuracy: 0.229167, mean_q: -69.852386, mean_eps: 0.100000\n",
      " 23042/50000: episode: 4768, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 13188.179362, mae: 549.181539, accuracy: 0.208333, mean_q: -50.690608, mean_eps: 0.100000\n",
      " 23045/50000: episode: 4769, duration: 0.019s, episode steps:   3, steps per second: 160, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 13298.183919, mae: 543.598104, accuracy: 0.270833, mean_q: -43.791886, mean_eps: 0.100000\n",
      " 23048/50000: episode: 4770, duration: 0.020s, episode steps:   3, steps per second: 151, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 11985.868490, mae: 534.563151, accuracy: 0.291667, mean_q: -31.271414, mean_eps: 0.100000\n",
      " 23052/50000: episode: 4771, duration: 0.018s, episode steps:   4, steps per second: 228, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 12200.757324, mae: 546.767830, accuracy: 0.312500, mean_q: -35.963255, mean_eps: 0.100000\n",
      " 23055/50000: episode: 4772, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 12252.716797, mae: 550.393005, accuracy: 0.312500, mean_q: -58.881392, mean_eps: 0.100000\n",
      " 23058/50000: episode: 4773, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 12707.745117, mae: 554.285156, accuracy: 0.229167, mean_q: -44.642770, mean_eps: 0.100000\n",
      " 23061/50000: episode: 4774, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 13520.783529, mae: 552.388509, accuracy: 0.322917, mean_q: -57.122813, mean_eps: 0.100000\n",
      " 23064/50000: episode: 4775, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 14915.349609, mae: 547.542562, accuracy: 0.166667, mean_q: -34.268898, mean_eps: 0.100000\n",
      " 23067/50000: episode: 4776, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 16836.005208, mae: 562.875346, accuracy: 0.197917, mean_q: -39.954903, mean_eps: 0.100000\n",
      " 23070/50000: episode: 4777, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 13256.744141, mae: 541.995829, accuracy: 0.302083, mean_q: -42.754562, mean_eps: 0.100000\n",
      " 23073/50000: episode: 4778, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 13430.855794, mae: 546.445089, accuracy: 0.239583, mean_q: -51.014707, mean_eps: 0.100000\n",
      " 23076/50000: episode: 4779, duration: 0.012s, episode steps:   3, steps per second: 257, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 14889.531250, mae: 545.933309, accuracy: 0.197917, mean_q: -77.081462, mean_eps: 0.100000\n",
      " 23080/50000: episode: 4780, duration: 0.015s, episode steps:   4, steps per second: 262, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.750 [0.000, 3.000],  loss: 15154.023926, mae: 550.487427, accuracy: 0.195312, mean_q: -65.613645, mean_eps: 0.100000\n",
      " 23083/50000: episode: 4781, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 14965.438477, mae: 556.563436, accuracy: 0.239583, mean_q: -57.655964, mean_eps: 0.100000\n",
      " 23086/50000: episode: 4782, duration: 0.013s, episode steps:   3, steps per second: 227, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 13953.724609, mae: 545.617188, accuracy: 0.270833, mean_q: -55.835381, mean_eps: 0.100000\n",
      " 23089/50000: episode: 4783, duration: 0.013s, episode steps:   3, steps per second: 223, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 13805.596029, mae: 537.697734, accuracy: 0.343750, mean_q: -97.052180, mean_eps: 0.100000\n",
      " 23093/50000: episode: 4784, duration: 0.019s, episode steps:   4, steps per second: 211, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 12686.412842, mae: 532.464874, accuracy: 0.312500, mean_q: -20.544950, mean_eps: 0.100000\n",
      " 23096/50000: episode: 4785, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.667 [0.000, 3.000],  loss: 12026.643555, mae: 527.189880, accuracy: 0.322917, mean_q: -45.883996, mean_eps: 0.100000\n",
      " 23099/50000: episode: 4786, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 11402.031250, mae: 528.648071, accuracy: 0.208333, mean_q: -31.636317, mean_eps: 0.100000\n",
      " 23102/50000: episode: 4787, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 13853.478516, mae: 541.548462, accuracy: 0.260417, mean_q: -42.674665, mean_eps: 0.100000\n",
      " 23105/50000: episode: 4788, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 14403.875000, mae: 561.449646, accuracy: 0.291667, mean_q: -42.382984, mean_eps: 0.100000\n",
      " 23109/50000: episode: 4789, duration: 0.017s, episode steps:   4, steps per second: 240, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 14038.757080, mae: 543.979660, accuracy: 0.148438, mean_q: -41.578726, mean_eps: 0.100000\n",
      " 23112/50000: episode: 4790, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 13966.628255, mae: 557.616842, accuracy: 0.322917, mean_q: -61.856719, mean_eps: 0.100000\n",
      " 23115/50000: episode: 4791, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 12953.454753, mae: 546.857239, accuracy: 0.208333, mean_q: -51.914381, mean_eps: 0.100000\n",
      " 23118/50000: episode: 4792, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 13585.083659, mae: 540.742879, accuracy: 0.312500, mean_q: -61.125743, mean_eps: 0.100000\n",
      " 23121/50000: episode: 4793, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 16356.416667, mae: 542.341960, accuracy: 0.260417, mean_q: -71.034899, mean_eps: 0.100000\n",
      " 23124/50000: episode: 4794, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 13426.828776, mae: 543.394104, accuracy: 0.197917, mean_q: -39.823341, mean_eps: 0.100000\n",
      " 23127/50000: episode: 4795, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 15130.938477, mae: 539.816528, accuracy: 0.197917, mean_q: -61.843562, mean_eps: 0.100000\n",
      " 23131/50000: episode: 4796, duration: 0.015s, episode steps:   4, steps per second: 263, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 14272.379883, mae: 557.146759, accuracy: 0.226562, mean_q: -24.678837, mean_eps: 0.100000\n",
      " 23134/50000: episode: 4797, duration: 0.014s, episode steps:   3, steps per second: 222, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 13335.878255, mae: 530.236959, accuracy: 0.218750, mean_q: -52.926276, mean_eps: 0.100000\n",
      " 23138/50000: episode: 4798, duration: 0.016s, episode steps:   4, steps per second: 246, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 14585.697510, mae: 544.040405, accuracy: 0.289062, mean_q: -43.751503, mean_eps: 0.100000\n",
      " 23141/50000: episode: 4799, duration: 0.014s, episode steps:   3, steps per second: 207, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 14821.048177, mae: 549.005127, accuracy: 0.218750, mean_q: -46.529915, mean_eps: 0.100000\n",
      " 23144/50000: episode: 4800, duration: 0.014s, episode steps:   3, steps per second: 215, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 14830.994466, mae: 564.850667, accuracy: 0.187500, mean_q: -39.586624, mean_eps: 0.100000\n",
      " 23147/50000: episode: 4801, duration: 0.013s, episode steps:   3, steps per second: 225, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 13778.950846, mae: 557.013631, accuracy: 0.281250, mean_q: -32.403418, mean_eps: 0.100000\n",
      " 23150/50000: episode: 4802, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 13568.919922, mae: 544.107320, accuracy: 0.270833, mean_q: -29.261308, mean_eps: 0.100000\n",
      " 23153/50000: episode: 4803, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 13640.572917, mae: 526.347361, accuracy: 0.218750, mean_q: -58.069934, mean_eps: 0.100000\n",
      " 23156/50000: episode: 4804, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 13431.860677, mae: 532.667948, accuracy: 0.270833, mean_q: -32.586395, mean_eps: 0.100000\n",
      " 23159/50000: episode: 4805, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 15514.668294, mae: 554.533997, accuracy: 0.291667, mean_q: -56.006229, mean_eps: 0.100000\n",
      " 23163/50000: episode: 4806, duration: 0.016s, episode steps:   4, steps per second: 255, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 13777.477295, mae: 545.381500, accuracy: 0.226562, mean_q: -41.867092, mean_eps: 0.100000\n",
      " 23166/50000: episode: 4807, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 13833.157552, mae: 526.869181, accuracy: 0.239583, mean_q: -46.722303, mean_eps: 0.100000\n",
      " 23169/50000: episode: 4808, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 13517.022461, mae: 531.841715, accuracy: 0.239583, mean_q: -24.823484, mean_eps: 0.100000\n",
      " 23172/50000: episode: 4809, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 13355.056315, mae: 537.410339, accuracy: 0.250000, mean_q: -41.601226, mean_eps: 0.100000\n",
      " 23175/50000: episode: 4810, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 14222.143880, mae: 559.952433, accuracy: 0.302083, mean_q: -45.601189, mean_eps: 0.100000\n",
      " 23178/50000: episode: 4811, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 14087.127930, mae: 532.990824, accuracy: 0.270833, mean_q: -52.428289, mean_eps: 0.100000\n",
      " 23182/50000: episode: 4812, duration: 0.016s, episode steps:   4, steps per second: 253, episode reward: -1223.000, mean reward: -305.750 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 16220.745361, mae: 548.627213, accuracy: 0.210938, mean_q: -49.424746, mean_eps: 0.100000\n",
      " 23185/50000: episode: 4813, duration: 0.013s, episode steps:   3, steps per second: 227, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 13373.867188, mae: 545.442830, accuracy: 0.260417, mean_q: -31.377249, mean_eps: 0.100000\n",
      " 23188/50000: episode: 4814, duration: 0.013s, episode steps:   3, steps per second: 222, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 14876.230143, mae: 547.245402, accuracy: 0.229167, mean_q: -47.572837, mean_eps: 0.100000\n",
      " 23191/50000: episode: 4815, duration: 0.032s, episode steps:   3, steps per second:  95, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 11247.014323, mae: 511.555898, accuracy: 0.250000, mean_q: -46.518330, mean_eps: 0.100000\n",
      " 23194/50000: episode: 4816, duration: 0.016s, episode steps:   3, steps per second: 187, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 1.667 [0.000, 3.000],  loss: 14863.936849, mae: 525.019999, accuracy: 0.218750, mean_q: -42.728991, mean_eps: 0.100000\n",
      " 23197/50000: episode: 4817, duration: 0.013s, episode steps:   3, steps per second: 230, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 15298.362630, mae: 543.316650, accuracy: 0.229167, mean_q: -33.014561, mean_eps: 0.100000\n",
      " 23200/50000: episode: 4818, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 13263.464844, mae: 532.147929, accuracy: 0.281250, mean_q: -39.321022, mean_eps: 0.100000\n",
      " 23204/50000: episode: 4819, duration: 0.016s, episode steps:   4, steps per second: 249, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 11764.885254, mae: 543.746643, accuracy: 0.257812, mean_q: -7.777467, mean_eps: 0.100000\n",
      " 23207/50000: episode: 4820, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 14404.548177, mae: 547.802104, accuracy: 0.218750, mean_q: -10.228308, mean_eps: 0.100000\n",
      " 23210/50000: episode: 4821, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 14882.229818, mae: 557.608480, accuracy: 0.218750, mean_q: -3.873058, mean_eps: 0.100000\n",
      " 23213/50000: episode: 4822, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 13462.046224, mae: 567.378967, accuracy: 0.197917, mean_q: -20.225253, mean_eps: 0.100000\n",
      " 23216/50000: episode: 4823, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 12450.508138, mae: 545.892721, accuracy: 0.333333, mean_q: -33.659273, mean_eps: 0.100000\n",
      " 23219/50000: episode: 4824, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 17482.424479, mae: 563.456543, accuracy: 0.239583, mean_q: -76.526910, mean_eps: 0.100000\n",
      " 23222/50000: episode: 4825, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 17710.048177, mae: 580.842794, accuracy: 0.218750, mean_q: -68.950940, mean_eps: 0.100000\n",
      " 23225/50000: episode: 4826, duration: 0.013s, episode steps:   3, steps per second: 226, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 12828.988607, mae: 550.742493, accuracy: 0.260417, mean_q: -43.152517, mean_eps: 0.100000\n",
      " 23228/50000: episode: 4827, duration: 0.013s, episode steps:   3, steps per second: 230, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 13270.371419, mae: 550.964600, accuracy: 0.208333, mean_q: -46.706991, mean_eps: 0.100000\n",
      " 23231/50000: episode: 4828, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 17770.668620, mae: 563.667969, accuracy: 0.187500, mean_q: -49.935000, mean_eps: 0.100000\n",
      " 23234/50000: episode: 4829, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 12716.166667, mae: 561.701314, accuracy: 0.177083, mean_q: -36.321488, mean_eps: 0.100000\n",
      " 23237/50000: episode: 4830, duration: 0.015s, episode steps:   3, steps per second: 200, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 13265.826497, mae: 539.545878, accuracy: 0.239583, mean_q: 1.362165, mean_eps: 0.100000\n",
      " 23241/50000: episode: 4831, duration: 0.018s, episode steps:   4, steps per second: 226, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 13174.303223, mae: 537.133247, accuracy: 0.367188, mean_q: -42.737510, mean_eps: 0.100000\n",
      " 23244/50000: episode: 4832, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 12191.856445, mae: 537.246826, accuracy: 0.302083, mean_q: -44.707749, mean_eps: 0.100000\n",
      " 23248/50000: episode: 4833, duration: 0.015s, episode steps:   4, steps per second: 260, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 14008.344971, mae: 550.729584, accuracy: 0.304688, mean_q: -51.462403, mean_eps: 0.100000\n",
      " 23251/50000: episode: 4834, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 13463.941732, mae: 530.514791, accuracy: 0.270833, mean_q: -36.118287, mean_eps: 0.100000\n",
      " 23254/50000: episode: 4835, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 13695.327148, mae: 550.539510, accuracy: 0.291667, mean_q: -33.933491, mean_eps: 0.100000\n",
      " 23257/50000: episode: 4836, duration: 0.012s, episode steps:   3, steps per second: 253, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 13510.677409, mae: 547.501221, accuracy: 0.208333, mean_q: -55.987888, mean_eps: 0.100000\n",
      " 23260/50000: episode: 4837, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 14906.596354, mae: 549.125651, accuracy: 0.229167, mean_q: -57.635414, mean_eps: 0.100000\n",
      " 23263/50000: episode: 4838, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 12619.724284, mae: 540.461650, accuracy: 0.312500, mean_q: -33.065964, mean_eps: 0.100000\n",
      " 23266/50000: episode: 4839, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 13143.575521, mae: 543.308350, accuracy: 0.281250, mean_q: -27.824942, mean_eps: 0.100000\n",
      " 23269/50000: episode: 4840, duration: 0.012s, episode steps:   3, steps per second: 254, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 14561.507487, mae: 550.835754, accuracy: 0.218750, mean_q: -59.481647, mean_eps: 0.100000\n",
      " 23272/50000: episode: 4841, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 15232.859375, mae: 564.637695, accuracy: 0.281250, mean_q: -53.096789, mean_eps: 0.100000\n",
      " 23275/50000: episode: 4842, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 17238.513672, mae: 575.910706, accuracy: 0.218750, mean_q: -57.042667, mean_eps: 0.100000\n",
      " 23278/50000: episode: 4843, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 14769.774740, mae: 541.829936, accuracy: 0.156250, mean_q: -64.051395, mean_eps: 0.100000\n",
      " 23281/50000: episode: 4844, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 13460.480143, mae: 551.784546, accuracy: 0.281250, mean_q: -41.153181, mean_eps: 0.100000\n",
      " 23284/50000: episode: 4845, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 12978.191732, mae: 533.104411, accuracy: 0.229167, mean_q: -31.361341, mean_eps: 0.100000\n",
      " 23287/50000: episode: 4846, duration: 0.018s, episode steps:   3, steps per second: 171, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 14868.430990, mae: 530.957947, accuracy: 0.239583, mean_q: -48.720438, mean_eps: 0.100000\n",
      " 23290/50000: episode: 4847, duration: 0.014s, episode steps:   3, steps per second: 208, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 14902.210286, mae: 546.722880, accuracy: 0.250000, mean_q: -32.850739, mean_eps: 0.100000\n",
      " 23293/50000: episode: 4848, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 14178.926432, mae: 546.993225, accuracy: 0.250000, mean_q: -46.267352, mean_eps: 0.100000\n",
      " 23296/50000: episode: 4849, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 12910.958333, mae: 543.782430, accuracy: 0.270833, mean_q: -34.168846, mean_eps: 0.100000\n",
      " 23299/50000: episode: 4850, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 14125.949544, mae: 548.068756, accuracy: 0.270833, mean_q: -24.119508, mean_eps: 0.100000\n",
      " 23302/50000: episode: 4851, duration: 0.012s, episode steps:   3, steps per second: 253, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 12963.288411, mae: 541.879883, accuracy: 0.270833, mean_q: -31.676144, mean_eps: 0.100000\n",
      " 23305/50000: episode: 4852, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 14690.155924, mae: 569.444519, accuracy: 0.250000, mean_q: -11.829105, mean_eps: 0.100000\n",
      " 23309/50000: episode: 4853, duration: 0.015s, episode steps:   4, steps per second: 260, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 15445.567383, mae: 551.629868, accuracy: 0.226562, mean_q: -59.526669, mean_eps: 0.100000\n",
      " 23312/50000: episode: 4854, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 18103.310547, mae: 561.305257, accuracy: 0.239583, mean_q: -48.171948, mean_eps: 0.100000\n",
      " 23316/50000: episode: 4855, duration: 0.016s, episode steps:   4, steps per second: 257, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 14930.530273, mae: 550.800949, accuracy: 0.265625, mean_q: -36.628118, mean_eps: 0.100000\n",
      " 23320/50000: episode: 4856, duration: 0.016s, episode steps:   4, steps per second: 257, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 2.250 [1.000, 3.000],  loss: 16531.103760, mae: 558.863510, accuracy: 0.179688, mean_q: -53.137488, mean_eps: 0.100000\n",
      " 23323/50000: episode: 4857, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 13077.865885, mae: 555.164632, accuracy: 0.281250, mean_q: -34.720168, mean_eps: 0.100000\n",
      " 23327/50000: episode: 4858, duration: 0.015s, episode steps:   4, steps per second: 261, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 2.250 [1.000, 3.000],  loss: 13801.653320, mae: 552.066986, accuracy: 0.187500, mean_q: -64.487090, mean_eps: 0.100000\n",
      " 23330/50000: episode: 4859, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 14363.683594, mae: 553.265096, accuracy: 0.218750, mean_q: -44.914227, mean_eps: 0.100000\n",
      " 23333/50000: episode: 4860, duration: 0.014s, episode steps:   3, steps per second: 215, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 11469.038411, mae: 517.386007, accuracy: 0.260417, mean_q: -28.234486, mean_eps: 0.100000\n",
      " 23336/50000: episode: 4861, duration: 0.018s, episode steps:   3, steps per second: 171, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 14068.933594, mae: 568.551615, accuracy: 0.218750, mean_q: -19.961677, mean_eps: 0.100000\n",
      " 23339/50000: episode: 4862, duration: 0.016s, episode steps:   3, steps per second: 192, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 13127.164388, mae: 548.604106, accuracy: 0.156250, mean_q: -26.024491, mean_eps: 0.100000\n",
      " 23343/50000: episode: 4863, duration: 0.016s, episode steps:   4, steps per second: 257, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 12610.880859, mae: 544.139969, accuracy: 0.156250, mean_q: -49.607324, mean_eps: 0.100000\n",
      " 23346/50000: episode: 4864, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 13455.295573, mae: 539.346212, accuracy: 0.270833, mean_q: -28.970755, mean_eps: 0.100000\n",
      " 23349/50000: episode: 4865, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 13766.455078, mae: 551.042480, accuracy: 0.302083, mean_q: -47.939194, mean_eps: 0.100000\n",
      " 23353/50000: episode: 4866, duration: 0.016s, episode steps:   4, steps per second: 250, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 13474.192871, mae: 547.311829, accuracy: 0.359375, mean_q: -46.332907, mean_eps: 0.100000\n",
      " 23356/50000: episode: 4867, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 12988.132812, mae: 559.326945, accuracy: 0.229167, mean_q: -35.155079, mean_eps: 0.100000\n",
      " 23359/50000: episode: 4868, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 13565.031250, mae: 548.806641, accuracy: 0.281250, mean_q: -47.238360, mean_eps: 0.100000\n",
      " 23362/50000: episode: 4869, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 13749.092448, mae: 539.357137, accuracy: 0.281250, mean_q: -40.820684, mean_eps: 0.100000\n",
      " 23365/50000: episode: 4870, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 13715.264648, mae: 540.105794, accuracy: 0.239583, mean_q: -39.294637, mean_eps: 0.100000\n",
      " 23368/50000: episode: 4871, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 14504.012695, mae: 538.902934, accuracy: 0.218750, mean_q: -33.467153, mean_eps: 0.100000\n",
      " 23371/50000: episode: 4872, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 13064.385742, mae: 535.059285, accuracy: 0.229167, mean_q: -24.460605, mean_eps: 0.100000\n",
      " 23375/50000: episode: 4873, duration: 0.016s, episode steps:   4, steps per second: 254, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 14303.640381, mae: 548.850510, accuracy: 0.382812, mean_q: -39.454330, mean_eps: 0.100000\n",
      " 23378/50000: episode: 4874, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 13130.093424, mae: 540.213857, accuracy: 0.250000, mean_q: -16.218741, mean_eps: 0.100000\n",
      " 23381/50000: episode: 4875, duration: 0.012s, episode steps:   3, steps per second: 255, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 12321.163411, mae: 539.839783, accuracy: 0.322917, mean_q: -44.167905, mean_eps: 0.100000\n",
      " 23384/50000: episode: 4876, duration: 0.014s, episode steps:   3, steps per second: 208, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 14366.374023, mae: 551.234253, accuracy: 0.260417, mean_q: -24.034803, mean_eps: 0.100000\n",
      " 23387/50000: episode: 4877, duration: 0.018s, episode steps:   3, steps per second: 169, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 14141.375977, mae: 543.799540, accuracy: 0.218750, mean_q: -10.660028, mean_eps: 0.100000\n",
      " 23390/50000: episode: 4878, duration: 0.014s, episode steps:   3, steps per second: 209, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 13919.996745, mae: 535.304708, accuracy: 0.281250, mean_q: -44.818499, mean_eps: 0.100000\n",
      " 23393/50000: episode: 4879, duration: 0.014s, episode steps:   3, steps per second: 220, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 14401.956380, mae: 537.983704, accuracy: 0.312500, mean_q: -0.361981, mean_eps: 0.100000\n",
      " 23396/50000: episode: 4880, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 14531.682943, mae: 542.879049, accuracy: 0.270833, mean_q: -23.755251, mean_eps: 0.100000\n",
      " 23399/50000: episode: 4881, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 14645.550456, mae: 532.995667, accuracy: 0.260417, mean_q: -23.358651, mean_eps: 0.100000\n",
      " 23402/50000: episode: 4882, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 15258.178711, mae: 548.355937, accuracy: 0.187500, mean_q: -51.286304, mean_eps: 0.100000\n",
      " 23405/50000: episode: 4883, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 13348.879883, mae: 535.277364, accuracy: 0.312500, mean_q: -44.154209, mean_eps: 0.100000\n",
      " 23408/50000: episode: 4884, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 14909.111328, mae: 558.922506, accuracy: 0.239583, mean_q: -43.822872, mean_eps: 0.100000\n",
      " 23411/50000: episode: 4885, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 13979.855794, mae: 545.342997, accuracy: 0.250000, mean_q: -33.534531, mean_eps: 0.100000\n",
      " 23414/50000: episode: 4886, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 15651.323242, mae: 551.991170, accuracy: 0.229167, mean_q: -45.248334, mean_eps: 0.100000\n",
      " 23417/50000: episode: 4887, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 15954.412760, mae: 540.561564, accuracy: 0.260417, mean_q: -63.051932, mean_eps: 0.100000\n",
      " 23420/50000: episode: 4888, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 16212.582682, mae: 557.598918, accuracy: 0.208333, mean_q: -42.317652, mean_eps: 0.100000\n",
      " 23423/50000: episode: 4889, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 14160.252604, mae: 540.463460, accuracy: 0.197917, mean_q: -22.069355, mean_eps: 0.100000\n",
      " 23426/50000: episode: 4890, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 14666.975911, mae: 548.980408, accuracy: 0.270833, mean_q: -35.604418, mean_eps: 0.100000\n",
      " 23429/50000: episode: 4891, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 15670.060221, mae: 550.682780, accuracy: 0.229167, mean_q: -2.324471, mean_eps: 0.100000\n",
      " 23432/50000: episode: 4892, duration: 0.015s, episode steps:   3, steps per second: 204, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 14032.586263, mae: 541.008097, accuracy: 0.239583, mean_q: -37.700291, mean_eps: 0.100000\n",
      " 23436/50000: episode: 4893, duration: 0.019s, episode steps:   4, steps per second: 216, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 13649.592041, mae: 538.765198, accuracy: 0.250000, mean_q: -32.971982, mean_eps: 0.100000\n",
      " 23439/50000: episode: 4894, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 14779.738607, mae: 547.547221, accuracy: 0.302083, mean_q: -20.368661, mean_eps: 0.100000\n",
      " 23442/50000: episode: 4895, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 16537.092773, mae: 567.096049, accuracy: 0.260417, mean_q: -12.766965, mean_eps: 0.100000\n",
      " 23445/50000: episode: 4896, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 13751.824870, mae: 533.747274, accuracy: 0.312500, mean_q: -30.643714, mean_eps: 0.100000\n",
      " 23448/50000: episode: 4897, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 13232.520833, mae: 523.627401, accuracy: 0.260417, mean_q: -61.978104, mean_eps: 0.100000\n",
      " 23452/50000: episode: 4898, duration: 0.015s, episode steps:   4, steps per second: 265, episode reward: -1238.000, mean reward: -309.500 [-999.000, -60.000], mean action: 2.000 [0.000, 3.000],  loss: 13094.874268, mae: 531.738388, accuracy: 0.242188, mean_q: -29.754779, mean_eps: 0.100000\n",
      " 23455/50000: episode: 4899, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 16196.050781, mae: 565.221395, accuracy: 0.250000, mean_q: -42.050266, mean_eps: 0.100000\n",
      " 23458/50000: episode: 4900, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 13399.569336, mae: 541.080627, accuracy: 0.250000, mean_q: -12.944390, mean_eps: 0.100000\n",
      " 23461/50000: episode: 4901, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 14304.768555, mae: 551.724080, accuracy: 0.145833, mean_q: -38.015933, mean_eps: 0.100000\n",
      " 23464/50000: episode: 4902, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 14314.436198, mae: 536.575378, accuracy: 0.270833, mean_q: -53.108190, mean_eps: 0.100000\n",
      " 23467/50000: episode: 4903, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 14227.709310, mae: 545.510050, accuracy: 0.270833, mean_q: -61.866186, mean_eps: 0.100000\n",
      " 23470/50000: episode: 4904, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 12867.065755, mae: 546.039897, accuracy: 0.270833, mean_q: -41.211429, mean_eps: 0.100000\n",
      " 23473/50000: episode: 4905, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 14133.176432, mae: 540.016479, accuracy: 0.260417, mean_q: -51.465519, mean_eps: 0.100000\n",
      " 23476/50000: episode: 4906, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 12934.973307, mae: 540.323385, accuracy: 0.260417, mean_q: -21.041866, mean_eps: 0.100000\n",
      " 23479/50000: episode: 4907, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 11349.954102, mae: 533.272502, accuracy: 0.218750, mean_q: -25.651681, mean_eps: 0.100000\n",
      " 23483/50000: episode: 4908, duration: 0.028s, episode steps:   4, steps per second: 141, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 14442.058594, mae: 534.138916, accuracy: 0.234375, mean_q: -28.122642, mean_eps: 0.100000\n",
      " 23486/50000: episode: 4909, duration: 0.023s, episode steps:   3, steps per second: 133, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 17026.203776, mae: 561.946208, accuracy: 0.156250, mean_q: -26.389783, mean_eps: 0.100000\n",
      " 23489/50000: episode: 4910, duration: 0.014s, episode steps:   3, steps per second: 212, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 13997.844727, mae: 539.283895, accuracy: 0.270833, mean_q: -21.006593, mean_eps: 0.100000\n",
      " 23492/50000: episode: 4911, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 13080.281901, mae: 519.423940, accuracy: 0.250000, mean_q: -59.095062, mean_eps: 0.100000\n",
      " 23495/50000: episode: 4912, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 14481.996419, mae: 543.029989, accuracy: 0.312500, mean_q: -34.983399, mean_eps: 0.100000\n",
      " 23498/50000: episode: 4913, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 12929.370117, mae: 522.917999, accuracy: 0.239583, mean_q: -21.688941, mean_eps: 0.100000\n",
      " 23501/50000: episode: 4914, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 14376.777995, mae: 535.897847, accuracy: 0.333333, mean_q: -15.377260, mean_eps: 0.100000\n",
      " 23504/50000: episode: 4915, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 14608.285482, mae: 554.627441, accuracy: 0.260417, mean_q: -12.976662, mean_eps: 0.100000\n",
      " 23507/50000: episode: 4916, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 14585.494792, mae: 551.634359, accuracy: 0.187500, mean_q: -33.844671, mean_eps: 0.100000\n",
      " 23510/50000: episode: 4917, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 16022.973958, mae: 566.187276, accuracy: 0.250000, mean_q: -60.732763, mean_eps: 0.100000\n",
      " 23513/50000: episode: 4918, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 14901.678711, mae: 545.011922, accuracy: 0.312500, mean_q: -45.419564, mean_eps: 0.100000\n",
      " 23516/50000: episode: 4919, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 13922.521484, mae: 554.468648, accuracy: 0.208333, mean_q: -75.197533, mean_eps: 0.100000\n",
      " 23519/50000: episode: 4920, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 14901.056315, mae: 561.324015, accuracy: 0.260417, mean_q: -32.822025, mean_eps: 0.100000\n",
      " 23522/50000: episode: 4921, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 13103.870117, mae: 541.727580, accuracy: 0.156250, mean_q: -47.170945, mean_eps: 0.100000\n",
      " 23526/50000: episode: 4922, duration: 0.015s, episode steps:   4, steps per second: 263, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 12522.175781, mae: 536.845291, accuracy: 0.234375, mean_q: -4.012363, mean_eps: 0.100000\n",
      " 23530/50000: episode: 4923, duration: 0.016s, episode steps:   4, steps per second: 245, episode reward: -1223.000, mean reward: -305.750 [-999.000, -58.000], mean action: 1.250 [0.000, 3.000],  loss: 13746.446777, mae: 548.120071, accuracy: 0.265625, mean_q: -36.548225, mean_eps: 0.100000\n",
      " 23533/50000: episode: 4924, duration: 0.015s, episode steps:   3, steps per second: 198, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 13991.068685, mae: 531.685425, accuracy: 0.156250, mean_q: -30.751212, mean_eps: 0.100000\n",
      " 23536/50000: episode: 4925, duration: 0.015s, episode steps:   3, steps per second: 199, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 15925.974284, mae: 537.123210, accuracy: 0.208333, mean_q: -47.123142, mean_eps: 0.100000\n",
      " 23539/50000: episode: 4926, duration: 0.014s, episode steps:   3, steps per second: 221, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 14118.350911, mae: 556.236287, accuracy: 0.270833, mean_q: -53.035253, mean_eps: 0.100000\n",
      " 23542/50000: episode: 4927, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 14983.468424, mae: 533.075358, accuracy: 0.270833, mean_q: -51.362165, mean_eps: 0.100000\n",
      " 23545/50000: episode: 4928, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 13612.514323, mae: 540.491638, accuracy: 0.218750, mean_q: -27.366585, mean_eps: 0.100000\n",
      " 23548/50000: episode: 4929, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 14324.462240, mae: 552.142578, accuracy: 0.239583, mean_q: -38.121548, mean_eps: 0.100000\n",
      " 23551/50000: episode: 4930, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 16117.310547, mae: 549.964030, accuracy: 0.312500, mean_q: -38.335711, mean_eps: 0.100000\n",
      " 23554/50000: episode: 4931, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 11614.433919, mae: 547.812398, accuracy: 0.260417, mean_q: -3.312425, mean_eps: 0.100000\n",
      " 23558/50000: episode: 4932, duration: 0.015s, episode steps:   4, steps per second: 262, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 14088.349609, mae: 558.327209, accuracy: 0.296875, mean_q: -35.528197, mean_eps: 0.100000\n",
      " 23561/50000: episode: 4933, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 13496.330078, mae: 535.344177, accuracy: 0.302083, mean_q: -61.275750, mean_eps: 0.100000\n",
      " 23564/50000: episode: 4934, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 13906.222656, mae: 544.242574, accuracy: 0.312500, mean_q: -49.573620, mean_eps: 0.100000\n",
      " 23567/50000: episode: 4935, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 14136.722331, mae: 539.637370, accuracy: 0.239583, mean_q: -15.377568, mean_eps: 0.100000\n",
      " 23570/50000: episode: 4936, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 13254.386719, mae: 537.383219, accuracy: 0.250000, mean_q: -45.876872, mean_eps: 0.100000\n",
      " 23573/50000: episode: 4937, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 13317.805339, mae: 546.499146, accuracy: 0.260417, mean_q: -45.743284, mean_eps: 0.100000\n",
      " 23576/50000: episode: 4938, duration: 0.019s, episode steps:   3, steps per second: 159, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.333 [0.000, 3.000],  loss: 15409.061849, mae: 548.555216, accuracy: 0.239583, mean_q: -60.935933, mean_eps: 0.100000\n",
      " 23579/50000: episode: 4939, duration: 0.015s, episode steps:   3, steps per second: 205, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 14235.817708, mae: 546.373962, accuracy: 0.354167, mean_q: -70.332057, mean_eps: 0.100000\n",
      " 23582/50000: episode: 4940, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 14875.089844, mae: 556.911031, accuracy: 0.229167, mean_q: -55.870501, mean_eps: 0.100000\n",
      " 23585/50000: episode: 4941, duration: 0.014s, episode steps:   3, steps per second: 219, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 12721.657878, mae: 532.982279, accuracy: 0.385417, mean_q: -42.599332, mean_eps: 0.100000\n",
      " 23588/50000: episode: 4942, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 13411.315755, mae: 548.512858, accuracy: 0.270833, mean_q: -33.199614, mean_eps: 0.100000\n",
      " 23591/50000: episode: 4943, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 15108.344076, mae: 548.486226, accuracy: 0.229167, mean_q: -28.993638, mean_eps: 0.100000\n",
      " 23594/50000: episode: 4944, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.667 [0.000, 3.000],  loss: 14347.326497, mae: 540.817159, accuracy: 0.302083, mean_q: -43.587434, mean_eps: 0.100000\n",
      " 23597/50000: episode: 4945, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 15669.311523, mae: 551.263163, accuracy: 0.229167, mean_q: -31.166305, mean_eps: 0.100000\n",
      " 23600/50000: episode: 4946, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 12083.468099, mae: 538.087565, accuracy: 0.270833, mean_q: -18.096537, mean_eps: 0.100000\n",
      " 23603/50000: episode: 4947, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 13281.767904, mae: 545.266113, accuracy: 0.270833, mean_q: -39.861635, mean_eps: 0.100000\n",
      " 23606/50000: episode: 4948, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 13820.706055, mae: 550.373088, accuracy: 0.281250, mean_q: -46.157571, mean_eps: 0.100000\n",
      " 23609/50000: episode: 4949, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 13874.520182, mae: 547.016947, accuracy: 0.250000, mean_q: -34.342840, mean_eps: 0.100000\n",
      " 23612/50000: episode: 4950, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 10901.640299, mae: 542.270304, accuracy: 0.250000, mean_q: -37.186776, mean_eps: 0.100000\n",
      " 23615/50000: episode: 4951, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 12211.230143, mae: 524.740377, accuracy: 0.208333, mean_q: -25.676113, mean_eps: 0.100000\n",
      " 23618/50000: episode: 4952, duration: 0.014s, episode steps:   3, steps per second: 211, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 14138.448893, mae: 538.225179, accuracy: 0.208333, mean_q: -26.757773, mean_eps: 0.100000\n",
      " 23621/50000: episode: 4953, duration: 0.016s, episode steps:   3, steps per second: 185, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.667 [0.000, 3.000],  loss: 12432.306966, mae: 516.791534, accuracy: 0.250000, mean_q: -16.179666, mean_eps: 0.100000\n",
      " 23624/50000: episode: 4954, duration: 0.015s, episode steps:   3, steps per second: 199, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 13004.305664, mae: 545.200297, accuracy: 0.218750, mean_q: -18.856910, mean_eps: 0.100000\n",
      " 23627/50000: episode: 4955, duration: 0.014s, episode steps:   3, steps per second: 207, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.667 [0.000, 3.000],  loss: 13853.460938, mae: 540.555501, accuracy: 0.187500, mean_q: -59.447379, mean_eps: 0.100000\n",
      " 23631/50000: episode: 4956, duration: 0.016s, episode steps:   4, steps per second: 247, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 2.250 [1.000, 3.000],  loss: 15740.245605, mae: 555.636551, accuracy: 0.210938, mean_q: -2.657927, mean_eps: 0.100000\n",
      " 23634/50000: episode: 4957, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 15548.857422, mae: 530.306508, accuracy: 0.208333, mean_q: -55.790793, mean_eps: 0.100000\n",
      " 23637/50000: episode: 4958, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 14012.170573, mae: 554.905599, accuracy: 0.270833, mean_q: -24.677013, mean_eps: 0.100000\n",
      " 23640/50000: episode: 4959, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 12623.230794, mae: 542.198832, accuracy: 0.354167, mean_q: -27.231172, mean_eps: 0.100000\n",
      " 23643/50000: episode: 4960, duration: 0.012s, episode steps:   3, steps per second: 255, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 15077.699544, mae: 552.730916, accuracy: 0.343750, mean_q: -47.760117, mean_eps: 0.100000\n",
      " 23647/50000: episode: 4961, duration: 0.015s, episode steps:   4, steps per second: 260, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 11530.201904, mae: 538.775375, accuracy: 0.250000, mean_q: -54.840168, mean_eps: 0.100000\n",
      " 23650/50000: episode: 4962, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 12476.461263, mae: 553.496012, accuracy: 0.229167, mean_q: -52.622108, mean_eps: 0.100000\n",
      " 23653/50000: episode: 4963, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 14883.873698, mae: 552.936198, accuracy: 0.229167, mean_q: -38.356986, mean_eps: 0.100000\n",
      " 23656/50000: episode: 4964, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 15804.781576, mae: 546.195028, accuracy: 0.281250, mean_q: -19.756344, mean_eps: 0.100000\n",
      " 23659/50000: episode: 4965, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 14303.193359, mae: 549.560628, accuracy: 0.260417, mean_q: -57.083803, mean_eps: 0.100000\n",
      " 23662/50000: episode: 4966, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 14737.256836, mae: 528.447876, accuracy: 0.291667, mean_q: -29.359139, mean_eps: 0.100000\n",
      " 23665/50000: episode: 4967, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 13409.240885, mae: 537.349508, accuracy: 0.312500, mean_q: -31.741777, mean_eps: 0.100000\n",
      " 23668/50000: episode: 4968, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 13174.496419, mae: 549.788656, accuracy: 0.281250, mean_q: -26.108675, mean_eps: 0.100000\n",
      " 23671/50000: episode: 4969, duration: 0.018s, episode steps:   3, steps per second: 163, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 12478.332357, mae: 550.656352, accuracy: 0.229167, mean_q: -1.475400, mean_eps: 0.100000\n",
      " 23674/50000: episode: 4970, duration: 0.015s, episode steps:   3, steps per second: 201, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 12789.036784, mae: 539.622640, accuracy: 0.291667, mean_q: -16.376908, mean_eps: 0.100000\n",
      " 23677/50000: episode: 4971, duration: 0.014s, episode steps:   3, steps per second: 221, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 12599.955078, mae: 532.883443, accuracy: 0.270833, mean_q: -34.058514, mean_eps: 0.100000\n",
      " 23680/50000: episode: 4972, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 13140.146810, mae: 538.871236, accuracy: 0.218750, mean_q: -26.696110, mean_eps: 0.100000\n",
      " 23683/50000: episode: 4973, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 15609.045898, mae: 563.382589, accuracy: 0.197917, mean_q: -37.560494, mean_eps: 0.100000\n",
      " 23686/50000: episode: 4974, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 15035.531901, mae: 548.074748, accuracy: 0.250000, mean_q: -44.153706, mean_eps: 0.100000\n",
      " 23689/50000: episode: 4975, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 13771.323568, mae: 552.413635, accuracy: 0.333333, mean_q: -63.533157, mean_eps: 0.100000\n",
      " 23692/50000: episode: 4976, duration: 0.012s, episode steps:   3, steps per second: 255, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 13086.550456, mae: 561.422404, accuracy: 0.270833, mean_q: -43.495625, mean_eps: 0.100000\n",
      " 23695/50000: episode: 4977, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 13481.389323, mae: 555.287598, accuracy: 0.322917, mean_q: -18.160197, mean_eps: 0.100000\n",
      " 23698/50000: episode: 4978, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 16095.824219, mae: 549.608398, accuracy: 0.364583, mean_q: -57.774333, mean_eps: 0.100000\n",
      " 23701/50000: episode: 4979, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 12121.820312, mae: 538.592753, accuracy: 0.375000, mean_q: -28.639872, mean_eps: 0.100000\n",
      " 23704/50000: episode: 4980, duration: 0.012s, episode steps:   3, steps per second: 254, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 14403.950521, mae: 554.809692, accuracy: 0.281250, mean_q: -72.696367, mean_eps: 0.100000\n",
      " 23707/50000: episode: 4981, duration: 0.013s, episode steps:   3, steps per second: 226, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 14417.866862, mae: 548.488770, accuracy: 0.250000, mean_q: -42.164652, mean_eps: 0.100000\n",
      " 23710/50000: episode: 4982, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 14265.175781, mae: 534.994914, accuracy: 0.250000, mean_q: -51.936673, mean_eps: 0.100000\n",
      " 23713/50000: episode: 4983, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 15566.986328, mae: 552.037577, accuracy: 0.250000, mean_q: -22.817630, mean_eps: 0.100000\n",
      " 23717/50000: episode: 4984, duration: 0.016s, episode steps:   4, steps per second: 253, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 13775.380859, mae: 532.641068, accuracy: 0.335938, mean_q: -31.652916, mean_eps: 0.100000\n",
      " 23720/50000: episode: 4985, duration: 0.014s, episode steps:   3, steps per second: 215, episode reward: -224.000, mean reward: -74.667 [-116.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 13629.160807, mae: 538.030070, accuracy: 0.197917, mean_q: -39.303328, mean_eps: 0.100000\n",
      " 23723/50000: episode: 4986, duration: 0.015s, episode steps:   3, steps per second: 197, episode reward: -224.000, mean reward: -74.667 [-116.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 13809.245117, mae: 549.037557, accuracy: 0.270833, mean_q: -28.510127, mean_eps: 0.100000\n",
      " 23726/50000: episode: 4987, duration: 0.016s, episode steps:   3, steps per second: 193, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 14629.562500, mae: 550.709656, accuracy: 0.239583, mean_q: -44.290273, mean_eps: 0.100000\n",
      " 23729/50000: episode: 4988, duration: 0.018s, episode steps:   3, steps per second: 171, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 14689.673828, mae: 537.877441, accuracy: 0.291667, mean_q: -54.255801, mean_eps: 0.100000\n",
      " 23732/50000: episode: 4989, duration: 0.014s, episode steps:   3, steps per second: 215, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 13644.755534, mae: 544.466736, accuracy: 0.260417, mean_q: -31.221263, mean_eps: 0.100000\n",
      " 23735/50000: episode: 4990, duration: 0.014s, episode steps:   3, steps per second: 208, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 14499.265625, mae: 544.220439, accuracy: 0.187500, mean_q: -75.043177, mean_eps: 0.100000\n",
      " 23738/50000: episode: 4991, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 13565.123047, mae: 549.388672, accuracy: 0.229167, mean_q: -32.076190, mean_eps: 0.100000\n",
      " 23741/50000: episode: 4992, duration: 0.014s, episode steps:   3, steps per second: 222, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 12882.077474, mae: 557.900838, accuracy: 0.291667, mean_q: -42.703003, mean_eps: 0.100000\n",
      " 23744/50000: episode: 4993, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 12613.475911, mae: 535.796021, accuracy: 0.291667, mean_q: -50.893872, mean_eps: 0.100000\n",
      " 23749/50000: episode: 4994, duration: 0.021s, episode steps:   5, steps per second: 238, episode reward: -2193.000, mean reward: -438.600 [-999.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 13557.450586, mae: 547.988123, accuracy: 0.187500, mean_q: -28.647798, mean_eps: 0.100000\n",
      " 23752/50000: episode: 4995, duration: 0.014s, episode steps:   3, steps per second: 216, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 12794.225911, mae: 550.201029, accuracy: 0.312500, mean_q: -51.478315, mean_eps: 0.100000\n",
      " 23755/50000: episode: 4996, duration: 0.014s, episode steps:   3, steps per second: 208, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 12978.867188, mae: 536.130025, accuracy: 0.322917, mean_q: -46.489567, mean_eps: 0.100000\n",
      " 23758/50000: episode: 4997, duration: 0.013s, episode steps:   3, steps per second: 230, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 15033.707357, mae: 549.031637, accuracy: 0.239583, mean_q: -30.733186, mean_eps: 0.100000\n",
      " 23761/50000: episode: 4998, duration: 0.013s, episode steps:   3, steps per second: 224, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 14145.942057, mae: 546.119507, accuracy: 0.343750, mean_q: -37.873313, mean_eps: 0.100000\n",
      " 23764/50000: episode: 4999, duration: 0.015s, episode steps:   3, steps per second: 203, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 15161.869792, mae: 549.423889, accuracy: 0.281250, mean_q: -50.425499, mean_eps: 0.100000\n",
      " 23767/50000: episode: 5000, duration: 0.021s, episode steps:   3, steps per second: 140, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 12375.610677, mae: 545.399780, accuracy: 0.270833, mean_q: -53.472468, mean_eps: 0.100000\n",
      " 23770/50000: episode: 5001, duration: 0.015s, episode steps:   3, steps per second: 194, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 12421.564128, mae: 541.782043, accuracy: 0.281250, mean_q: -42.173680, mean_eps: 0.100000\n",
      " 23773/50000: episode: 5002, duration: 0.013s, episode steps:   3, steps per second: 224, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 13186.459961, mae: 548.683879, accuracy: 0.312500, mean_q: -41.234666, mean_eps: 0.100000\n",
      " 23776/50000: episode: 5003, duration: 0.014s, episode steps:   3, steps per second: 213, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 12782.445964, mae: 561.254354, accuracy: 0.187500, mean_q: -46.839240, mean_eps: 0.100000\n",
      " 23779/50000: episode: 5004, duration: 0.013s, episode steps:   3, steps per second: 225, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 13456.521484, mae: 570.040161, accuracy: 0.187500, mean_q: -27.440924, mean_eps: 0.100000\n",
      " 23782/50000: episode: 5005, duration: 0.014s, episode steps:   3, steps per second: 221, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 14961.149414, mae: 568.974813, accuracy: 0.229167, mean_q: -45.411439, mean_eps: 0.100000\n",
      " 23785/50000: episode: 5006, duration: 0.014s, episode steps:   3, steps per second: 219, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 13506.866211, mae: 540.094523, accuracy: 0.302083, mean_q: -15.480760, mean_eps: 0.100000\n",
      " 23788/50000: episode: 5007, duration: 0.014s, episode steps:   3, steps per second: 212, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 11812.711914, mae: 553.183004, accuracy: 0.281250, mean_q: -62.073420, mean_eps: 0.100000\n",
      " 23792/50000: episode: 5008, duration: 0.017s, episode steps:   4, steps per second: 239, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 14249.467773, mae: 541.596313, accuracy: 0.250000, mean_q: -66.442376, mean_eps: 0.100000\n",
      " 23796/50000: episode: 5009, duration: 0.016s, episode steps:   4, steps per second: 257, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.000 [0.000, 3.000],  loss: 13349.629883, mae: 544.445129, accuracy: 0.273438, mean_q: -45.468269, mean_eps: 0.100000\n",
      " 23799/50000: episode: 5010, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 15002.544922, mae: 555.653422, accuracy: 0.312500, mean_q: -41.508291, mean_eps: 0.100000\n",
      " 23802/50000: episode: 5011, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 13340.303711, mae: 545.444397, accuracy: 0.239583, mean_q: -56.037899, mean_eps: 0.100000\n",
      " 23805/50000: episode: 5012, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 12539.155599, mae: 533.970601, accuracy: 0.260417, mean_q: -43.248880, mean_eps: 0.100000\n",
      " 23808/50000: episode: 5013, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 12666.161458, mae: 542.268921, accuracy: 0.291667, mean_q: -29.416823, mean_eps: 0.100000\n",
      " 23811/50000: episode: 5014, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 14021.200195, mae: 538.339233, accuracy: 0.250000, mean_q: -42.558695, mean_eps: 0.100000\n",
      " 23814/50000: episode: 5015, duration: 0.015s, episode steps:   3, steps per second: 200, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 15046.904297, mae: 550.497355, accuracy: 0.187500, mean_q: -37.308723, mean_eps: 0.100000\n",
      " 23817/50000: episode: 5016, duration: 0.015s, episode steps:   3, steps per second: 205, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 13335.972982, mae: 540.114665, accuracy: 0.312500, mean_q: -43.397893, mean_eps: 0.100000\n",
      " 23820/50000: episode: 5017, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 14555.528320, mae: 538.829854, accuracy: 0.239583, mean_q: -40.325264, mean_eps: 0.100000\n",
      " 23823/50000: episode: 5018, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 14534.761719, mae: 556.908305, accuracy: 0.260417, mean_q: -52.743024, mean_eps: 0.100000\n",
      " 23827/50000: episode: 5019, duration: 0.015s, episode steps:   4, steps per second: 260, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 2.000 [0.000, 3.000],  loss: 13663.601318, mae: 551.225418, accuracy: 0.281250, mean_q: -43.795969, mean_eps: 0.100000\n",
      " 23830/50000: episode: 5020, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 12024.139974, mae: 539.404521, accuracy: 0.239583, mean_q: -42.474162, mean_eps: 0.100000\n",
      " 23833/50000: episode: 5021, duration: 0.015s, episode steps:   3, steps per second: 194, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 12578.061523, mae: 533.987010, accuracy: 0.250000, mean_q: -65.173990, mean_eps: 0.100000\n",
      " 23836/50000: episode: 5022, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 2.000 [1.000, 3.000],  loss: 14810.207682, mae: 546.542155, accuracy: 0.177083, mean_q: -40.668706, mean_eps: 0.100000\n",
      " 23839/50000: episode: 5023, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 12570.151693, mae: 529.921163, accuracy: 0.291667, mean_q: -47.570788, mean_eps: 0.100000\n",
      " 23843/50000: episode: 5024, duration: 0.016s, episode steps:   4, steps per second: 257, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 15303.706055, mae: 543.740425, accuracy: 0.203125, mean_q: -32.181685, mean_eps: 0.100000\n",
      " 23846/50000: episode: 5025, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 14302.952148, mae: 547.344564, accuracy: 0.208333, mean_q: -28.089453, mean_eps: 0.100000\n",
      " 23850/50000: episode: 5026, duration: 0.015s, episode steps:   4, steps per second: 265, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 13322.434082, mae: 537.304581, accuracy: 0.242188, mean_q: -42.485715, mean_eps: 0.100000\n",
      " 23853/50000: episode: 5027, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 12421.775391, mae: 533.601176, accuracy: 0.218750, mean_q: -53.035666, mean_eps: 0.100000\n",
      " 23856/50000: episode: 5028, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 16331.180664, mae: 552.145467, accuracy: 0.260417, mean_q: -49.971402, mean_eps: 0.100000\n",
      " 23859/50000: episode: 5029, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 13124.880534, mae: 549.466390, accuracy: 0.302083, mean_q: -19.532922, mean_eps: 0.100000\n",
      " 23862/50000: episode: 5030, duration: 0.021s, episode steps:   3, steps per second: 141, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 14875.162760, mae: 550.903971, accuracy: 0.135417, mean_q: -54.556345, mean_eps: 0.100000\n",
      " 23865/50000: episode: 5031, duration: 0.020s, episode steps:   3, steps per second: 148, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 12379.868490, mae: 533.932922, accuracy: 0.187500, mean_q: -23.302399, mean_eps: 0.100000\n",
      " 23868/50000: episode: 5032, duration: 0.015s, episode steps:   3, steps per second: 200, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 14783.206055, mae: 538.283020, accuracy: 0.250000, mean_q: -39.089965, mean_eps: 0.100000\n",
      " 23871/50000: episode: 5033, duration: 0.013s, episode steps:   3, steps per second: 227, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 13317.384766, mae: 549.859762, accuracy: 0.260417, mean_q: -47.564725, mean_eps: 0.100000\n",
      " 23874/50000: episode: 5034, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 14809.467448, mae: 543.345439, accuracy: 0.354167, mean_q: -47.969290, mean_eps: 0.100000\n",
      " 23877/50000: episode: 5035, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 12184.805339, mae: 542.665548, accuracy: 0.281250, mean_q: -41.231612, mean_eps: 0.100000\n",
      " 23880/50000: episode: 5036, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 13220.423177, mae: 531.598633, accuracy: 0.302083, mean_q: -27.798315, mean_eps: 0.100000\n",
      " 23883/50000: episode: 5037, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 14772.479818, mae: 529.059733, accuracy: 0.281250, mean_q: -69.064167, mean_eps: 0.100000\n",
      " 23886/50000: episode: 5038, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 14319.865234, mae: 542.868937, accuracy: 0.291667, mean_q: -43.292982, mean_eps: 0.100000\n",
      " 23889/50000: episode: 5039, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 13609.997721, mae: 559.575948, accuracy: 0.270833, mean_q: -35.161616, mean_eps: 0.100000\n",
      " 23892/50000: episode: 5040, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 15272.812500, mae: 542.552022, accuracy: 0.197917, mean_q: -52.060167, mean_eps: 0.100000\n",
      " 23896/50000: episode: 5041, duration: 0.016s, episode steps:   4, steps per second: 257, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 14978.989014, mae: 542.432816, accuracy: 0.195312, mean_q: -53.754532, mean_eps: 0.100000\n",
      " 23899/50000: episode: 5042, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 11825.265951, mae: 529.730713, accuracy: 0.270833, mean_q: -16.222390, mean_eps: 0.100000\n",
      " 23902/50000: episode: 5043, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 14126.626953, mae: 548.017497, accuracy: 0.270833, mean_q: -24.158257, mean_eps: 0.100000\n",
      " 23905/50000: episode: 5044, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 11146.573893, mae: 543.274618, accuracy: 0.250000, mean_q: -44.193504, mean_eps: 0.100000\n",
      " 23908/50000: episode: 5045, duration: 0.018s, episode steps:   3, steps per second: 169, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 11423.183594, mae: 535.741394, accuracy: 0.239583, mean_q: -34.660532, mean_eps: 0.100000\n",
      " 23912/50000: episode: 5046, duration: 0.021s, episode steps:   4, steps per second: 192, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 15831.256592, mae: 555.165787, accuracy: 0.312500, mean_q: -56.677871, mean_eps: 0.100000\n",
      " 23916/50000: episode: 5047, duration: 0.016s, episode steps:   4, steps per second: 252, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 13539.017578, mae: 539.925873, accuracy: 0.226562, mean_q: -57.348186, mean_eps: 0.100000\n",
      " 23919/50000: episode: 5048, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 13210.883138, mae: 543.071370, accuracy: 0.239583, mean_q: -39.780983, mean_eps: 0.100000\n",
      " 23922/50000: episode: 5049, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 12040.628906, mae: 533.294535, accuracy: 0.270833, mean_q: -18.602236, mean_eps: 0.100000\n",
      " 23926/50000: episode: 5050, duration: 0.015s, episode steps:   4, steps per second: 262, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 13900.646484, mae: 534.171127, accuracy: 0.328125, mean_q: -65.869938, mean_eps: 0.100000\n",
      " 23929/50000: episode: 5051, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 14244.558919, mae: 524.300578, accuracy: 0.322917, mean_q: -55.739403, mean_eps: 0.100000\n",
      " 23932/50000: episode: 5052, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 13520.608724, mae: 537.067301, accuracy: 0.197917, mean_q: -30.062492, mean_eps: 0.100000\n",
      " 23935/50000: episode: 5053, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 14799.553060, mae: 548.524211, accuracy: 0.250000, mean_q: -31.793168, mean_eps: 0.100000\n",
      " 23938/50000: episode: 5054, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 12405.660807, mae: 532.041982, accuracy: 0.302083, mean_q: -59.357540, mean_eps: 0.100000\n",
      " 23941/50000: episode: 5055, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 14896.908854, mae: 549.557638, accuracy: 0.197917, mean_q: -33.682897, mean_eps: 0.100000\n",
      " 23944/50000: episode: 5056, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 13650.502604, mae: 552.633504, accuracy: 0.250000, mean_q: -10.484833, mean_eps: 0.100000\n",
      " 23947/50000: episode: 5057, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 16548.404297, mae: 558.820882, accuracy: 0.281250, mean_q: -12.444813, mean_eps: 0.100000\n",
      " 23950/50000: episode: 5058, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 12951.542318, mae: 529.466003, accuracy: 0.260417, mean_q: -36.893593, mean_eps: 0.100000\n",
      " 23953/50000: episode: 5059, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 13717.363281, mae: 549.393412, accuracy: 0.281250, mean_q: -31.861513, mean_eps: 0.100000\n",
      " 23956/50000: episode: 5060, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 13042.158854, mae: 555.184937, accuracy: 0.260417, mean_q: -33.450731, mean_eps: 0.100000\n",
      " 23959/50000: episode: 5061, duration: 0.018s, episode steps:   3, steps per second: 169, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 13008.623372, mae: 554.589213, accuracy: 0.239583, mean_q: -47.098928, mean_eps: 0.100000\n",
      " 23962/50000: episode: 5062, duration: 0.015s, episode steps:   3, steps per second: 201, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 13509.513021, mae: 555.843465, accuracy: 0.229167, mean_q: -46.820930, mean_eps: 0.100000\n",
      " 23966/50000: episode: 5063, duration: 0.016s, episode steps:   4, steps per second: 252, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 14073.980957, mae: 553.341721, accuracy: 0.210938, mean_q: -62.050773, mean_eps: 0.100000\n",
      " 23969/50000: episode: 5064, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 13018.164714, mae: 542.562032, accuracy: 0.302083, mean_q: -61.829895, mean_eps: 0.100000\n",
      " 23972/50000: episode: 5065, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 11867.403971, mae: 538.778483, accuracy: 0.312500, mean_q: -31.483340, mean_eps: 0.100000\n",
      " 23975/50000: episode: 5066, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 14281.855469, mae: 535.014099, accuracy: 0.354167, mean_q: -51.105068, mean_eps: 0.100000\n",
      " 23978/50000: episode: 5067, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 14158.972331, mae: 559.167379, accuracy: 0.322917, mean_q: -45.358261, mean_eps: 0.100000\n",
      " 23981/50000: episode: 5068, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 11796.771810, mae: 547.492106, accuracy: 0.260417, mean_q: -27.629583, mean_eps: 0.100000\n",
      " 23984/50000: episode: 5069, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 14401.823242, mae: 542.710510, accuracy: 0.260417, mean_q: -74.049867, mean_eps: 0.100000\n",
      " 23987/50000: episode: 5070, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 12577.691081, mae: 540.708903, accuracy: 0.333333, mean_q: -39.684555, mean_eps: 0.100000\n",
      " 23990/50000: episode: 5071, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 12207.121094, mae: 556.553772, accuracy: 0.218750, mean_q: -21.396736, mean_eps: 0.100000\n",
      " 23993/50000: episode: 5072, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 14773.813802, mae: 559.196879, accuracy: 0.250000, mean_q: -48.638842, mean_eps: 0.100000\n",
      " 23996/50000: episode: 5073, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 13739.167643, mae: 553.924072, accuracy: 0.354167, mean_q: -33.581165, mean_eps: 0.100000\n",
      " 23999/50000: episode: 5074, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 14149.711589, mae: 548.446493, accuracy: 0.208333, mean_q: -55.485320, mean_eps: 0.100000\n",
      " 24002/50000: episode: 5075, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 12170.091146, mae: 544.047974, accuracy: 0.239583, mean_q: -29.217297, mean_eps: 0.100000\n",
      " 24005/50000: episode: 5076, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 13975.017253, mae: 551.723063, accuracy: 0.302083, mean_q: -45.042391, mean_eps: 0.100000\n",
      " 24008/50000: episode: 5077, duration: 0.013s, episode steps:   3, steps per second: 223, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 12445.604492, mae: 529.570760, accuracy: 0.260417, mean_q: -38.002618, mean_eps: 0.100000\n",
      " 24011/50000: episode: 5078, duration: 0.015s, episode steps:   3, steps per second: 206, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 13379.460286, mae: 539.825378, accuracy: 0.187500, mean_q: -20.925118, mean_eps: 0.100000\n",
      " 24014/50000: episode: 5079, duration: 0.013s, episode steps:   3, steps per second: 225, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 12418.918620, mae: 538.999146, accuracy: 0.187500, mean_q: -55.578913, mean_eps: 0.100000\n",
      " 24017/50000: episode: 5080, duration: 0.014s, episode steps:   3, steps per second: 217, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 14077.330078, mae: 547.887268, accuracy: 0.250000, mean_q: -43.946039, mean_eps: 0.100000\n",
      " 24020/50000: episode: 5081, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 12431.469401, mae: 521.957825, accuracy: 0.239583, mean_q: -64.984723, mean_eps: 0.100000\n",
      " 24023/50000: episode: 5082, duration: 0.014s, episode steps:   3, steps per second: 208, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 14668.365885, mae: 538.011230, accuracy: 0.208333, mean_q: -36.219152, mean_eps: 0.100000\n",
      " 24026/50000: episode: 5083, duration: 0.015s, episode steps:   3, steps per second: 205, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 13614.568034, mae: 526.717428, accuracy: 0.197917, mean_q: -38.123368, mean_eps: 0.100000\n",
      " 24030/50000: episode: 5084, duration: 0.017s, episode steps:   4, steps per second: 236, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 12762.935059, mae: 542.937317, accuracy: 0.226562, mean_q: -20.323255, mean_eps: 0.100000\n",
      " 24033/50000: episode: 5085, duration: 0.014s, episode steps:   3, steps per second: 211, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 14764.545898, mae: 543.855733, accuracy: 0.260417, mean_q: -45.478493, mean_eps: 0.100000\n",
      " 24036/50000: episode: 5086, duration: 0.013s, episode steps:   3, steps per second: 223, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 12502.553060, mae: 549.581807, accuracy: 0.260417, mean_q: -78.690470, mean_eps: 0.100000\n",
      " 24039/50000: episode: 5087, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 13975.748372, mae: 566.366923, accuracy: 0.197917, mean_q: -65.759888, mean_eps: 0.100000\n",
      " 24042/50000: episode: 5088, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 12602.299805, mae: 557.090637, accuracy: 0.260417, mean_q: -55.595477, mean_eps: 0.100000\n",
      " 24046/50000: episode: 5089, duration: 0.017s, episode steps:   4, steps per second: 233, episode reward: -1238.000, mean reward: -309.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 12728.898926, mae: 539.052521, accuracy: 0.257812, mean_q: -65.038118, mean_eps: 0.100000\n",
      " 24049/50000: episode: 5090, duration: 0.015s, episode steps:   3, steps per second: 195, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 13341.748698, mae: 542.800252, accuracy: 0.239583, mean_q: -41.350937, mean_eps: 0.100000\n",
      " 24053/50000: episode: 5091, duration: 0.026s, episode steps:   4, steps per second: 157, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 15542.894043, mae: 542.719772, accuracy: 0.226562, mean_q: -36.079759, mean_eps: 0.100000\n",
      " 24056/50000: episode: 5092, duration: 0.016s, episode steps:   3, steps per second: 188, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 13317.213867, mae: 539.258993, accuracy: 0.281250, mean_q: -27.449711, mean_eps: 0.100000\n",
      " 24059/50000: episode: 5093, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 14602.477865, mae: 538.796061, accuracy: 0.281250, mean_q: -55.980128, mean_eps: 0.100000\n",
      " 24062/50000: episode: 5094, duration: 0.013s, episode steps:   3, steps per second: 224, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 14256.400391, mae: 546.018209, accuracy: 0.229167, mean_q: -18.476523, mean_eps: 0.100000\n",
      " 24065/50000: episode: 5095, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 13831.390299, mae: 542.976054, accuracy: 0.250000, mean_q: -37.201026, mean_eps: 0.100000\n",
      " 24069/50000: episode: 5096, duration: 0.015s, episode steps:   4, steps per second: 263, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 13653.981201, mae: 555.293701, accuracy: 0.257812, mean_q: -39.262522, mean_eps: 0.100000\n",
      " 24072/50000: episode: 5097, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 13184.945964, mae: 552.378011, accuracy: 0.208333, mean_q: -61.219238, mean_eps: 0.100000\n",
      " 24075/50000: episode: 5098, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 13968.919596, mae: 559.072795, accuracy: 0.177083, mean_q: -53.101211, mean_eps: 0.100000\n",
      " 24078/50000: episode: 5099, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 12227.683594, mae: 536.570964, accuracy: 0.260417, mean_q: -33.651461, mean_eps: 0.100000\n",
      " 24081/50000: episode: 5100, duration: 0.013s, episode steps:   3, steps per second: 225, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 12174.174154, mae: 533.490540, accuracy: 0.239583, mean_q: -32.349538, mean_eps: 0.100000\n",
      " 24085/50000: episode: 5101, duration: 0.016s, episode steps:   4, steps per second: 247, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 14789.511719, mae: 542.214264, accuracy: 0.250000, mean_q: -53.936187, mean_eps: 0.100000\n",
      " 24088/50000: episode: 5102, duration: 0.014s, episode steps:   3, steps per second: 218, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 14281.824870, mae: 563.531677, accuracy: 0.250000, mean_q: -15.869347, mean_eps: 0.100000\n",
      " 24091/50000: episode: 5103, duration: 0.014s, episode steps:   3, steps per second: 215, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 13361.109049, mae: 543.200155, accuracy: 0.354167, mean_q: -61.217575, mean_eps: 0.100000\n",
      " 24094/50000: episode: 5104, duration: 0.014s, episode steps:   3, steps per second: 220, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 12987.267578, mae: 555.493958, accuracy: 0.343750, mean_q: -39.138666, mean_eps: 0.100000\n",
      " 24097/50000: episode: 5105, duration: 0.015s, episode steps:   3, steps per second: 205, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 14102.809570, mae: 555.323832, accuracy: 0.281250, mean_q: -47.268550, mean_eps: 0.100000\n",
      " 24100/50000: episode: 5106, duration: 0.016s, episode steps:   3, steps per second: 185, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 14145.588216, mae: 537.880249, accuracy: 0.250000, mean_q: -42.416113, mean_eps: 0.100000\n",
      " 24103/50000: episode: 5107, duration: 0.014s, episode steps:   3, steps per second: 215, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 12170.046875, mae: 525.515198, accuracy: 0.250000, mean_q: -29.911900, mean_eps: 0.100000\n",
      " 24106/50000: episode: 5108, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 12164.327148, mae: 541.051676, accuracy: 0.218750, mean_q: -19.780519, mean_eps: 0.100000\n",
      " 24109/50000: episode: 5109, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 11898.417318, mae: 542.161926, accuracy: 0.322917, mean_q: -40.571986, mean_eps: 0.100000\n",
      " 24112/50000: episode: 5110, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 11616.510742, mae: 521.522715, accuracy: 0.239583, mean_q: -50.815393, mean_eps: 0.100000\n",
      " 24115/50000: episode: 5111, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 12787.614909, mae: 535.302368, accuracy: 0.218750, mean_q: -51.418175, mean_eps: 0.100000\n",
      " 24118/50000: episode: 5112, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 13715.111979, mae: 550.693319, accuracy: 0.239583, mean_q: -41.851265, mean_eps: 0.100000\n",
      " 24121/50000: episode: 5113, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 14847.500326, mae: 566.354146, accuracy: 0.291667, mean_q: -46.808204, mean_eps: 0.100000\n",
      " 24124/50000: episode: 5114, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 14446.094401, mae: 541.003927, accuracy: 0.312500, mean_q: -70.936236, mean_eps: 0.100000\n",
      " 24129/50000: episode: 5115, duration: 0.019s, episode steps:   5, steps per second: 265, episode reward: -2193.000, mean reward: -438.600 [-999.000, -58.000], mean action: 1.600 [0.000, 3.000],  loss: 12581.271094, mae: 546.371802, accuracy: 0.212500, mean_q: -43.384763, mean_eps: 0.100000\n",
      " 24132/50000: episode: 5116, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 12019.117839, mae: 545.973185, accuracy: 0.270833, mean_q: -29.336365, mean_eps: 0.100000\n",
      " 24135/50000: episode: 5117, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 13874.222982, mae: 546.471334, accuracy: 0.239583, mean_q: -24.802134, mean_eps: 0.100000\n",
      " 24139/50000: episode: 5118, duration: 0.016s, episode steps:   4, steps per second: 258, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 15032.311523, mae: 565.270340, accuracy: 0.242188, mean_q: -12.108664, mean_eps: 0.100000\n",
      " 24144/50000: episode: 5119, duration: 0.020s, episode steps:   5, steps per second: 254, episode reward: -2222.000, mean reward: -444.400 [-999.000, -58.000], mean action: 1.600 [0.000, 3.000],  loss: 15182.927539, mae: 539.657861, accuracy: 0.287500, mean_q: -50.452649, mean_eps: 0.100000\n",
      " 24147/50000: episode: 5120, duration: 0.015s, episode steps:   3, steps per second: 202, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 14456.468424, mae: 557.642598, accuracy: 0.270833, mean_q: -44.247751, mean_eps: 0.100000\n",
      " 24150/50000: episode: 5121, duration: 0.020s, episode steps:   3, steps per second: 151, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 11495.967773, mae: 549.202983, accuracy: 0.208333, mean_q: -56.783632, mean_eps: 0.100000\n",
      " 24153/50000: episode: 5122, duration: 0.014s, episode steps:   3, steps per second: 216, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 13140.457031, mae: 541.220479, accuracy: 0.270833, mean_q: -60.776905, mean_eps: 0.100000\n",
      " 24156/50000: episode: 5123, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 11379.198893, mae: 534.655355, accuracy: 0.197917, mean_q: -42.496694, mean_eps: 0.100000\n",
      " 24159/50000: episode: 5124, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 14398.754883, mae: 560.808919, accuracy: 0.270833, mean_q: -48.901592, mean_eps: 0.100000\n",
      " 24163/50000: episode: 5125, duration: 0.016s, episode steps:   4, steps per second: 245, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 11937.782959, mae: 556.068512, accuracy: 0.265625, mean_q: -43.576071, mean_eps: 0.100000\n",
      " 24166/50000: episode: 5126, duration: 0.013s, episode steps:   3, steps per second: 227, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 13107.905599, mae: 551.873596, accuracy: 0.281250, mean_q: -34.479718, mean_eps: 0.100000\n",
      " 24169/50000: episode: 5127, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 13020.208008, mae: 554.225769, accuracy: 0.250000, mean_q: -55.967693, mean_eps: 0.100000\n",
      " 24172/50000: episode: 5128, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 12894.165365, mae: 525.870321, accuracy: 0.281250, mean_q: -51.653379, mean_eps: 0.100000\n",
      " 24176/50000: episode: 5129, duration: 0.016s, episode steps:   4, steps per second: 256, episode reward: -1238.000, mean reward: -309.500 [-999.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 12957.639648, mae: 545.207230, accuracy: 0.289062, mean_q: -41.634109, mean_eps: 0.100000\n",
      " 24179/50000: episode: 5130, duration: 0.012s, episode steps:   3, steps per second: 257, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 12145.332682, mae: 541.133677, accuracy: 0.291667, mean_q: -54.546804, mean_eps: 0.100000\n",
      " 24182/50000: episode: 5131, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 15342.693359, mae: 550.811544, accuracy: 0.354167, mean_q: -66.116247, mean_eps: 0.100000\n",
      " 24185/50000: episode: 5132, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 11682.897461, mae: 538.338806, accuracy: 0.270833, mean_q: -32.911105, mean_eps: 0.100000\n",
      " 24189/50000: episode: 5133, duration: 0.016s, episode steps:   4, steps per second: 250, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 10917.794922, mae: 539.096725, accuracy: 0.281250, mean_q: -30.864399, mean_eps: 0.100000\n",
      " 24192/50000: episode: 5134, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 12436.279622, mae: 554.627502, accuracy: 0.281250, mean_q: -53.935010, mean_eps: 0.100000\n",
      " 24195/50000: episode: 5135, duration: 0.014s, episode steps:   3, steps per second: 220, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 13706.675781, mae: 547.252055, accuracy: 0.270833, mean_q: -29.951638, mean_eps: 0.100000\n",
      " 24198/50000: episode: 5136, duration: 0.019s, episode steps:   3, steps per second: 161, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 14068.826823, mae: 539.159444, accuracy: 0.322917, mean_q: -50.546577, mean_eps: 0.100000\n",
      " 24202/50000: episode: 5137, duration: 0.019s, episode steps:   4, steps per second: 213, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 13610.430908, mae: 554.470306, accuracy: 0.171875, mean_q: -45.635618, mean_eps: 0.100000\n",
      " 24206/50000: episode: 5138, duration: 0.016s, episode steps:   4, steps per second: 251, episode reward: -1238.000, mean reward: -309.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 16769.491455, mae: 567.211197, accuracy: 0.257812, mean_q: -49.049809, mean_eps: 0.100000\n",
      " 24209/50000: episode: 5139, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 12691.823568, mae: 539.112101, accuracy: 0.208333, mean_q: -22.797852, mean_eps: 0.100000\n",
      " 24212/50000: episode: 5140, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 13157.161784, mae: 540.575785, accuracy: 0.291667, mean_q: -36.777535, mean_eps: 0.100000\n",
      " 24215/50000: episode: 5141, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 12360.521484, mae: 548.064331, accuracy: 0.281250, mean_q: -71.209445, mean_eps: 0.100000\n",
      " 24218/50000: episode: 5142, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 13282.054688, mae: 540.551086, accuracy: 0.229167, mean_q: -62.191363, mean_eps: 0.100000\n",
      " 24221/50000: episode: 5143, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 12877.132487, mae: 533.870036, accuracy: 0.302083, mean_q: -44.819562, mean_eps: 0.100000\n",
      " 24224/50000: episode: 5144, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 13289.625977, mae: 546.176188, accuracy: 0.302083, mean_q: -47.811436, mean_eps: 0.100000\n",
      " 24227/50000: episode: 5145, duration: 0.013s, episode steps:   3, steps per second: 227, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 14161.426432, mae: 546.137410, accuracy: 0.270833, mean_q: -66.191889, mean_eps: 0.100000\n",
      " 24230/50000: episode: 5146, duration: 0.014s, episode steps:   3, steps per second: 216, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 12567.565430, mae: 552.413269, accuracy: 0.322917, mean_q: -51.305330, mean_eps: 0.100000\n",
      " 24233/50000: episode: 5147, duration: 0.013s, episode steps:   3, steps per second: 225, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 12677.507487, mae: 544.865153, accuracy: 0.312500, mean_q: -55.242239, mean_eps: 0.100000\n",
      " 24236/50000: episode: 5148, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 13172.167643, mae: 547.960042, accuracy: 0.250000, mean_q: -39.799819, mean_eps: 0.100000\n",
      " 24239/50000: episode: 5149, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 12040.317057, mae: 544.375814, accuracy: 0.281250, mean_q: -38.197776, mean_eps: 0.100000\n",
      " 24242/50000: episode: 5150, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 12597.218750, mae: 545.746257, accuracy: 0.322917, mean_q: -8.475260, mean_eps: 0.100000\n",
      " 24245/50000: episode: 5151, duration: 0.018s, episode steps:   3, steps per second: 169, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 14779.863281, mae: 537.631042, accuracy: 0.302083, mean_q: -39.738182, mean_eps: 0.100000\n",
      " 24248/50000: episode: 5152, duration: 0.015s, episode steps:   3, steps per second: 194, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 14520.304362, mae: 547.227763, accuracy: 0.229167, mean_q: -14.796479, mean_eps: 0.100000\n",
      " 24251/50000: episode: 5153, duration: 0.015s, episode steps:   3, steps per second: 203, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 13812.125000, mae: 553.920654, accuracy: 0.302083, mean_q: -35.063904, mean_eps: 0.100000\n",
      " 24254/50000: episode: 5154, duration: 0.013s, episode steps:   3, steps per second: 226, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 12464.648763, mae: 559.283488, accuracy: 0.239583, mean_q: -45.802546, mean_eps: 0.100000\n",
      " 24257/50000: episode: 5155, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 13953.454427, mae: 540.823344, accuracy: 0.229167, mean_q: -61.871693, mean_eps: 0.100000\n",
      " 24260/50000: episode: 5156, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 13888.518555, mae: 541.803019, accuracy: 0.260417, mean_q: -36.423402, mean_eps: 0.100000\n",
      " 24264/50000: episode: 5157, duration: 0.015s, episode steps:   4, steps per second: 259, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 12852.942871, mae: 539.665665, accuracy: 0.281250, mean_q: -75.015213, mean_eps: 0.100000\n",
      " 24267/50000: episode: 5158, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 12068.904948, mae: 544.397624, accuracy: 0.177083, mean_q: -25.232634, mean_eps: 0.100000\n",
      " 24271/50000: episode: 5159, duration: 0.016s, episode steps:   4, steps per second: 254, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 11119.069824, mae: 529.067276, accuracy: 0.289062, mean_q: -72.488339, mean_eps: 0.100000\n",
      " 24274/50000: episode: 5160, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 13662.602214, mae: 548.588826, accuracy: 0.218750, mean_q: -67.843683, mean_eps: 0.100000\n",
      " 24278/50000: episode: 5161, duration: 0.015s, episode steps:   4, steps per second: 269, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 13410.877686, mae: 543.683975, accuracy: 0.265625, mean_q: -47.240149, mean_eps: 0.100000\n",
      " 24281/50000: episode: 5162, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 12738.348633, mae: 521.304667, accuracy: 0.260417, mean_q: -43.474217, mean_eps: 0.100000\n",
      " 24284/50000: episode: 5163, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 12484.528971, mae: 524.858093, accuracy: 0.239583, mean_q: -42.639207, mean_eps: 0.100000\n",
      " 24287/50000: episode: 5164, duration: 0.013s, episode steps:   3, steps per second: 230, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 12541.555339, mae: 533.718404, accuracy: 0.239583, mean_q: -3.855508, mean_eps: 0.100000\n",
      " 24290/50000: episode: 5165, duration: 0.020s, episode steps:   3, steps per second: 150, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 14870.514974, mae: 526.537944, accuracy: 0.197917, mean_q: -41.987581, mean_eps: 0.100000\n",
      " 24294/50000: episode: 5166, duration: 0.028s, episode steps:   4, steps per second: 144, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 14188.441650, mae: 545.867188, accuracy: 0.242188, mean_q: -60.171747, mean_eps: 0.100000\n",
      " 24297/50000: episode: 5167, duration: 0.014s, episode steps:   3, steps per second: 219, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 13407.232747, mae: 514.291748, accuracy: 0.197917, mean_q: -38.362307, mean_eps: 0.100000\n",
      " 24300/50000: episode: 5168, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 14705.077799, mae: 562.891418, accuracy: 0.177083, mean_q: -29.086735, mean_eps: 0.100000\n",
      " 24303/50000: episode: 5169, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 13340.391927, mae: 540.728658, accuracy: 0.239583, mean_q: -24.819979, mean_eps: 0.100000\n",
      " 24307/50000: episode: 5170, duration: 0.015s, episode steps:   4, steps per second: 263, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 12212.812500, mae: 541.804237, accuracy: 0.328125, mean_q: -62.824959, mean_eps: 0.100000\n",
      " 24310/50000: episode: 5171, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 12923.062174, mae: 548.074687, accuracy: 0.333333, mean_q: -47.198053, mean_eps: 0.100000\n",
      " 24313/50000: episode: 5172, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 12556.999023, mae: 555.830973, accuracy: 0.302083, mean_q: -44.313486, mean_eps: 0.100000\n",
      " 24316/50000: episode: 5173, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 11669.916341, mae: 546.902954, accuracy: 0.239583, mean_q: -38.413321, mean_eps: 0.100000\n",
      " 24320/50000: episode: 5174, duration: 0.015s, episode steps:   4, steps per second: 261, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.000 [0.000, 3.000],  loss: 13496.070312, mae: 544.851822, accuracy: 0.351562, mean_q: -51.186747, mean_eps: 0.100000\n",
      " 24323/50000: episode: 5175, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 12978.885091, mae: 558.910645, accuracy: 0.229167, mean_q: -26.816743, mean_eps: 0.100000\n",
      " 24326/50000: episode: 5176, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 12777.612305, mae: 551.363546, accuracy: 0.354167, mean_q: -23.272055, mean_eps: 0.100000\n",
      " 24329/50000: episode: 5177, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 13759.260742, mae: 542.126404, accuracy: 0.343750, mean_q: -72.519081, mean_eps: 0.100000\n",
      " 24332/50000: episode: 5178, duration: 0.012s, episode steps:   3, steps per second: 254, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 12387.765625, mae: 535.258769, accuracy: 0.250000, mean_q: -58.653470, mean_eps: 0.100000\n",
      " 24335/50000: episode: 5179, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 12440.520833, mae: 538.853597, accuracy: 0.270833, mean_q: -29.950712, mean_eps: 0.100000\n",
      " 24339/50000: episode: 5180, duration: 0.018s, episode steps:   4, steps per second: 223, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 11471.158447, mae: 540.267120, accuracy: 0.250000, mean_q: -19.734516, mean_eps: 0.100000\n",
      " 24342/50000: episode: 5181, duration: 0.017s, episode steps:   3, steps per second: 181, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 13702.395833, mae: 548.165324, accuracy: 0.322917, mean_q: -17.420955, mean_eps: 0.100000\n",
      " 24345/50000: episode: 5182, duration: 0.014s, episode steps:   3, steps per second: 207, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 13681.440104, mae: 548.745138, accuracy: 0.270833, mean_q: -50.908038, mean_eps: 0.100000\n",
      " 24348/50000: episode: 5183, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 12842.642578, mae: 553.683004, accuracy: 0.270833, mean_q: -40.070907, mean_eps: 0.100000\n",
      " 24351/50000: episode: 5184, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 15663.022135, mae: 547.024699, accuracy: 0.281250, mean_q: -64.064206, mean_eps: 0.100000\n",
      " 24354/50000: episode: 5185, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 13065.045247, mae: 544.672262, accuracy: 0.291667, mean_q: -65.772029, mean_eps: 0.100000\n",
      " 24357/50000: episode: 5186, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 12584.424805, mae: 536.697957, accuracy: 0.250000, mean_q: -21.344446, mean_eps: 0.100000\n",
      " 24361/50000: episode: 5187, duration: 0.015s, episode steps:   4, steps per second: 263, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 12470.569092, mae: 540.414886, accuracy: 0.328125, mean_q: -28.360323, mean_eps: 0.100000\n",
      " 24364/50000: episode: 5188, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 12086.036784, mae: 539.443970, accuracy: 0.260417, mean_q: -23.831061, mean_eps: 0.100000\n",
      " 24367/50000: episode: 5189, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 14379.596029, mae: 548.870097, accuracy: 0.333333, mean_q: -3.045774, mean_eps: 0.100000\n",
      " 24370/50000: episode: 5190, duration: 0.012s, episode steps:   3, steps per second: 253, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 14095.622396, mae: 552.258911, accuracy: 0.260417, mean_q: -42.353297, mean_eps: 0.100000\n",
      " 24373/50000: episode: 5191, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 11727.684896, mae: 533.899333, accuracy: 0.291667, mean_q: -44.106824, mean_eps: 0.100000\n",
      " 24376/50000: episode: 5192, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 13460.464193, mae: 544.373840, accuracy: 0.250000, mean_q: -41.743004, mean_eps: 0.100000\n",
      " 24380/50000: episode: 5193, duration: 0.015s, episode steps:   4, steps per second: 263, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 12773.316650, mae: 538.521523, accuracy: 0.335938, mean_q: -36.928695, mean_eps: 0.100000\n",
      " 24383/50000: episode: 5194, duration: 0.012s, episode steps:   3, steps per second: 253, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 12855.083984, mae: 551.863851, accuracy: 0.281250, mean_q: -16.636675, mean_eps: 0.100000\n",
      " 24386/50000: episode: 5195, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 12549.384440, mae: 538.312113, accuracy: 0.229167, mean_q: -25.036848, mean_eps: 0.100000\n",
      " 24389/50000: episode: 5196, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 12718.102539, mae: 543.424459, accuracy: 0.322917, mean_q: -33.548279, mean_eps: 0.100000\n",
      " 24393/50000: episode: 5197, duration: 0.018s, episode steps:   4, steps per second: 220, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 2.250 [1.000, 3.000],  loss: 12882.351807, mae: 534.724030, accuracy: 0.250000, mean_q: -47.061241, mean_eps: 0.100000\n",
      " 24396/50000: episode: 5198, duration: 0.015s, episode steps:   3, steps per second: 204, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 12332.264323, mae: 545.196431, accuracy: 0.229167, mean_q: -42.249481, mean_eps: 0.100000\n",
      " 24399/50000: episode: 5199, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 13625.611328, mae: 561.901510, accuracy: 0.281250, mean_q: -33.477886, mean_eps: 0.100000\n",
      " 24402/50000: episode: 5200, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 12349.794596, mae: 557.624227, accuracy: 0.322917, mean_q: -44.634646, mean_eps: 0.100000\n",
      " 24405/50000: episode: 5201, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 12082.137695, mae: 542.698547, accuracy: 0.291667, mean_q: -42.506032, mean_eps: 0.100000\n",
      " 24408/50000: episode: 5202, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 13943.338542, mae: 553.609660, accuracy: 0.229167, mean_q: -21.621442, mean_eps: 0.100000\n",
      " 24411/50000: episode: 5203, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 11995.807617, mae: 552.620463, accuracy: 0.166667, mean_q: -32.141270, mean_eps: 0.100000\n",
      " 24414/50000: episode: 5204, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 13037.429688, mae: 556.271505, accuracy: 0.250000, mean_q: -38.392630, mean_eps: 0.100000\n",
      " 24417/50000: episode: 5205, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 12262.253255, mae: 545.943766, accuracy: 0.322917, mean_q: -45.332653, mean_eps: 0.100000\n",
      " 24420/50000: episode: 5206, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 10496.246745, mae: 534.201782, accuracy: 0.260417, mean_q: -45.955554, mean_eps: 0.100000\n",
      " 24423/50000: episode: 5207, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 14913.599609, mae: 559.526794, accuracy: 0.250000, mean_q: -51.894773, mean_eps: 0.100000\n",
      " 24427/50000: episode: 5208, duration: 0.015s, episode steps:   4, steps per second: 263, episode reward: -1238.000, mean reward: -309.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 14885.419922, mae: 551.037361, accuracy: 0.273438, mean_q: -67.596883, mean_eps: 0.100000\n",
      " 24430/50000: episode: 5209, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 11288.896159, mae: 530.813578, accuracy: 0.333333, mean_q: -42.189601, mean_eps: 0.100000\n",
      " 24434/50000: episode: 5210, duration: 0.016s, episode steps:   4, steps per second: 255, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 13097.486816, mae: 519.004852, accuracy: 0.265625, mean_q: -41.505903, mean_eps: 0.100000\n",
      " 24437/50000: episode: 5211, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 13435.712240, mae: 545.036438, accuracy: 0.250000, mean_q: -22.215081, mean_eps: 0.100000\n",
      " 24441/50000: episode: 5212, duration: 0.015s, episode steps:   4, steps per second: 261, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 13386.077637, mae: 525.190018, accuracy: 0.304688, mean_q: -49.622799, mean_eps: 0.100000\n",
      " 24445/50000: episode: 5213, duration: 0.016s, episode steps:   4, steps per second: 253, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 2.250 [1.000, 3.000],  loss: 12788.682861, mae: 535.993958, accuracy: 0.234375, mean_q: -40.372449, mean_eps: 0.100000\n",
      " 24448/50000: episode: 5214, duration: 0.017s, episode steps:   3, steps per second: 181, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 12981.868490, mae: 522.480001, accuracy: 0.208333, mean_q: -0.877585, mean_eps: 0.100000\n",
      " 24451/50000: episode: 5215, duration: 0.015s, episode steps:   3, steps per second: 197, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 12082.350586, mae: 532.817078, accuracy: 0.281250, mean_q: -31.778355, mean_eps: 0.100000\n",
      " 24454/50000: episode: 5216, duration: 0.015s, episode steps:   3, steps per second: 194, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 11576.334961, mae: 531.140910, accuracy: 0.197917, mean_q: -2.257312, mean_eps: 0.100000\n",
      " 24458/50000: episode: 5217, duration: 0.016s, episode steps:   4, steps per second: 249, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 13070.454834, mae: 557.329742, accuracy: 0.250000, mean_q: -22.897096, mean_eps: 0.100000\n",
      " 24463/50000: episode: 5218, duration: 0.018s, episode steps:   5, steps per second: 271, episode reward: -2193.000, mean reward: -438.600 [-999.000, -32.000], mean action: 1.200 [0.000, 3.000],  loss: 13402.666992, mae: 538.944568, accuracy: 0.287500, mean_q: -40.816781, mean_eps: 0.100000\n",
      " 24466/50000: episode: 5219, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 11330.878581, mae: 556.626322, accuracy: 0.312500, mean_q: -22.225865, mean_eps: 0.100000\n",
      " 24469/50000: episode: 5220, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 13184.544596, mae: 547.253988, accuracy: 0.208333, mean_q: -44.457656, mean_eps: 0.100000\n",
      " 24472/50000: episode: 5221, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 10789.763997, mae: 544.428284, accuracy: 0.322917, mean_q: -50.116557, mean_eps: 0.100000\n",
      " 24475/50000: episode: 5222, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 12065.299154, mae: 544.748698, accuracy: 0.218750, mean_q: -35.793226, mean_eps: 0.100000\n",
      " 24478/50000: episode: 5223, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 13332.201172, mae: 543.094442, accuracy: 0.239583, mean_q: -51.268696, mean_eps: 0.100000\n",
      " 24481/50000: episode: 5224, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 13477.015299, mae: 536.896098, accuracy: 0.187500, mean_q: -71.527425, mean_eps: 0.100000\n",
      " 24484/50000: episode: 5225, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 12439.227539, mae: 549.865072, accuracy: 0.197917, mean_q: -38.528583, mean_eps: 0.100000\n",
      " 24487/50000: episode: 5226, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 13874.685872, mae: 536.778971, accuracy: 0.270833, mean_q: -41.093175, mean_eps: 0.100000\n",
      " 24490/50000: episode: 5227, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 11310.399740, mae: 537.663859, accuracy: 0.166667, mean_q: -46.041855, mean_eps: 0.100000\n",
      " 24493/50000: episode: 5228, duration: 0.017s, episode steps:   3, steps per second: 175, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 13980.948242, mae: 544.064189, accuracy: 0.250000, mean_q: -33.816798, mean_eps: 0.100000\n",
      " 24497/50000: episode: 5229, duration: 0.021s, episode steps:   4, steps per second: 193, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 13140.773926, mae: 536.802460, accuracy: 0.218750, mean_q: -28.425806, mean_eps: 0.100000\n",
      " 24500/50000: episode: 5230, duration: 0.015s, episode steps:   3, steps per second: 204, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 12044.322591, mae: 537.070374, accuracy: 0.270833, mean_q: -35.512200, mean_eps: 0.100000\n",
      " 24504/50000: episode: 5231, duration: 0.016s, episode steps:   4, steps per second: 244, episode reward: -1223.000, mean reward: -305.750 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 12315.644165, mae: 525.315308, accuracy: 0.234375, mean_q: -44.837998, mean_eps: 0.100000\n",
      " 24507/50000: episode: 5232, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 13353.974935, mae: 526.987661, accuracy: 0.239583, mean_q: -47.482412, mean_eps: 0.100000\n",
      " 24510/50000: episode: 5233, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 14243.897135, mae: 532.761434, accuracy: 0.281250, mean_q: -57.958150, mean_eps: 0.100000\n",
      " 24513/50000: episode: 5234, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 13260.225911, mae: 534.564331, accuracy: 0.333333, mean_q: -21.191487, mean_eps: 0.100000\n",
      " 24517/50000: episode: 5235, duration: 0.015s, episode steps:   4, steps per second: 259, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 2.250 [1.000, 3.000],  loss: 13461.205078, mae: 544.198013, accuracy: 0.273438, mean_q: -34.316281, mean_eps: 0.100000\n",
      " 24520/50000: episode: 5236, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 11612.480469, mae: 526.775513, accuracy: 0.406250, mean_q: -26.117406, mean_eps: 0.100000\n",
      " 24524/50000: episode: 5237, duration: 0.015s, episode steps:   4, steps per second: 261, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 13632.137207, mae: 545.576202, accuracy: 0.304688, mean_q: -39.888545, mean_eps: 0.100000\n",
      " 24529/50000: episode: 5238, duration: 0.019s, episode steps:   5, steps per second: 268, episode reward: -2222.000, mean reward: -444.400 [-999.000, -58.000], mean action: 1.600 [0.000, 3.000],  loss: 12181.302930, mae: 547.591699, accuracy: 0.312500, mean_q: -44.881490, mean_eps: 0.100000\n",
      " 24533/50000: episode: 5239, duration: 0.015s, episode steps:   4, steps per second: 258, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 13719.523682, mae: 550.752625, accuracy: 0.210938, mean_q: -34.111466, mean_eps: 0.100000\n",
      " 24536/50000: episode: 5240, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 13308.222656, mae: 549.323608, accuracy: 0.250000, mean_q: -48.195851, mean_eps: 0.100000\n",
      " 24539/50000: episode: 5241, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 11433.776042, mae: 537.541504, accuracy: 0.270833, mean_q: -48.970507, mean_eps: 0.100000\n",
      " 24542/50000: episode: 5242, duration: 0.018s, episode steps:   3, steps per second: 163, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 11015.773112, mae: 521.280986, accuracy: 0.218750, mean_q: -20.457544, mean_eps: 0.100000\n",
      " 24545/50000: episode: 5243, duration: 0.015s, episode steps:   3, steps per second: 199, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 13647.152344, mae: 563.170898, accuracy: 0.250000, mean_q: -42.977242, mean_eps: 0.100000\n",
      " 24548/50000: episode: 5244, duration: 0.014s, episode steps:   3, steps per second: 222, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 11715.794271, mae: 535.909648, accuracy: 0.229167, mean_q: -23.462794, mean_eps: 0.100000\n",
      " 24551/50000: episode: 5245, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 13593.977214, mae: 536.044800, accuracy: 0.291667, mean_q: -49.124942, mean_eps: 0.100000\n",
      " 24554/50000: episode: 5246, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 12163.741536, mae: 526.544474, accuracy: 0.270833, mean_q: -47.757565, mean_eps: 0.100000\n",
      " 24557/50000: episode: 5247, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 15172.851562, mae: 544.995199, accuracy: 0.270833, mean_q: -56.966335, mean_eps: 0.100000\n",
      " 24560/50000: episode: 5248, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 13376.438151, mae: 559.811625, accuracy: 0.177083, mean_q: -53.844026, mean_eps: 0.100000\n",
      " 24564/50000: episode: 5249, duration: 0.015s, episode steps:   4, steps per second: 262, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 11933.806396, mae: 539.012772, accuracy: 0.226562, mean_q: -38.752364, mean_eps: 0.100000\n",
      " 24567/50000: episode: 5250, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 10418.501302, mae: 526.626567, accuracy: 0.229167, mean_q: -37.737932, mean_eps: 0.100000\n",
      " 24571/50000: episode: 5251, duration: 0.015s, episode steps:   4, steps per second: 262, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 12646.477295, mae: 533.633606, accuracy: 0.257812, mean_q: -51.904358, mean_eps: 0.100000\n",
      " 24574/50000: episode: 5252, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 14974.017578, mae: 526.600688, accuracy: 0.302083, mean_q: -67.601270, mean_eps: 0.100000\n",
      " 24577/50000: episode: 5253, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 11872.487630, mae: 540.385152, accuracy: 0.114583, mean_q: -43.426484, mean_eps: 0.100000\n",
      " 24580/50000: episode: 5254, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 13422.136393, mae: 543.894796, accuracy: 0.208333, mean_q: -59.926287, mean_eps: 0.100000\n",
      " 24583/50000: episode: 5255, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 12784.251953, mae: 538.044088, accuracy: 0.322917, mean_q: -33.364236, mean_eps: 0.100000\n",
      " 24587/50000: episode: 5256, duration: 0.016s, episode steps:   4, steps per second: 254, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 12050.958008, mae: 533.153000, accuracy: 0.171875, mean_q: -28.854768, mean_eps: 0.100000\n",
      " 24590/50000: episode: 5257, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 13033.569987, mae: 543.387573, accuracy: 0.218750, mean_q: 0.831468, mean_eps: 0.100000\n",
      " 24593/50000: episode: 5258, duration: 0.015s, episode steps:   3, steps per second: 203, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 14214.825521, mae: 544.241781, accuracy: 0.208333, mean_q: -36.272496, mean_eps: 0.100000\n",
      " 24596/50000: episode: 5259, duration: 0.015s, episode steps:   3, steps per second: 205, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 13489.924479, mae: 557.734619, accuracy: 0.218750, mean_q: -38.258313, mean_eps: 0.100000\n",
      " 24599/50000: episode: 5260, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 12452.797526, mae: 540.240784, accuracy: 0.250000, mean_q: -45.845225, mean_eps: 0.100000\n",
      " 24602/50000: episode: 5261, duration: 0.016s, episode steps:   3, steps per second: 191, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 14070.927734, mae: 540.013285, accuracy: 0.312500, mean_q: -71.686424, mean_eps: 0.100000\n",
      " 24605/50000: episode: 5262, duration: 0.019s, episode steps:   3, steps per second: 161, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 11532.081706, mae: 536.877380, accuracy: 0.229167, mean_q: -47.013159, mean_eps: 0.100000\n",
      " 24608/50000: episode: 5263, duration: 0.015s, episode steps:   3, steps per second: 203, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 13426.128581, mae: 553.174927, accuracy: 0.218750, mean_q: -70.159236, mean_eps: 0.100000\n",
      " 24612/50000: episode: 5264, duration: 0.015s, episode steps:   4, steps per second: 260, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 2.250 [1.000, 3.000],  loss: 12494.613281, mae: 538.920319, accuracy: 0.265625, mean_q: -29.605673, mean_eps: 0.100000\n",
      " 24615/50000: episode: 5265, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 13566.584635, mae: 574.758586, accuracy: 0.218750, mean_q: -33.200688, mean_eps: 0.100000\n",
      " 24620/50000: episode: 5266, duration: 0.018s, episode steps:   5, steps per second: 274, episode reward: -2222.000, mean reward: -444.400 [-999.000, -32.000], mean action: 1.400 [0.000, 3.000],  loss: 11846.670508, mae: 533.356836, accuracy: 0.250000, mean_q: -62.761890, mean_eps: 0.100000\n",
      " 24623/50000: episode: 5267, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 12718.796224, mae: 533.071899, accuracy: 0.208333, mean_q: -28.910942, mean_eps: 0.100000\n",
      " 24628/50000: episode: 5268, duration: 0.019s, episode steps:   5, steps per second: 268, episode reward: -2193.000, mean reward: -438.600 [-999.000, -45.000], mean action: 1.800 [0.000, 3.000],  loss: 12617.862695, mae: 533.314355, accuracy: 0.256250, mean_q: -61.221816, mean_eps: 0.100000\n",
      " 24631/50000: episode: 5269, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 13236.423177, mae: 555.905924, accuracy: 0.239583, mean_q: -52.512421, mean_eps: 0.100000\n",
      " 24634/50000: episode: 5270, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 10530.521484, mae: 523.920848, accuracy: 0.302083, mean_q: -29.766453, mean_eps: 0.100000\n",
      " 24637/50000: episode: 5271, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 12346.515951, mae: 559.472290, accuracy: 0.218750, mean_q: -37.267268, mean_eps: 0.100000\n",
      " 24640/50000: episode: 5272, duration: 0.014s, episode steps:   3, steps per second: 219, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 12304.905273, mae: 520.893250, accuracy: 0.208333, mean_q: -41.133559, mean_eps: 0.100000\n",
      " 24643/50000: episode: 5273, duration: 0.018s, episode steps:   3, steps per second: 165, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 12033.850586, mae: 540.961283, accuracy: 0.229167, mean_q: -20.556722, mean_eps: 0.100000\n",
      " 24646/50000: episode: 5274, duration: 0.015s, episode steps:   3, steps per second: 201, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 13676.518880, mae: 556.248535, accuracy: 0.239583, mean_q: -21.374599, mean_eps: 0.100000\n",
      " 24649/50000: episode: 5275, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 12544.665690, mae: 545.121175, accuracy: 0.197917, mean_q: -39.326726, mean_eps: 0.100000\n",
      " 24652/50000: episode: 5276, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 14491.323242, mae: 540.260701, accuracy: 0.270833, mean_q: -42.050769, mean_eps: 0.100000\n",
      " 24655/50000: episode: 5277, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 12469.690755, mae: 538.995443, accuracy: 0.156250, mean_q: -49.758227, mean_eps: 0.100000\n",
      " 24658/50000: episode: 5278, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 13038.944336, mae: 561.123474, accuracy: 0.239583, mean_q: -43.904705, mean_eps: 0.100000\n",
      " 24661/50000: episode: 5279, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 12531.201497, mae: 571.064290, accuracy: 0.166667, mean_q: -61.782518, mean_eps: 0.100000\n",
      " 24664/50000: episode: 5280, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 12258.602214, mae: 545.380880, accuracy: 0.229167, mean_q: -56.379976, mean_eps: 0.100000\n",
      " 24667/50000: episode: 5281, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 11458.743164, mae: 544.554667, accuracy: 0.260417, mean_q: -61.880040, mean_eps: 0.100000\n",
      " 24670/50000: episode: 5282, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 13195.890299, mae: 542.079854, accuracy: 0.281250, mean_q: -70.010153, mean_eps: 0.100000\n",
      " 24674/50000: episode: 5283, duration: 0.016s, episode steps:   4, steps per second: 257, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.000 [0.000, 3.000],  loss: 12470.793457, mae: 526.698944, accuracy: 0.234375, mean_q: -63.481637, mean_eps: 0.100000\n",
      " 24678/50000: episode: 5284, duration: 0.015s, episode steps:   4, steps per second: 268, episode reward: -1238.000, mean reward: -309.500 [-999.000, -58.000], mean action: 1.750 [0.000, 3.000],  loss: 10491.792236, mae: 540.777214, accuracy: 0.226562, mean_q: -13.801665, mean_eps: 0.100000\n",
      " 24681/50000: episode: 5285, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 9664.376628, mae: 524.405975, accuracy: 0.270833, mean_q: -35.556209, mean_eps: 0.100000\n",
      " 24684/50000: episode: 5286, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 13402.594401, mae: 546.940491, accuracy: 0.260417, mean_q: -25.905291, mean_eps: 0.100000\n",
      " 24689/50000: episode: 5287, duration: 0.019s, episode steps:   5, steps per second: 268, episode reward: -2193.000, mean reward: -438.600 [-999.000, -32.000], mean action: 1.200 [0.000, 3.000],  loss: 15939.539453, mae: 539.969324, accuracy: 0.212500, mean_q: -53.355221, mean_eps: 0.100000\n",
      " 24692/50000: episode: 5288, duration: 0.015s, episode steps:   3, steps per second: 205, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 12518.049154, mae: 513.566335, accuracy: 0.197917, mean_q: -31.139999, mean_eps: 0.100000\n",
      " 24695/50000: episode: 5289, duration: 0.015s, episode steps:   3, steps per second: 202, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 13975.029297, mae: 544.858297, accuracy: 0.270833, mean_q: 4.213788, mean_eps: 0.100000\n",
      " 24698/50000: episode: 5290, duration: 0.014s, episode steps:   3, steps per second: 213, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 14806.360026, mae: 542.442566, accuracy: 0.260417, mean_q: -61.601084, mean_eps: 0.100000\n",
      " 24701/50000: episode: 5291, duration: 0.013s, episode steps:   3, steps per second: 225, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 11683.445638, mae: 542.072632, accuracy: 0.260417, mean_q: -32.873772, mean_eps: 0.100000\n",
      " 24704/50000: episode: 5292, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 12328.657878, mae: 533.644287, accuracy: 0.239583, mean_q: -46.682935, mean_eps: 0.100000\n",
      " 24707/50000: episode: 5293, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 14745.971029, mae: 537.541626, accuracy: 0.218750, mean_q: -33.659481, mean_eps: 0.100000\n",
      " 24710/50000: episode: 5294, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 11607.904297, mae: 523.433105, accuracy: 0.260417, mean_q: -42.494570, mean_eps: 0.100000\n",
      " 24713/50000: episode: 5295, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 11153.722982, mae: 541.646423, accuracy: 0.218750, mean_q: -47.829315, mean_eps: 0.100000\n",
      " 24716/50000: episode: 5296, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 14078.138997, mae: 539.460307, accuracy: 0.208333, mean_q: -37.811368, mean_eps: 0.100000\n",
      " 24719/50000: episode: 5297, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 13553.381510, mae: 534.184692, accuracy: 0.218750, mean_q: -19.781658, mean_eps: 0.100000\n",
      " 24722/50000: episode: 5298, duration: 0.015s, episode steps:   3, steps per second: 205, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 12532.938151, mae: 544.440694, accuracy: 0.239583, mean_q: -74.477000, mean_eps: 0.100000\n",
      " 24725/50000: episode: 5299, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 12419.780599, mae: 539.910116, accuracy: 0.177083, mean_q: -31.611376, mean_eps: 0.100000\n",
      " 24728/50000: episode: 5300, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 12695.046875, mae: 539.564535, accuracy: 0.333333, mean_q: -66.503614, mean_eps: 0.100000\n",
      " 24731/50000: episode: 5301, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 15350.484375, mae: 565.733866, accuracy: 0.208333, mean_q: -59.644605, mean_eps: 0.100000\n",
      " 24734/50000: episode: 5302, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 12423.511719, mae: 529.947276, accuracy: 0.270833, mean_q: -44.999150, mean_eps: 0.100000\n",
      " 24737/50000: episode: 5303, duration: 0.016s, episode steps:   3, steps per second: 191, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 13868.866211, mae: 554.430827, accuracy: 0.197917, mean_q: -34.882710, mean_eps: 0.100000\n",
      " 24740/50000: episode: 5304, duration: 0.017s, episode steps:   3, steps per second: 175, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 15684.270508, mae: 538.323669, accuracy: 0.166667, mean_q: -31.571692, mean_eps: 0.100000\n",
      " 24743/50000: episode: 5305, duration: 0.015s, episode steps:   3, steps per second: 201, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 11492.574870, mae: 548.550252, accuracy: 0.250000, mean_q: -33.600635, mean_eps: 0.100000\n",
      " 24746/50000: episode: 5306, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 12617.662760, mae: 547.444051, accuracy: 0.208333, mean_q: -54.340375, mean_eps: 0.100000\n",
      " 24749/50000: episode: 5307, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 12679.523438, mae: 542.881266, accuracy: 0.260417, mean_q: -45.680899, mean_eps: 0.100000\n",
      " 24752/50000: episode: 5308, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 12791.328125, mae: 533.134969, accuracy: 0.208333, mean_q: -61.623288, mean_eps: 0.100000\n",
      " 24755/50000: episode: 5309, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 10345.664062, mae: 544.569377, accuracy: 0.177083, mean_q: -33.495942, mean_eps: 0.100000\n",
      " 24760/50000: episode: 5310, duration: 0.018s, episode steps:   5, steps per second: 270, episode reward: -2193.000, mean reward: -438.600 [-999.000, -32.000], mean action: 1.800 [0.000, 3.000],  loss: 14089.197852, mae: 550.228552, accuracy: 0.206250, mean_q: -43.200936, mean_eps: 0.100000\n",
      " 24763/50000: episode: 5311, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 13683.279297, mae: 549.350179, accuracy: 0.197917, mean_q: -31.204001, mean_eps: 0.100000\n",
      " 24766/50000: episode: 5312, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 12269.902018, mae: 560.826050, accuracy: 0.208333, mean_q: -15.469502, mean_eps: 0.100000\n",
      " 24769/50000: episode: 5313, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 11663.229492, mae: 533.663818, accuracy: 0.187500, mean_q: -24.001804, mean_eps: 0.100000\n",
      " 24772/50000: episode: 5314, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 10390.954753, mae: 546.834717, accuracy: 0.250000, mean_q: -57.906127, mean_eps: 0.100000\n",
      " 24775/50000: episode: 5315, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 12001.250977, mae: 541.302979, accuracy: 0.291667, mean_q: -37.249325, mean_eps: 0.100000\n",
      " 24778/50000: episode: 5316, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 13509.880859, mae: 538.104411, accuracy: 0.239583, mean_q: -70.991379, mean_eps: 0.100000\n",
      " 24781/50000: episode: 5317, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 11028.537760, mae: 541.843343, accuracy: 0.197917, mean_q: -62.120028, mean_eps: 0.100000\n",
      " 24784/50000: episode: 5318, duration: 0.019s, episode steps:   3, steps per second: 158, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 12514.427409, mae: 534.468262, accuracy: 0.208333, mean_q: -37.569506, mean_eps: 0.100000\n",
      " 24787/50000: episode: 5319, duration: 0.016s, episode steps:   3, steps per second: 186, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 11596.777344, mae: 550.748271, accuracy: 0.239583, mean_q: -39.673733, mean_eps: 0.100000\n",
      " 24790/50000: episode: 5320, duration: 0.013s, episode steps:   3, steps per second: 230, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 13718.402018, mae: 558.897746, accuracy: 0.218750, mean_q: -54.820253, mean_eps: 0.100000\n",
      " 24793/50000: episode: 5321, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 13168.980143, mae: 559.067912, accuracy: 0.229167, mean_q: -48.311417, mean_eps: 0.100000\n",
      " 24796/50000: episode: 5322, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 12666.466146, mae: 544.149028, accuracy: 0.187500, mean_q: -37.178272, mean_eps: 0.100000\n",
      " 24800/50000: episode: 5323, duration: 0.015s, episode steps:   4, steps per second: 263, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 12794.523438, mae: 546.208008, accuracy: 0.242188, mean_q: -36.749607, mean_eps: 0.100000\n",
      " 24803/50000: episode: 5324, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 12631.409505, mae: 554.580505, accuracy: 0.229167, mean_q: -33.751091, mean_eps: 0.100000\n",
      " 24806/50000: episode: 5325, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 11073.818034, mae: 538.089294, accuracy: 0.270833, mean_q: -46.899307, mean_eps: 0.100000\n",
      " 24809/50000: episode: 5326, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 13119.208659, mae: 556.001933, accuracy: 0.187500, mean_q: -76.699333, mean_eps: 0.100000\n",
      " 24813/50000: episode: 5327, duration: 0.016s, episode steps:   4, steps per second: 255, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 11950.705566, mae: 555.675140, accuracy: 0.250000, mean_q: -40.314846, mean_eps: 0.100000\n",
      " 24816/50000: episode: 5328, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 15200.878906, mae: 556.333496, accuracy: 0.250000, mean_q: -66.658723, mean_eps: 0.100000\n",
      " 24820/50000: episode: 5329, duration: 0.016s, episode steps:   4, steps per second: 256, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.000 [0.000, 3.000],  loss: 14407.033203, mae: 545.794678, accuracy: 0.304688, mean_q: -62.235362, mean_eps: 0.100000\n",
      " 24824/50000: episode: 5330, duration: 0.015s, episode steps:   4, steps per second: 261, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 11973.298340, mae: 540.531708, accuracy: 0.218750, mean_q: -38.300751, mean_eps: 0.100000\n",
      " 24827/50000: episode: 5331, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 11225.677734, mae: 556.477620, accuracy: 0.177083, mean_q: -23.815472, mean_eps: 0.100000\n",
      " 24830/50000: episode: 5332, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 13692.949544, mae: 554.403178, accuracy: 0.177083, mean_q: -43.090130, mean_eps: 0.100000\n",
      " 24833/50000: episode: 5333, duration: 0.015s, episode steps:   3, steps per second: 194, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 11423.490234, mae: 543.127258, accuracy: 0.177083, mean_q: -21.205839, mean_eps: 0.100000\n",
      " 24836/50000: episode: 5334, duration: 0.015s, episode steps:   3, steps per second: 197, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 12534.457682, mae: 551.672506, accuracy: 0.229167, mean_q: -42.352556, mean_eps: 0.100000\n",
      " 24839/50000: episode: 5335, duration: 0.014s, episode steps:   3, steps per second: 208, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 12658.928385, mae: 554.228678, accuracy: 0.166667, mean_q: -34.692858, mean_eps: 0.100000\n",
      " 24842/50000: episode: 5336, duration: 0.014s, episode steps:   3, steps per second: 208, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 12375.309896, mae: 558.648560, accuracy: 0.166667, mean_q: -66.106758, mean_eps: 0.100000\n",
      " 24846/50000: episode: 5337, duration: 0.017s, episode steps:   4, steps per second: 240, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 13142.146973, mae: 545.220436, accuracy: 0.234375, mean_q: -75.484936, mean_eps: 0.100000\n",
      " 24849/50000: episode: 5338, duration: 0.014s, episode steps:   3, steps per second: 210, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 12362.658854, mae: 540.532593, accuracy: 0.218750, mean_q: -57.544504, mean_eps: 0.100000\n",
      " 24852/50000: episode: 5339, duration: 0.014s, episode steps:   3, steps per second: 219, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 12656.972656, mae: 542.937561, accuracy: 0.291667, mean_q: -64.565379, mean_eps: 0.100000\n",
      " 24855/50000: episode: 5340, duration: 0.015s, episode steps:   3, steps per second: 205, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 14934.184570, mae: 556.657308, accuracy: 0.218750, mean_q: -52.520486, mean_eps: 0.100000\n",
      " 24858/50000: episode: 5341, duration: 0.015s, episode steps:   3, steps per second: 199, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 12385.527018, mae: 566.193359, accuracy: 0.229167, mean_q: -40.641576, mean_eps: 0.100000\n",
      " 24862/50000: episode: 5342, duration: 0.017s, episode steps:   4, steps per second: 237, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 12843.645508, mae: 553.203217, accuracy: 0.289062, mean_q: -52.019403, mean_eps: 0.100000\n",
      " 24865/50000: episode: 5343, duration: 0.013s, episode steps:   3, steps per second: 224, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 12719.579427, mae: 537.368225, accuracy: 0.270833, mean_q: -59.554385, mean_eps: 0.100000\n",
      " 24869/50000: episode: 5344, duration: 0.017s, episode steps:   4, steps per second: 242, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 13337.421875, mae: 543.626862, accuracy: 0.234375, mean_q: -33.003823, mean_eps: 0.100000\n",
      " 24872/50000: episode: 5345, duration: 0.013s, episode steps:   3, steps per second: 223, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 10874.750651, mae: 554.796834, accuracy: 0.250000, mean_q: -50.074987, mean_eps: 0.100000\n",
      " 24875/50000: episode: 5346, duration: 0.015s, episode steps:   3, steps per second: 200, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 11651.501302, mae: 550.304728, accuracy: 0.239583, mean_q: -29.558241, mean_eps: 0.100000\n",
      " 24878/50000: episode: 5347, duration: 0.015s, episode steps:   3, steps per second: 199, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 12328.481120, mae: 538.002360, accuracy: 0.312500, mean_q: -49.135921, mean_eps: 0.100000\n",
      " 24881/50000: episode: 5348, duration: 0.015s, episode steps:   3, steps per second: 203, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 10921.258138, mae: 532.233297, accuracy: 0.281250, mean_q: -44.296089, mean_eps: 0.100000\n",
      " 24884/50000: episode: 5349, duration: 0.018s, episode steps:   3, steps per second: 167, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 10846.662760, mae: 558.011292, accuracy: 0.291667, mean_q: -15.143473, mean_eps: 0.100000\n",
      " 24887/50000: episode: 5350, duration: 0.015s, episode steps:   3, steps per second: 199, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 14298.480794, mae: 559.444926, accuracy: 0.208333, mean_q: -50.247622, mean_eps: 0.100000\n",
      " 24890/50000: episode: 5351, duration: 0.015s, episode steps:   3, steps per second: 202, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 12069.753906, mae: 547.510946, accuracy: 0.093750, mean_q: -50.774682, mean_eps: 0.100000\n",
      " 24894/50000: episode: 5352, duration: 0.017s, episode steps:   4, steps per second: 240, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 12126.530273, mae: 566.993286, accuracy: 0.171875, mean_q: -32.504894, mean_eps: 0.100000\n",
      " 24898/50000: episode: 5353, duration: 0.016s, episode steps:   4, steps per second: 245, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 12170.303711, mae: 549.387421, accuracy: 0.234375, mean_q: -49.794607, mean_eps: 0.100000\n",
      " 24901/50000: episode: 5354, duration: 0.014s, episode steps:   3, steps per second: 218, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 11209.645833, mae: 555.184041, accuracy: 0.208333, mean_q: -62.210032, mean_eps: 0.100000\n",
      " 24904/50000: episode: 5355, duration: 0.014s, episode steps:   3, steps per second: 209, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 11697.737305, mae: 558.619446, accuracy: 0.208333, mean_q: -62.060496, mean_eps: 0.100000\n",
      " 24907/50000: episode: 5356, duration: 0.014s, episode steps:   3, steps per second: 217, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 11181.092448, mae: 552.622192, accuracy: 0.177083, mean_q: -40.290914, mean_eps: 0.100000\n",
      " 24910/50000: episode: 5357, duration: 0.014s, episode steps:   3, steps per second: 215, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 12581.133789, mae: 539.387797, accuracy: 0.312500, mean_q: -56.757268, mean_eps: 0.100000\n",
      " 24914/50000: episode: 5358, duration: 0.017s, episode steps:   4, steps per second: 241, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 11599.863770, mae: 544.864799, accuracy: 0.179688, mean_q: -42.824899, mean_eps: 0.100000\n",
      " 24917/50000: episode: 5359, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 11068.609049, mae: 550.735616, accuracy: 0.260417, mean_q: -60.388812, mean_eps: 0.100000\n",
      " 24920/50000: episode: 5360, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 10223.929688, mae: 548.528198, accuracy: 0.187500, mean_q: -63.588523, mean_eps: 0.100000\n",
      " 24924/50000: episode: 5361, duration: 0.017s, episode steps:   4, steps per second: 240, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 9536.390137, mae: 526.125237, accuracy: 0.195312, mean_q: -56.479737, mean_eps: 0.100000\n",
      " 24927/50000: episode: 5362, duration: 0.015s, episode steps:   3, steps per second: 197, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 10883.019857, mae: 545.722453, accuracy: 0.156250, mean_q: -58.008694, mean_eps: 0.100000\n",
      " 24930/50000: episode: 5363, duration: 0.017s, episode steps:   3, steps per second: 178, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 12943.483724, mae: 559.825907, accuracy: 0.145833, mean_q: -69.300626, mean_eps: 0.100000\n",
      " 24933/50000: episode: 5364, duration: 0.014s, episode steps:   3, steps per second: 209, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 9042.167969, mae: 538.150777, accuracy: 0.197917, mean_q: -12.385060, mean_eps: 0.100000\n",
      " 24936/50000: episode: 5365, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 10134.233724, mae: 554.111003, accuracy: 0.208333, mean_q: -70.759654, mean_eps: 0.100000\n",
      " 24939/50000: episode: 5366, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 11737.793620, mae: 537.858358, accuracy: 0.177083, mean_q: -55.002047, mean_eps: 0.100000\n",
      " 24942/50000: episode: 5367, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 12212.631185, mae: 534.940308, accuracy: 0.312500, mean_q: -50.037675, mean_eps: 0.100000\n",
      " 24946/50000: episode: 5368, duration: 0.015s, episode steps:   4, steps per second: 262, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.000 [0.000, 3.000],  loss: 11786.424316, mae: 553.551575, accuracy: 0.203125, mean_q: -48.495630, mean_eps: 0.100000\n",
      " 24949/50000: episode: 5369, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 11236.911458, mae: 557.388590, accuracy: 0.197917, mean_q: -58.459087, mean_eps: 0.100000\n",
      " 24952/50000: episode: 5370, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 13104.483073, mae: 559.855835, accuracy: 0.166667, mean_q: -69.206687, mean_eps: 0.100000\n",
      " 24955/50000: episode: 5371, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -224.000, mean reward: -74.667 [-116.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 12314.074870, mae: 545.783976, accuracy: 0.156250, mean_q: -18.009185, mean_eps: 0.100000\n",
      " 24958/50000: episode: 5372, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 11935.194987, mae: 548.833171, accuracy: 0.197917, mean_q: -42.662905, mean_eps: 0.100000\n",
      " 24961/50000: episode: 5373, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 13381.870768, mae: 547.998596, accuracy: 0.260417, mean_q: -49.302099, mean_eps: 0.100000\n",
      " 24964/50000: episode: 5374, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 12489.195964, mae: 545.298014, accuracy: 0.218750, mean_q: -57.670034, mean_eps: 0.100000\n",
      " 24967/50000: episode: 5375, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 14021.125000, mae: 547.302836, accuracy: 0.208333, mean_q: -75.068307, mean_eps: 0.100000\n",
      " 24970/50000: episode: 5376, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 11518.455078, mae: 541.971069, accuracy: 0.239583, mean_q: -51.698761, mean_eps: 0.100000\n",
      " 24973/50000: episode: 5377, duration: 0.022s, episode steps:   3, steps per second: 135, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 11549.032878, mae: 534.568441, accuracy: 0.187500, mean_q: -27.612624, mean_eps: 0.100000\n",
      " 24976/50000: episode: 5378, duration: 0.018s, episode steps:   3, steps per second: 169, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 11144.370117, mae: 553.501709, accuracy: 0.145833, mean_q: -39.970570, mean_eps: 0.100000\n",
      " 24979/50000: episode: 5379, duration: 0.015s, episode steps:   3, steps per second: 197, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 13315.302734, mae: 555.541768, accuracy: 0.187500, mean_q: -43.379537, mean_eps: 0.100000\n",
      " 24982/50000: episode: 5380, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 13508.716471, mae: 565.795898, accuracy: 0.177083, mean_q: -37.937202, mean_eps: 0.100000\n",
      " 24985/50000: episode: 5381, duration: 0.014s, episode steps:   3, steps per second: 216, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 11239.363607, mae: 545.200582, accuracy: 0.187500, mean_q: -41.566525, mean_eps: 0.100000\n",
      " 24988/50000: episode: 5382, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 12900.630534, mae: 550.493306, accuracy: 0.197917, mean_q: -40.402706, mean_eps: 0.100000\n",
      " 24991/50000: episode: 5383, duration: 0.013s, episode steps:   3, steps per second: 224, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 13618.919596, mae: 572.338298, accuracy: 0.187500, mean_q: -25.836435, mean_eps: 0.100000\n",
      " 24996/50000: episode: 5384, duration: 0.018s, episode steps:   5, steps per second: 274, episode reward: -2193.000, mean reward: -438.600 [-999.000, -32.000], mean action: 1.400 [0.000, 3.000],  loss: 11567.157227, mae: 527.768042, accuracy: 0.212500, mean_q: -50.118614, mean_eps: 0.100000\n",
      " 24999/50000: episode: 5385, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 11435.756510, mae: 550.179281, accuracy: 0.218750, mean_q: -62.703724, mean_eps: 0.100000\n",
      " 25002/50000: episode: 5386, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 11452.229818, mae: 548.928101, accuracy: 0.197917, mean_q: -36.353288, mean_eps: 0.100000\n",
      " 25005/50000: episode: 5387, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 12523.436849, mae: 564.108276, accuracy: 0.156250, mean_q: -62.695172, mean_eps: 0.100000\n",
      " 25009/50000: episode: 5388, duration: 0.015s, episode steps:   4, steps per second: 265, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 10617.227539, mae: 538.041924, accuracy: 0.234375, mean_q: -39.141631, mean_eps: 0.100000\n",
      " 25012/50000: episode: 5389, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 12458.648112, mae: 550.137451, accuracy: 0.239583, mean_q: -73.089007, mean_eps: 0.100000\n",
      " 25015/50000: episode: 5390, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 11802.897135, mae: 551.681356, accuracy: 0.197917, mean_q: -76.780192, mean_eps: 0.100000\n",
      " 25019/50000: episode: 5391, duration: 0.016s, episode steps:   4, steps per second: 244, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 11008.797363, mae: 561.186249, accuracy: 0.171875, mean_q: -32.441549, mean_eps: 0.100000\n",
      " 25023/50000: episode: 5392, duration: 0.018s, episode steps:   4, steps per second: 216, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 11214.024414, mae: 570.560440, accuracy: 0.171875, mean_q: -42.008153, mean_eps: 0.100000\n",
      " 25027/50000: episode: 5393, duration: 0.017s, episode steps:   4, steps per second: 229, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 11973.304199, mae: 552.855621, accuracy: 0.226562, mean_q: -66.924896, mean_eps: 0.100000\n",
      " 25030/50000: episode: 5394, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 12756.113607, mae: 548.584025, accuracy: 0.208333, mean_q: -42.653470, mean_eps: 0.100000\n",
      " 25033/50000: episode: 5395, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 11274.143229, mae: 540.860962, accuracy: 0.250000, mean_q: -42.311251, mean_eps: 0.100000\n",
      " 25036/50000: episode: 5396, duration: 0.012s, episode steps:   3, steps per second: 254, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 12777.283854, mae: 552.666199, accuracy: 0.197917, mean_q: -48.826440, mean_eps: 0.100000\n",
      " 25039/50000: episode: 5397, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 9943.163737, mae: 548.797058, accuracy: 0.239583, mean_q: -51.010436, mean_eps: 0.100000\n",
      " 25043/50000: episode: 5398, duration: 0.016s, episode steps:   4, steps per second: 252, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 12479.574951, mae: 547.127670, accuracy: 0.195312, mean_q: -71.511433, mean_eps: 0.100000\n",
      " 25046/50000: episode: 5399, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 11059.492839, mae: 562.495280, accuracy: 0.177083, mean_q: -28.761538, mean_eps: 0.100000\n",
      " 25049/50000: episode: 5400, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 12447.032227, mae: 561.436564, accuracy: 0.239583, mean_q: -52.297485, mean_eps: 0.100000\n",
      " 25052/50000: episode: 5401, duration: 0.012s, episode steps:   3, steps per second: 254, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 13606.012695, mae: 537.775248, accuracy: 0.177083, mean_q: -63.313323, mean_eps: 0.100000\n",
      " 25055/50000: episode: 5402, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 14083.357422, mae: 536.073954, accuracy: 0.239583, mean_q: -49.586285, mean_eps: 0.100000\n",
      " 25058/50000: episode: 5403, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 11362.527669, mae: 542.670990, accuracy: 0.218750, mean_q: -67.593608, mean_eps: 0.100000\n",
      " 25061/50000: episode: 5404, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 11094.318034, mae: 548.438395, accuracy: 0.177083, mean_q: -88.100235, mean_eps: 0.100000\n",
      " 25064/50000: episode: 5405, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 11949.712240, mae: 537.501139, accuracy: 0.229167, mean_q: -33.115906, mean_eps: 0.100000\n",
      " 25067/50000: episode: 5406, duration: 0.016s, episode steps:   3, steps per second: 192, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 13357.057617, mae: 538.802307, accuracy: 0.177083, mean_q: -48.286152, mean_eps: 0.100000\n",
      " 25073/50000: episode: 5407, duration: 0.033s, episode steps:   6, steps per second: 182, episode reward: -3192.000, mean reward: -532.000 [-999.000, -58.000], mean action: 1.833 [0.000, 3.000],  loss: 11927.372396, mae: 543.492167, accuracy: 0.234375, mean_q: -34.343970, mean_eps: 0.100000\n",
      " 25076/50000: episode: 5408, duration: 0.013s, episode steps:   3, steps per second: 223, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 10881.816406, mae: 523.879130, accuracy: 0.197917, mean_q: -38.824179, mean_eps: 0.100000\n",
      " 25079/50000: episode: 5409, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 10438.180664, mae: 525.136698, accuracy: 0.093750, mean_q: -55.414453, mean_eps: 0.100000\n",
      " 25083/50000: episode: 5410, duration: 0.015s, episode steps:   4, steps per second: 263, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 2.250 [1.000, 3.000],  loss: 14122.399902, mae: 555.884598, accuracy: 0.179688, mean_q: -45.676682, mean_eps: 0.100000\n",
      " 25086/50000: episode: 5411, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 15023.351888, mae: 561.013387, accuracy: 0.156250, mean_q: -51.407763, mean_eps: 0.100000\n",
      " 25089/50000: episode: 5412, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 12982.847331, mae: 540.155416, accuracy: 0.218750, mean_q: -28.422002, mean_eps: 0.100000\n",
      " 25092/50000: episode: 5413, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 11845.374674, mae: 570.488403, accuracy: 0.229167, mean_q: -50.830202, mean_eps: 0.100000\n",
      " 25095/50000: episode: 5414, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 11998.782552, mae: 556.076986, accuracy: 0.104167, mean_q: -25.273402, mean_eps: 0.100000\n",
      " 25098/50000: episode: 5415, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 11850.123698, mae: 555.755920, accuracy: 0.135417, mean_q: -46.068947, mean_eps: 0.100000\n",
      " 25101/50000: episode: 5416, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 11074.155924, mae: 547.132955, accuracy: 0.260417, mean_q: -70.211114, mean_eps: 0.100000\n",
      " 25104/50000: episode: 5417, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 12332.336263, mae: 554.984233, accuracy: 0.250000, mean_q: -46.292209, mean_eps: 0.100000\n",
      " 25107/50000: episode: 5418, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 10483.625326, mae: 557.350321, accuracy: 0.197917, mean_q: -52.105464, mean_eps: 0.100000\n",
      " 25110/50000: episode: 5419, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 10403.174805, mae: 540.251383, accuracy: 0.145833, mean_q: -45.361603, mean_eps: 0.100000\n",
      " 25113/50000: episode: 5420, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 10329.560872, mae: 542.435649, accuracy: 0.125000, mean_q: -59.680676, mean_eps: 0.100000\n",
      " 25116/50000: episode: 5421, duration: 0.014s, episode steps:   3, steps per second: 219, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 13682.887370, mae: 558.382039, accuracy: 0.239583, mean_q: -57.487979, mean_eps: 0.100000\n",
      " 25119/50000: episode: 5422, duration: 0.014s, episode steps:   3, steps per second: 219, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 11313.846680, mae: 548.595113, accuracy: 0.187500, mean_q: -59.385337, mean_eps: 0.100000\n",
      " 25123/50000: episode: 5423, duration: 0.018s, episode steps:   4, steps per second: 222, episode reward: -1238.000, mean reward: -309.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 13383.932617, mae: 538.081848, accuracy: 0.171875, mean_q: -38.327349, mean_eps: 0.100000\n",
      " 25126/50000: episode: 5424, duration: 0.014s, episode steps:   3, steps per second: 208, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 12172.545898, mae: 548.544820, accuracy: 0.166667, mean_q: -23.808660, mean_eps: 0.100000\n",
      " 25130/50000: episode: 5425, duration: 0.016s, episode steps:   4, steps per second: 253, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.000 [0.000, 3.000],  loss: 11991.598877, mae: 557.320007, accuracy: 0.156250, mean_q: -20.343644, mean_eps: 0.100000\n",
      " 25133/50000: episode: 5426, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 12468.609701, mae: 544.878357, accuracy: 0.260417, mean_q: -42.714551, mean_eps: 0.100000\n",
      " 25136/50000: episode: 5427, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 11661.288086, mae: 544.407288, accuracy: 0.229167, mean_q: -63.169451, mean_eps: 0.100000\n",
      " 25139/50000: episode: 5428, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 12291.649414, mae: 553.230367, accuracy: 0.218750, mean_q: -45.325639, mean_eps: 0.100000\n",
      " 25142/50000: episode: 5429, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 12720.886719, mae: 550.038635, accuracy: 0.197917, mean_q: -49.799795, mean_eps: 0.100000\n",
      " 25145/50000: episode: 5430, duration: 0.014s, episode steps:   3, steps per second: 220, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 10686.423177, mae: 533.957906, accuracy: 0.177083, mean_q: -55.192253, mean_eps: 0.100000\n",
      " 25148/50000: episode: 5431, duration: 0.014s, episode steps:   3, steps per second: 210, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 11337.820964, mae: 555.946208, accuracy: 0.177083, mean_q: -19.827551, mean_eps: 0.100000\n",
      " 25151/50000: episode: 5432, duration: 0.014s, episode steps:   3, steps per second: 220, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 11382.725586, mae: 545.074036, accuracy: 0.250000, mean_q: -51.432678, mean_eps: 0.100000\n",
      " 25154/50000: episode: 5433, duration: 0.014s, episode steps:   3, steps per second: 220, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 11567.664388, mae: 561.925578, accuracy: 0.166667, mean_q: -41.749966, mean_eps: 0.100000\n",
      " 25157/50000: episode: 5434, duration: 0.015s, episode steps:   3, steps per second: 197, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 11213.291341, mae: 544.247843, accuracy: 0.187500, mean_q: -54.975867, mean_eps: 0.100000\n",
      " 25160/50000: episode: 5435, duration: 0.015s, episode steps:   3, steps per second: 199, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 11547.263021, mae: 543.401876, accuracy: 0.260417, mean_q: -50.370649, mean_eps: 0.100000\n",
      " 25163/50000: episode: 5436, duration: 0.016s, episode steps:   3, steps per second: 188, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 13608.340495, mae: 549.636780, accuracy: 0.218750, mean_q: -57.888256, mean_eps: 0.100000\n",
      " 25166/50000: episode: 5437, duration: 0.016s, episode steps:   3, steps per second: 185, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 13472.130534, mae: 554.698222, accuracy: 0.302083, mean_q: -53.221873, mean_eps: 0.100000\n",
      " 25169/50000: episode: 5438, duration: 0.015s, episode steps:   3, steps per second: 198, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 11658.668294, mae: 544.648254, accuracy: 0.260417, mean_q: -23.610402, mean_eps: 0.100000\n",
      " 25172/50000: episode: 5439, duration: 0.013s, episode steps:   3, steps per second: 223, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 12874.777018, mae: 548.099996, accuracy: 0.239583, mean_q: -72.987528, mean_eps: 0.100000\n",
      " 25175/50000: episode: 5440, duration: 0.014s, episode steps:   3, steps per second: 211, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 11726.044271, mae: 556.933248, accuracy: 0.187500, mean_q: -51.125944, mean_eps: 0.100000\n",
      " 25178/50000: episode: 5441, duration: 0.013s, episode steps:   3, steps per second: 225, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 11669.248047, mae: 537.500875, accuracy: 0.135417, mean_q: -56.559790, mean_eps: 0.100000\n",
      " 25181/50000: episode: 5442, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 11259.941732, mae: 536.293396, accuracy: 0.218750, mean_q: -43.856926, mean_eps: 0.100000\n",
      " 25185/50000: episode: 5443, duration: 0.016s, episode steps:   4, steps per second: 255, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 14253.119385, mae: 529.005203, accuracy: 0.187500, mean_q: -56.637733, mean_eps: 0.100000\n",
      " 25188/50000: episode: 5444, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 13336.157878, mae: 578.235535, accuracy: 0.104167, mean_q: -39.939093, mean_eps: 0.100000\n",
      " 25193/50000: episode: 5445, duration: 0.018s, episode steps:   5, steps per second: 277, episode reward: -2193.000, mean reward: -438.600 [-999.000, -32.000], mean action: 1.200 [0.000, 3.000],  loss: 11582.495508, mae: 541.677924, accuracy: 0.187500, mean_q: -86.687849, mean_eps: 0.100000\n",
      " 25196/50000: episode: 5446, duration: 0.013s, episode steps:   3, steps per second: 230, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 11333.601888, mae: 545.361959, accuracy: 0.312500, mean_q: -52.390345, mean_eps: 0.100000\n",
      " 25199/50000: episode: 5447, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 11461.438802, mae: 548.612854, accuracy: 0.187500, mean_q: -26.376757, mean_eps: 0.100000\n",
      " 25202/50000: episode: 5448, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 11038.968099, mae: 537.364990, accuracy: 0.208333, mean_q: -69.968525, mean_eps: 0.100000\n",
      " 25205/50000: episode: 5449, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 11612.529948, mae: 546.941610, accuracy: 0.197917, mean_q: -37.754642, mean_eps: 0.100000\n",
      " 25208/50000: episode: 5450, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 11720.513346, mae: 552.445536, accuracy: 0.114583, mean_q: -59.117096, mean_eps: 0.100000\n",
      " 25211/50000: episode: 5451, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 10986.813477, mae: 556.345662, accuracy: 0.156250, mean_q: -38.739361, mean_eps: 0.100000\n",
      " 25214/50000: episode: 5452, duration: 0.020s, episode steps:   3, steps per second: 153, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 12226.427734, mae: 533.179026, accuracy: 0.239583, mean_q: -57.111606, mean_eps: 0.100000\n",
      " 25218/50000: episode: 5453, duration: 0.018s, episode steps:   4, steps per second: 219, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 11477.793701, mae: 551.026825, accuracy: 0.156250, mean_q: -58.673355, mean_eps: 0.100000\n",
      " 25221/50000: episode: 5454, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 10483.916016, mae: 525.477336, accuracy: 0.177083, mean_q: -45.266418, mean_eps: 0.100000\n",
      " 25224/50000: episode: 5455, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 10845.034180, mae: 531.508158, accuracy: 0.270833, mean_q: -51.974881, mean_eps: 0.100000\n",
      " 25227/50000: episode: 5456, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 10429.573893, mae: 536.243164, accuracy: 0.208333, mean_q: -55.457549, mean_eps: 0.100000\n",
      " 25230/50000: episode: 5457, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 12885.432292, mae: 554.128092, accuracy: 0.187500, mean_q: -52.652009, mean_eps: 0.100000\n",
      " 25233/50000: episode: 5458, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 12315.522135, mae: 539.371012, accuracy: 0.208333, mean_q: -44.056157, mean_eps: 0.100000\n",
      " 25236/50000: episode: 5459, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 11997.623698, mae: 544.717875, accuracy: 0.218750, mean_q: -36.815119, mean_eps: 0.100000\n",
      " 25240/50000: episode: 5460, duration: 0.016s, episode steps:   4, steps per second: 251, episode reward: -1238.000, mean reward: -309.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 13009.452148, mae: 551.041748, accuracy: 0.179688, mean_q: -65.959876, mean_eps: 0.100000\n",
      " 25243/50000: episode: 5461, duration: 0.013s, episode steps:   3, steps per second: 230, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 11683.712565, mae: 529.966573, accuracy: 0.260417, mean_q: -33.283026, mean_eps: 0.100000\n",
      " 25247/50000: episode: 5462, duration: 0.016s, episode steps:   4, steps per second: 243, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 13358.174072, mae: 545.456772, accuracy: 0.257812, mean_q: -44.474091, mean_eps: 0.100000\n",
      " 25250/50000: episode: 5463, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 11832.384766, mae: 538.975789, accuracy: 0.208333, mean_q: -42.715922, mean_eps: 0.100000\n",
      " 25253/50000: episode: 5464, duration: 0.013s, episode steps:   3, steps per second: 230, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 12100.652669, mae: 551.081543, accuracy: 0.250000, mean_q: -36.284953, mean_eps: 0.100000\n",
      " 25256/50000: episode: 5465, duration: 0.014s, episode steps:   3, steps per second: 217, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 12263.232747, mae: 552.653117, accuracy: 0.177083, mean_q: -53.929478, mean_eps: 0.100000\n",
      " 25259/50000: episode: 5466, duration: 0.015s, episode steps:   3, steps per second: 207, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 10404.028646, mae: 523.908813, accuracy: 0.197917, mean_q: -34.663421, mean_eps: 0.100000\n",
      " 25262/50000: episode: 5467, duration: 0.016s, episode steps:   3, steps per second: 191, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 11080.732422, mae: 540.885620, accuracy: 0.208333, mean_q: -26.933496, mean_eps: 0.100000\n",
      " 25265/50000: episode: 5468, duration: 0.015s, episode steps:   3, steps per second: 203, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 13911.108073, mae: 571.619914, accuracy: 0.166667, mean_q: -25.780441, mean_eps: 0.100000\n",
      " 25269/50000: episode: 5469, duration: 0.017s, episode steps:   4, steps per second: 242, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 11397.956787, mae: 553.572433, accuracy: 0.187500, mean_q: -61.594565, mean_eps: 0.100000\n",
      " 25272/50000: episode: 5470, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 11551.921224, mae: 542.562093, accuracy: 0.208333, mean_q: -52.965640, mean_eps: 0.100000\n",
      " 25275/50000: episode: 5471, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 12367.796549, mae: 546.964620, accuracy: 0.187500, mean_q: -24.917240, mean_eps: 0.100000\n",
      " 25278/50000: episode: 5472, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 9957.967122, mae: 545.793274, accuracy: 0.166667, mean_q: -49.127141, mean_eps: 0.100000\n",
      " 25281/50000: episode: 5473, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 12419.388997, mae: 551.235352, accuracy: 0.156250, mean_q: -48.822175, mean_eps: 0.100000\n",
      " 25284/50000: episode: 5474, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 11294.746094, mae: 554.792277, accuracy: 0.145833, mean_q: -41.564636, mean_eps: 0.100000\n",
      " 25287/50000: episode: 5475, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 12116.513997, mae: 551.010905, accuracy: 0.218750, mean_q: -38.950948, mean_eps: 0.100000\n",
      " 25290/50000: episode: 5476, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 10765.865885, mae: 546.553101, accuracy: 0.145833, mean_q: -39.459703, mean_eps: 0.100000\n",
      " 25293/50000: episode: 5477, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 11129.415039, mae: 554.748901, accuracy: 0.177083, mean_q: -36.554281, mean_eps: 0.100000\n",
      " 25296/50000: episode: 5478, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 12512.561849, mae: 566.345520, accuracy: 0.145833, mean_q: -82.805761, mean_eps: 0.100000\n",
      " 25299/50000: episode: 5479, duration: 0.012s, episode steps:   3, steps per second: 254, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 12100.547526, mae: 550.624186, accuracy: 0.291667, mean_q: -67.402756, mean_eps: 0.100000\n",
      " 25302/50000: episode: 5480, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 9518.375977, mae: 539.199972, accuracy: 0.218750, mean_q: -0.014575, mean_eps: 0.100000\n",
      " 25305/50000: episode: 5481, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 16037.536784, mae: 559.617859, accuracy: 0.239583, mean_q: -40.763223, mean_eps: 0.100000\n",
      " 25308/50000: episode: 5482, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 13472.189453, mae: 581.294759, accuracy: 0.177083, mean_q: -51.594861, mean_eps: 0.100000\n",
      " 25313/50000: episode: 5483, duration: 0.021s, episode steps:   5, steps per second: 233, episode reward: -2193.000, mean reward: -438.600 [-999.000, -32.000], mean action: 1.400 [0.000, 3.000],  loss: 11059.753906, mae: 557.808325, accuracy: 0.206250, mean_q: -41.107222, mean_eps: 0.100000\n",
      " 25317/50000: episode: 5484, duration: 0.018s, episode steps:   4, steps per second: 226, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 11925.924561, mae: 562.672668, accuracy: 0.171875, mean_q: -45.832773, mean_eps: 0.100000\n",
      " 25321/50000: episode: 5485, duration: 0.016s, episode steps:   4, steps per second: 257, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 9611.375977, mae: 550.882965, accuracy: 0.203125, mean_q: -14.598420, mean_eps: 0.100000\n",
      " 25325/50000: episode: 5486, duration: 0.016s, episode steps:   4, steps per second: 257, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 11443.978760, mae: 545.745911, accuracy: 0.179688, mean_q: -55.291893, mean_eps: 0.100000\n",
      " 25328/50000: episode: 5487, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 11503.925456, mae: 546.447876, accuracy: 0.260417, mean_q: -50.002702, mean_eps: 0.100000\n",
      " 25331/50000: episode: 5488, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 12188.353190, mae: 539.220744, accuracy: 0.166667, mean_q: -70.864919, mean_eps: 0.100000\n",
      " 25334/50000: episode: 5489, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 9953.606120, mae: 546.126444, accuracy: 0.239583, mean_q: -47.692402, mean_eps: 0.100000\n",
      " 25337/50000: episode: 5490, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 13902.259766, mae: 567.921000, accuracy: 0.125000, mean_q: -41.606321, mean_eps: 0.100000\n",
      " 25340/50000: episode: 5491, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -224.000, mean reward: -74.667 [-116.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 13954.431315, mae: 566.912292, accuracy: 0.187500, mean_q: -59.148336, mean_eps: 0.100000\n",
      " 25343/50000: episode: 5492, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 12538.594401, mae: 546.077209, accuracy: 0.250000, mean_q: -51.876597, mean_eps: 0.100000\n",
      " 25346/50000: episode: 5493, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 12148.353516, mae: 544.607422, accuracy: 0.177083, mean_q: -56.331059, mean_eps: 0.100000\n",
      " 25349/50000: episode: 5494, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 11119.920573, mae: 543.182983, accuracy: 0.218750, mean_q: -57.292920, mean_eps: 0.100000\n",
      " 25352/50000: episode: 5495, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 10989.164388, mae: 538.022786, accuracy: 0.145833, mean_q: -44.706220, mean_eps: 0.100000\n",
      " 25355/50000: episode: 5496, duration: 0.014s, episode steps:   3, steps per second: 218, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 9875.793620, mae: 551.289836, accuracy: 0.166667, mean_q: -44.998610, mean_eps: 0.100000\n",
      " 25359/50000: episode: 5497, duration: 0.027s, episode steps:   4, steps per second: 147, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.000 [0.000, 3.000],  loss: 12363.212646, mae: 545.893280, accuracy: 0.242188, mean_q: -49.394384, mean_eps: 0.100000\n",
      " 25362/50000: episode: 5498, duration: 0.030s, episode steps:   3, steps per second: 100, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 14122.191732, mae: 553.323914, accuracy: 0.187500, mean_q: -56.159551, mean_eps: 0.100000\n",
      " 25365/50000: episode: 5499, duration: 0.015s, episode steps:   3, steps per second: 195, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 10643.474609, mae: 549.294617, accuracy: 0.135417, mean_q: -36.623894, mean_eps: 0.100000\n",
      " 25368/50000: episode: 5500, duration: 0.014s, episode steps:   3, steps per second: 217, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 12904.379557, mae: 555.742289, accuracy: 0.125000, mean_q: -64.208819, mean_eps: 0.100000\n",
      " 25371/50000: episode: 5501, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 12169.878255, mae: 551.735331, accuracy: 0.322917, mean_q: -38.882653, mean_eps: 0.100000\n",
      " 25374/50000: episode: 5502, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 11474.106445, mae: 536.296672, accuracy: 0.229167, mean_q: -50.385815, mean_eps: 0.100000\n",
      " 25377/50000: episode: 5503, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 10120.079102, mae: 539.263957, accuracy: 0.156250, mean_q: -41.345605, mean_eps: 0.100000\n",
      " 25380/50000: episode: 5504, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 12011.586263, mae: 557.020833, accuracy: 0.145833, mean_q: -37.609078, mean_eps: 0.100000\n",
      " 25383/50000: episode: 5505, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 13911.572917, mae: 552.587077, accuracy: 0.197917, mean_q: -55.480307, mean_eps: 0.100000\n",
      " 25386/50000: episode: 5506, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 11921.768880, mae: 569.073120, accuracy: 0.197917, mean_q: -30.486692, mean_eps: 0.100000\n",
      " 25389/50000: episode: 5507, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 11097.370443, mae: 542.747498, accuracy: 0.239583, mean_q: -71.191577, mean_eps: 0.100000\n",
      " 25392/50000: episode: 5508, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 12369.947591, mae: 556.833618, accuracy: 0.187500, mean_q: -70.802872, mean_eps: 0.100000\n",
      " 25395/50000: episode: 5509, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 12975.613932, mae: 554.309530, accuracy: 0.208333, mean_q: -65.969045, mean_eps: 0.100000\n",
      " 25398/50000: episode: 5510, duration: 0.013s, episode steps:   3, steps per second: 226, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 10689.858724, mae: 534.639832, accuracy: 0.166667, mean_q: -53.566452, mean_eps: 0.100000\n",
      " 25401/50000: episode: 5511, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 11394.008464, mae: 527.262756, accuracy: 0.208333, mean_q: -31.255466, mean_eps: 0.100000\n",
      " 25404/50000: episode: 5512, duration: 0.014s, episode steps:   3, steps per second: 210, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 13220.040039, mae: 554.585103, accuracy: 0.218750, mean_q: -33.350817, mean_eps: 0.100000\n",
      " 25407/50000: episode: 5513, duration: 0.014s, episode steps:   3, steps per second: 212, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 12717.689779, mae: 539.819743, accuracy: 0.218750, mean_q: -26.170886, mean_eps: 0.100000\n",
      " 25410/50000: episode: 5514, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 12162.178711, mae: 525.769653, accuracy: 0.239583, mean_q: -42.688318, mean_eps: 0.100000\n",
      " 25413/50000: episode: 5515, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 11054.664714, mae: 557.521362, accuracy: 0.145833, mean_q: -42.390311, mean_eps: 0.100000\n",
      " 25416/50000: episode: 5516, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 11325.699219, mae: 544.440776, accuracy: 0.250000, mean_q: -65.052034, mean_eps: 0.100000\n",
      " 25419/50000: episode: 5517, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 11808.808594, mae: 540.488892, accuracy: 0.239583, mean_q: -36.286743, mean_eps: 0.100000\n",
      " 25422/50000: episode: 5518, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 11636.734375, mae: 528.368652, accuracy: 0.218750, mean_q: -94.099289, mean_eps: 0.100000\n",
      " 25426/50000: episode: 5519, duration: 0.016s, episode steps:   4, steps per second: 250, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 12982.480469, mae: 565.602203, accuracy: 0.234375, mean_q: -42.517172, mean_eps: 0.100000\n",
      " 25430/50000: episode: 5520, duration: 0.016s, episode steps:   4, steps per second: 247, episode reward: -1223.000, mean reward: -305.750 [-999.000, -58.000], mean action: 1.250 [0.000, 3.000],  loss: 10952.090332, mae: 534.645111, accuracy: 0.171875, mean_q: -47.867696, mean_eps: 0.100000\n",
      " 25434/50000: episode: 5521, duration: 0.016s, episode steps:   4, steps per second: 257, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 13051.629150, mae: 550.241180, accuracy: 0.187500, mean_q: -52.266056, mean_eps: 0.100000\n",
      " 25437/50000: episode: 5522, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 11193.264974, mae: 556.032654, accuracy: 0.229167, mean_q: -39.775642, mean_eps: 0.100000\n",
      " 25440/50000: episode: 5523, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 11370.060547, mae: 534.302002, accuracy: 0.187500, mean_q: -54.438490, mean_eps: 0.100000\n",
      " 25443/50000: episode: 5524, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 12163.122721, mae: 546.999715, accuracy: 0.156250, mean_q: -66.258809, mean_eps: 0.100000\n",
      " 25447/50000: episode: 5525, duration: 0.017s, episode steps:   4, steps per second: 235, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 11971.941406, mae: 544.048599, accuracy: 0.203125, mean_q: -50.245826, mean_eps: 0.100000\n",
      " 25451/50000: episode: 5526, duration: 0.022s, episode steps:   4, steps per second: 184, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 12310.519287, mae: 542.650574, accuracy: 0.234375, mean_q: -31.591108, mean_eps: 0.100000\n",
      " 25454/50000: episode: 5527, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 11281.186198, mae: 554.478617, accuracy: 0.156250, mean_q: -37.475024, mean_eps: 0.100000\n",
      " 25457/50000: episode: 5528, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 12261.694336, mae: 555.823730, accuracy: 0.229167, mean_q: -34.585310, mean_eps: 0.100000\n",
      " 25460/50000: episode: 5529, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 9497.550781, mae: 545.884745, accuracy: 0.187500, mean_q: -37.814130, mean_eps: 0.100000\n",
      " 25463/50000: episode: 5530, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 11112.909505, mae: 544.023417, accuracy: 0.145833, mean_q: -55.324408, mean_eps: 0.100000\n",
      " 25466/50000: episode: 5531, duration: 0.012s, episode steps:   3, steps per second: 253, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 9309.702474, mae: 541.961589, accuracy: 0.218750, mean_q: -32.532206, mean_eps: 0.100000\n",
      " 25469/50000: episode: 5532, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 11022.220703, mae: 542.772115, accuracy: 0.229167, mean_q: -22.897863, mean_eps: 0.100000\n",
      " 25472/50000: episode: 5533, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 9452.989583, mae: 547.915243, accuracy: 0.218750, mean_q: -39.228635, mean_eps: 0.100000\n",
      " 25475/50000: episode: 5534, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 10703.956055, mae: 549.701945, accuracy: 0.250000, mean_q: -42.336664, mean_eps: 0.100000\n",
      " 25478/50000: episode: 5535, duration: 0.012s, episode steps:   3, steps per second: 256, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 12256.777995, mae: 533.378662, accuracy: 0.197917, mean_q: -53.994034, mean_eps: 0.100000\n",
      " 25481/50000: episode: 5536, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 11194.969076, mae: 566.056295, accuracy: 0.208333, mean_q: -41.025598, mean_eps: 0.100000\n",
      " 25484/50000: episode: 5537, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 11303.276693, mae: 533.236816, accuracy: 0.239583, mean_q: -46.071882, mean_eps: 0.100000\n",
      " 25487/50000: episode: 5538, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 12154.455729, mae: 547.888753, accuracy: 0.229167, mean_q: -49.510172, mean_eps: 0.100000\n",
      " 25490/50000: episode: 5539, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 10522.610677, mae: 548.298848, accuracy: 0.197917, mean_q: -58.052686, mean_eps: 0.100000\n",
      " 25493/50000: episode: 5540, duration: 0.013s, episode steps:   3, steps per second: 226, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 12307.490885, mae: 557.873698, accuracy: 0.156250, mean_q: -78.208569, mean_eps: 0.100000\n",
      " 25496/50000: episode: 5541, duration: 0.015s, episode steps:   3, steps per second: 199, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 11479.101237, mae: 540.664754, accuracy: 0.197917, mean_q: -43.682467, mean_eps: 0.100000\n",
      " 25499/50000: episode: 5542, duration: 0.015s, episode steps:   3, steps per second: 206, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 9717.554036, mae: 531.999797, accuracy: 0.229167, mean_q: -48.045508, mean_eps: 0.100000\n",
      " 25502/50000: episode: 5543, duration: 0.013s, episode steps:   3, steps per second: 227, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 12372.944336, mae: 564.854533, accuracy: 0.187500, mean_q: -73.245597, mean_eps: 0.100000\n",
      " 25505/50000: episode: 5544, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.333 [0.000, 3.000],  loss: 11596.723958, mae: 536.211243, accuracy: 0.208333, mean_q: -59.735730, mean_eps: 0.100000\n",
      " 25508/50000: episode: 5545, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 12431.697591, mae: 549.024943, accuracy: 0.156250, mean_q: -63.795427, mean_eps: 0.100000\n",
      " 25511/50000: episode: 5546, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 10304.911458, mae: 541.625773, accuracy: 0.218750, mean_q: -39.825651, mean_eps: 0.100000\n",
      " 25514/50000: episode: 5547, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 10959.302734, mae: 537.480916, accuracy: 0.218750, mean_q: -49.805079, mean_eps: 0.100000\n",
      " 25517/50000: episode: 5548, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 12287.498047, mae: 532.332581, accuracy: 0.229167, mean_q: -55.589508, mean_eps: 0.100000\n",
      " 25520/50000: episode: 5549, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 10053.652995, mae: 534.388062, accuracy: 0.229167, mean_q: -48.879130, mean_eps: 0.100000\n",
      " 25523/50000: episode: 5550, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 10097.946615, mae: 544.229126, accuracy: 0.218750, mean_q: -41.529703, mean_eps: 0.100000\n",
      " 25526/50000: episode: 5551, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 10893.108073, mae: 563.053996, accuracy: 0.125000, mean_q: -41.359978, mean_eps: 0.100000\n",
      " 25529/50000: episode: 5552, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 11638.450195, mae: 547.818522, accuracy: 0.177083, mean_q: -25.907512, mean_eps: 0.100000\n",
      " 25532/50000: episode: 5553, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 11723.423828, mae: 545.456828, accuracy: 0.135417, mean_q: -43.792682, mean_eps: 0.100000\n",
      " 25536/50000: episode: 5554, duration: 0.016s, episode steps:   4, steps per second: 257, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 11646.170654, mae: 537.783493, accuracy: 0.234375, mean_q: -48.544878, mean_eps: 0.100000\n",
      " 25539/50000: episode: 5555, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 12950.973633, mae: 555.121480, accuracy: 0.114583, mean_q: -27.221663, mean_eps: 0.100000\n",
      " 25542/50000: episode: 5556, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 11428.698568, mae: 541.944010, accuracy: 0.260417, mean_q: -48.236351, mean_eps: 0.100000\n",
      " 25545/50000: episode: 5557, duration: 0.018s, episode steps:   3, steps per second: 163, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 11560.852539, mae: 547.388082, accuracy: 0.156250, mean_q: -37.765931, mean_eps: 0.100000\n",
      " 25549/50000: episode: 5558, duration: 0.017s, episode steps:   4, steps per second: 229, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 10773.386230, mae: 546.386612, accuracy: 0.218750, mean_q: -61.907108, mean_eps: 0.100000\n",
      " 25552/50000: episode: 5559, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 11806.677083, mae: 555.099609, accuracy: 0.156250, mean_q: -41.181513, mean_eps: 0.100000\n",
      " 25555/50000: episode: 5560, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 13246.961589, mae: 547.994792, accuracy: 0.260417, mean_q: -64.119080, mean_eps: 0.100000\n",
      " 25558/50000: episode: 5561, duration: 0.012s, episode steps:   3, steps per second: 255, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 12406.958333, mae: 549.714457, accuracy: 0.187500, mean_q: -62.410673, mean_eps: 0.100000\n",
      " 25561/50000: episode: 5562, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 10985.265299, mae: 531.948771, accuracy: 0.166667, mean_q: -53.513013, mean_eps: 0.100000\n",
      " 25565/50000: episode: 5563, duration: 0.016s, episode steps:   4, steps per second: 256, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 10654.523682, mae: 552.305817, accuracy: 0.203125, mean_q: -33.806792, mean_eps: 0.100000\n",
      " 25570/50000: episode: 5564, duration: 0.018s, episode steps:   5, steps per second: 278, episode reward: -2193.000, mean reward: -438.600 [-999.000, -32.000], mean action: 1.600 [0.000, 3.000],  loss: 11390.732422, mae: 530.951611, accuracy: 0.168750, mean_q: -54.207815, mean_eps: 0.100000\n",
      " 25573/50000: episode: 5565, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 10624.749349, mae: 552.569621, accuracy: 0.218750, mean_q: -33.501733, mean_eps: 0.100000\n",
      " 25577/50000: episode: 5566, duration: 0.016s, episode steps:   4, steps per second: 257, episode reward: -1223.000, mean reward: -305.750 [-999.000, -58.000], mean action: 1.250 [0.000, 2.000],  loss: 11933.343750, mae: 569.011627, accuracy: 0.226562, mean_q: -51.453226, mean_eps: 0.100000\n",
      " 25580/50000: episode: 5567, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -224.000, mean reward: -74.667 [-116.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 11605.781901, mae: 544.896830, accuracy: 0.218750, mean_q: -69.489363, mean_eps: 0.100000\n",
      " 25583/50000: episode: 5568, duration: 0.012s, episode steps:   3, steps per second: 255, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 10563.927083, mae: 563.384440, accuracy: 0.166667, mean_q: -54.913681, mean_eps: 0.100000\n",
      " 25586/50000: episode: 5569, duration: 0.012s, episode steps:   3, steps per second: 254, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 11732.366862, mae: 566.727519, accuracy: 0.208333, mean_q: -37.996833, mean_eps: 0.100000\n",
      " 25590/50000: episode: 5570, duration: 0.016s, episode steps:   4, steps per second: 244, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 11479.258545, mae: 540.180679, accuracy: 0.210938, mean_q: -43.495405, mean_eps: 0.100000\n",
      " 25593/50000: episode: 5571, duration: 0.014s, episode steps:   3, steps per second: 209, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 10466.104167, mae: 559.835225, accuracy: 0.166667, mean_q: -30.756714, mean_eps: 0.100000\n",
      " 25596/50000: episode: 5572, duration: 0.017s, episode steps:   3, steps per second: 177, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 12049.274089, mae: 547.618266, accuracy: 0.218750, mean_q: -63.667942, mean_eps: 0.100000\n",
      " 25599/50000: episode: 5573, duration: 0.015s, episode steps:   3, steps per second: 194, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 9283.594076, mae: 545.132243, accuracy: 0.229167, mean_q: -43.920203, mean_eps: 0.100000\n",
      " 25602/50000: episode: 5574, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 10984.648438, mae: 535.020569, accuracy: 0.177083, mean_q: -50.957694, mean_eps: 0.100000\n",
      " 25605/50000: episode: 5575, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 10527.253255, mae: 565.748311, accuracy: 0.177083, mean_q: -54.507977, mean_eps: 0.100000\n",
      " 25608/50000: episode: 5576, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 12852.542643, mae: 572.796061, accuracy: 0.187500, mean_q: -48.399820, mean_eps: 0.100000\n",
      " 25611/50000: episode: 5577, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 13978.301432, mae: 561.481750, accuracy: 0.208333, mean_q: -70.532023, mean_eps: 0.100000\n",
      " 25614/50000: episode: 5578, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 11411.978516, mae: 549.594767, accuracy: 0.218750, mean_q: -47.268091, mean_eps: 0.100000\n",
      " 25617/50000: episode: 5579, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 12030.566081, mae: 573.582153, accuracy: 0.177083, mean_q: -60.844474, mean_eps: 0.100000\n",
      " 25620/50000: episode: 5580, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 13413.398438, mae: 563.825480, accuracy: 0.250000, mean_q: -59.182346, mean_eps: 0.100000\n",
      " 25623/50000: episode: 5581, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 10690.978841, mae: 563.137024, accuracy: 0.197917, mean_q: -48.581351, mean_eps: 0.100000\n",
      " 25628/50000: episode: 5582, duration: 0.018s, episode steps:   5, steps per second: 275, episode reward: -2193.000, mean reward: -438.600 [-999.000, -45.000], mean action: 1.600 [0.000, 3.000],  loss: 11492.125781, mae: 544.645703, accuracy: 0.231250, mean_q: -52.720513, mean_eps: 0.100000\n",
      " 25631/50000: episode: 5583, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 12019.995443, mae: 548.362651, accuracy: 0.260417, mean_q: -59.799952, mean_eps: 0.100000\n",
      " 25634/50000: episode: 5584, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 10582.131510, mae: 557.536519, accuracy: 0.177083, mean_q: -52.012934, mean_eps: 0.100000\n",
      " 25637/50000: episode: 5585, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 11932.079427, mae: 542.503947, accuracy: 0.208333, mean_q: -49.156852, mean_eps: 0.100000\n",
      " 25640/50000: episode: 5586, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 12293.087565, mae: 568.761739, accuracy: 0.208333, mean_q: -44.404832, mean_eps: 0.100000\n",
      " 25643/50000: episode: 5587, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 12975.391602, mae: 562.245850, accuracy: 0.208333, mean_q: -57.871063, mean_eps: 0.100000\n",
      " 25646/50000: episode: 5588, duration: 0.019s, episode steps:   3, steps per second: 156, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 10954.206055, mae: 542.563904, accuracy: 0.208333, mean_q: -50.212316, mean_eps: 0.100000\n",
      " 25650/50000: episode: 5589, duration: 0.018s, episode steps:   4, steps per second: 228, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 11843.822021, mae: 551.175812, accuracy: 0.234375, mean_q: -39.594241, mean_eps: 0.100000\n",
      " 25653/50000: episode: 5590, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 11894.357096, mae: 561.462911, accuracy: 0.187500, mean_q: -29.439136, mean_eps: 0.100000\n",
      " 25656/50000: episode: 5591, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 12459.672526, mae: 549.260335, accuracy: 0.104167, mean_q: -60.165501, mean_eps: 0.100000\n",
      " 25659/50000: episode: 5592, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 13229.635417, mae: 562.449951, accuracy: 0.114583, mean_q: -44.267240, mean_eps: 0.100000\n",
      " 25665/50000: episode: 5593, duration: 0.022s, episode steps:   6, steps per second: 278, episode reward: -3192.000, mean reward: -532.000 [-999.000, -32.000], mean action: 1.167 [0.000, 3.000],  loss: 11444.214030, mae: 546.451253, accuracy: 0.161458, mean_q: -51.705314, mean_eps: 0.100000\n",
      " 25668/50000: episode: 5594, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 12878.888021, mae: 559.602315, accuracy: 0.187500, mean_q: -58.296417, mean_eps: 0.100000\n",
      " 25671/50000: episode: 5595, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 11030.971680, mae: 555.801229, accuracy: 0.239583, mean_q: -39.354683, mean_eps: 0.100000\n",
      " 25674/50000: episode: 5596, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 11182.125000, mae: 561.950684, accuracy: 0.135417, mean_q: -53.462882, mean_eps: 0.100000\n",
      " 25677/50000: episode: 5597, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 12119.394531, mae: 544.534241, accuracy: 0.312500, mean_q: -76.928136, mean_eps: 0.100000\n",
      " 25680/50000: episode: 5598, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 12621.315104, mae: 552.994588, accuracy: 0.208333, mean_q: -79.146844, mean_eps: 0.100000\n",
      " 25683/50000: episode: 5599, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 9957.050781, mae: 543.256734, accuracy: 0.229167, mean_q: -49.526193, mean_eps: 0.100000\n",
      " 25687/50000: episode: 5600, duration: 0.015s, episode steps:   4, steps per second: 259, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 10939.804688, mae: 556.110077, accuracy: 0.109375, mean_q: -54.195136, mean_eps: 0.100000\n",
      " 25690/50000: episode: 5601, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 10534.294596, mae: 551.501058, accuracy: 0.135417, mean_q: -29.351220, mean_eps: 0.100000\n",
      " 25694/50000: episode: 5602, duration: 0.016s, episode steps:   4, steps per second: 256, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 12483.144531, mae: 549.520157, accuracy: 0.171875, mean_q: -69.503979, mean_eps: 0.100000\n",
      " 25697/50000: episode: 5603, duration: 0.015s, episode steps:   3, steps per second: 196, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 11604.633464, mae: 558.098836, accuracy: 0.197917, mean_q: -54.592243, mean_eps: 0.100000\n",
      " 25700/50000: episode: 5604, duration: 0.014s, episode steps:   3, steps per second: 209, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 12326.841146, mae: 572.559672, accuracy: 0.125000, mean_q: -59.287137, mean_eps: 0.100000\n",
      " 25703/50000: episode: 5605, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 10895.157715, mae: 565.039815, accuracy: 0.135417, mean_q: -51.958314, mean_eps: 0.100000\n",
      " 25706/50000: episode: 5606, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 10360.524740, mae: 560.998576, accuracy: 0.208333, mean_q: -54.207411, mean_eps: 0.100000\n",
      " 25709/50000: episode: 5607, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 11413.501953, mae: 568.939921, accuracy: 0.229167, mean_q: -65.915064, mean_eps: 0.100000\n",
      " 25713/50000: episode: 5608, duration: 0.016s, episode steps:   4, steps per second: 250, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 11084.149414, mae: 541.120880, accuracy: 0.226562, mean_q: -73.574372, mean_eps: 0.100000\n",
      " 25718/50000: episode: 5609, duration: 0.019s, episode steps:   5, steps per second: 261, episode reward: -2193.000, mean reward: -438.600 [-999.000, -32.000], mean action: 1.000 [0.000, 3.000],  loss: 10878.110156, mae: 555.415613, accuracy: 0.193750, mean_q: -61.656786, mean_eps: 0.100000\n",
      " 25721/50000: episode: 5610, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 9934.555339, mae: 542.994568, accuracy: 0.187500, mean_q: -52.673334, mean_eps: 0.100000\n",
      " 25724/50000: episode: 5611, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 10570.522135, mae: 560.612874, accuracy: 0.156250, mean_q: -46.287980, mean_eps: 0.100000\n",
      " 25727/50000: episode: 5612, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 11381.519857, mae: 548.369731, accuracy: 0.135417, mean_q: -41.004487, mean_eps: 0.100000\n",
      " 25730/50000: episode: 5613, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 13033.549154, mae: 524.782715, accuracy: 0.250000, mean_q: -55.375754, mean_eps: 0.100000\n",
      " 25733/50000: episode: 5614, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 12884.774414, mae: 550.669373, accuracy: 0.145833, mean_q: -38.661032, mean_eps: 0.100000\n",
      " 25736/50000: episode: 5615, duration: 0.022s, episode steps:   3, steps per second: 138, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 11465.487956, mae: 533.588969, accuracy: 0.250000, mean_q: -42.653720, mean_eps: 0.100000\n",
      " 25739/50000: episode: 5616, duration: 0.015s, episode steps:   3, steps per second: 197, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 12836.557943, mae: 546.967407, accuracy: 0.239583, mean_q: -46.758045, mean_eps: 0.100000\n",
      " 25742/50000: episode: 5617, duration: 0.016s, episode steps:   3, steps per second: 183, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 11760.428060, mae: 558.745870, accuracy: 0.177083, mean_q: -56.902212, mean_eps: 0.100000\n",
      " 25745/50000: episode: 5618, duration: 0.015s, episode steps:   3, steps per second: 201, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 11722.115234, mae: 553.030090, accuracy: 0.197917, mean_q: -58.604258, mean_eps: 0.100000\n",
      " 25749/50000: episode: 5619, duration: 0.018s, episode steps:   4, steps per second: 228, episode reward: -1238.000, mean reward: -309.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 11169.056641, mae: 550.893036, accuracy: 0.179688, mean_q: -46.821074, mean_eps: 0.100000\n",
      " 25752/50000: episode: 5620, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 12493.124349, mae: 557.827983, accuracy: 0.156250, mean_q: -72.017220, mean_eps: 0.100000\n",
      " 25755/50000: episode: 5621, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 11372.068359, mae: 565.262024, accuracy: 0.145833, mean_q: -63.733673, mean_eps: 0.100000\n",
      " 25758/50000: episode: 5622, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 10547.228841, mae: 564.317586, accuracy: 0.208333, mean_q: -41.369911, mean_eps: 0.100000\n",
      " 25761/50000: episode: 5623, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 11501.333008, mae: 522.480306, accuracy: 0.260417, mean_q: -54.732465, mean_eps: 0.100000\n",
      " 25764/50000: episode: 5624, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 10830.942708, mae: 537.623698, accuracy: 0.187500, mean_q: -50.088539, mean_eps: 0.100000\n",
      " 25767/50000: episode: 5625, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 9848.471029, mae: 541.255636, accuracy: 0.239583, mean_q: -39.211327, mean_eps: 0.100000\n",
      " 25770/50000: episode: 5626, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 11121.764323, mae: 540.898295, accuracy: 0.229167, mean_q: -59.203075, mean_eps: 0.100000\n",
      " 25773/50000: episode: 5627, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 11713.188477, mae: 546.156453, accuracy: 0.229167, mean_q: -69.110878, mean_eps: 0.100000\n",
      " 25776/50000: episode: 5628, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 11245.916016, mae: 534.585815, accuracy: 0.177083, mean_q: -59.311278, mean_eps: 0.100000\n",
      " 25779/50000: episode: 5629, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 11857.170898, mae: 549.143555, accuracy: 0.145833, mean_q: -38.725423, mean_eps: 0.100000\n",
      " 25782/50000: episode: 5630, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 9924.909505, mae: 537.015605, accuracy: 0.156250, mean_q: -48.092934, mean_eps: 0.100000\n",
      " 25785/50000: episode: 5631, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 12376.146484, mae: 556.445190, accuracy: 0.239583, mean_q: -57.405242, mean_eps: 0.100000\n",
      " 25788/50000: episode: 5632, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 11522.441081, mae: 550.963033, accuracy: 0.239583, mean_q: -32.414315, mean_eps: 0.100000\n",
      " 25791/50000: episode: 5633, duration: 0.014s, episode steps:   3, steps per second: 214, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 10500.707357, mae: 533.435262, accuracy: 0.218750, mean_q: -59.161809, mean_eps: 0.100000\n",
      " 25794/50000: episode: 5634, duration: 0.015s, episode steps:   3, steps per second: 200, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 11199.420898, mae: 546.920349, accuracy: 0.156250, mean_q: -42.106581, mean_eps: 0.100000\n",
      " 25797/50000: episode: 5635, duration: 0.015s, episode steps:   3, steps per second: 195, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 10193.380859, mae: 549.900370, accuracy: 0.260417, mean_q: -35.950670, mean_eps: 0.100000\n",
      " 25800/50000: episode: 5636, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 11386.275716, mae: 551.014730, accuracy: 0.166667, mean_q: -29.927017, mean_eps: 0.100000\n",
      " 25803/50000: episode: 5637, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 11231.311849, mae: 556.939901, accuracy: 0.166667, mean_q: -51.624947, mean_eps: 0.100000\n",
      " 25806/50000: episode: 5638, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 12273.176758, mae: 550.232910, accuracy: 0.208333, mean_q: -55.752674, mean_eps: 0.100000\n",
      " 25809/50000: episode: 5639, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 9343.940104, mae: 548.970235, accuracy: 0.083333, mean_q: -50.054755, mean_eps: 0.100000\n",
      " 25812/50000: episode: 5640, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 11892.813151, mae: 553.689636, accuracy: 0.239583, mean_q: -51.777278, mean_eps: 0.100000\n",
      " 25815/50000: episode: 5641, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 10995.286784, mae: 558.822367, accuracy: 0.187500, mean_q: -44.812604, mean_eps: 0.100000\n",
      " 25818/50000: episode: 5642, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 11653.243490, mae: 561.336141, accuracy: 0.208333, mean_q: -74.502321, mean_eps: 0.100000\n",
      " 25821/50000: episode: 5643, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 10327.007975, mae: 557.923279, accuracy: 0.156250, mean_q: -66.582984, mean_eps: 0.100000\n",
      " 25824/50000: episode: 5644, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 8828.518066, mae: 561.552368, accuracy: 0.145833, mean_q: -33.390071, mean_eps: 0.100000\n",
      " 25828/50000: episode: 5645, duration: 0.016s, episode steps:   4, steps per second: 254, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 10087.150146, mae: 556.899857, accuracy: 0.148438, mean_q: -47.005738, mean_eps: 0.100000\n",
      " 25831/50000: episode: 5646, duration: 0.013s, episode steps:   3, steps per second: 226, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 8503.173665, mae: 531.598043, accuracy: 0.218750, mean_q: -31.593946, mean_eps: 0.100000\n",
      " 25835/50000: episode: 5647, duration: 0.016s, episode steps:   4, steps per second: 250, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 11287.036621, mae: 553.747543, accuracy: 0.195312, mean_q: -49.444862, mean_eps: 0.100000\n",
      " 25838/50000: episode: 5648, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 11606.268880, mae: 562.667826, accuracy: 0.156250, mean_q: -51.688394, mean_eps: 0.100000\n",
      " 25841/50000: episode: 5649, duration: 0.023s, episode steps:   3, steps per second: 133, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 11393.808268, mae: 563.511190, accuracy: 0.187500, mean_q: -55.566505, mean_eps: 0.100000\n",
      " 25845/50000: episode: 5650, duration: 0.019s, episode steps:   4, steps per second: 206, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 9596.799316, mae: 551.436615, accuracy: 0.179688, mean_q: -37.082200, mean_eps: 0.100000\n",
      " 25849/50000: episode: 5651, duration: 0.016s, episode steps:   4, steps per second: 252, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 10481.790527, mae: 575.386490, accuracy: 0.171875, mean_q: -29.003627, mean_eps: 0.100000\n",
      " 25852/50000: episode: 5652, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 10043.646484, mae: 564.461405, accuracy: 0.197917, mean_q: -51.017301, mean_eps: 0.100000\n",
      " 25855/50000: episode: 5653, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 10420.415365, mae: 560.263184, accuracy: 0.114583, mean_q: -59.178196, mean_eps: 0.100000\n",
      " 25859/50000: episode: 5654, duration: 0.016s, episode steps:   4, steps per second: 255, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 12113.949707, mae: 561.040024, accuracy: 0.195312, mean_q: -45.807218, mean_eps: 0.100000\n",
      " 25862/50000: episode: 5655, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 10681.279297, mae: 547.193583, accuracy: 0.208333, mean_q: -54.763686, mean_eps: 0.100000\n",
      " 25866/50000: episode: 5656, duration: 0.015s, episode steps:   4, steps per second: 258, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 9983.541260, mae: 560.365524, accuracy: 0.164062, mean_q: -50.642176, mean_eps: 0.100000\n",
      " 25869/50000: episode: 5657, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 11015.378255, mae: 545.447347, accuracy: 0.156250, mean_q: -85.589991, mean_eps: 0.100000\n",
      " 25872/50000: episode: 5658, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 11092.684896, mae: 558.738261, accuracy: 0.239583, mean_q: -103.540599, mean_eps: 0.100000\n",
      " 25875/50000: episode: 5659, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 10199.808594, mae: 542.194784, accuracy: 0.177083, mean_q: -61.094976, mean_eps: 0.100000\n",
      " 25878/50000: episode: 5660, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.333 [0.000, 3.000],  loss: 12185.193685, mae: 563.908997, accuracy: 0.145833, mean_q: -60.231647, mean_eps: 0.100000\n",
      " 25881/50000: episode: 5661, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 10550.065755, mae: 553.538269, accuracy: 0.218750, mean_q: -58.168241, mean_eps: 0.100000\n",
      " 25884/50000: episode: 5662, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 10064.234049, mae: 557.503418, accuracy: 0.166667, mean_q: -71.782210, mean_eps: 0.100000\n",
      " 25887/50000: episode: 5663, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 9774.921224, mae: 559.911560, accuracy: 0.125000, mean_q: -45.223310, mean_eps: 0.100000\n",
      " 25890/50000: episode: 5664, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 10869.011393, mae: 542.933757, accuracy: 0.291667, mean_q: -63.447407, mean_eps: 0.100000\n",
      " 25893/50000: episode: 5665, duration: 0.015s, episode steps:   3, steps per second: 198, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 11156.477539, mae: 553.668132, accuracy: 0.229167, mean_q: -75.149656, mean_eps: 0.100000\n",
      " 25897/50000: episode: 5666, duration: 0.018s, episode steps:   4, steps per second: 224, episode reward: -1223.000, mean reward: -305.750 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 11404.397705, mae: 551.909744, accuracy: 0.171875, mean_q: -62.577524, mean_eps: 0.100000\n",
      " 25900/50000: episode: 5667, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 9215.510091, mae: 543.507670, accuracy: 0.135417, mean_q: -41.360522, mean_eps: 0.100000\n",
      " 25903/50000: episode: 5668, duration: 0.013s, episode steps:   3, steps per second: 227, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 9877.834961, mae: 539.992513, accuracy: 0.218750, mean_q: -41.499842, mean_eps: 0.100000\n",
      " 25906/50000: episode: 5669, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 10543.580078, mae: 557.004598, accuracy: 0.166667, mean_q: -55.953429, mean_eps: 0.100000\n",
      " 25909/50000: episode: 5670, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 11443.870117, mae: 540.409058, accuracy: 0.166667, mean_q: -46.640067, mean_eps: 0.100000\n",
      " 25912/50000: episode: 5671, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 9993.927083, mae: 544.985840, accuracy: 0.197917, mean_q: -48.830088, mean_eps: 0.100000\n",
      " 25915/50000: episode: 5672, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 9615.232096, mae: 537.547302, accuracy: 0.125000, mean_q: -45.360573, mean_eps: 0.100000\n",
      " 25918/50000: episode: 5673, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 11141.995443, mae: 546.981934, accuracy: 0.177083, mean_q: -61.553354, mean_eps: 0.100000\n",
      " 25921/50000: episode: 5674, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 11552.073568, mae: 547.345357, accuracy: 0.177083, mean_q: -69.658178, mean_eps: 0.100000\n",
      " 25924/50000: episode: 5675, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 1.667 [0.000, 3.000],  loss: 11163.893555, mae: 561.446920, accuracy: 0.145833, mean_q: -36.992564, mean_eps: 0.100000\n",
      " 25927/50000: episode: 5676, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 10636.175781, mae: 541.709696, accuracy: 0.145833, mean_q: -40.444462, mean_eps: 0.100000\n",
      " 25930/50000: episode: 5677, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 12315.510742, mae: 578.663086, accuracy: 0.197917, mean_q: -47.970345, mean_eps: 0.100000\n",
      " 25933/50000: episode: 5678, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 9901.968750, mae: 542.396606, accuracy: 0.270833, mean_q: -45.801413, mean_eps: 0.100000\n",
      " 25936/50000: episode: 5679, duration: 0.013s, episode steps:   3, steps per second: 225, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 9445.399089, mae: 542.599386, accuracy: 0.135417, mean_q: -75.582010, mean_eps: 0.100000\n",
      " 25939/50000: episode: 5680, duration: 0.014s, episode steps:   3, steps per second: 209, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 9444.504232, mae: 551.056234, accuracy: 0.114583, mean_q: -47.905561, mean_eps: 0.100000\n",
      " 25942/50000: episode: 5681, duration: 0.014s, episode steps:   3, steps per second: 211, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 10933.148438, mae: 550.057536, accuracy: 0.229167, mean_q: -113.139186, mean_eps: 0.100000\n",
      " 25945/50000: episode: 5682, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 11447.670898, mae: 542.509827, accuracy: 0.145833, mean_q: -90.996511, mean_eps: 0.100000\n",
      " 25948/50000: episode: 5683, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 10151.933594, mae: 553.038635, accuracy: 0.187500, mean_q: -74.363558, mean_eps: 0.100000\n",
      " 25951/50000: episode: 5684, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 10240.715495, mae: 549.578308, accuracy: 0.177083, mean_q: -66.647616, mean_eps: 0.100000\n",
      " 25954/50000: episode: 5685, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 9548.937174, mae: 541.792908, accuracy: 0.218750, mean_q: -55.458227, mean_eps: 0.100000\n",
      " 25957/50000: episode: 5686, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 8611.471029, mae: 535.575277, accuracy: 0.166667, mean_q: -54.428441, mean_eps: 0.100000\n",
      " 25960/50000: episode: 5687, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 8565.605794, mae: 533.655070, accuracy: 0.177083, mean_q: -34.669831, mean_eps: 0.100000\n",
      " 25963/50000: episode: 5688, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 10946.670573, mae: 554.896301, accuracy: 0.208333, mean_q: -39.605460, mean_eps: 0.100000\n",
      " 25967/50000: episode: 5689, duration: 0.016s, episode steps:   4, steps per second: 257, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 10209.128174, mae: 557.852142, accuracy: 0.218750, mean_q: -43.027376, mean_eps: 0.100000\n",
      " 25971/50000: episode: 5690, duration: 0.016s, episode steps:   4, steps per second: 254, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 2.000 [0.000, 3.000],  loss: 12334.573730, mae: 548.849121, accuracy: 0.210938, mean_q: -84.053827, mean_eps: 0.100000\n",
      " 25975/50000: episode: 5691, duration: 0.018s, episode steps:   4, steps per second: 226, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 10463.330078, mae: 551.784531, accuracy: 0.195312, mean_q: -72.259342, mean_eps: 0.100000\n",
      " 25978/50000: episode: 5692, duration: 0.014s, episode steps:   3, steps per second: 210, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 10775.593099, mae: 549.603088, accuracy: 0.166667, mean_q: -56.325817, mean_eps: 0.100000\n",
      " 25981/50000: episode: 5693, duration: 0.013s, episode steps:   3, steps per second: 222, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 10229.165365, mae: 530.021912, accuracy: 0.125000, mean_q: -60.052429, mean_eps: 0.100000\n",
      " 25984/50000: episode: 5694, duration: 0.020s, episode steps:   3, steps per second: 153, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 11757.101562, mae: 557.429403, accuracy: 0.114583, mean_q: -54.945658, mean_eps: 0.100000\n",
      " 25988/50000: episode: 5695, duration: 0.023s, episode steps:   4, steps per second: 177, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 11052.466553, mae: 563.037415, accuracy: 0.179688, mean_q: -61.200135, mean_eps: 0.100000\n",
      " 25991/50000: episode: 5696, duration: 0.014s, episode steps:   3, steps per second: 216, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 9621.647786, mae: 536.875041, accuracy: 0.187500, mean_q: -60.596601, mean_eps: 0.100000\n",
      " 25994/50000: episode: 5697, duration: 0.016s, episode steps:   3, steps per second: 184, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.333 [0.000, 3.000],  loss: 11867.990560, mae: 553.093424, accuracy: 0.145833, mean_q: -56.563683, mean_eps: 0.100000\n",
      " 25999/50000: episode: 5698, duration: 0.028s, episode steps:   5, steps per second: 175, episode reward: -2193.000, mean reward: -438.600 [-999.000, -58.000], mean action: 1.800 [0.000, 3.000],  loss: 10964.358984, mae: 556.516675, accuracy: 0.193750, mean_q: -57.855283, mean_eps: 0.100000\n",
      " 26003/50000: episode: 5699, duration: 0.018s, episode steps:   4, steps per second: 218, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 2.250 [1.000, 3.000],  loss: 10275.560791, mae: 560.584274, accuracy: 0.125000, mean_q: -46.203246, mean_eps: 0.100000\n",
      " 26006/50000: episode: 5700, duration: 0.014s, episode steps:   3, steps per second: 210, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 11463.328125, mae: 547.088440, accuracy: 0.239583, mean_q: -59.648946, mean_eps: 0.100000\n",
      " 26010/50000: episode: 5701, duration: 0.017s, episode steps:   4, steps per second: 235, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 11983.632812, mae: 550.338226, accuracy: 0.195312, mean_q: -70.709538, mean_eps: 0.100000\n",
      " 26013/50000: episode: 5702, duration: 0.013s, episode steps:   3, steps per second: 223, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 9000.531901, mae: 543.968852, accuracy: 0.218750, mean_q: -61.477651, mean_eps: 0.100000\n",
      " 26016/50000: episode: 5703, duration: 0.013s, episode steps:   3, steps per second: 223, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 10907.481445, mae: 559.322286, accuracy: 0.239583, mean_q: -65.485424, mean_eps: 0.100000\n",
      " 26019/50000: episode: 5704, duration: 0.014s, episode steps:   3, steps per second: 211, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 12122.787760, mae: 574.535848, accuracy: 0.145833, mean_q: -61.588201, mean_eps: 0.100000\n",
      " 26022/50000: episode: 5705, duration: 0.014s, episode steps:   3, steps per second: 208, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 10265.348958, mae: 552.634501, accuracy: 0.239583, mean_q: -64.968866, mean_eps: 0.100000\n",
      " 26025/50000: episode: 5706, duration: 0.014s, episode steps:   3, steps per second: 209, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 12255.052083, mae: 543.035034, accuracy: 0.177083, mean_q: -69.121984, mean_eps: 0.100000\n",
      " 26028/50000: episode: 5707, duration: 0.016s, episode steps:   3, steps per second: 183, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 9449.249674, mae: 547.102397, accuracy: 0.218750, mean_q: -61.149408, mean_eps: 0.100000\n",
      " 26031/50000: episode: 5708, duration: 0.016s, episode steps:   3, steps per second: 193, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 11700.272786, mae: 557.628357, accuracy: 0.166667, mean_q: -60.519875, mean_eps: 0.100000\n",
      " 26034/50000: episode: 5709, duration: 0.016s, episode steps:   3, steps per second: 191, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 9439.742513, mae: 540.874756, accuracy: 0.208333, mean_q: -28.336417, mean_eps: 0.100000\n",
      " 26037/50000: episode: 5710, duration: 0.015s, episode steps:   3, steps per second: 202, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 11503.175781, mae: 560.439046, accuracy: 0.208333, mean_q: -39.700581, mean_eps: 0.100000\n",
      " 26040/50000: episode: 5711, duration: 0.014s, episode steps:   3, steps per second: 216, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 10076.708984, mae: 535.593608, accuracy: 0.239583, mean_q: -50.708627, mean_eps: 0.100000\n",
      " 26043/50000: episode: 5712, duration: 0.013s, episode steps:   3, steps per second: 227, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 10297.800456, mae: 546.007243, accuracy: 0.135417, mean_q: -47.908049, mean_eps: 0.100000\n",
      " 26046/50000: episode: 5713, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 11149.230469, mae: 565.584615, accuracy: 0.187500, mean_q: -57.358471, mean_eps: 0.100000\n",
      " 26049/50000: episode: 5714, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 10539.656901, mae: 540.728516, accuracy: 0.187500, mean_q: -58.774202, mean_eps: 0.100000\n",
      " 26052/50000: episode: 5715, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 11606.872884, mae: 544.292806, accuracy: 0.166667, mean_q: -64.613003, mean_eps: 0.100000\n",
      " 26055/50000: episode: 5716, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 9707.992839, mae: 561.846659, accuracy: 0.156250, mean_q: -41.098708, mean_eps: 0.100000\n",
      " 26058/50000: episode: 5717, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 11666.657552, mae: 551.462850, accuracy: 0.239583, mean_q: -55.445722, mean_eps: 0.100000\n",
      " 26063/50000: episode: 5718, duration: 0.019s, episode steps:   5, steps per second: 267, episode reward: -2193.000, mean reward: -438.600 [-999.000, -32.000], mean action: 1.200 [0.000, 3.000],  loss: 10624.993555, mae: 565.780713, accuracy: 0.150000, mean_q: -60.321127, mean_eps: 0.100000\n",
      " 26066/50000: episode: 5719, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 11614.058268, mae: 528.578084, accuracy: 0.156250, mean_q: -38.950290, mean_eps: 0.100000\n",
      " 26069/50000: episode: 5720, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 13355.641927, mae: 551.712036, accuracy: 0.218750, mean_q: -74.421150, mean_eps: 0.100000\n",
      " 26072/50000: episode: 5721, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 10271.879557, mae: 554.711650, accuracy: 0.239583, mean_q: -62.665530, mean_eps: 0.100000\n",
      " 26075/50000: episode: 5722, duration: 0.016s, episode steps:   3, steps per second: 188, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 9839.242025, mae: 557.219727, accuracy: 0.177083, mean_q: -51.334502, mean_eps: 0.100000\n",
      " 26078/50000: episode: 5723, duration: 0.023s, episode steps:   3, steps per second: 130, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 9952.214844, mae: 552.153341, accuracy: 0.093750, mean_q: -63.816709, mean_eps: 0.100000\n",
      " 26081/50000: episode: 5724, duration: 0.014s, episode steps:   3, steps per second: 218, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 10521.086914, mae: 544.834310, accuracy: 0.260417, mean_q: -68.264388, mean_eps: 0.100000\n",
      " 26084/50000: episode: 5725, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 10260.748698, mae: 549.745260, accuracy: 0.187500, mean_q: -55.040291, mean_eps: 0.100000\n",
      " 26087/50000: episode: 5726, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 9650.785482, mae: 557.332825, accuracy: 0.187500, mean_q: -48.323961, mean_eps: 0.100000\n",
      " 26090/50000: episode: 5727, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 8630.840332, mae: 551.371908, accuracy: 0.197917, mean_q: -50.946200, mean_eps: 0.100000\n",
      " 26093/50000: episode: 5728, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 11023.752279, mae: 542.465698, accuracy: 0.208333, mean_q: -61.894721, mean_eps: 0.100000\n",
      " 26096/50000: episode: 5729, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 10736.400716, mae: 551.538411, accuracy: 0.156250, mean_q: -48.585347, mean_eps: 0.100000\n",
      " 26099/50000: episode: 5730, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 9846.208984, mae: 562.699788, accuracy: 0.145833, mean_q: -69.640071, mean_eps: 0.100000\n",
      " 26102/50000: episode: 5731, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 10249.708333, mae: 554.113505, accuracy: 0.177083, mean_q: -49.164759, mean_eps: 0.100000\n",
      " 26105/50000: episode: 5732, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 11731.676107, mae: 557.734111, accuracy: 0.239583, mean_q: -119.951696, mean_eps: 0.100000\n",
      " 26109/50000: episode: 5733, duration: 0.016s, episode steps:   4, steps per second: 255, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 9823.675049, mae: 559.075363, accuracy: 0.179688, mean_q: -43.411386, mean_eps: 0.100000\n",
      " 26112/50000: episode: 5734, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 10667.757812, mae: 570.811300, accuracy: 0.197917, mean_q: -64.343765, mean_eps: 0.100000\n",
      " 26115/50000: episode: 5735, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 9857.831706, mae: 558.659688, accuracy: 0.177083, mean_q: -50.320892, mean_eps: 0.100000\n",
      " 26118/50000: episode: 5736, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.333 [0.000, 3.000],  loss: 11119.475911, mae: 559.196533, accuracy: 0.156250, mean_q: -57.273271, mean_eps: 0.100000\n",
      " 26122/50000: episode: 5737, duration: 0.015s, episode steps:   4, steps per second: 258, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 10416.214111, mae: 553.487000, accuracy: 0.250000, mean_q: -39.093777, mean_eps: 0.100000\n",
      " 26125/50000: episode: 5738, duration: 0.015s, episode steps:   3, steps per second: 203, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 11325.082682, mae: 560.880758, accuracy: 0.145833, mean_q: -74.945405, mean_eps: 0.100000\n",
      " 26128/50000: episode: 5739, duration: 0.016s, episode steps:   3, steps per second: 192, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 10607.634115, mae: 539.683736, accuracy: 0.145833, mean_q: -92.959656, mean_eps: 0.100000\n",
      " 26131/50000: episode: 5740, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 10325.010417, mae: 548.456014, accuracy: 0.145833, mean_q: -56.850285, mean_eps: 0.100000\n",
      " 26135/50000: episode: 5741, duration: 0.015s, episode steps:   4, steps per second: 262, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 10668.824219, mae: 545.006485, accuracy: 0.203125, mean_q: -73.253749, mean_eps: 0.100000\n",
      " 26138/50000: episode: 5742, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 8643.165527, mae: 548.606934, accuracy: 0.270833, mean_q: -68.845154, mean_eps: 0.100000\n",
      " 26142/50000: episode: 5743, duration: 0.015s, episode steps:   4, steps per second: 263, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 11524.068604, mae: 545.806870, accuracy: 0.218750, mean_q: -77.077375, mean_eps: 0.100000\n",
      " 26145/50000: episode: 5744, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 9682.846029, mae: 540.775208, accuracy: 0.218750, mean_q: -59.193747, mean_eps: 0.100000\n",
      " 26148/50000: episode: 5745, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 8517.349284, mae: 553.414225, accuracy: 0.135417, mean_q: -50.313527, mean_eps: 0.100000\n",
      " 26152/50000: episode: 5746, duration: 0.016s, episode steps:   4, steps per second: 255, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 10309.082031, mae: 550.710312, accuracy: 0.210938, mean_q: -65.643664, mean_eps: 0.100000\n",
      " 26155/50000: episode: 5747, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 10112.495280, mae: 572.830221, accuracy: 0.239583, mean_q: -54.716199, mean_eps: 0.100000\n",
      " 26158/50000: episode: 5748, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 9150.388672, mae: 541.186320, accuracy: 0.156250, mean_q: -52.250459, mean_eps: 0.100000\n",
      " 26161/50000: episode: 5749, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 9981.740397, mae: 552.879171, accuracy: 0.114583, mean_q: -62.856110, mean_eps: 0.100000\n",
      " 26165/50000: episode: 5750, duration: 0.016s, episode steps:   4, steps per second: 256, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 2.250 [1.000, 3.000],  loss: 10053.173584, mae: 558.182037, accuracy: 0.171875, mean_q: -59.012698, mean_eps: 0.100000\n",
      " 26168/50000: episode: 5751, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 10693.440430, mae: 558.844096, accuracy: 0.197917, mean_q: -46.344213, mean_eps: 0.100000\n",
      " 26173/50000: episode: 5752, duration: 0.019s, episode steps:   5, steps per second: 270, episode reward: -2193.000, mean reward: -438.600 [-999.000, -58.000], mean action: 1.800 [0.000, 3.000],  loss: 11064.966211, mae: 568.903809, accuracy: 0.181250, mean_q: -80.699199, mean_eps: 0.100000\n",
      " 26176/50000: episode: 5753, duration: 0.014s, episode steps:   3, steps per second: 222, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 9397.627279, mae: 554.931905, accuracy: 0.239583, mean_q: -55.600734, mean_eps: 0.100000\n",
      " 26179/50000: episode: 5754, duration: 0.018s, episode steps:   3, steps per second: 164, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 11180.616536, mae: 549.802165, accuracy: 0.177083, mean_q: -61.914958, mean_eps: 0.100000\n",
      " 26182/50000: episode: 5755, duration: 0.015s, episode steps:   3, steps per second: 201, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 8844.811523, mae: 561.865702, accuracy: 0.187500, mean_q: -53.063370, mean_eps: 0.100000\n",
      " 26185/50000: episode: 5756, duration: 0.013s, episode steps:   3, steps per second: 230, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.333 [0.000, 3.000],  loss: 10419.571777, mae: 538.508911, accuracy: 0.177083, mean_q: -55.046787, mean_eps: 0.100000\n",
      " 26188/50000: episode: 5757, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 9961.523763, mae: 558.074666, accuracy: 0.145833, mean_q: -41.905084, mean_eps: 0.100000\n",
      " 26191/50000: episode: 5758, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 9950.019857, mae: 558.587708, accuracy: 0.187500, mean_q: -48.996218, mean_eps: 0.100000\n",
      " 26194/50000: episode: 5759, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 9873.990560, mae: 551.928101, accuracy: 0.145833, mean_q: -87.935242, mean_eps: 0.100000\n",
      " 26198/50000: episode: 5760, duration: 0.015s, episode steps:   4, steps per second: 262, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 10723.677856, mae: 547.411133, accuracy: 0.210938, mean_q: -72.496079, mean_eps: 0.100000\n",
      " 26201/50000: episode: 5761, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 10197.942708, mae: 548.683594, accuracy: 0.281250, mean_q: -46.534978, mean_eps: 0.100000\n",
      " 26204/50000: episode: 5762, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 11787.786784, mae: 559.320699, accuracy: 0.177083, mean_q: -89.449973, mean_eps: 0.100000\n",
      " 26208/50000: episode: 5763, duration: 0.015s, episode steps:   4, steps per second: 264, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 8470.268555, mae: 567.609024, accuracy: 0.203125, mean_q: -66.709288, mean_eps: 0.100000\n",
      " 26211/50000: episode: 5764, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 11903.410156, mae: 541.795878, accuracy: 0.229167, mean_q: -111.960318, mean_eps: 0.100000\n",
      " 26214/50000: episode: 5765, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 10602.585612, mae: 555.823792, accuracy: 0.208333, mean_q: -62.318194, mean_eps: 0.100000\n",
      " 26217/50000: episode: 5766, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 8041.124023, mae: 557.950338, accuracy: 0.187500, mean_q: -57.903732, mean_eps: 0.100000\n",
      " 26220/50000: episode: 5767, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 10142.181966, mae: 556.447896, accuracy: 0.239583, mean_q: -44.715090, mean_eps: 0.100000\n",
      " 26225/50000: episode: 5768, duration: 0.021s, episode steps:   5, steps per second: 233, episode reward: -2193.000, mean reward: -438.600 [-999.000, -45.000], mean action: 1.800 [0.000, 3.000],  loss: 11493.026758, mae: 555.383716, accuracy: 0.218750, mean_q: -76.782293, mean_eps: 0.100000\n",
      " 26228/50000: episode: 5769, duration: 0.015s, episode steps:   3, steps per second: 198, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 10270.556641, mae: 553.543620, accuracy: 0.239583, mean_q: -77.821503, mean_eps: 0.100000\n",
      " 26231/50000: episode: 5770, duration: 0.013s, episode steps:   3, steps per second: 223, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 9502.762370, mae: 552.056824, accuracy: 0.218750, mean_q: -63.917403, mean_eps: 0.100000\n",
      " 26234/50000: episode: 5771, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 10916.041992, mae: 550.818726, accuracy: 0.156250, mean_q: -77.956127, mean_eps: 0.100000\n",
      " 26237/50000: episode: 5772, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 10518.616211, mae: 546.855347, accuracy: 0.218750, mean_q: -65.724884, mean_eps: 0.100000\n",
      " 26240/50000: episode: 5773, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 10552.779297, mae: 555.787435, accuracy: 0.208333, mean_q: -55.103947, mean_eps: 0.100000\n",
      " 26243/50000: episode: 5774, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 9510.850260, mae: 555.478861, accuracy: 0.218750, mean_q: -27.195530, mean_eps: 0.100000\n",
      " 26246/50000: episode: 5775, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 9575.872070, mae: 545.661153, accuracy: 0.187500, mean_q: -68.924248, mean_eps: 0.100000\n",
      " 26249/50000: episode: 5776, duration: 0.015s, episode steps:   3, steps per second: 197, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 9518.970540, mae: 548.859192, accuracy: 0.125000, mean_q: -66.870591, mean_eps: 0.100000\n",
      " 26252/50000: episode: 5777, duration: 0.017s, episode steps:   3, steps per second: 179, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 9711.516927, mae: 561.204997, accuracy: 0.197917, mean_q: -47.605051, mean_eps: 0.100000\n",
      " 26255/50000: episode: 5778, duration: 0.014s, episode steps:   3, steps per second: 215, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 1.667 [0.000, 3.000],  loss: 9380.594727, mae: 556.228190, accuracy: 0.125000, mean_q: -66.577395, mean_eps: 0.100000\n",
      " 26258/50000: episode: 5779, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 10663.486003, mae: 569.125305, accuracy: 0.197917, mean_q: -88.163747, mean_eps: 0.100000\n",
      " 26261/50000: episode: 5780, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 9053.002604, mae: 552.256958, accuracy: 0.135417, mean_q: -57.934967, mean_eps: 0.100000\n",
      " 26264/50000: episode: 5781, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 9844.076823, mae: 558.363546, accuracy: 0.208333, mean_q: -79.061963, mean_eps: 0.100000\n",
      " 26267/50000: episode: 5782, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 9136.306803, mae: 553.576640, accuracy: 0.197917, mean_q: -60.237041, mean_eps: 0.100000\n",
      " 26270/50000: episode: 5783, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 9747.444010, mae: 556.756348, accuracy: 0.239583, mean_q: -59.624231, mean_eps: 0.100000\n",
      " 26273/50000: episode: 5784, duration: 0.021s, episode steps:   3, steps per second: 142, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 9268.867513, mae: 549.656942, accuracy: 0.166667, mean_q: -37.727556, mean_eps: 0.100000\n",
      " 26277/50000: episode: 5785, duration: 0.021s, episode steps:   4, steps per second: 188, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 10301.235352, mae: 567.997086, accuracy: 0.242188, mean_q: -48.226490, mean_eps: 0.100000\n",
      " 26280/50000: episode: 5786, duration: 0.014s, episode steps:   3, steps per second: 211, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 12370.009115, mae: 574.707784, accuracy: 0.093750, mean_q: -69.455345, mean_eps: 0.100000\n",
      " 26283/50000: episode: 5787, duration: 0.014s, episode steps:   3, steps per second: 214, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 9713.561849, mae: 544.084737, accuracy: 0.197917, mean_q: -56.364048, mean_eps: 0.100000\n",
      " 26286/50000: episode: 5788, duration: 0.014s, episode steps:   3, steps per second: 221, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 10740.302083, mae: 544.205383, accuracy: 0.156250, mean_q: -73.687525, mean_eps: 0.100000\n",
      " 26289/50000: episode: 5789, duration: 0.014s, episode steps:   3, steps per second: 222, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 10812.834310, mae: 560.572144, accuracy: 0.093750, mean_q: -62.131710, mean_eps: 0.100000\n",
      " 26294/50000: episode: 5790, duration: 0.021s, episode steps:   5, steps per second: 240, episode reward: -2193.000, mean reward: -438.600 [-999.000, -45.000], mean action: 1.400 [0.000, 3.000],  loss: 9908.395898, mae: 556.861145, accuracy: 0.200000, mean_q: -55.895900, mean_eps: 0.100000\n",
      " 26297/50000: episode: 5791, duration: 0.013s, episode steps:   3, steps per second: 225, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 9883.451172, mae: 537.849426, accuracy: 0.229167, mean_q: -76.294815, mean_eps: 0.100000\n",
      " 26300/50000: episode: 5792, duration: 0.014s, episode steps:   3, steps per second: 215, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 9282.565430, mae: 544.586405, accuracy: 0.166667, mean_q: -47.335642, mean_eps: 0.100000\n",
      " 26303/50000: episode: 5793, duration: 0.014s, episode steps:   3, steps per second: 213, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 10067.145833, mae: 556.473287, accuracy: 0.177083, mean_q: -69.650683, mean_eps: 0.100000\n",
      " 26306/50000: episode: 5794, duration: 0.014s, episode steps:   3, steps per second: 210, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 10561.107422, mae: 562.786621, accuracy: 0.156250, mean_q: -33.761005, mean_eps: 0.100000\n",
      " 26310/50000: episode: 5795, duration: 0.016s, episode steps:   4, steps per second: 245, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 10051.659668, mae: 571.094833, accuracy: 0.148438, mean_q: -74.459094, mean_eps: 0.100000\n",
      " 26313/50000: episode: 5796, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 10136.977702, mae: 572.983134, accuracy: 0.197917, mean_q: -63.652532, mean_eps: 0.100000\n",
      " 26316/50000: episode: 5797, duration: 0.014s, episode steps:   3, steps per second: 218, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 7941.199056, mae: 539.939382, accuracy: 0.197917, mean_q: -51.123238, mean_eps: 0.100000\n",
      " 26319/50000: episode: 5798, duration: 0.016s, episode steps:   3, steps per second: 189, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 9446.050293, mae: 549.299581, accuracy: 0.197917, mean_q: -63.951013, mean_eps: 0.100000\n",
      " 26322/50000: episode: 5799, duration: 0.016s, episode steps:   3, steps per second: 186, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 10569.445638, mae: 574.938904, accuracy: 0.177083, mean_q: -81.903296, mean_eps: 0.100000\n",
      " 26325/50000: episode: 5800, duration: 0.015s, episode steps:   3, steps per second: 204, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 10581.812174, mae: 564.051880, accuracy: 0.208333, mean_q: -82.552559, mean_eps: 0.100000\n",
      " 26328/50000: episode: 5801, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 9864.790039, mae: 561.224040, accuracy: 0.177083, mean_q: -69.877660, mean_eps: 0.100000\n",
      " 26331/50000: episode: 5802, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 8990.704427, mae: 549.219360, accuracy: 0.229167, mean_q: -58.065179, mean_eps: 0.100000\n",
      " 26335/50000: episode: 5803, duration: 0.016s, episode steps:   4, steps per second: 252, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 9422.654785, mae: 555.906387, accuracy: 0.179688, mean_q: -73.405060, mean_eps: 0.100000\n",
      " 26338/50000: episode: 5804, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 8466.182617, mae: 561.918986, accuracy: 0.166667, mean_q: -40.550438, mean_eps: 0.100000\n",
      " 26341/50000: episode: 5805, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 8986.124023, mae: 569.187378, accuracy: 0.156250, mean_q: -72.260207, mean_eps: 0.100000\n",
      " 26344/50000: episode: 5806, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 10406.285807, mae: 541.876282, accuracy: 0.239583, mean_q: -65.293617, mean_eps: 0.100000\n",
      " 26347/50000: episode: 5807, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 7827.374837, mae: 544.940328, accuracy: 0.187500, mean_q: -39.787392, mean_eps: 0.100000\n",
      " 26351/50000: episode: 5808, duration: 0.015s, episode steps:   4, steps per second: 264, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.000 [0.000, 3.000],  loss: 10807.906738, mae: 577.107834, accuracy: 0.171875, mean_q: -61.761374, mean_eps: 0.100000\n",
      " 26354/50000: episode: 5809, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 8965.912598, mae: 552.591756, accuracy: 0.187500, mean_q: -74.030952, mean_eps: 0.100000\n",
      " 26357/50000: episode: 5810, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 9074.028646, mae: 558.943258, accuracy: 0.177083, mean_q: -63.753249, mean_eps: 0.100000\n",
      " 26360/50000: episode: 5811, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 8092.702799, mae: 556.641195, accuracy: 0.250000, mean_q: -75.054647, mean_eps: 0.100000\n",
      " 26363/50000: episode: 5812, duration: 0.017s, episode steps:   3, steps per second: 180, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 9150.492513, mae: 563.097209, accuracy: 0.239583, mean_q: -38.395102, mean_eps: 0.100000\n",
      " 26366/50000: episode: 5813, duration: 0.017s, episode steps:   3, steps per second: 180, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 8922.272298, mae: 560.848551, accuracy: 0.218750, mean_q: -61.373066, mean_eps: 0.100000\n",
      " 26369/50000: episode: 5814, duration: 0.015s, episode steps:   3, steps per second: 198, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 9124.751302, mae: 556.219849, accuracy: 0.166667, mean_q: -86.633436, mean_eps: 0.100000\n",
      " 26372/50000: episode: 5815, duration: 0.013s, episode steps:   3, steps per second: 227, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 11691.173828, mae: 551.436056, accuracy: 0.187500, mean_q: -59.315135, mean_eps: 0.100000\n",
      " 26376/50000: episode: 5816, duration: 0.016s, episode steps:   4, steps per second: 243, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 8526.233398, mae: 570.159653, accuracy: 0.101562, mean_q: -54.056445, mean_eps: 0.100000\n",
      " 26379/50000: episode: 5817, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 9487.456706, mae: 562.963725, accuracy: 0.250000, mean_q: -56.315008, mean_eps: 0.100000\n",
      " 26383/50000: episode: 5818, duration: 0.016s, episode steps:   4, steps per second: 249, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 10181.978760, mae: 570.339706, accuracy: 0.132812, mean_q: -61.175345, mean_eps: 0.100000\n",
      " 26386/50000: episode: 5819, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 11874.704427, mae: 574.915792, accuracy: 0.187500, mean_q: -60.316083, mean_eps: 0.100000\n",
      " 26389/50000: episode: 5820, duration: 0.013s, episode steps:   3, steps per second: 230, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 9663.070312, mae: 568.278910, accuracy: 0.114583, mean_q: -48.846550, mean_eps: 0.100000\n",
      " 26392/50000: episode: 5821, duration: 0.014s, episode steps:   3, steps per second: 221, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 9273.813965, mae: 550.711955, accuracy: 0.208333, mean_q: -59.370213, mean_eps: 0.100000\n",
      " 26396/50000: episode: 5822, duration: 0.017s, episode steps:   4, steps per second: 237, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 9739.543213, mae: 554.473892, accuracy: 0.265625, mean_q: -75.039631, mean_eps: 0.100000\n",
      " 26399/50000: episode: 5823, duration: 0.014s, episode steps:   3, steps per second: 212, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 10463.449219, mae: 561.290385, accuracy: 0.177083, mean_q: -71.750921, mean_eps: 0.100000\n",
      " 26402/50000: episode: 5824, duration: 0.014s, episode steps:   3, steps per second: 215, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 9481.813965, mae: 562.676290, accuracy: 0.208333, mean_q: -48.390157, mean_eps: 0.100000\n",
      " 26405/50000: episode: 5825, duration: 0.014s, episode steps:   3, steps per second: 211, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 10873.946940, mae: 570.463928, accuracy: 0.208333, mean_q: -58.672326, mean_eps: 0.100000\n",
      " 26409/50000: episode: 5826, duration: 0.020s, episode steps:   4, steps per second: 197, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 2.250 [1.000, 3.000],  loss: 10183.098145, mae: 559.093521, accuracy: 0.164062, mean_q: -61.509612, mean_eps: 0.100000\n",
      " 26412/50000: episode: 5827, duration: 0.016s, episode steps:   3, steps per second: 186, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 10963.847982, mae: 562.932943, accuracy: 0.177083, mean_q: -60.471433, mean_eps: 0.100000\n",
      " 26415/50000: episode: 5828, duration: 0.015s, episode steps:   3, steps per second: 204, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 9162.750326, mae: 546.412292, accuracy: 0.166667, mean_q: -67.526121, mean_eps: 0.100000\n",
      " 26418/50000: episode: 5829, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 8705.705729, mae: 545.732503, accuracy: 0.156250, mean_q: -90.996768, mean_eps: 0.100000\n",
      " 26421/50000: episode: 5830, duration: 0.014s, episode steps:   3, steps per second: 217, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 9305.196615, mae: 564.477376, accuracy: 0.197917, mean_q: -45.122214, mean_eps: 0.100000\n",
      " 26424/50000: episode: 5831, duration: 0.014s, episode steps:   3, steps per second: 211, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 12157.619466, mae: 575.352173, accuracy: 0.156250, mean_q: -32.706474, mean_eps: 0.100000\n",
      " 26427/50000: episode: 5832, duration: 0.014s, episode steps:   3, steps per second: 210, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 9083.783203, mae: 543.478170, accuracy: 0.208333, mean_q: -43.288542, mean_eps: 0.100000\n",
      " 26430/50000: episode: 5833, duration: 0.014s, episode steps:   3, steps per second: 218, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 11368.084961, mae: 554.980469, accuracy: 0.177083, mean_q: -65.674605, mean_eps: 0.100000\n",
      " 26433/50000: episode: 5834, duration: 0.015s, episode steps:   3, steps per second: 203, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 8664.239583, mae: 548.959880, accuracy: 0.083333, mean_q: -75.177915, mean_eps: 0.100000\n",
      " 26436/50000: episode: 5835, duration: 0.014s, episode steps:   3, steps per second: 221, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 10157.451986, mae: 552.123739, accuracy: 0.156250, mean_q: -83.467150, mean_eps: 0.100000\n",
      " 26439/50000: episode: 5836, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 8813.090658, mae: 555.095032, accuracy: 0.156250, mean_q: -80.039591, mean_eps: 0.100000\n",
      " 26442/50000: episode: 5837, duration: 0.014s, episode steps:   3, steps per second: 221, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 8807.833659, mae: 558.636373, accuracy: 0.187500, mean_q: -45.435774, mean_eps: 0.100000\n",
      " 26445/50000: episode: 5838, duration: 0.014s, episode steps:   3, steps per second: 221, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 9447.579427, mae: 559.285441, accuracy: 0.166667, mean_q: -63.728151, mean_eps: 0.100000\n",
      " 26448/50000: episode: 5839, duration: 0.014s, episode steps:   3, steps per second: 211, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 10815.070312, mae: 565.435547, accuracy: 0.250000, mean_q: -64.812899, mean_eps: 0.100000\n",
      " 26453/50000: episode: 5840, duration: 0.020s, episode steps:   5, steps per second: 246, episode reward: -2193.000, mean reward: -438.600 [-999.000, -58.000], mean action: 1.200 [0.000, 3.000],  loss: 9935.104883, mae: 549.816809, accuracy: 0.175000, mean_q: -72.432764, mean_eps: 0.100000\n",
      " 26456/50000: episode: 5841, duration: 0.020s, episode steps:   3, steps per second: 152, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 9474.614909, mae: 578.614115, accuracy: 0.145833, mean_q: -54.041885, mean_eps: 0.100000\n",
      " 26459/50000: episode: 5842, duration: 0.017s, episode steps:   3, steps per second: 177, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 8536.729655, mae: 552.903483, accuracy: 0.239583, mean_q: -73.464386, mean_eps: 0.100000\n",
      " 26462/50000: episode: 5843, duration: 0.015s, episode steps:   3, steps per second: 200, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 9730.052409, mae: 541.724060, accuracy: 0.197917, mean_q: -65.195765, mean_eps: 0.100000\n",
      " 26465/50000: episode: 5844, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 11337.403320, mae: 563.427714, accuracy: 0.250000, mean_q: -65.392756, mean_eps: 0.100000\n",
      " 26468/50000: episode: 5845, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 9447.803060, mae: 553.778422, accuracy: 0.156250, mean_q: -56.191107, mean_eps: 0.100000\n",
      " 26471/50000: episode: 5846, duration: 0.014s, episode steps:   3, steps per second: 212, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 9570.098307, mae: 545.163656, accuracy: 0.166667, mean_q: -65.450256, mean_eps: 0.100000\n",
      " 26474/50000: episode: 5847, duration: 0.014s, episode steps:   3, steps per second: 208, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 9296.034342, mae: 553.351176, accuracy: 0.218750, mean_q: -54.134221, mean_eps: 0.100000\n",
      " 26477/50000: episode: 5848, duration: 0.014s, episode steps:   3, steps per second: 222, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 10363.891276, mae: 556.849569, accuracy: 0.114583, mean_q: -79.647837, mean_eps: 0.100000\n",
      " 26480/50000: episode: 5849, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 9323.569336, mae: 560.131205, accuracy: 0.239583, mean_q: -47.632561, mean_eps: 0.100000\n",
      " 26483/50000: episode: 5850, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 10447.598633, mae: 551.583049, accuracy: 0.187500, mean_q: -55.400778, mean_eps: 0.100000\n",
      " 26486/50000: episode: 5851, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 10170.213542, mae: 557.312764, accuracy: 0.187500, mean_q: -61.882559, mean_eps: 0.100000\n",
      " 26490/50000: episode: 5852, duration: 0.016s, episode steps:   4, steps per second: 252, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 8657.650391, mae: 554.235077, accuracy: 0.203125, mean_q: -73.084335, mean_eps: 0.100000\n",
      " 26493/50000: episode: 5853, duration: 0.013s, episode steps:   3, steps per second: 230, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 8581.035482, mae: 543.385783, accuracy: 0.208333, mean_q: -57.716620, mean_eps: 0.100000\n",
      " 26496/50000: episode: 5854, duration: 0.013s, episode steps:   3, steps per second: 223, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 9534.892904, mae: 565.504944, accuracy: 0.135417, mean_q: -90.469655, mean_eps: 0.100000\n",
      " 26499/50000: episode: 5855, duration: 0.013s, episode steps:   3, steps per second: 225, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 9641.684082, mae: 540.018819, accuracy: 0.197917, mean_q: -53.060102, mean_eps: 0.100000\n",
      " 26502/50000: episode: 5856, duration: 0.013s, episode steps:   3, steps per second: 224, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 9219.020833, mae: 561.437602, accuracy: 0.177083, mean_q: -56.644688, mean_eps: 0.100000\n",
      " 26505/50000: episode: 5857, duration: 0.019s, episode steps:   3, steps per second: 154, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 10122.766276, mae: 558.602966, accuracy: 0.166667, mean_q: -71.803040, mean_eps: 0.100000\n",
      " 26509/50000: episode: 5858, duration: 0.026s, episode steps:   4, steps per second: 157, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 11218.066406, mae: 563.467468, accuracy: 0.148438, mean_q: -50.236840, mean_eps: 0.100000\n",
      " 26512/50000: episode: 5859, duration: 0.014s, episode steps:   3, steps per second: 218, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 9505.370117, mae: 570.478617, accuracy: 0.177083, mean_q: -42.112399, mean_eps: 0.100000\n",
      " 26515/50000: episode: 5860, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 10083.168294, mae: 532.326233, accuracy: 0.166667, mean_q: -72.321245, mean_eps: 0.100000\n",
      " 26518/50000: episode: 5861, duration: 0.013s, episode steps:   3, steps per second: 230, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 11159.198242, mae: 554.657227, accuracy: 0.208333, mean_q: -67.673358, mean_eps: 0.100000\n",
      " 26521/50000: episode: 5862, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 10569.829427, mae: 558.735209, accuracy: 0.166667, mean_q: -66.555284, mean_eps: 0.100000\n",
      " 26524/50000: episode: 5863, duration: 0.013s, episode steps:   3, steps per second: 227, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 10307.417643, mae: 545.416768, accuracy: 0.135417, mean_q: -23.056761, mean_eps: 0.100000\n",
      " 26527/50000: episode: 5864, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 8713.729818, mae: 556.726461, accuracy: 0.135417, mean_q: -49.403615, mean_eps: 0.100000\n",
      " 26531/50000: episode: 5865, duration: 0.016s, episode steps:   4, steps per second: 248, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 9925.147461, mae: 547.265503, accuracy: 0.210938, mean_q: -28.928816, mean_eps: 0.100000\n",
      " 26534/50000: episode: 5866, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 10317.555176, mae: 548.870260, accuracy: 0.250000, mean_q: -56.787116, mean_eps: 0.100000\n",
      " 26537/50000: episode: 5867, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 12414.686849, mae: 561.517029, accuracy: 0.218750, mean_q: -65.784349, mean_eps: 0.100000\n",
      " 26540/50000: episode: 5868, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 9562.386393, mae: 564.736267, accuracy: 0.166667, mean_q: -68.412544, mean_eps: 0.100000\n",
      " 26544/50000: episode: 5869, duration: 0.016s, episode steps:   4, steps per second: 245, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 2.250 [1.000, 3.000],  loss: 9086.083496, mae: 541.531616, accuracy: 0.187500, mean_q: -46.929850, mean_eps: 0.100000\n",
      " 26547/50000: episode: 5870, duration: 0.019s, episode steps:   3, steps per second: 161, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 10328.668294, mae: 549.142293, accuracy: 0.239583, mean_q: -45.948189, mean_eps: 0.100000\n",
      " 26550/50000: episode: 5871, duration: 0.020s, episode steps:   3, steps per second: 153, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 8275.547526, mae: 554.732178, accuracy: 0.114583, mean_q: -48.734767, mean_eps: 0.100000\n",
      " 26553/50000: episode: 5872, duration: 0.015s, episode steps:   3, steps per second: 197, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 10664.591797, mae: 565.706807, accuracy: 0.166667, mean_q: -69.709229, mean_eps: 0.100000\n",
      " 26556/50000: episode: 5873, duration: 0.013s, episode steps:   3, steps per second: 222, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 10585.586263, mae: 559.987325, accuracy: 0.177083, mean_q: -83.712466, mean_eps: 0.100000\n",
      " 26560/50000: episode: 5874, duration: 0.016s, episode steps:   4, steps per second: 253, episode reward: -1223.000, mean reward: -305.750 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 8101.784668, mae: 562.942535, accuracy: 0.203125, mean_q: -64.864673, mean_eps: 0.100000\n",
      " 26563/50000: episode: 5875, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 10904.327148, mae: 553.309814, accuracy: 0.208333, mean_q: -61.901117, mean_eps: 0.100000\n",
      " 26566/50000: episode: 5876, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 9312.944987, mae: 529.961894, accuracy: 0.208333, mean_q: -57.899378, mean_eps: 0.100000\n",
      " 26570/50000: episode: 5877, duration: 0.016s, episode steps:   4, steps per second: 250, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 10214.343750, mae: 551.999847, accuracy: 0.109375, mean_q: -56.324728, mean_eps: 0.100000\n",
      " 26573/50000: episode: 5878, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 9892.176921, mae: 571.269206, accuracy: 0.187500, mean_q: -60.413930, mean_eps: 0.100000\n",
      " 26576/50000: episode: 5879, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 8810.323242, mae: 542.532389, accuracy: 0.166667, mean_q: -43.112230, mean_eps: 0.100000\n",
      " 26579/50000: episode: 5880, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 8665.636068, mae: 542.565674, accuracy: 0.229167, mean_q: -48.057486, mean_eps: 0.100000\n",
      " 26582/50000: episode: 5881, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 9316.756022, mae: 552.578166, accuracy: 0.166667, mean_q: -40.350759, mean_eps: 0.100000\n",
      " 26585/50000: episode: 5882, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 10542.150716, mae: 564.544678, accuracy: 0.156250, mean_q: -74.438779, mean_eps: 0.100000\n",
      " 26588/50000: episode: 5883, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 9062.322591, mae: 550.643778, accuracy: 0.250000, mean_q: -49.149311, mean_eps: 0.100000\n",
      " 26591/50000: episode: 5884, duration: 0.014s, episode steps:   3, steps per second: 221, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 10613.005534, mae: 574.350484, accuracy: 0.197917, mean_q: -46.149965, mean_eps: 0.100000\n",
      " 26594/50000: episode: 5885, duration: 0.013s, episode steps:   3, steps per second: 223, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 11643.234701, mae: 565.530253, accuracy: 0.187500, mean_q: -57.118065, mean_eps: 0.100000\n",
      " 26597/50000: episode: 5886, duration: 0.015s, episode steps:   3, steps per second: 195, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 9164.151367, mae: 539.281779, accuracy: 0.177083, mean_q: -70.103424, mean_eps: 0.100000\n",
      " 26601/50000: episode: 5887, duration: 0.018s, episode steps:   4, steps per second: 220, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 9780.406006, mae: 570.577530, accuracy: 0.179688, mean_q: -58.539915, mean_eps: 0.100000\n",
      " 26606/50000: episode: 5888, duration: 0.020s, episode steps:   5, steps per second: 254, episode reward: -2193.000, mean reward: -438.600 [-999.000, -58.000], mean action: 2.400 [1.000, 3.000],  loss: 9473.196680, mae: 549.387732, accuracy: 0.187500, mean_q: -42.355742, mean_eps: 0.100000\n",
      " 26609/50000: episode: 5889, duration: 0.013s, episode steps:   3, steps per second: 225, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 8895.557617, mae: 549.426310, accuracy: 0.145833, mean_q: -69.574128, mean_eps: 0.100000\n",
      " 26612/50000: episode: 5890, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 9737.918294, mae: 561.338908, accuracy: 0.270833, mean_q: -69.368159, mean_eps: 0.100000\n",
      " 26615/50000: episode: 5891, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 9173.903646, mae: 560.331991, accuracy: 0.156250, mean_q: -37.673279, mean_eps: 0.100000\n",
      " 26618/50000: episode: 5892, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 11093.337240, mae: 573.666199, accuracy: 0.156250, mean_q: -59.206198, mean_eps: 0.100000\n",
      " 26621/50000: episode: 5893, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 9506.867839, mae: 550.043538, accuracy: 0.145833, mean_q: -51.995539, mean_eps: 0.100000\n",
      " 26624/50000: episode: 5894, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 9148.799316, mae: 542.118225, accuracy: 0.125000, mean_q: -66.732376, mean_eps: 0.100000\n",
      " 26627/50000: episode: 5895, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 9760.511393, mae: 554.730306, accuracy: 0.177083, mean_q: -25.232297, mean_eps: 0.100000\n",
      " 26630/50000: episode: 5896, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 9746.630859, mae: 544.375203, accuracy: 0.208333, mean_q: -56.836917, mean_eps: 0.100000\n",
      " 26633/50000: episode: 5897, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 9014.825195, mae: 554.008606, accuracy: 0.197917, mean_q: -61.815034, mean_eps: 0.100000\n",
      " 26636/50000: episode: 5898, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 9902.047526, mae: 559.867981, accuracy: 0.197917, mean_q: -59.672174, mean_eps: 0.100000\n",
      " 26639/50000: episode: 5899, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 12117.957031, mae: 563.989095, accuracy: 0.250000, mean_q: -51.816096, mean_eps: 0.100000\n",
      " 26642/50000: episode: 5900, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 8904.590495, mae: 562.027954, accuracy: 0.145833, mean_q: -41.313897, mean_eps: 0.100000\n",
      " 26646/50000: episode: 5901, duration: 0.018s, episode steps:   4, steps per second: 224, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 9104.500488, mae: 567.339020, accuracy: 0.218750, mean_q: -33.109078, mean_eps: 0.100000\n",
      " 26649/50000: episode: 5902, duration: 0.016s, episode steps:   3, steps per second: 189, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 9800.435872, mae: 568.930420, accuracy: 0.145833, mean_q: -59.611627, mean_eps: 0.100000\n",
      " 26652/50000: episode: 5903, duration: 0.014s, episode steps:   3, steps per second: 222, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 7847.899740, mae: 549.541789, accuracy: 0.114583, mean_q: -57.025089, mean_eps: 0.100000\n",
      " 26655/50000: episode: 5904, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 10516.754557, mae: 545.706807, accuracy: 0.156250, mean_q: -97.451385, mean_eps: 0.100000\n",
      " 26658/50000: episode: 5905, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 8197.793783, mae: 558.429179, accuracy: 0.177083, mean_q: -68.380201, mean_eps: 0.100000\n",
      " 26661/50000: episode: 5906, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 9653.728516, mae: 555.861979, accuracy: 0.187500, mean_q: -62.972581, mean_eps: 0.100000\n",
      " 26664/50000: episode: 5907, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 9662.105469, mae: 567.735779, accuracy: 0.156250, mean_q: -60.352942, mean_eps: 0.100000\n",
      " 26667/50000: episode: 5908, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 9490.467448, mae: 538.035014, accuracy: 0.145833, mean_q: -32.679495, mean_eps: 0.100000\n",
      " 26670/50000: episode: 5909, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 9111.805664, mae: 564.897807, accuracy: 0.135417, mean_q: -37.455530, mean_eps: 0.100000\n",
      " 26673/50000: episode: 5910, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 10844.722005, mae: 531.559794, accuracy: 0.135417, mean_q: -42.278734, mean_eps: 0.100000\n",
      " 26676/50000: episode: 5911, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 8586.897786, mae: 573.015055, accuracy: 0.260417, mean_q: -49.970170, mean_eps: 0.100000\n",
      " 26679/50000: episode: 5912, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 9775.980143, mae: 558.995544, accuracy: 0.187500, mean_q: -42.280656, mean_eps: 0.100000\n",
      " 26682/50000: episode: 5913, duration: 0.013s, episode steps:   3, steps per second: 230, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 8403.000000, mae: 567.363078, accuracy: 0.187500, mean_q: -49.571704, mean_eps: 0.100000\n",
      " 26685/50000: episode: 5914, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 8251.889648, mae: 550.774048, accuracy: 0.177083, mean_q: -60.992339, mean_eps: 0.100000\n",
      " 26689/50000: episode: 5915, duration: 0.016s, episode steps:   4, steps per second: 245, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 9094.682495, mae: 552.365158, accuracy: 0.195312, mean_q: -64.319589, mean_eps: 0.100000\n",
      " 26692/50000: episode: 5916, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 10025.696126, mae: 545.865987, accuracy: 0.187500, mean_q: -62.399679, mean_eps: 0.100000\n",
      " 26695/50000: episode: 5917, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 8992.591471, mae: 555.949443, accuracy: 0.177083, mean_q: -37.245396, mean_eps: 0.100000\n",
      " 26698/50000: episode: 5918, duration: 0.015s, episode steps:   3, steps per second: 203, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 7816.920736, mae: 553.892965, accuracy: 0.156250, mean_q: -62.343203, mean_eps: 0.100000\n",
      " 26701/50000: episode: 5919, duration: 0.015s, episode steps:   3, steps per second: 205, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 9643.376628, mae: 551.410970, accuracy: 0.197917, mean_q: -66.356003, mean_eps: 0.100000\n",
      " 26704/50000: episode: 5920, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 10322.771484, mae: 566.467916, accuracy: 0.208333, mean_q: -59.461833, mean_eps: 0.100000\n",
      " 26707/50000: episode: 5921, duration: 0.013s, episode steps:   3, steps per second: 225, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 10689.859701, mae: 548.641520, accuracy: 0.187500, mean_q: -71.149021, mean_eps: 0.100000\n",
      " 26710/50000: episode: 5922, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 8192.544759, mae: 545.639099, accuracy: 0.239583, mean_q: -35.042899, mean_eps: 0.100000\n",
      " 26713/50000: episode: 5923, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 10117.412435, mae: 558.062744, accuracy: 0.187500, mean_q: -57.239581, mean_eps: 0.100000\n",
      " 26716/50000: episode: 5924, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 9171.269206, mae: 557.638997, accuracy: 0.208333, mean_q: -67.464561, mean_eps: 0.100000\n",
      " 26719/50000: episode: 5925, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 10963.190430, mae: 565.219381, accuracy: 0.208333, mean_q: -71.828654, mean_eps: 0.100000\n",
      " 26722/50000: episode: 5926, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 7583.916829, mae: 559.100077, accuracy: 0.270833, mean_q: -41.877728, mean_eps: 0.100000\n",
      " 26725/50000: episode: 5927, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 10331.482259, mae: 578.607198, accuracy: 0.239583, mean_q: -66.616236, mean_eps: 0.100000\n",
      " 26728/50000: episode: 5928, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 7951.182617, mae: 569.774556, accuracy: 0.145833, mean_q: -73.034644, mean_eps: 0.100000\n",
      " 26731/50000: episode: 5929, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 10347.069010, mae: 563.967733, accuracy: 0.187500, mean_q: -73.944242, mean_eps: 0.100000\n",
      " 26734/50000: episode: 5930, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 8995.064128, mae: 562.815918, accuracy: 0.229167, mean_q: -35.580700, mean_eps: 0.100000\n",
      " 26737/50000: episode: 5931, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.667 [0.000, 3.000],  loss: 8980.447591, mae: 552.500102, accuracy: 0.208333, mean_q: -88.283051, mean_eps: 0.100000\n",
      " 26740/50000: episode: 5932, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 9536.114258, mae: 557.322611, accuracy: 0.156250, mean_q: -58.394608, mean_eps: 0.100000\n",
      " 26743/50000: episode: 5933, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 8597.102539, mae: 549.585775, accuracy: 0.229167, mean_q: -34.415262, mean_eps: 0.100000\n",
      " 26747/50000: episode: 5934, duration: 0.024s, episode steps:   4, steps per second: 168, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 9659.736084, mae: 550.804092, accuracy: 0.164062, mean_q: -72.700016, mean_eps: 0.100000\n",
      " 26750/50000: episode: 5935, duration: 0.015s, episode steps:   3, steps per second: 202, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 10080.701823, mae: 560.780558, accuracy: 0.208333, mean_q: -71.498095, mean_eps: 0.100000\n",
      " 26753/50000: episode: 5936, duration: 0.014s, episode steps:   3, steps per second: 221, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 7972.501302, mae: 558.615865, accuracy: 0.208333, mean_q: -70.550616, mean_eps: 0.100000\n",
      " 26756/50000: episode: 5937, duration: 0.013s, episode steps:   3, steps per second: 224, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 9319.904948, mae: 547.037496, accuracy: 0.229167, mean_q: -52.318876, mean_eps: 0.100000\n",
      " 26759/50000: episode: 5938, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 6419.949056, mae: 555.066406, accuracy: 0.145833, mean_q: -47.925494, mean_eps: 0.100000\n",
      " 26762/50000: episode: 5939, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 8957.896810, mae: 577.317586, accuracy: 0.229167, mean_q: -41.547115, mean_eps: 0.100000\n",
      " 26765/50000: episode: 5940, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.333 [0.000, 3.000],  loss: 9839.608724, mae: 550.115682, accuracy: 0.208333, mean_q: -78.644446, mean_eps: 0.100000\n",
      " 26768/50000: episode: 5941, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 9193.086589, mae: 556.417847, accuracy: 0.218750, mean_q: -66.865372, mean_eps: 0.100000\n",
      " 26771/50000: episode: 5942, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 10494.487630, mae: 563.923910, accuracy: 0.218750, mean_q: -56.111285, mean_eps: 0.100000\n",
      " 26774/50000: episode: 5943, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 9468.493490, mae: 567.194499, accuracy: 0.239583, mean_q: -60.930431, mean_eps: 0.100000\n",
      " 26777/50000: episode: 5944, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 8531.308594, mae: 550.896606, accuracy: 0.177083, mean_q: -60.850483, mean_eps: 0.100000\n",
      " 26780/50000: episode: 5945, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 10343.520508, mae: 560.889872, accuracy: 0.166667, mean_q: -60.932477, mean_eps: 0.100000\n",
      " 26783/50000: episode: 5946, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 8440.553385, mae: 555.597900, accuracy: 0.177083, mean_q: -30.834513, mean_eps: 0.100000\n",
      " 26786/50000: episode: 5947, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 9774.330729, mae: 562.858541, accuracy: 0.125000, mean_q: -56.379922, mean_eps: 0.100000\n",
      " 26789/50000: episode: 5948, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 7292.562826, mae: 558.265116, accuracy: 0.187500, mean_q: -41.770487, mean_eps: 0.100000\n",
      " 26792/50000: episode: 5949, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 9786.106120, mae: 547.793376, accuracy: 0.250000, mean_q: -48.158830, mean_eps: 0.100000\n",
      " 26795/50000: episode: 5950, duration: 0.017s, episode steps:   3, steps per second: 175, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 10512.234375, mae: 568.700358, accuracy: 0.218750, mean_q: -33.775166, mean_eps: 0.100000\n",
      " 26798/50000: episode: 5951, duration: 0.017s, episode steps:   3, steps per second: 180, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 7508.274902, mae: 552.366577, accuracy: 0.197917, mean_q: -75.425762, mean_eps: 0.100000\n",
      " 26801/50000: episode: 5952, duration: 0.018s, episode steps:   3, steps per second: 171, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 7303.626139, mae: 537.626750, accuracy: 0.229167, mean_q: -72.728399, mean_eps: 0.100000\n",
      " 26804/50000: episode: 5953, duration: 0.013s, episode steps:   3, steps per second: 230, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 8634.842936, mae: 563.089966, accuracy: 0.125000, mean_q: -53.697731, mean_eps: 0.100000\n",
      " 26807/50000: episode: 5954, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 7971.391439, mae: 552.364339, accuracy: 0.187500, mean_q: -44.490139, mean_eps: 0.100000\n",
      " 26810/50000: episode: 5955, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 8318.391764, mae: 564.118164, accuracy: 0.197917, mean_q: -49.484604, mean_eps: 0.100000\n",
      " 26813/50000: episode: 5956, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 10609.497721, mae: 568.196208, accuracy: 0.135417, mean_q: -65.133837, mean_eps: 0.100000\n",
      " 26817/50000: episode: 5957, duration: 0.016s, episode steps:   4, steps per second: 256, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 9756.823486, mae: 539.179108, accuracy: 0.171875, mean_q: -61.560370, mean_eps: 0.100000\n",
      " 26821/50000: episode: 5958, duration: 0.017s, episode steps:   4, steps per second: 236, episode reward: -1223.000, mean reward: -305.750 [-999.000, -58.000], mean action: 1.750 [0.000, 3.000],  loss: 9953.634033, mae: 537.389679, accuracy: 0.164062, mean_q: -98.905209, mean_eps: 0.100000\n",
      " 26824/50000: episode: 5959, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 9997.520182, mae: 565.530497, accuracy: 0.145833, mean_q: -66.101143, mean_eps: 0.100000\n",
      " 26828/50000: episode: 5960, duration: 0.017s, episode steps:   4, steps per second: 237, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 10431.850098, mae: 573.079971, accuracy: 0.187500, mean_q: -47.698639, mean_eps: 0.100000\n",
      " 26831/50000: episode: 5961, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 11113.706706, mae: 546.862406, accuracy: 0.197917, mean_q: -66.399934, mean_eps: 0.100000\n",
      " 26834/50000: episode: 5962, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 9720.984375, mae: 565.897359, accuracy: 0.218750, mean_q: -60.002675, mean_eps: 0.100000\n",
      " 26837/50000: episode: 5963, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 9706.342122, mae: 551.588582, accuracy: 0.177083, mean_q: -42.316601, mean_eps: 0.100000\n",
      " 26840/50000: episode: 5964, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 9249.954753, mae: 548.589640, accuracy: 0.187500, mean_q: -48.516247, mean_eps: 0.100000\n",
      " 26843/50000: episode: 5965, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 11365.420573, mae: 556.208089, accuracy: 0.218750, mean_q: -49.134013, mean_eps: 0.100000\n",
      " 26846/50000: episode: 5966, duration: 0.019s, episode steps:   3, steps per second: 158, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 8784.283854, mae: 554.001200, accuracy: 0.135417, mean_q: -60.466338, mean_eps: 0.100000\n",
      " 26849/50000: episode: 5967, duration: 0.015s, episode steps:   3, steps per second: 201, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 9556.513997, mae: 561.860209, accuracy: 0.197917, mean_q: -61.725063, mean_eps: 0.100000\n",
      " 26852/50000: episode: 5968, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 9008.663900, mae: 545.654409, accuracy: 0.218750, mean_q: -63.270472, mean_eps: 0.100000\n",
      " 26855/50000: episode: 5969, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 7869.597331, mae: 541.926921, accuracy: 0.208333, mean_q: -49.939345, mean_eps: 0.100000\n",
      " 26858/50000: episode: 5970, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 8526.898275, mae: 534.372314, accuracy: 0.166667, mean_q: -39.174452, mean_eps: 0.100000\n",
      " 26861/50000: episode: 5971, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 9553.434082, mae: 560.093709, accuracy: 0.218750, mean_q: -77.874242, mean_eps: 0.100000\n",
      " 26864/50000: episode: 5972, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 7410.673014, mae: 546.318888, accuracy: 0.270833, mean_q: -48.805121, mean_eps: 0.100000\n",
      " 26867/50000: episode: 5973, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 8211.877604, mae: 546.974101, accuracy: 0.166667, mean_q: -73.889737, mean_eps: 0.100000\n",
      " 26871/50000: episode: 5974, duration: 0.015s, episode steps:   4, steps per second: 261, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 8450.920898, mae: 550.628494, accuracy: 0.234375, mean_q: -50.279259, mean_eps: 0.100000\n",
      " 26874/50000: episode: 5975, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 9194.425130, mae: 560.844604, accuracy: 0.229167, mean_q: -66.197550, mean_eps: 0.100000\n",
      " 26878/50000: episode: 5976, duration: 0.016s, episode steps:   4, steps per second: 249, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 2.000 [0.000, 3.000],  loss: 10139.417969, mae: 565.758224, accuracy: 0.179688, mean_q: -61.796019, mean_eps: 0.100000\n",
      " 26881/50000: episode: 5977, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 10192.136719, mae: 592.716492, accuracy: 0.229167, mean_q: -22.024567, mean_eps: 0.100000\n",
      " 26884/50000: episode: 5978, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 8820.320312, mae: 555.271322, accuracy: 0.218750, mean_q: -43.618786, mean_eps: 0.100000\n",
      " 26887/50000: episode: 5979, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 8370.117513, mae: 562.116699, accuracy: 0.197917, mean_q: -51.120956, mean_eps: 0.100000\n",
      " 26890/50000: episode: 5980, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 8593.383464, mae: 575.667704, accuracy: 0.187500, mean_q: -57.234941, mean_eps: 0.100000\n",
      " 26894/50000: episode: 5981, duration: 0.016s, episode steps:   4, steps per second: 243, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 2.000 [0.000, 3.000],  loss: 9897.213135, mae: 570.312653, accuracy: 0.195312, mean_q: -73.842786, mean_eps: 0.100000\n",
      " 26897/50000: episode: 5982, duration: 0.023s, episode steps:   3, steps per second: 131, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 8329.163737, mae: 549.024963, accuracy: 0.218750, mean_q: -78.573217, mean_eps: 0.100000\n",
      " 26900/50000: episode: 5983, duration: 0.017s, episode steps:   3, steps per second: 180, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 7332.853516, mae: 559.300842, accuracy: 0.125000, mean_q: -59.835449, mean_eps: 0.100000\n",
      " 26903/50000: episode: 5984, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 10515.494792, mae: 573.916219, accuracy: 0.156250, mean_q: -69.656648, mean_eps: 0.100000\n",
      " 26906/50000: episode: 5985, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 10121.277995, mae: 560.441264, accuracy: 0.156250, mean_q: -74.214375, mean_eps: 0.100000\n",
      " 26909/50000: episode: 5986, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 8693.916341, mae: 558.656372, accuracy: 0.218750, mean_q: -53.428975, mean_eps: 0.100000\n",
      " 26912/50000: episode: 5987, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 9671.771159, mae: 541.412720, accuracy: 0.177083, mean_q: -66.135852, mean_eps: 0.100000\n",
      " 26916/50000: episode: 5988, duration: 0.015s, episode steps:   4, steps per second: 259, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 8898.451904, mae: 560.645142, accuracy: 0.179688, mean_q: -50.986485, mean_eps: 0.100000\n",
      " 26919/50000: episode: 5989, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 8850.359701, mae: 555.772909, accuracy: 0.177083, mean_q: -42.261262, mean_eps: 0.100000\n",
      " 26922/50000: episode: 5990, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 9281.961263, mae: 551.939473, accuracy: 0.145833, mean_q: -60.457637, mean_eps: 0.100000\n",
      " 26925/50000: episode: 5991, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 8601.534505, mae: 567.453898, accuracy: 0.166667, mean_q: -51.193381, mean_eps: 0.100000\n",
      " 26929/50000: episode: 5992, duration: 0.015s, episode steps:   4, steps per second: 263, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.000 [0.000, 3.000],  loss: 9285.755249, mae: 552.182434, accuracy: 0.195312, mean_q: -37.018037, mean_eps: 0.100000\n",
      " 26933/50000: episode: 5993, duration: 0.016s, episode steps:   4, steps per second: 254, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 2.250 [1.000, 3.000],  loss: 9124.430176, mae: 551.620544, accuracy: 0.179688, mean_q: -54.613055, mean_eps: 0.100000\n",
      " 26936/50000: episode: 5994, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 10596.999023, mae: 564.393148, accuracy: 0.187500, mean_q: -69.821208, mean_eps: 0.100000\n",
      " 26939/50000: episode: 5995, duration: 0.016s, episode steps:   3, steps per second: 185, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 8173.571615, mae: 566.736104, accuracy: 0.187500, mean_q: -71.475085, mean_eps: 0.100000\n",
      " 26942/50000: episode: 5996, duration: 0.016s, episode steps:   3, steps per second: 187, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 8797.609701, mae: 560.525635, accuracy: 0.229167, mean_q: -67.141037, mean_eps: 0.100000\n",
      " 26946/50000: episode: 5997, duration: 0.019s, episode steps:   4, steps per second: 215, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 8343.857178, mae: 559.504608, accuracy: 0.195312, mean_q: -74.836890, mean_eps: 0.100000\n",
      " 26949/50000: episode: 5998, duration: 0.013s, episode steps:   3, steps per second: 227, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 9631.891276, mae: 574.894592, accuracy: 0.281250, mean_q: -56.714478, mean_eps: 0.100000\n",
      " 26952/50000: episode: 5999, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 9494.160156, mae: 535.373718, accuracy: 0.135417, mean_q: -81.201416, mean_eps: 0.100000\n",
      " 26955/50000: episode: 6000, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 8304.431315, mae: 547.030680, accuracy: 0.260417, mean_q: -32.211013, mean_eps: 0.100000\n",
      " 26958/50000: episode: 6001, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 9838.887695, mae: 553.285909, accuracy: 0.166667, mean_q: -54.491179, mean_eps: 0.100000\n",
      " 26961/50000: episode: 6002, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 8475.105469, mae: 549.652445, accuracy: 0.208333, mean_q: -36.841593, mean_eps: 0.100000\n",
      " 26964/50000: episode: 6003, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 10538.247721, mae: 559.241028, accuracy: 0.250000, mean_q: -63.109675, mean_eps: 0.100000\n",
      " 26967/50000: episode: 6004, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 9761.387370, mae: 570.998230, accuracy: 0.156250, mean_q: -54.933362, mean_eps: 0.100000\n",
      " 26970/50000: episode: 6005, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 9831.813802, mae: 557.095907, accuracy: 0.250000, mean_q: -54.332433, mean_eps: 0.100000\n",
      " 26973/50000: episode: 6006, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 7475.153320, mae: 559.961650, accuracy: 0.208333, mean_q: -33.207817, mean_eps: 0.100000\n",
      " 26978/50000: episode: 6007, duration: 0.018s, episode steps:   5, steps per second: 273, episode reward: -2193.000, mean reward: -438.600 [-999.000, -58.000], mean action: 2.200 [1.000, 3.000],  loss: 8095.359180, mae: 562.309521, accuracy: 0.156250, mean_q: -50.696531, mean_eps: 0.100000\n",
      " 26981/50000: episode: 6008, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 9184.049805, mae: 558.264038, accuracy: 0.177083, mean_q: -57.624547, mean_eps: 0.100000\n",
      " 26984/50000: episode: 6009, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 8084.373535, mae: 559.083150, accuracy: 0.156250, mean_q: -58.726384, mean_eps: 0.100000\n",
      " 26987/50000: episode: 6010, duration: 0.013s, episode steps:   3, steps per second: 225, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 8445.523926, mae: 554.040710, accuracy: 0.177083, mean_q: -51.992934, mean_eps: 0.100000\n",
      " 26991/50000: episode: 6011, duration: 0.018s, episode steps:   4, steps per second: 218, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 10530.144409, mae: 573.069565, accuracy: 0.187500, mean_q: -94.620720, mean_eps: 0.100000\n",
      " 26994/50000: episode: 6012, duration: 0.015s, episode steps:   3, steps per second: 206, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 7410.907878, mae: 544.243042, accuracy: 0.197917, mean_q: -39.958879, mean_eps: 0.100000\n",
      " 26998/50000: episode: 6013, duration: 0.016s, episode steps:   4, steps per second: 252, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 8095.057251, mae: 560.649857, accuracy: 0.226562, mean_q: -69.633320, mean_eps: 0.100000\n",
      " 27002/50000: episode: 6014, duration: 0.016s, episode steps:   4, steps per second: 255, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 9343.976685, mae: 563.536560, accuracy: 0.195312, mean_q: -60.131947, mean_eps: 0.100000\n",
      " 27005/50000: episode: 6015, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 8349.483724, mae: 555.733459, accuracy: 0.177083, mean_q: -47.632578, mean_eps: 0.100000\n",
      " 27008/50000: episode: 6016, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 8913.058594, mae: 544.209269, accuracy: 0.260417, mean_q: -58.446313, mean_eps: 0.100000\n",
      " 27011/50000: episode: 6017, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 8246.381348, mae: 568.702108, accuracy: 0.187500, mean_q: -67.397045, mean_eps: 0.100000\n",
      " 27014/50000: episode: 6018, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 8492.486328, mae: 562.443461, accuracy: 0.166667, mean_q: -43.768960, mean_eps: 0.100000\n",
      " 27019/50000: episode: 6019, duration: 0.019s, episode steps:   5, steps per second: 265, episode reward: -2237.000, mean reward: -447.400 [-999.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 8565.873828, mae: 555.830603, accuracy: 0.200000, mean_q: -53.294657, mean_eps: 0.100000\n",
      " 27024/50000: episode: 6020, duration: 0.019s, episode steps:   5, steps per second: 261, episode reward: -2193.000, mean reward: -438.600 [-999.000, -58.000], mean action: 2.400 [1.000, 3.000],  loss: 8144.750195, mae: 545.460535, accuracy: 0.168750, mean_q: -74.858450, mean_eps: 0.100000\n",
      " 27027/50000: episode: 6021, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 10303.491862, mae: 552.130391, accuracy: 0.208333, mean_q: -64.596224, mean_eps: 0.100000\n",
      " 27030/50000: episode: 6022, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 9807.444987, mae: 550.109721, accuracy: 0.229167, mean_q: -73.979833, mean_eps: 0.100000\n",
      " 27034/50000: episode: 6023, duration: 0.016s, episode steps:   4, steps per second: 254, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 8420.435303, mae: 548.488968, accuracy: 0.218750, mean_q: -62.465548, mean_eps: 0.100000\n",
      " 27037/50000: episode: 6024, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 8398.009277, mae: 538.582947, accuracy: 0.135417, mean_q: -38.248486, mean_eps: 0.100000\n",
      " 27041/50000: episode: 6025, duration: 0.021s, episode steps:   4, steps per second: 190, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 8847.386719, mae: 547.422729, accuracy: 0.226562, mean_q: -46.484938, mean_eps: 0.100000\n",
      " 27044/50000: episode: 6026, duration: 0.019s, episode steps:   3, steps per second: 158, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 7683.457357, mae: 553.124552, accuracy: 0.166667, mean_q: -54.993928, mean_eps: 0.100000\n",
      " 27047/50000: episode: 6027, duration: 0.015s, episode steps:   3, steps per second: 201, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 7630.812337, mae: 539.775981, accuracy: 0.208333, mean_q: -62.087997, mean_eps: 0.100000\n",
      " 27051/50000: episode: 6028, duration: 0.016s, episode steps:   4, steps per second: 250, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 9527.621338, mae: 548.978088, accuracy: 0.179688, mean_q: -45.902200, mean_eps: 0.100000\n",
      " 27054/50000: episode: 6029, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 7825.482422, mae: 544.758179, accuracy: 0.177083, mean_q: -27.354864, mean_eps: 0.100000\n",
      " 27058/50000: episode: 6030, duration: 0.016s, episode steps:   4, steps per second: 255, episode reward: -1223.000, mean reward: -305.750 [-999.000, -58.000], mean action: 0.750 [0.000, 2.000],  loss: 8176.586548, mae: 545.215210, accuracy: 0.156250, mean_q: -59.601621, mean_eps: 0.100000\n",
      " 27061/50000: episode: 6031, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 9465.170898, mae: 558.637085, accuracy: 0.156250, mean_q: -68.375659, mean_eps: 0.100000\n",
      " 27064/50000: episode: 6032, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 8012.361654, mae: 546.879120, accuracy: 0.197917, mean_q: -52.763196, mean_eps: 0.100000\n",
      " 27067/50000: episode: 6033, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 8293.558594, mae: 528.724955, accuracy: 0.218750, mean_q: -61.782223, mean_eps: 0.100000\n",
      " 27071/50000: episode: 6034, duration: 0.016s, episode steps:   4, steps per second: 250, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 2.250 [1.000, 3.000],  loss: 7996.266724, mae: 564.701309, accuracy: 0.085938, mean_q: -65.411592, mean_eps: 0.100000\n",
      " 27074/50000: episode: 6035, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 8434.160807, mae: 559.009359, accuracy: 0.208333, mean_q: -66.996827, mean_eps: 0.100000\n",
      " 27077/50000: episode: 6036, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 8909.761393, mae: 556.333740, accuracy: 0.218750, mean_q: -76.818494, mean_eps: 0.100000\n",
      " 27080/50000: episode: 6037, duration: 0.014s, episode steps:   3, steps per second: 212, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 10012.593750, mae: 566.479574, accuracy: 0.177083, mean_q: -56.663507, mean_eps: 0.100000\n",
      " 27083/50000: episode: 6038, duration: 0.014s, episode steps:   3, steps per second: 218, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 9299.345378, mae: 551.033366, accuracy: 0.177083, mean_q: -81.865824, mean_eps: 0.100000\n",
      " 27087/50000: episode: 6039, duration: 0.019s, episode steps:   4, steps per second: 208, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.000 [0.000, 3.000],  loss: 10130.635986, mae: 548.241974, accuracy: 0.171875, mean_q: -71.634583, mean_eps: 0.100000\n",
      " 27090/50000: episode: 6040, duration: 0.018s, episode steps:   3, steps per second: 167, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 8938.775228, mae: 555.692546, accuracy: 0.250000, mean_q: -39.582906, mean_eps: 0.100000\n",
      " 27093/50000: episode: 6041, duration: 0.015s, episode steps:   3, steps per second: 203, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 8117.532878, mae: 555.853475, accuracy: 0.177083, mean_q: -62.732793, mean_eps: 0.100000\n",
      " 27096/50000: episode: 6042, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 6434.856120, mae: 568.587504, accuracy: 0.218750, mean_q: -47.407762, mean_eps: 0.100000\n",
      " 27099/50000: episode: 6043, duration: 0.014s, episode steps:   3, steps per second: 219, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 8432.613118, mae: 558.959351, accuracy: 0.239583, mean_q: -68.341324, mean_eps: 0.100000\n",
      " 27102/50000: episode: 6044, duration: 0.014s, episode steps:   3, steps per second: 219, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 9475.403809, mae: 581.287699, accuracy: 0.156250, mean_q: -94.708532, mean_eps: 0.100000\n",
      " 27106/50000: episode: 6045, duration: 0.017s, episode steps:   4, steps per second: 229, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 2.250 [1.000, 3.000],  loss: 8838.451660, mae: 552.004745, accuracy: 0.187500, mean_q: -69.887243, mean_eps: 0.100000\n",
      " 27109/50000: episode: 6046, duration: 0.016s, episode steps:   3, steps per second: 191, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 9922.380859, mae: 564.675883, accuracy: 0.197917, mean_q: -67.912722, mean_eps: 0.100000\n",
      " 27112/50000: episode: 6047, duration: 0.014s, episode steps:   3, steps per second: 215, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 9047.184896, mae: 561.213562, accuracy: 0.208333, mean_q: -74.782519, mean_eps: 0.100000\n",
      " 27116/50000: episode: 6048, duration: 0.017s, episode steps:   4, steps per second: 229, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 7291.158813, mae: 553.631393, accuracy: 0.242188, mean_q: -51.162903, mean_eps: 0.100000\n",
      " 27119/50000: episode: 6049, duration: 0.014s, episode steps:   3, steps per second: 218, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 8386.951172, mae: 561.537699, accuracy: 0.197917, mean_q: -39.432479, mean_eps: 0.100000\n",
      " 27122/50000: episode: 6050, duration: 0.013s, episode steps:   3, steps per second: 223, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 8735.778971, mae: 544.954203, accuracy: 0.166667, mean_q: -71.834078, mean_eps: 0.100000\n",
      " 27126/50000: episode: 6051, duration: 0.018s, episode steps:   4, steps per second: 219, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 10953.360107, mae: 569.522446, accuracy: 0.164062, mean_q: -52.259296, mean_eps: 0.100000\n",
      " 27130/50000: episode: 6052, duration: 0.018s, episode steps:   4, steps per second: 225, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 9192.882568, mae: 557.838272, accuracy: 0.195312, mean_q: -66.070797, mean_eps: 0.100000\n",
      " 27133/50000: episode: 6053, duration: 0.019s, episode steps:   3, steps per second: 158, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 6669.321452, mae: 545.951619, accuracy: 0.229167, mean_q: -59.899475, mean_eps: 0.100000\n",
      " 27136/50000: episode: 6054, duration: 0.016s, episode steps:   3, steps per second: 183, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 9256.388346, mae: 589.868286, accuracy: 0.166667, mean_q: -50.537373, mean_eps: 0.100000\n",
      " 27139/50000: episode: 6055, duration: 0.017s, episode steps:   3, steps per second: 180, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 9426.828125, mae: 565.586446, accuracy: 0.166667, mean_q: -86.616679, mean_eps: 0.100000\n",
      " 27142/50000: episode: 6056, duration: 0.014s, episode steps:   3, steps per second: 208, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 8357.421224, mae: 553.519185, accuracy: 0.187500, mean_q: -76.023422, mean_eps: 0.100000\n",
      " 27145/50000: episode: 6057, duration: 0.014s, episode steps:   3, steps per second: 210, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 7530.644694, mae: 552.210795, accuracy: 0.197917, mean_q: -49.024736, mean_eps: 0.100000\n",
      " 27148/50000: episode: 6058, duration: 0.014s, episode steps:   3, steps per second: 213, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 9585.894531, mae: 566.461609, accuracy: 0.156250, mean_q: -76.542928, mean_eps: 0.100000\n",
      " 27151/50000: episode: 6059, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 9689.883789, mae: 559.216838, accuracy: 0.156250, mean_q: -73.962465, mean_eps: 0.100000\n",
      " 27154/50000: episode: 6060, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 8504.819336, mae: 566.989766, accuracy: 0.156250, mean_q: -54.801420, mean_eps: 0.100000\n",
      " 27159/50000: episode: 6061, duration: 0.019s, episode steps:   5, steps per second: 261, episode reward: -2193.000, mean reward: -438.600 [-999.000, -32.000], mean action: 1.200 [0.000, 3.000],  loss: 8981.199219, mae: 571.358337, accuracy: 0.168750, mean_q: -41.876958, mean_eps: 0.100000\n",
      " 27162/50000: episode: 6062, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 8389.686849, mae: 552.330505, accuracy: 0.177083, mean_q: -62.778740, mean_eps: 0.100000\n",
      " 27165/50000: episode: 6063, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 7468.369466, mae: 539.098775, accuracy: 0.166667, mean_q: -66.563810, mean_eps: 0.100000\n",
      " 27170/50000: episode: 6064, duration: 0.018s, episode steps:   5, steps per second: 278, episode reward: -2193.000, mean reward: -438.600 [-999.000, -45.000], mean action: 1.400 [0.000, 2.000],  loss: 8480.421973, mae: 557.722192, accuracy: 0.168750, mean_q: -55.563809, mean_eps: 0.100000\n",
      " 27173/50000: episode: 6065, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 9085.862467, mae: 532.851400, accuracy: 0.270833, mean_q: -62.977315, mean_eps: 0.100000\n",
      " 27178/50000: episode: 6066, duration: 0.019s, episode steps:   5, steps per second: 270, episode reward: -2222.000, mean reward: -444.400 [-999.000, -58.000], mean action: 1.200 [0.000, 3.000],  loss: 8522.971289, mae: 566.424817, accuracy: 0.156250, mean_q: -70.880408, mean_eps: 0.100000\n",
      " 27181/50000: episode: 6067, duration: 0.013s, episode steps:   3, steps per second: 227, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 9180.236654, mae: 561.274211, accuracy: 0.250000, mean_q: -72.697662, mean_eps: 0.100000\n",
      " 27184/50000: episode: 6068, duration: 0.013s, episode steps:   3, steps per second: 226, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 8581.791016, mae: 564.953817, accuracy: 0.135417, mean_q: -70.949473, mean_eps: 0.100000\n",
      " 27187/50000: episode: 6069, duration: 0.014s, episode steps:   3, steps per second: 210, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 9741.167969, mae: 562.407410, accuracy: 0.208333, mean_q: -54.168456, mean_eps: 0.100000\n",
      " 27190/50000: episode: 6070, duration: 0.015s, episode steps:   3, steps per second: 200, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 9351.190104, mae: 542.819865, accuracy: 0.239583, mean_q: -52.649840, mean_eps: 0.100000\n",
      " 27194/50000: episode: 6071, duration: 0.016s, episode steps:   4, steps per second: 249, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 8974.571289, mae: 557.889984, accuracy: 0.179688, mean_q: -55.396257, mean_eps: 0.100000\n",
      " 27197/50000: episode: 6072, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 9476.241862, mae: 556.023885, accuracy: 0.218750, mean_q: -55.045201, mean_eps: 0.100000\n",
      " 27200/50000: episode: 6073, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 9213.432943, mae: 562.790710, accuracy: 0.208333, mean_q: -72.451094, mean_eps: 0.100000\n",
      " 27203/50000: episode: 6074, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 9724.522298, mae: 557.926432, accuracy: 0.197917, mean_q: -55.098362, mean_eps: 0.100000\n",
      " 27207/50000: episode: 6075, duration: 0.015s, episode steps:   4, steps per second: 262, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 8363.734863, mae: 560.699203, accuracy: 0.171875, mean_q: -47.388242, mean_eps: 0.100000\n",
      " 27210/50000: episode: 6076, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 8992.024577, mae: 554.061890, accuracy: 0.156250, mean_q: -93.481344, mean_eps: 0.100000\n",
      " 27213/50000: episode: 6077, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 9167.911784, mae: 552.461019, accuracy: 0.166667, mean_q: -82.242863, mean_eps: 0.100000\n",
      " 27216/50000: episode: 6078, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 10350.444010, mae: 565.248006, accuracy: 0.145833, mean_q: -62.213875, mean_eps: 0.100000\n",
      " 27220/50000: episode: 6079, duration: 0.015s, episode steps:   4, steps per second: 263, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 8937.445068, mae: 554.597000, accuracy: 0.187500, mean_q: -54.816699, mean_eps: 0.100000\n",
      " 27223/50000: episode: 6080, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 9170.975586, mae: 554.823629, accuracy: 0.270833, mean_q: -51.640645, mean_eps: 0.100000\n",
      " 27226/50000: episode: 6081, duration: 0.021s, episode steps:   3, steps per second: 142, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 9811.088216, mae: 569.101746, accuracy: 0.166667, mean_q: -52.596812, mean_eps: 0.100000\n",
      " 27229/50000: episode: 6082, duration: 0.017s, episode steps:   3, steps per second: 174, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 9989.024740, mae: 575.946330, accuracy: 0.229167, mean_q: -40.278395, mean_eps: 0.100000\n",
      " 27232/50000: episode: 6083, duration: 0.016s, episode steps:   3, steps per second: 184, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 8714.873698, mae: 554.649190, accuracy: 0.145833, mean_q: -52.820824, mean_eps: 0.100000\n",
      " 27235/50000: episode: 6084, duration: 0.014s, episode steps:   3, steps per second: 218, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 9625.638997, mae: 532.097839, accuracy: 0.145833, mean_q: -71.487011, mean_eps: 0.100000\n",
      " 27238/50000: episode: 6085, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 9215.598958, mae: 579.757853, accuracy: 0.114583, mean_q: -63.955866, mean_eps: 0.100000\n",
      " 27242/50000: episode: 6086, duration: 0.016s, episode steps:   4, steps per second: 254, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 9671.620728, mae: 541.110344, accuracy: 0.281250, mean_q: -57.776781, mean_eps: 0.100000\n",
      " 27246/50000: episode: 6087, duration: 0.016s, episode steps:   4, steps per second: 253, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 9745.577148, mae: 569.719955, accuracy: 0.203125, mean_q: -65.156204, mean_eps: 0.100000\n",
      " 27250/50000: episode: 6088, duration: 0.016s, episode steps:   4, steps per second: 251, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 2.250 [1.000, 3.000],  loss: 9700.801025, mae: 559.284439, accuracy: 0.296875, mean_q: -52.881647, mean_eps: 0.100000\n",
      " 27254/50000: episode: 6089, duration: 0.016s, episode steps:   4, steps per second: 252, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 2.250 [1.000, 3.000],  loss: 10249.288330, mae: 568.568130, accuracy: 0.179688, mean_q: -58.052889, mean_eps: 0.100000\n",
      " 27257/50000: episode: 6090, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 8951.351725, mae: 556.431559, accuracy: 0.218750, mean_q: -33.299474, mean_eps: 0.100000\n",
      " 27260/50000: episode: 6091, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 9740.964844, mae: 573.732788, accuracy: 0.229167, mean_q: -46.203467, mean_eps: 0.100000\n",
      " 27264/50000: episode: 6092, duration: 0.017s, episode steps:   4, steps per second: 236, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 8365.085938, mae: 541.771866, accuracy: 0.195312, mean_q: -61.558412, mean_eps: 0.100000\n",
      " 27267/50000: episode: 6093, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 7699.787272, mae: 559.424418, accuracy: 0.270833, mean_q: -62.094943, mean_eps: 0.100000\n",
      " 27271/50000: episode: 6094, duration: 0.015s, episode steps:   4, steps per second: 260, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 8348.285645, mae: 568.135544, accuracy: 0.179688, mean_q: -61.254680, mean_eps: 0.100000\n",
      " 27275/50000: episode: 6095, duration: 0.016s, episode steps:   4, steps per second: 255, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 9339.932617, mae: 555.006577, accuracy: 0.265625, mean_q: -68.933323, mean_eps: 0.100000\n",
      " 27279/50000: episode: 6096, duration: 0.018s, episode steps:   4, steps per second: 222, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 8906.561890, mae: 564.276245, accuracy: 0.210938, mean_q: -53.901478, mean_eps: 0.100000\n",
      " 27283/50000: episode: 6097, duration: 0.021s, episode steps:   4, steps per second: 194, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 7505.800293, mae: 547.192780, accuracy: 0.195312, mean_q: -41.540245, mean_eps: 0.100000\n",
      " 27286/50000: episode: 6098, duration: 0.015s, episode steps:   3, steps per second: 204, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 9566.159180, mae: 560.905721, accuracy: 0.145833, mean_q: -84.867475, mean_eps: 0.100000\n",
      " 27289/50000: episode: 6099, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 7006.667155, mae: 540.603088, accuracy: 0.156250, mean_q: -41.984456, mean_eps: 0.100000\n",
      " 27293/50000: episode: 6100, duration: 0.016s, episode steps:   4, steps per second: 254, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 7486.457886, mae: 555.015976, accuracy: 0.148438, mean_q: -50.735549, mean_eps: 0.100000\n",
      " 27296/50000: episode: 6101, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 9343.025065, mae: 563.200928, accuracy: 0.145833, mean_q: -81.680918, mean_eps: 0.100000\n",
      " 27299/50000: episode: 6102, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 9021.665039, mae: 564.079122, accuracy: 0.177083, mean_q: -55.641514, mean_eps: 0.100000\n",
      " 27303/50000: episode: 6103, duration: 0.016s, episode steps:   4, steps per second: 258, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 2.250 [1.000, 3.000],  loss: 8403.927734, mae: 576.862473, accuracy: 0.218750, mean_q: -60.523288, mean_eps: 0.100000\n",
      " 27306/50000: episode: 6104, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 9031.199056, mae: 578.112183, accuracy: 0.197917, mean_q: -67.314044, mean_eps: 0.100000\n",
      " 27309/50000: episode: 6105, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 7078.381348, mae: 555.341329, accuracy: 0.197917, mean_q: -66.752632, mean_eps: 0.100000\n",
      " 27312/50000: episode: 6106, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 11152.592448, mae: 562.880249, accuracy: 0.135417, mean_q: -84.210475, mean_eps: 0.100000\n",
      " 27315/50000: episode: 6107, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 8177.610189, mae: 547.427002, accuracy: 0.197917, mean_q: -58.263182, mean_eps: 0.100000\n",
      " 27318/50000: episode: 6108, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 8892.978516, mae: 553.899333, accuracy: 0.177083, mean_q: -62.462797, mean_eps: 0.100000\n",
      " 27321/50000: episode: 6109, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 8202.252767, mae: 550.355164, accuracy: 0.208333, mean_q: -60.036095, mean_eps: 0.100000\n",
      " 27324/50000: episode: 6110, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 9045.731934, mae: 566.131877, accuracy: 0.187500, mean_q: -70.115040, mean_eps: 0.100000\n",
      " 27327/50000: episode: 6111, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 9883.738281, mae: 547.822327, accuracy: 0.239583, mean_q: -43.453958, mean_eps: 0.100000\n",
      " 27330/50000: episode: 6112, duration: 0.016s, episode steps:   3, steps per second: 185, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 8997.548177, mae: 550.958476, accuracy: 0.239583, mean_q: -54.954000, mean_eps: 0.100000\n",
      " 27333/50000: episode: 6113, duration: 0.016s, episode steps:   3, steps per second: 190, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 8034.812663, mae: 562.603739, accuracy: 0.229167, mean_q: -41.893344, mean_eps: 0.100000\n",
      " 27336/50000: episode: 6114, duration: 0.015s, episode steps:   3, steps per second: 207, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 2.000 [1.000, 3.000],  loss: 9249.955892, mae: 569.660665, accuracy: 0.197917, mean_q: -58.392870, mean_eps: 0.100000\n",
      " 27339/50000: episode: 6115, duration: 0.013s, episode steps:   3, steps per second: 230, episode reward: -224.000, mean reward: -74.667 [-116.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 9121.697917, mae: 551.070231, accuracy: 0.239583, mean_q: -62.159930, mean_eps: 0.100000\n",
      " 27342/50000: episode: 6116, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 8369.392253, mae: 574.735758, accuracy: 0.156250, mean_q: -74.174329, mean_eps: 0.100000\n",
      " 27345/50000: episode: 6117, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 8060.056315, mae: 549.255615, accuracy: 0.187500, mean_q: -71.412771, mean_eps: 0.100000\n",
      " 27348/50000: episode: 6118, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 8936.733887, mae: 543.210856, accuracy: 0.197917, mean_q: -67.136378, mean_eps: 0.100000\n",
      " 27351/50000: episode: 6119, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 10649.170573, mae: 560.107849, accuracy: 0.218750, mean_q: -64.640999, mean_eps: 0.100000\n",
      " 27354/50000: episode: 6120, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 8174.571126, mae: 533.701111, accuracy: 0.177083, mean_q: -62.443218, mean_eps: 0.100000\n",
      " 27357/50000: episode: 6121, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 7096.067220, mae: 537.012441, accuracy: 0.166667, mean_q: -62.139786, mean_eps: 0.100000\n",
      " 27361/50000: episode: 6122, duration: 0.016s, episode steps:   4, steps per second: 255, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 9645.480957, mae: 568.884918, accuracy: 0.148438, mean_q: -65.339298, mean_eps: 0.100000\n",
      " 27364/50000: episode: 6123, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 7727.420573, mae: 551.094157, accuracy: 0.177083, mean_q: -47.630814, mean_eps: 0.100000\n",
      " 27367/50000: episode: 6124, duration: 0.013s, episode steps:   3, steps per second: 226, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 7870.478027, mae: 559.879395, accuracy: 0.166667, mean_q: -69.107157, mean_eps: 0.100000\n",
      " 27370/50000: episode: 6125, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 9519.045898, mae: 559.912150, accuracy: 0.187500, mean_q: -70.375248, mean_eps: 0.100000\n",
      " 27373/50000: episode: 6126, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 8359.622233, mae: 559.628418, accuracy: 0.208333, mean_q: -65.210437, mean_eps: 0.100000\n",
      " 27376/50000: episode: 6127, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 8039.160645, mae: 555.690226, accuracy: 0.208333, mean_q: -53.938925, mean_eps: 0.100000\n",
      " 27379/50000: episode: 6128, duration: 0.015s, episode steps:   3, steps per second: 205, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 8716.306152, mae: 570.769613, accuracy: 0.166667, mean_q: -55.612016, mean_eps: 0.100000\n",
      " 27382/50000: episode: 6129, duration: 0.017s, episode steps:   3, steps per second: 172, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 7799.157064, mae: 555.493164, accuracy: 0.156250, mean_q: -78.119977, mean_eps: 0.100000\n",
      " 27386/50000: episode: 6130, duration: 0.019s, episode steps:   4, steps per second: 206, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 8709.494629, mae: 569.715622, accuracy: 0.250000, mean_q: -69.946510, mean_eps: 0.100000\n",
      " 27389/50000: episode: 6131, duration: 0.015s, episode steps:   3, steps per second: 195, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 6759.104492, mae: 547.807271, accuracy: 0.187500, mean_q: -49.978330, mean_eps: 0.100000\n",
      " 27392/50000: episode: 6132, duration: 0.014s, episode steps:   3, steps per second: 211, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 1.667 [0.000, 3.000],  loss: 8708.939290, mae: 565.067647, accuracy: 0.177083, mean_q: -51.345502, mean_eps: 0.100000\n",
      " 27395/50000: episode: 6133, duration: 0.013s, episode steps:   3, steps per second: 225, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 7187.261230, mae: 561.692871, accuracy: 0.135417, mean_q: -58.067507, mean_eps: 0.100000\n",
      " 27398/50000: episode: 6134, duration: 0.013s, episode steps:   3, steps per second: 225, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 9259.914062, mae: 574.913595, accuracy: 0.072917, mean_q: -42.637071, mean_eps: 0.100000\n",
      " 27401/50000: episode: 6135, duration: 0.014s, episode steps:   3, steps per second: 222, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 9331.719727, mae: 551.123149, accuracy: 0.135417, mean_q: -60.137149, mean_eps: 0.100000\n",
      " 27404/50000: episode: 6136, duration: 0.015s, episode steps:   3, steps per second: 204, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 8351.706055, mae: 557.207845, accuracy: 0.156250, mean_q: -59.263967, mean_eps: 0.100000\n",
      " 27407/50000: episode: 6137, duration: 0.013s, episode steps:   3, steps per second: 223, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 9529.057129, mae: 557.803304, accuracy: 0.208333, mean_q: -92.552284, mean_eps: 0.100000\n",
      " 27410/50000: episode: 6138, duration: 0.014s, episode steps:   3, steps per second: 213, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 8958.255534, mae: 569.262716, accuracy: 0.156250, mean_q: -49.484262, mean_eps: 0.100000\n",
      " 27413/50000: episode: 6139, duration: 0.014s, episode steps:   3, steps per second: 214, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 7255.664876, mae: 565.193319, accuracy: 0.239583, mean_q: -45.337072, mean_eps: 0.100000\n",
      " 27416/50000: episode: 6140, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 8164.233073, mae: 551.841085, accuracy: 0.156250, mean_q: -75.361914, mean_eps: 0.100000\n",
      " 27419/50000: episode: 6141, duration: 0.013s, episode steps:   3, steps per second: 224, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 8778.518880, mae: 563.513041, accuracy: 0.229167, mean_q: -45.295367, mean_eps: 0.100000\n",
      " 27422/50000: episode: 6142, duration: 0.017s, episode steps:   3, steps per second: 174, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 9386.680013, mae: 572.564351, accuracy: 0.177083, mean_q: -55.395084, mean_eps: 0.100000\n",
      " 27425/50000: episode: 6143, duration: 0.022s, episode steps:   3, steps per second: 138, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 8878.719401, mae: 563.687073, accuracy: 0.197917, mean_q: -77.020765, mean_eps: 0.100000\n",
      " 27428/50000: episode: 6144, duration: 0.015s, episode steps:   3, steps per second: 196, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 8479.241699, mae: 559.752035, accuracy: 0.156250, mean_q: -66.973981, mean_eps: 0.100000\n",
      " 27432/50000: episode: 6145, duration: 0.016s, episode steps:   4, steps per second: 248, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 7828.591431, mae: 549.249466, accuracy: 0.148438, mean_q: -66.791393, mean_eps: 0.100000\n",
      " 27436/50000: episode: 6146, duration: 0.015s, episode steps:   4, steps per second: 263, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 8132.116333, mae: 550.652359, accuracy: 0.171875, mean_q: -63.651007, mean_eps: 0.100000\n",
      " 27439/50000: episode: 6147, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 8532.511719, mae: 553.293518, accuracy: 0.250000, mean_q: -26.029456, mean_eps: 0.100000\n",
      " 27443/50000: episode: 6148, duration: 0.016s, episode steps:   4, steps per second: 255, episode reward: -1238.000, mean reward: -309.500 [-999.000, -60.000], mean action: 1.500 [0.000, 3.000],  loss: 9332.901123, mae: 559.773972, accuracy: 0.210938, mean_q: -59.257028, mean_eps: 0.100000\n",
      " 27447/50000: episode: 6149, duration: 0.016s, episode steps:   4, steps per second: 255, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 9585.149048, mae: 550.199417, accuracy: 0.218750, mean_q: -55.389386, mean_eps: 0.100000\n",
      " 27450/50000: episode: 6150, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 9061.401855, mae: 535.649801, accuracy: 0.229167, mean_q: -51.886130, mean_eps: 0.100000\n",
      " 27453/50000: episode: 6151, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 8859.410807, mae: 568.867615, accuracy: 0.177083, mean_q: -62.135642, mean_eps: 0.100000\n",
      " 27456/50000: episode: 6152, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.333 [0.000, 3.000],  loss: 8955.536784, mae: 557.089376, accuracy: 0.208333, mean_q: -65.471845, mean_eps: 0.100000\n",
      " 27459/50000: episode: 6153, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 9361.899089, mae: 558.040080, accuracy: 0.166667, mean_q: -64.658251, mean_eps: 0.100000\n",
      " 27462/50000: episode: 6154, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 8723.717773, mae: 540.072998, accuracy: 0.218750, mean_q: -35.387241, mean_eps: 0.100000\n",
      " 27466/50000: episode: 6155, duration: 0.016s, episode steps:   4, steps per second: 254, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 9236.396240, mae: 548.861359, accuracy: 0.242188, mean_q: -55.472669, mean_eps: 0.100000\n",
      " 27469/50000: episode: 6156, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 9264.666341, mae: 546.187622, accuracy: 0.260417, mean_q: -55.327254, mean_eps: 0.100000\n",
      " 27472/50000: episode: 6157, duration: 0.015s, episode steps:   3, steps per second: 204, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 8624.273275, mae: 537.600464, accuracy: 0.250000, mean_q: -71.011017, mean_eps: 0.100000\n",
      " 27475/50000: episode: 6158, duration: 0.015s, episode steps:   3, steps per second: 201, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 9305.045898, mae: 566.639486, accuracy: 0.239583, mean_q: -43.565899, mean_eps: 0.100000\n",
      " 27478/50000: episode: 6159, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 8557.706217, mae: 563.886251, accuracy: 0.218750, mean_q: -55.519016, mean_eps: 0.100000\n",
      " 27481/50000: episode: 6160, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 8142.010417, mae: 567.277527, accuracy: 0.145833, mean_q: -58.452923, mean_eps: 0.100000\n",
      " 27484/50000: episode: 6161, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 6891.417155, mae: 536.991974, accuracy: 0.270833, mean_q: -52.659987, mean_eps: 0.100000\n",
      " 27487/50000: episode: 6162, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 8221.406901, mae: 559.993103, accuracy: 0.239583, mean_q: -56.351495, mean_eps: 0.100000\n",
      " 27490/50000: episode: 6163, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 8332.055827, mae: 550.677734, accuracy: 0.177083, mean_q: -66.342946, mean_eps: 0.100000\n",
      " 27494/50000: episode: 6164, duration: 0.015s, episode steps:   4, steps per second: 263, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 7974.905518, mae: 544.535629, accuracy: 0.203125, mean_q: -72.500210, mean_eps: 0.100000\n",
      " 27497/50000: episode: 6165, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 10976.416341, mae: 553.081177, accuracy: 0.177083, mean_q: -80.237813, mean_eps: 0.100000\n",
      " 27500/50000: episode: 6166, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 8492.681152, mae: 544.310628, accuracy: 0.156250, mean_q: -64.017550, mean_eps: 0.100000\n",
      " 27503/50000: episode: 6167, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 8649.276367, mae: 552.464193, accuracy: 0.197917, mean_q: -47.674666, mean_eps: 0.100000\n",
      " 27506/50000: episode: 6168, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 6960.920085, mae: 557.330648, accuracy: 0.177083, mean_q: -49.950396, mean_eps: 0.100000\n",
      " 27510/50000: episode: 6169, duration: 0.016s, episode steps:   4, steps per second: 258, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.000 [0.000, 3.000],  loss: 8817.149780, mae: 551.325317, accuracy: 0.195312, mean_q: -85.232218, mean_eps: 0.100000\n",
      " 27514/50000: episode: 6170, duration: 0.015s, episode steps:   4, steps per second: 258, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 2.000 [0.000, 3.000],  loss: 8693.206543, mae: 548.873032, accuracy: 0.164062, mean_q: -76.615600, mean_eps: 0.100000\n",
      " 27517/50000: episode: 6171, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 7927.994141, mae: 559.221842, accuracy: 0.187500, mean_q: -68.451820, mean_eps: 0.100000\n",
      " 27520/50000: episode: 6172, duration: 0.019s, episode steps:   3, steps per second: 156, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 8659.414714, mae: 549.057007, accuracy: 0.177083, mean_q: -62.201876, mean_eps: 0.100000\n",
      " 27523/50000: episode: 6173, duration: 0.018s, episode steps:   3, steps per second: 166, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 7948.155924, mae: 582.508911, accuracy: 0.104167, mean_q: -51.323943, mean_eps: 0.100000\n",
      " 27526/50000: episode: 6174, duration: 0.015s, episode steps:   3, steps per second: 203, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 7565.533854, mae: 565.047302, accuracy: 0.177083, mean_q: -39.865266, mean_eps: 0.100000\n",
      " 27529/50000: episode: 6175, duration: 0.014s, episode steps:   3, steps per second: 215, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 8851.014974, mae: 540.423136, accuracy: 0.250000, mean_q: -83.552391, mean_eps: 0.100000\n",
      " 27532/50000: episode: 6176, duration: 0.013s, episode steps:   3, steps per second: 223, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 8171.337077, mae: 535.462341, accuracy: 0.239583, mean_q: -83.382182, mean_eps: 0.100000\n",
      " 27535/50000: episode: 6177, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 9707.135091, mae: 568.403381, accuracy: 0.166667, mean_q: -78.903093, mean_eps: 0.100000\n",
      " 27539/50000: episode: 6178, duration: 0.016s, episode steps:   4, steps per second: 244, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 8952.973389, mae: 565.874969, accuracy: 0.226562, mean_q: -59.702743, mean_eps: 0.100000\n",
      " 27542/50000: episode: 6179, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 8750.308919, mae: 553.139364, accuracy: 0.145833, mean_q: -66.857622, mean_eps: 0.100000\n",
      " 27545/50000: episode: 6180, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 8346.936035, mae: 550.627096, accuracy: 0.208333, mean_q: -46.645067, mean_eps: 0.100000\n",
      " 27548/50000: episode: 6181, duration: 0.014s, episode steps:   3, steps per second: 221, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 7722.721029, mae: 551.545308, accuracy: 0.177083, mean_q: -70.008081, mean_eps: 0.100000\n",
      " 27552/50000: episode: 6182, duration: 0.017s, episode steps:   4, steps per second: 240, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 8728.178955, mae: 549.469528, accuracy: 0.187500, mean_q: -43.283301, mean_eps: 0.100000\n",
      " 27555/50000: episode: 6183, duration: 0.014s, episode steps:   3, steps per second: 217, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 7626.948568, mae: 543.189168, accuracy: 0.135417, mean_q: -51.328299, mean_eps: 0.100000\n",
      " 27558/50000: episode: 6184, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 7539.036296, mae: 549.101725, accuracy: 0.239583, mean_q: -70.754777, mean_eps: 0.100000\n",
      " 27561/50000: episode: 6185, duration: 0.013s, episode steps:   3, steps per second: 230, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 6894.865234, mae: 558.577799, accuracy: 0.166667, mean_q: -62.224063, mean_eps: 0.100000\n",
      " 27565/50000: episode: 6186, duration: 0.016s, episode steps:   4, steps per second: 249, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 2.000 [0.000, 3.000],  loss: 8978.139038, mae: 559.016098, accuracy: 0.195312, mean_q: -95.593716, mean_eps: 0.100000\n",
      " 27569/50000: episode: 6187, duration: 0.018s, episode steps:   4, steps per second: 224, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 2.000 [0.000, 3.000],  loss: 8260.655884, mae: 569.629044, accuracy: 0.210938, mean_q: -61.090649, mean_eps: 0.100000\n",
      " 27572/50000: episode: 6188, duration: 0.015s, episode steps:   3, steps per second: 195, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 8918.755534, mae: 574.743754, accuracy: 0.229167, mean_q: -89.353406, mean_eps: 0.100000\n",
      " 27575/50000: episode: 6189, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 8710.584635, mae: 548.714498, accuracy: 0.229167, mean_q: -45.744343, mean_eps: 0.100000\n",
      " 27579/50000: episode: 6190, duration: 0.016s, episode steps:   4, steps per second: 253, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 8876.170898, mae: 546.441071, accuracy: 0.195312, mean_q: -40.654761, mean_eps: 0.100000\n",
      " 27582/50000: episode: 6191, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 7520.564616, mae: 549.407694, accuracy: 0.114583, mean_q: -81.670486, mean_eps: 0.100000\n",
      " 27585/50000: episode: 6192, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 8735.942871, mae: 560.072754, accuracy: 0.229167, mean_q: -51.284879, mean_eps: 0.100000\n",
      " 27588/50000: episode: 6193, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 7938.189941, mae: 532.860555, accuracy: 0.218750, mean_q: -50.916808, mean_eps: 0.100000\n",
      " 27591/50000: episode: 6194, duration: 0.014s, episode steps:   3, steps per second: 218, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 8743.062826, mae: 555.954651, accuracy: 0.218750, mean_q: -84.912348, mean_eps: 0.100000\n",
      " 27594/50000: episode: 6195, duration: 0.013s, episode steps:   3, steps per second: 225, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 9138.547038, mae: 542.429474, accuracy: 0.218750, mean_q: -83.587494, mean_eps: 0.100000\n",
      " 27597/50000: episode: 6196, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 9461.156738, mae: 544.206380, accuracy: 0.197917, mean_q: -56.143964, mean_eps: 0.100000\n",
      " 27600/50000: episode: 6197, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 6875.000651, mae: 543.808350, accuracy: 0.218750, mean_q: -65.432955, mean_eps: 0.100000\n",
      " 27603/50000: episode: 6198, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 9791.338542, mae: 547.000509, accuracy: 0.229167, mean_q: -54.558360, mean_eps: 0.100000\n",
      " 27606/50000: episode: 6199, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 8089.482096, mae: 562.129110, accuracy: 0.208333, mean_q: -61.014370, mean_eps: 0.100000\n",
      " 27609/50000: episode: 6200, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 8282.115234, mae: 563.705485, accuracy: 0.166667, mean_q: -61.690434, mean_eps: 0.100000\n",
      " 27612/50000: episode: 6201, duration: 0.015s, episode steps:   3, steps per second: 200, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 9080.395671, mae: 556.360738, accuracy: 0.197917, mean_q: -62.468479, mean_eps: 0.100000\n",
      " 27615/50000: episode: 6202, duration: 0.017s, episode steps:   3, steps per second: 173, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 7463.104818, mae: 525.208923, accuracy: 0.177083, mean_q: -81.440371, mean_eps: 0.100000\n",
      " 27618/50000: episode: 6203, duration: 0.013s, episode steps:   3, steps per second: 227, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 8840.787109, mae: 544.496623, accuracy: 0.187500, mean_q: -93.021146, mean_eps: 0.100000\n",
      " 27622/50000: episode: 6204, duration: 0.017s, episode steps:   4, steps per second: 231, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 8312.198853, mae: 558.442184, accuracy: 0.171875, mean_q: -62.638748, mean_eps: 0.100000\n",
      " 27625/50000: episode: 6205, duration: 0.014s, episode steps:   3, steps per second: 215, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 8065.302083, mae: 556.340271, accuracy: 0.239583, mean_q: -61.054457, mean_eps: 0.100000\n",
      " 27628/50000: episode: 6206, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 7777.218099, mae: 572.882690, accuracy: 0.187500, mean_q: -61.017972, mean_eps: 0.100000\n",
      " 27631/50000: episode: 6207, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 8233.388184, mae: 559.939392, accuracy: 0.187500, mean_q: -53.986759, mean_eps: 0.100000\n",
      " 27634/50000: episode: 6208, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 8846.870443, mae: 565.214233, accuracy: 0.229167, mean_q: -56.530340, mean_eps: 0.100000\n",
      " 27637/50000: episode: 6209, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 7173.049479, mae: 541.731384, accuracy: 0.229167, mean_q: -75.138723, mean_eps: 0.100000\n",
      " 27640/50000: episode: 6210, duration: 0.012s, episode steps:   3, steps per second: 255, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 7066.645345, mae: 574.060832, accuracy: 0.177083, mean_q: -75.811101, mean_eps: 0.100000\n",
      " 27643/50000: episode: 6211, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 9775.936849, mae: 562.065653, accuracy: 0.135417, mean_q: -73.754297, mean_eps: 0.100000\n",
      " 27646/50000: episode: 6212, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 9007.229818, mae: 548.755615, accuracy: 0.208333, mean_q: -101.807587, mean_eps: 0.100000\n",
      " 27649/50000: episode: 6213, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 6238.203451, mae: 557.148376, accuracy: 0.260417, mean_q: -57.800893, mean_eps: 0.100000\n",
      " 27653/50000: episode: 6214, duration: 0.016s, episode steps:   4, steps per second: 253, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 8458.455444, mae: 568.549225, accuracy: 0.226562, mean_q: -49.069668, mean_eps: 0.100000\n",
      " 27656/50000: episode: 6215, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 8331.653483, mae: 558.798157, accuracy: 0.239583, mean_q: -72.075897, mean_eps: 0.100000\n",
      " 27660/50000: episode: 6216, duration: 0.016s, episode steps:   4, steps per second: 247, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 8545.541260, mae: 546.443130, accuracy: 0.179688, mean_q: -72.812023, mean_eps: 0.100000\n",
      " 27663/50000: episode: 6217, duration: 0.023s, episode steps:   3, steps per second: 131, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 9023.140299, mae: 559.739075, accuracy: 0.145833, mean_q: -54.932292, mean_eps: 0.100000\n",
      " 27667/50000: episode: 6218, duration: 0.020s, episode steps:   4, steps per second: 196, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 9652.852051, mae: 556.558121, accuracy: 0.281250, mean_q: -56.421991, mean_eps: 0.100000\n",
      " 27670/50000: episode: 6219, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 7398.608398, mae: 531.297485, accuracy: 0.260417, mean_q: -53.543616, mean_eps: 0.100000\n",
      " 27675/50000: episode: 6220, duration: 0.019s, episode steps:   5, steps per second: 266, episode reward: -2237.000, mean reward: -447.400 [-999.000, -60.000], mean action: 2.200 [0.000, 3.000],  loss: 8897.572168, mae: 550.387024, accuracy: 0.193750, mean_q: -69.163373, mean_eps: 0.100000\n",
      " 27678/50000: episode: 6221, duration: 0.013s, episode steps:   3, steps per second: 230, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 9039.993815, mae: 546.206767, accuracy: 0.125000, mean_q: -55.349952, mean_eps: 0.100000\n",
      " 27681/50000: episode: 6222, duration: 0.012s, episode steps:   3, steps per second: 253, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 7816.725260, mae: 549.013407, accuracy: 0.218750, mean_q: -45.619095, mean_eps: 0.100000\n",
      " 27684/50000: episode: 6223, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 8477.900065, mae: 560.598674, accuracy: 0.177083, mean_q: -75.373112, mean_eps: 0.100000\n",
      " 27687/50000: episode: 6224, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 7589.418945, mae: 563.021200, accuracy: 0.177083, mean_q: -69.655706, mean_eps: 0.100000\n",
      " 27690/50000: episode: 6225, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 9055.392253, mae: 555.609172, accuracy: 0.177083, mean_q: -52.837995, mean_eps: 0.100000\n",
      " 27693/50000: episode: 6226, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 7615.235026, mae: 531.222270, accuracy: 0.218750, mean_q: -53.742373, mean_eps: 0.100000\n",
      " 27696/50000: episode: 6227, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 8643.789388, mae: 575.008423, accuracy: 0.114583, mean_q: -81.639208, mean_eps: 0.100000\n",
      " 27699/50000: episode: 6228, duration: 0.013s, episode steps:   3, steps per second: 226, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 9029.421224, mae: 570.410929, accuracy: 0.177083, mean_q: -65.838013, mean_eps: 0.100000\n",
      " 27702/50000: episode: 6229, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 8873.331868, mae: 535.586426, accuracy: 0.218750, mean_q: -85.170837, mean_eps: 0.100000\n",
      " 27705/50000: episode: 6230, duration: 0.019s, episode steps:   3, steps per second: 162, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 6133.960938, mae: 535.058512, accuracy: 0.197917, mean_q: -88.504461, mean_eps: 0.100000\n",
      " 27708/50000: episode: 6231, duration: 0.016s, episode steps:   3, steps per second: 188, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 6131.544596, mae: 565.031331, accuracy: 0.208333, mean_q: -71.738108, mean_eps: 0.100000\n",
      " 27711/50000: episode: 6232, duration: 0.013s, episode steps:   3, steps per second: 227, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 7187.026204, mae: 562.583028, accuracy: 0.166667, mean_q: -58.749503, mean_eps: 0.100000\n",
      " 27714/50000: episode: 6233, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 8885.925944, mae: 544.258199, accuracy: 0.187500, mean_q: -48.410261, mean_eps: 0.100000\n",
      " 27718/50000: episode: 6234, duration: 0.015s, episode steps:   4, steps per second: 262, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 7501.994873, mae: 547.770477, accuracy: 0.203125, mean_q: -47.225121, mean_eps: 0.100000\n",
      " 27721/50000: episode: 6235, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 9043.934896, mae: 561.665833, accuracy: 0.208333, mean_q: -88.795812, mean_eps: 0.100000\n",
      " 27724/50000: episode: 6236, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 7956.099447, mae: 551.732117, accuracy: 0.156250, mean_q: -61.116262, mean_eps: 0.100000\n",
      " 27727/50000: episode: 6237, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 10279.726237, mae: 557.450948, accuracy: 0.250000, mean_q: -50.998711, mean_eps: 0.100000\n",
      " 27732/50000: episode: 6238, duration: 0.019s, episode steps:   5, steps per second: 262, episode reward: -2193.000, mean reward: -438.600 [-999.000, -32.000], mean action: 1.600 [0.000, 3.000],  loss: 8938.428516, mae: 554.176672, accuracy: 0.156250, mean_q: -43.343362, mean_eps: 0.100000\n",
      " 27735/50000: episode: 6239, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 8835.444010, mae: 557.313395, accuracy: 0.218750, mean_q: -79.911690, mean_eps: 0.100000\n",
      " 27738/50000: episode: 6240, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 9317.131836, mae: 552.197673, accuracy: 0.250000, mean_q: -57.627759, mean_eps: 0.100000\n",
      " 27741/50000: episode: 6241, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 8393.239583, mae: 571.833618, accuracy: 0.250000, mean_q: -49.635900, mean_eps: 0.100000\n",
      " 27744/50000: episode: 6242, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 7437.268392, mae: 539.761251, accuracy: 0.156250, mean_q: -62.922035, mean_eps: 0.100000\n",
      " 27747/50000: episode: 6243, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 9118.240885, mae: 569.324626, accuracy: 0.187500, mean_q: -59.285903, mean_eps: 0.100000\n",
      " 27750/50000: episode: 6244, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 7400.896647, mae: 562.520182, accuracy: 0.156250, mean_q: -69.460192, mean_eps: 0.100000\n",
      " 27753/50000: episode: 6245, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 8463.793620, mae: 576.011637, accuracy: 0.135417, mean_q: -68.406844, mean_eps: 0.100000\n",
      " 27756/50000: episode: 6246, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 8625.609863, mae: 578.571025, accuracy: 0.166667, mean_q: -58.514549, mean_eps: 0.100000\n",
      " 27759/50000: episode: 6247, duration: 0.017s, episode steps:   3, steps per second: 176, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 7260.978678, mae: 559.564779, accuracy: 0.177083, mean_q: -67.082628, mean_eps: 0.100000\n",
      " 27762/50000: episode: 6248, duration: 0.016s, episode steps:   3, steps per second: 192, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 8951.540853, mae: 551.542664, accuracy: 0.145833, mean_q: -70.159419, mean_eps: 0.100000\n",
      " 27765/50000: episode: 6249, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 7117.812337, mae: 563.753845, accuracy: 0.156250, mean_q: -70.967079, mean_eps: 0.100000\n",
      " 27769/50000: episode: 6250, duration: 0.016s, episode steps:   4, steps per second: 255, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 7254.025146, mae: 557.009033, accuracy: 0.203125, mean_q: -56.514826, mean_eps: 0.100000\n",
      " 27772/50000: episode: 6251, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 6991.834147, mae: 563.434448, accuracy: 0.197917, mean_q: -76.911344, mean_eps: 0.100000\n",
      " 27776/50000: episode: 6252, duration: 0.015s, episode steps:   4, steps per second: 263, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 9682.400391, mae: 563.350754, accuracy: 0.140625, mean_q: -67.107088, mean_eps: 0.100000\n",
      " 27779/50000: episode: 6253, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 8728.696615, mae: 561.299316, accuracy: 0.125000, mean_q: -62.122442, mean_eps: 0.100000\n",
      " 27782/50000: episode: 6254, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 7311.409180, mae: 559.098145, accuracy: 0.187500, mean_q: -63.480699, mean_eps: 0.100000\n",
      " 27785/50000: episode: 6255, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 9642.717122, mae: 577.901876, accuracy: 0.218750, mean_q: -79.500099, mean_eps: 0.100000\n",
      " 27788/50000: episode: 6256, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 7476.318359, mae: 554.730347, accuracy: 0.114583, mean_q: -48.148969, mean_eps: 0.100000\n",
      " 27791/50000: episode: 6257, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 9001.917643, mae: 567.970296, accuracy: 0.177083, mean_q: -64.939938, mean_eps: 0.100000\n",
      " 27795/50000: episode: 6258, duration: 0.015s, episode steps:   4, steps per second: 262, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 8723.155029, mae: 565.532364, accuracy: 0.210938, mean_q: -86.390837, mean_eps: 0.100000\n",
      " 27798/50000: episode: 6259, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 8176.483073, mae: 588.521403, accuracy: 0.104167, mean_q: -48.254206, mean_eps: 0.100000\n",
      " 27801/50000: episode: 6260, duration: 0.014s, episode steps:   3, steps per second: 217, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 7196.864258, mae: 549.299805, accuracy: 0.187500, mean_q: -73.511505, mean_eps: 0.100000\n",
      " 27804/50000: episode: 6261, duration: 0.019s, episode steps:   3, steps per second: 155, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 8081.270182, mae: 558.831523, accuracy: 0.250000, mean_q: -78.553935, mean_eps: 0.100000\n",
      " 27807/50000: episode: 6262, duration: 0.015s, episode steps:   3, steps per second: 200, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 7487.719727, mae: 569.567383, accuracy: 0.260417, mean_q: -71.999827, mean_eps: 0.100000\n",
      " 27810/50000: episode: 6263, duration: 0.013s, episode steps:   3, steps per second: 223, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 6440.374023, mae: 565.332174, accuracy: 0.166667, mean_q: -71.287601, mean_eps: 0.100000\n",
      " 27814/50000: episode: 6264, duration: 0.016s, episode steps:   4, steps per second: 256, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 7999.804199, mae: 563.184845, accuracy: 0.210938, mean_q: -73.995266, mean_eps: 0.100000\n",
      " 27817/50000: episode: 6265, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 8368.053711, mae: 554.864766, accuracy: 0.114583, mean_q: -64.386673, mean_eps: 0.100000\n",
      " 27820/50000: episode: 6266, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 9152.231445, mae: 541.322550, accuracy: 0.197917, mean_q: -97.155022, mean_eps: 0.100000\n",
      " 27823/50000: episode: 6267, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 6784.387695, mae: 579.485046, accuracy: 0.145833, mean_q: -53.044669, mean_eps: 0.100000\n",
      " 27826/50000: episode: 6268, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 9047.703776, mae: 565.584534, accuracy: 0.135417, mean_q: -81.442126, mean_eps: 0.100000\n",
      " 27829/50000: episode: 6269, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 8326.419271, mae: 579.527222, accuracy: 0.135417, mean_q: -79.575493, mean_eps: 0.100000\n",
      " 27832/50000: episode: 6270, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 8096.732259, mae: 574.314168, accuracy: 0.125000, mean_q: -77.327217, mean_eps: 0.100000\n",
      " 27835/50000: episode: 6271, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 8313.443034, mae: 564.409749, accuracy: 0.197917, mean_q: -67.022058, mean_eps: 0.100000\n",
      " 27838/50000: episode: 6272, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 6937.178223, mae: 530.996562, accuracy: 0.166667, mean_q: -65.316157, mean_eps: 0.100000\n",
      " 27841/50000: episode: 6273, duration: 0.013s, episode steps:   3, steps per second: 227, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 7641.132650, mae: 549.407654, accuracy: 0.145833, mean_q: -61.283260, mean_eps: 0.100000\n",
      " 27844/50000: episode: 6274, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 7504.794434, mae: 555.208984, accuracy: 0.135417, mean_q: -78.740486, mean_eps: 0.100000\n",
      " 27847/50000: episode: 6275, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 6729.773275, mae: 566.190511, accuracy: 0.177083, mean_q: -61.442177, mean_eps: 0.100000\n",
      " 27850/50000: episode: 6276, duration: 0.014s, episode steps:   3, steps per second: 214, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 8121.545898, mae: 575.479960, accuracy: 0.104167, mean_q: -57.862640, mean_eps: 0.100000\n",
      " 27853/50000: episode: 6277, duration: 0.016s, episode steps:   3, steps per second: 191, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 8257.291016, mae: 570.294067, accuracy: 0.135417, mean_q: -58.454753, mean_eps: 0.100000\n",
      " 27856/50000: episode: 6278, duration: 0.015s, episode steps:   3, steps per second: 201, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 7969.444661, mae: 549.692647, accuracy: 0.197917, mean_q: -68.743257, mean_eps: 0.100000\n",
      " 27859/50000: episode: 6279, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 7205.926758, mae: 571.348246, accuracy: 0.177083, mean_q: -71.209321, mean_eps: 0.100000\n",
      " 27862/50000: episode: 6280, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 7787.156576, mae: 571.516663, accuracy: 0.177083, mean_q: -82.761322, mean_eps: 0.100000\n",
      " 27865/50000: episode: 6281, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 6115.314779, mae: 568.308329, accuracy: 0.135417, mean_q: -84.480283, mean_eps: 0.100000\n",
      " 27869/50000: episode: 6282, duration: 0.016s, episode steps:   4, steps per second: 248, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 8192.275757, mae: 574.261627, accuracy: 0.218750, mean_q: -89.959846, mean_eps: 0.100000\n",
      " 27872/50000: episode: 6283, duration: 0.013s, episode steps:   3, steps per second: 226, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 9643.443685, mae: 563.714600, accuracy: 0.166667, mean_q: -88.527074, mean_eps: 0.100000\n",
      " 27875/50000: episode: 6284, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 7389.887533, mae: 562.219666, accuracy: 0.166667, mean_q: -77.252909, mean_eps: 0.100000\n",
      " 27878/50000: episode: 6285, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 6999.883464, mae: 538.609070, accuracy: 0.135417, mean_q: -75.989438, mean_eps: 0.100000\n",
      " 27881/50000: episode: 6286, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 7994.405436, mae: 552.899211, accuracy: 0.208333, mean_q: -54.919387, mean_eps: 0.100000\n",
      " 27884/50000: episode: 6287, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 7416.363444, mae: 556.397278, accuracy: 0.145833, mean_q: -43.620528, mean_eps: 0.100000\n",
      " 27887/50000: episode: 6288, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 7449.831217, mae: 569.967468, accuracy: 0.104167, mean_q: -65.336159, mean_eps: 0.100000\n",
      " 27890/50000: episode: 6289, duration: 0.013s, episode steps:   3, steps per second: 226, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 9420.574056, mae: 538.171326, accuracy: 0.187500, mean_q: -87.224932, mean_eps: 0.100000\n",
      " 27893/50000: episode: 6290, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 8365.681315, mae: 565.206034, accuracy: 0.125000, mean_q: -71.020228, mean_eps: 0.100000\n",
      " 27897/50000: episode: 6291, duration: 0.022s, episode steps:   4, steps per second: 184, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 8025.615723, mae: 574.130692, accuracy: 0.125000, mean_q: -64.767728, mean_eps: 0.100000\n",
      " 27900/50000: episode: 6292, duration: 0.016s, episode steps:   3, steps per second: 190, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 7969.377279, mae: 559.419393, accuracy: 0.114583, mean_q: -82.385137, mean_eps: 0.100000\n",
      " 27903/50000: episode: 6293, duration: 0.013s, episode steps:   3, steps per second: 226, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 7413.735026, mae: 566.742574, accuracy: 0.125000, mean_q: -65.124437, mean_eps: 0.100000\n",
      " 27907/50000: episode: 6294, duration: 0.015s, episode steps:   4, steps per second: 261, episode reward: -1238.000, mean reward: -309.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 6964.604858, mae: 562.110306, accuracy: 0.093750, mean_q: -87.240053, mean_eps: 0.100000\n",
      " 27910/50000: episode: 6295, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 8319.538086, mae: 561.072001, accuracy: 0.166667, mean_q: -100.073209, mean_eps: 0.100000\n",
      " 27913/50000: episode: 6296, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 7248.907064, mae: 532.014669, accuracy: 0.177083, mean_q: -90.284894, mean_eps: 0.100000\n",
      " 27916/50000: episode: 6297, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 10214.702474, mae: 570.876587, accuracy: 0.114583, mean_q: -83.384722, mean_eps: 0.100000\n",
      " 27919/50000: episode: 6298, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 8467.672852, mae: 556.250895, accuracy: 0.156250, mean_q: -78.683563, mean_eps: 0.100000\n",
      " 27922/50000: episode: 6299, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 9022.703288, mae: 571.928792, accuracy: 0.093750, mean_q: -56.595385, mean_eps: 0.100000\n",
      " 27925/50000: episode: 6300, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 9071.066081, mae: 568.653829, accuracy: 0.135417, mean_q: -57.577836, mean_eps: 0.100000\n",
      " 27928/50000: episode: 6301, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 8912.470052, mae: 562.949626, accuracy: 0.177083, mean_q: -61.836605, mean_eps: 0.100000\n",
      " 27931/50000: episode: 6302, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 8040.610677, mae: 564.901733, accuracy: 0.166667, mean_q: -72.075882, mean_eps: 0.100000\n",
      " 27934/50000: episode: 6303, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 8122.344076, mae: 552.154419, accuracy: 0.208333, mean_q: -46.345924, mean_eps: 0.100000\n",
      " 27937/50000: episode: 6304, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 7932.248861, mae: 547.159444, accuracy: 0.197917, mean_q: -68.764503, mean_eps: 0.100000\n",
      " 27940/50000: episode: 6305, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 8711.656576, mae: 563.048604, accuracy: 0.166667, mean_q: -81.212512, mean_eps: 0.100000\n",
      " 27943/50000: episode: 6306, duration: 0.014s, episode steps:   3, steps per second: 210, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 6387.212402, mae: 565.647359, accuracy: 0.156250, mean_q: -85.025073, mean_eps: 0.100000\n",
      " 27946/50000: episode: 6307, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 8470.140625, mae: 545.107564, accuracy: 0.125000, mean_q: -89.778450, mean_eps: 0.100000\n",
      " 27949/50000: episode: 6308, duration: 0.013s, episode steps:   3, steps per second: 227, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 8933.659180, mae: 544.388916, accuracy: 0.145833, mean_q: -69.105480, mean_eps: 0.100000\n",
      " 27952/50000: episode: 6309, duration: 0.015s, episode steps:   3, steps per second: 198, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 7471.199870, mae: 570.244527, accuracy: 0.177083, mean_q: -54.478218, mean_eps: 0.100000\n",
      " 27955/50000: episode: 6310, duration: 0.014s, episode steps:   3, steps per second: 219, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 8343.147135, mae: 566.466675, accuracy: 0.177083, mean_q: -83.561864, mean_eps: 0.100000\n",
      " 27958/50000: episode: 6311, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 8433.265951, mae: 558.732442, accuracy: 0.166667, mean_q: -65.516446, mean_eps: 0.100000\n",
      " 27961/50000: episode: 6312, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 8078.239258, mae: 565.321289, accuracy: 0.135417, mean_q: -63.355954, mean_eps: 0.100000\n",
      " 27964/50000: episode: 6313, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 8322.984538, mae: 561.468913, accuracy: 0.145833, mean_q: -72.438123, mean_eps: 0.100000\n",
      " 27967/50000: episode: 6314, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -224.000, mean reward: -74.667 [-116.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 7367.875326, mae: 550.883748, accuracy: 0.177083, mean_q: -67.524055, mean_eps: 0.100000\n",
      " 27970/50000: episode: 6315, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 8535.698079, mae: 581.157532, accuracy: 0.104167, mean_q: -77.687383, mean_eps: 0.100000\n",
      " 27973/50000: episode: 6316, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 7344.154622, mae: 559.521200, accuracy: 0.145833, mean_q: -64.701590, mean_eps: 0.100000\n",
      " 27976/50000: episode: 6317, duration: 0.012s, episode steps:   3, steps per second: 254, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 7300.766927, mae: 577.130636, accuracy: 0.104167, mean_q: -60.884824, mean_eps: 0.100000\n",
      " 27979/50000: episode: 6318, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 8889.349609, mae: 570.252462, accuracy: 0.156250, mean_q: -57.964050, mean_eps: 0.100000\n",
      " 27982/50000: episode: 6319, duration: 0.012s, episode steps:   3, steps per second: 254, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 6696.256185, mae: 561.631022, accuracy: 0.104167, mean_q: -59.562420, mean_eps: 0.100000\n",
      " 27985/50000: episode: 6320, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 7798.730794, mae: 558.207235, accuracy: 0.145833, mean_q: -88.734581, mean_eps: 0.100000\n",
      " 27988/50000: episode: 6321, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 9079.277018, mae: 579.090434, accuracy: 0.125000, mean_q: -73.514633, mean_eps: 0.100000\n",
      " 27991/50000: episode: 6322, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 9258.559896, mae: 570.740438, accuracy: 0.208333, mean_q: -37.821054, mean_eps: 0.100000\n",
      " 27994/50000: episode: 6323, duration: 0.019s, episode steps:   3, steps per second: 159, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 6565.702311, mae: 565.763082, accuracy: 0.114583, mean_q: -78.709599, mean_eps: 0.100000\n",
      " 27998/50000: episode: 6324, duration: 0.022s, episode steps:   4, steps per second: 180, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 7244.367188, mae: 562.727692, accuracy: 0.187500, mean_q: -57.716881, mean_eps: 0.100000\n",
      " 28001/50000: episode: 6325, duration: 0.015s, episode steps:   3, steps per second: 206, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 8688.517578, mae: 566.192301, accuracy: 0.166667, mean_q: -64.670255, mean_eps: 0.100000\n",
      " 28004/50000: episode: 6326, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 8257.743490, mae: 573.192586, accuracy: 0.166667, mean_q: -77.606531, mean_eps: 0.100000\n",
      " 28007/50000: episode: 6327, duration: 0.013s, episode steps:   3, steps per second: 222, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 7873.083822, mae: 559.249634, accuracy: 0.114583, mean_q: -82.502401, mean_eps: 0.100000\n",
      " 28010/50000: episode: 6328, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 7487.996745, mae: 575.936137, accuracy: 0.072917, mean_q: -48.174253, mean_eps: 0.100000\n",
      " 28013/50000: episode: 6329, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 6739.087891, mae: 548.307617, accuracy: 0.197917, mean_q: -79.661799, mean_eps: 0.100000\n",
      " 28016/50000: episode: 6330, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 8534.591960, mae: 558.185059, accuracy: 0.166667, mean_q: -78.507927, mean_eps: 0.100000\n",
      " 28019/50000: episode: 6331, duration: 0.013s, episode steps:   3, steps per second: 226, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 6862.224772, mae: 576.354513, accuracy: 0.125000, mean_q: -72.553987, mean_eps: 0.100000\n",
      " 28022/50000: episode: 6332, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 1.667 [0.000, 3.000],  loss: 8918.945475, mae: 544.791280, accuracy: 0.187500, mean_q: -91.039266, mean_eps: 0.100000\n",
      " 28025/50000: episode: 6333, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 7507.184896, mae: 560.283508, accuracy: 0.145833, mean_q: -67.093367, mean_eps: 0.100000\n",
      " 28028/50000: episode: 6334, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 7696.787923, mae: 551.323324, accuracy: 0.177083, mean_q: -87.894155, mean_eps: 0.100000\n",
      " 28031/50000: episode: 6335, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 6225.651367, mae: 550.305298, accuracy: 0.072917, mean_q: -91.124502, mean_eps: 0.100000\n",
      " 28034/50000: episode: 6336, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 7467.784831, mae: 571.221110, accuracy: 0.114583, mean_q: -68.957751, mean_eps: 0.100000\n",
      " 28037/50000: episode: 6337, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 7741.503418, mae: 567.720357, accuracy: 0.145833, mean_q: -73.040759, mean_eps: 0.100000\n",
      " 28040/50000: episode: 6338, duration: 0.014s, episode steps:   3, steps per second: 214, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 7954.847331, mae: 562.677979, accuracy: 0.156250, mean_q: -63.440439, mean_eps: 0.100000\n",
      " 28043/50000: episode: 6339, duration: 0.022s, episode steps:   3, steps per second: 136, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 7384.191081, mae: 567.696899, accuracy: 0.041667, mean_q: -82.010635, mean_eps: 0.100000\n",
      " 28046/50000: episode: 6340, duration: 0.020s, episode steps:   3, steps per second: 152, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 7507.667969, mae: 573.410726, accuracy: 0.197917, mean_q: -60.995326, mean_eps: 0.100000\n",
      " 28049/50000: episode: 6341, duration: 0.014s, episode steps:   3, steps per second: 215, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 8212.685384, mae: 561.304220, accuracy: 0.250000, mean_q: -78.132337, mean_eps: 0.100000\n",
      " 28052/50000: episode: 6342, duration: 0.013s, episode steps:   3, steps per second: 225, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 8012.750163, mae: 557.545573, accuracy: 0.218750, mean_q: -76.959216, mean_eps: 0.100000\n",
      " 28055/50000: episode: 6343, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 6465.099935, mae: 574.859619, accuracy: 0.083333, mean_q: -44.357649, mean_eps: 0.100000\n",
      " 28058/50000: episode: 6344, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 9350.857747, mae: 564.196350, accuracy: 0.125000, mean_q: -79.170071, mean_eps: 0.100000\n",
      " 28061/50000: episode: 6345, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 8061.091309, mae: 559.078593, accuracy: 0.187500, mean_q: -80.689320, mean_eps: 0.100000\n",
      " 28064/50000: episode: 6346, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 6939.049479, mae: 566.310913, accuracy: 0.072917, mean_q: -66.904833, mean_eps: 0.100000\n",
      " 28067/50000: episode: 6347, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 7417.787923, mae: 551.792135, accuracy: 0.145833, mean_q: -85.245950, mean_eps: 0.100000\n",
      " 28070/50000: episode: 6348, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 8640.267741, mae: 571.870341, accuracy: 0.166667, mean_q: -111.204717, mean_eps: 0.100000\n",
      " 28073/50000: episode: 6349, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 7054.112467, mae: 560.352722, accuracy: 0.187500, mean_q: -70.652891, mean_eps: 0.100000\n",
      " 28076/50000: episode: 6350, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 8358.698893, mae: 560.972392, accuracy: 0.197917, mean_q: -60.648436, mean_eps: 0.100000\n",
      " 28079/50000: episode: 6351, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 6101.204590, mae: 550.425476, accuracy: 0.093750, mean_q: -65.078659, mean_eps: 0.100000\n",
      " 28082/50000: episode: 6352, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 7594.222005, mae: 555.719808, accuracy: 0.166667, mean_q: -83.469973, mean_eps: 0.100000\n",
      " 28085/50000: episode: 6353, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 8884.370768, mae: 580.086650, accuracy: 0.135417, mean_q: -66.048995, mean_eps: 0.100000\n",
      " 28089/50000: episode: 6354, duration: 0.016s, episode steps:   4, steps per second: 245, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 8445.010620, mae: 568.502869, accuracy: 0.085938, mean_q: -63.856681, mean_eps: 0.100000\n",
      " 28092/50000: episode: 6355, duration: 0.017s, episode steps:   3, steps per second: 174, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 7746.524902, mae: 553.769267, accuracy: 0.156250, mean_q: -67.193192, mean_eps: 0.100000\n",
      " 28095/50000: episode: 6356, duration: 0.015s, episode steps:   3, steps per second: 202, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 7877.295085, mae: 568.661947, accuracy: 0.145833, mean_q: -64.861568, mean_eps: 0.100000\n",
      " 28098/50000: episode: 6357, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 7163.515137, mae: 558.555115, accuracy: 0.114583, mean_q: -91.125984, mean_eps: 0.100000\n",
      " 28101/50000: episode: 6358, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 5630.791748, mae: 549.909831, accuracy: 0.104167, mean_q: -62.925165, mean_eps: 0.100000\n",
      " 28104/50000: episode: 6359, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -224.000, mean reward: -74.667 [-116.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 8432.291341, mae: 555.495280, accuracy: 0.156250, mean_q: -76.890828, mean_eps: 0.100000\n",
      " 28108/50000: episode: 6360, duration: 0.015s, episode steps:   4, steps per second: 262, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 8522.229126, mae: 570.179947, accuracy: 0.148438, mean_q: -92.476589, mean_eps: 0.100000\n",
      " 28111/50000: episode: 6361, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 8670.192708, mae: 555.834961, accuracy: 0.187500, mean_q: -79.407201, mean_eps: 0.100000\n",
      " 28114/50000: episode: 6362, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 7224.304199, mae: 564.282186, accuracy: 0.104167, mean_q: -95.438151, mean_eps: 0.100000\n",
      " 28118/50000: episode: 6363, duration: 0.015s, episode steps:   4, steps per second: 258, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 2.250 [1.000, 3.000],  loss: 6237.165405, mae: 561.034317, accuracy: 0.179688, mean_q: -62.473948, mean_eps: 0.100000\n",
      " 28121/50000: episode: 6364, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 7388.404297, mae: 549.226278, accuracy: 0.177083, mean_q: -80.389992, mean_eps: 0.100000\n",
      " 28124/50000: episode: 6365, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 8275.882324, mae: 573.185221, accuracy: 0.083333, mean_q: -52.839260, mean_eps: 0.100000\n",
      " 28127/50000: episode: 6366, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 8108.025716, mae: 561.981222, accuracy: 0.177083, mean_q: -80.549023, mean_eps: 0.100000\n",
      " 28130/50000: episode: 6367, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 9536.694987, mae: 576.274190, accuracy: 0.145833, mean_q: -88.835559, mean_eps: 0.100000\n",
      " 28133/50000: episode: 6368, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 6911.291341, mae: 554.685689, accuracy: 0.166667, mean_q: -83.291153, mean_eps: 0.100000\n",
      " 28136/50000: episode: 6369, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 7242.326172, mae: 552.967041, accuracy: 0.145833, mean_q: -72.747271, mean_eps: 0.100000\n",
      " 28139/50000: episode: 6370, duration: 0.015s, episode steps:   3, steps per second: 206, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 7828.750163, mae: 562.269775, accuracy: 0.187500, mean_q: -94.854319, mean_eps: 0.100000\n",
      " 28143/50000: episode: 6371, duration: 0.018s, episode steps:   4, steps per second: 228, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 5518.057861, mae: 551.878448, accuracy: 0.187500, mean_q: -81.312825, mean_eps: 0.100000\n",
      " 28146/50000: episode: 6372, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 7703.847331, mae: 570.593587, accuracy: 0.145833, mean_q: -67.103059, mean_eps: 0.100000\n",
      " 28150/50000: episode: 6373, duration: 0.016s, episode steps:   4, steps per second: 252, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 7764.778687, mae: 559.513870, accuracy: 0.109375, mean_q: -85.263836, mean_eps: 0.100000\n",
      " 28153/50000: episode: 6374, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 6792.007161, mae: 550.824707, accuracy: 0.083333, mean_q: -70.925784, mean_eps: 0.100000\n",
      " 28156/50000: episode: 6375, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 8828.441406, mae: 562.108968, accuracy: 0.135417, mean_q: -76.497253, mean_eps: 0.100000\n",
      " 28159/50000: episode: 6376, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 7394.976400, mae: 575.757894, accuracy: 0.125000, mean_q: -69.983573, mean_eps: 0.100000\n",
      " 28162/50000: episode: 6377, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 8391.040853, mae: 551.557739, accuracy: 0.145833, mean_q: -80.117030, mean_eps: 0.100000\n",
      " 28165/50000: episode: 6378, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 8419.572754, mae: 537.804545, accuracy: 0.135417, mean_q: -66.224864, mean_eps: 0.100000\n",
      " 28169/50000: episode: 6379, duration: 0.016s, episode steps:   4, steps per second: 247, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 2.250 [1.000, 3.000],  loss: 6839.676514, mae: 557.297836, accuracy: 0.085938, mean_q: -77.998285, mean_eps: 0.100000\n",
      " 28173/50000: episode: 6380, duration: 0.016s, episode steps:   4, steps per second: 254, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 7499.416138, mae: 566.099762, accuracy: 0.117188, mean_q: -74.817580, mean_eps: 0.100000\n",
      " 28176/50000: episode: 6381, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 7582.789551, mae: 552.385498, accuracy: 0.166667, mean_q: -58.616674, mean_eps: 0.100000\n",
      " 28179/50000: episode: 6382, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 8516.556315, mae: 567.999593, accuracy: 0.125000, mean_q: -34.288489, mean_eps: 0.100000\n",
      " 28182/50000: episode: 6383, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 7384.298014, mae: 555.223490, accuracy: 0.156250, mean_q: -76.390007, mean_eps: 0.100000\n",
      " 28185/50000: episode: 6384, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 7081.364421, mae: 552.053324, accuracy: 0.114583, mean_q: -77.704557, mean_eps: 0.100000\n",
      " 28188/50000: episode: 6385, duration: 0.016s, episode steps:   3, steps per second: 193, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 6242.285482, mae: 556.642090, accuracy: 0.114583, mean_q: -71.469681, mean_eps: 0.100000\n",
      " 28191/50000: episode: 6386, duration: 0.016s, episode steps:   3, steps per second: 187, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 7292.653809, mae: 559.248210, accuracy: 0.135417, mean_q: -85.879625, mean_eps: 0.100000\n",
      " 28194/50000: episode: 6387, duration: 0.015s, episode steps:   3, steps per second: 200, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 8242.981120, mae: 551.140503, accuracy: 0.145833, mean_q: -42.358578, mean_eps: 0.100000\n",
      " 28197/50000: episode: 6388, duration: 0.015s, episode steps:   3, steps per second: 203, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 7265.521973, mae: 553.969666, accuracy: 0.135417, mean_q: -77.405694, mean_eps: 0.100000\n",
      " 28200/50000: episode: 6389, duration: 0.015s, episode steps:   3, steps per second: 203, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 7890.563965, mae: 540.617737, accuracy: 0.166667, mean_q: -83.286382, mean_eps: 0.100000\n",
      " 28203/50000: episode: 6390, duration: 0.014s, episode steps:   3, steps per second: 217, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 6719.979004, mae: 546.489665, accuracy: 0.125000, mean_q: -79.189898, mean_eps: 0.100000\n",
      " 28208/50000: episode: 6391, duration: 0.021s, episode steps:   5, steps per second: 234, episode reward: -2193.000, mean reward: -438.600 [-999.000, -45.000], mean action: 1.400 [0.000, 3.000],  loss: 7694.974316, mae: 567.457935, accuracy: 0.093750, mean_q: -67.645821, mean_eps: 0.100000\n",
      " 28211/50000: episode: 6392, duration: 0.013s, episode steps:   3, steps per second: 223, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 6241.862142, mae: 563.504089, accuracy: 0.114583, mean_q: -55.036954, mean_eps: 0.100000\n",
      " 28214/50000: episode: 6393, duration: 0.013s, episode steps:   3, steps per second: 230, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 7480.436361, mae: 538.063273, accuracy: 0.166667, mean_q: -74.941461, mean_eps: 0.100000\n",
      " 28218/50000: episode: 6394, duration: 0.017s, episode steps:   4, steps per second: 240, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 5843.276001, mae: 561.577194, accuracy: 0.117188, mean_q: -49.580708, mean_eps: 0.100000\n",
      " 28221/50000: episode: 6395, duration: 0.013s, episode steps:   3, steps per second: 227, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 7703.244303, mae: 578.156535, accuracy: 0.156250, mean_q: -58.172096, mean_eps: 0.100000\n",
      " 28224/50000: episode: 6396, duration: 0.014s, episode steps:   3, steps per second: 209, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 7183.788086, mae: 560.429647, accuracy: 0.166667, mean_q: -64.115938, mean_eps: 0.100000\n",
      " 28227/50000: episode: 6397, duration: 0.015s, episode steps:   3, steps per second: 195, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 7459.937826, mae: 574.979594, accuracy: 0.104167, mean_q: -79.815786, mean_eps: 0.100000\n",
      " 28231/50000: episode: 6398, duration: 0.018s, episode steps:   4, steps per second: 222, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 8287.177124, mae: 553.074997, accuracy: 0.171875, mean_q: -84.085005, mean_eps: 0.100000\n",
      " 28234/50000: episode: 6399, duration: 0.017s, episode steps:   3, steps per second: 172, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 8015.038574, mae: 571.356283, accuracy: 0.083333, mean_q: -79.803750, mean_eps: 0.100000\n",
      " 28237/50000: episode: 6400, duration: 0.017s, episode steps:   3, steps per second: 175, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 6341.103841, mae: 556.961812, accuracy: 0.135417, mean_q: -63.367516, mean_eps: 0.100000\n",
      " 28242/50000: episode: 6401, duration: 0.023s, episode steps:   5, steps per second: 214, episode reward: -2222.000, mean reward: -444.400 [-999.000, -32.000], mean action: 1.200 [0.000, 3.000],  loss: 7998.551367, mae: 546.677954, accuracy: 0.187500, mean_q: -63.825524, mean_eps: 0.100000\n",
      " 28245/50000: episode: 6402, duration: 0.015s, episode steps:   3, steps per second: 195, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 6990.693685, mae: 577.724121, accuracy: 0.093750, mean_q: -77.525154, mean_eps: 0.100000\n",
      " 28248/50000: episode: 6403, duration: 0.015s, episode steps:   3, steps per second: 200, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 8151.028483, mae: 563.236471, accuracy: 0.156250, mean_q: -78.159927, mean_eps: 0.100000\n",
      " 28252/50000: episode: 6404, duration: 0.017s, episode steps:   4, steps per second: 233, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 7750.391357, mae: 566.817337, accuracy: 0.132812, mean_q: -67.558407, mean_eps: 0.100000\n",
      " 28255/50000: episode: 6405, duration: 0.014s, episode steps:   3, steps per second: 219, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 7086.544271, mae: 553.506571, accuracy: 0.166667, mean_q: -56.550357, mean_eps: 0.100000\n",
      " 28258/50000: episode: 6406, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 8031.433919, mae: 571.032125, accuracy: 0.114583, mean_q: -62.878860, mean_eps: 0.100000\n",
      " 28262/50000: episode: 6407, duration: 0.017s, episode steps:   4, steps per second: 229, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 8203.959717, mae: 541.668304, accuracy: 0.156250, mean_q: -73.555323, mean_eps: 0.100000\n",
      " 28265/50000: episode: 6408, duration: 0.014s, episode steps:   3, steps per second: 213, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 6762.290202, mae: 571.107544, accuracy: 0.104167, mean_q: -81.220268, mean_eps: 0.100000\n",
      " 28268/50000: episode: 6409, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.333 [0.000, 3.000],  loss: 7552.264974, mae: 572.913127, accuracy: 0.114583, mean_q: -101.832240, mean_eps: 0.100000\n",
      " 28272/50000: episode: 6410, duration: 0.017s, episode steps:   4, steps per second: 241, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 2.000 [0.000, 3.000],  loss: 7710.798218, mae: 569.929199, accuracy: 0.140625, mean_q: -78.355370, mean_eps: 0.100000\n",
      " 28275/50000: episode: 6411, duration: 0.013s, episode steps:   3, steps per second: 227, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 6590.011068, mae: 543.512695, accuracy: 0.125000, mean_q: -66.278978, mean_eps: 0.100000\n",
      " 28278/50000: episode: 6412, duration: 0.016s, episode steps:   3, steps per second: 188, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 6358.266927, mae: 561.758993, accuracy: 0.104167, mean_q: -61.022340, mean_eps: 0.100000\n",
      " 28281/50000: episode: 6413, duration: 0.016s, episode steps:   3, steps per second: 184, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 7320.237793, mae: 562.203064, accuracy: 0.031250, mean_q: -76.976893, mean_eps: 0.100000\n",
      " 28284/50000: episode: 6414, duration: 0.015s, episode steps:   3, steps per second: 204, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 6685.058594, mae: 548.894775, accuracy: 0.135417, mean_q: -83.234983, mean_eps: 0.100000\n",
      " 28287/50000: episode: 6415, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 6702.655924, mae: 546.356689, accuracy: 0.145833, mean_q: -70.555534, mean_eps: 0.100000\n",
      " 28291/50000: episode: 6416, duration: 0.015s, episode steps:   4, steps per second: 264, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 6847.198364, mae: 569.983322, accuracy: 0.085938, mean_q: -88.666071, mean_eps: 0.100000\n",
      " 28294/50000: episode: 6417, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 5641.218750, mae: 564.050008, accuracy: 0.104167, mean_q: -67.404823, mean_eps: 0.100000\n",
      " 28297/50000: episode: 6418, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 8152.600749, mae: 589.262492, accuracy: 0.145833, mean_q: -54.433084, mean_eps: 0.100000\n",
      " 28301/50000: episode: 6419, duration: 0.016s, episode steps:   4, steps per second: 256, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 2.000 [0.000, 3.000],  loss: 7715.576294, mae: 568.979706, accuracy: 0.132812, mean_q: -68.567486, mean_eps: 0.100000\n",
      " 28305/50000: episode: 6420, duration: 0.015s, episode steps:   4, steps per second: 265, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.000 [0.000, 3.000],  loss: 7627.937500, mae: 571.247711, accuracy: 0.101562, mean_q: -64.265231, mean_eps: 0.100000\n",
      " 28308/50000: episode: 6421, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 7561.502441, mae: 577.158712, accuracy: 0.104167, mean_q: -79.948018, mean_eps: 0.100000\n",
      " 28311/50000: episode: 6422, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 7317.214518, mae: 576.416870, accuracy: 0.093750, mean_q: -71.721537, mean_eps: 0.100000\n",
      " 28314/50000: episode: 6423, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 8055.489909, mae: 558.340413, accuracy: 0.125000, mean_q: -60.952169, mean_eps: 0.100000\n",
      " 28317/50000: episode: 6424, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 7347.943197, mae: 568.046509, accuracy: 0.135417, mean_q: -88.069570, mean_eps: 0.100000\n",
      " 28320/50000: episode: 6425, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 6141.215658, mae: 568.795410, accuracy: 0.145833, mean_q: -59.196828, mean_eps: 0.100000\n",
      " 28323/50000: episode: 6426, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 8995.380859, mae: 572.838155, accuracy: 0.166667, mean_q: -51.935893, mean_eps: 0.100000\n",
      " 28326/50000: episode: 6427, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 7328.072917, mae: 560.985575, accuracy: 0.145833, mean_q: -77.408038, mean_eps: 0.100000\n",
      " 28329/50000: episode: 6428, duration: 0.017s, episode steps:   3, steps per second: 176, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 8269.214030, mae: 576.023763, accuracy: 0.177083, mean_q: -84.198705, mean_eps: 0.100000\n",
      " 28332/50000: episode: 6429, duration: 0.020s, episode steps:   3, steps per second: 151, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 8842.009766, mae: 588.820760, accuracy: 0.145833, mean_q: -79.018561, mean_eps: 0.100000\n",
      " 28337/50000: episode: 6430, duration: 0.022s, episode steps:   5, steps per second: 224, episode reward: -2193.000, mean reward: -438.600 [-999.000, -58.000], mean action: 1.800 [0.000, 3.000],  loss: 7284.967871, mae: 557.677063, accuracy: 0.087500, mean_q: -83.022711, mean_eps: 0.100000\n",
      " 28340/50000: episode: 6431, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 6693.906738, mae: 565.179688, accuracy: 0.093750, mean_q: -62.880884, mean_eps: 0.100000\n",
      " 28344/50000: episode: 6432, duration: 0.016s, episode steps:   4, steps per second: 246, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 8317.868652, mae: 562.585831, accuracy: 0.132812, mean_q: -67.646139, mean_eps: 0.100000\n",
      " 28347/50000: episode: 6433, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 8797.250651, mae: 566.558085, accuracy: 0.166667, mean_q: -66.492085, mean_eps: 0.100000\n",
      " 28350/50000: episode: 6434, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 6408.569173, mae: 567.654500, accuracy: 0.145833, mean_q: -77.904350, mean_eps: 0.100000\n",
      " 28353/50000: episode: 6435, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 5998.894368, mae: 539.443095, accuracy: 0.208333, mean_q: -63.116006, mean_eps: 0.100000\n",
      " 28356/50000: episode: 6436, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 8607.217122, mae: 570.792196, accuracy: 0.166667, mean_q: -66.652400, mean_eps: 0.100000\n",
      " 28359/50000: episode: 6437, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 7187.474447, mae: 545.254537, accuracy: 0.135417, mean_q: -72.706650, mean_eps: 0.100000\n",
      " 28362/50000: episode: 6438, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 6926.674154, mae: 574.017904, accuracy: 0.114583, mean_q: -75.925947, mean_eps: 0.100000\n",
      " 28365/50000: episode: 6439, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 7436.582520, mae: 564.108521, accuracy: 0.145833, mean_q: -67.246813, mean_eps: 0.100000\n",
      " 28368/50000: episode: 6440, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 9308.987142, mae: 571.155131, accuracy: 0.166667, mean_q: -94.680384, mean_eps: 0.100000\n",
      " 28371/50000: episode: 6441, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 8656.578288, mae: 566.285238, accuracy: 0.093750, mean_q: -73.647006, mean_eps: 0.100000\n",
      " 28374/50000: episode: 6442, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.333 [0.000, 3.000],  loss: 6436.587565, mae: 539.004557, accuracy: 0.145833, mean_q: -77.939143, mean_eps: 0.100000\n",
      " 28377/50000: episode: 6443, duration: 0.017s, episode steps:   3, steps per second: 178, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 7395.459473, mae: 563.525899, accuracy: 0.125000, mean_q: -88.703603, mean_eps: 0.100000\n",
      " 28380/50000: episode: 6444, duration: 0.015s, episode steps:   3, steps per second: 197, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 7328.339193, mae: 580.361389, accuracy: 0.125000, mean_q: -52.990035, mean_eps: 0.100000\n",
      " 28383/50000: episode: 6445, duration: 0.014s, episode steps:   3, steps per second: 208, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 6863.358887, mae: 561.895386, accuracy: 0.104167, mean_q: -93.706871, mean_eps: 0.100000\n",
      " 28386/50000: episode: 6446, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 8259.674805, mae: 570.441691, accuracy: 0.104167, mean_q: -90.934252, mean_eps: 0.100000\n",
      " 28389/50000: episode: 6447, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 8082.133138, mae: 554.814209, accuracy: 0.187500, mean_q: -70.506383, mean_eps: 0.100000\n",
      " 28393/50000: episode: 6448, duration: 0.016s, episode steps:   4, steps per second: 248, episode reward: -1223.000, mean reward: -305.750 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 7604.166992, mae: 560.288849, accuracy: 0.109375, mean_q: -70.417521, mean_eps: 0.100000\n",
      " 28396/50000: episode: 6449, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 6911.228841, mae: 560.667135, accuracy: 0.166667, mean_q: -77.444478, mean_eps: 0.100000\n",
      " 28399/50000: episode: 6450, duration: 0.012s, episode steps:   3, steps per second: 254, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 7155.593750, mae: 559.421529, accuracy: 0.197917, mean_q: -81.456525, mean_eps: 0.100000\n",
      " 28402/50000: episode: 6451, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 6788.281413, mae: 545.436564, accuracy: 0.135417, mean_q: -72.017731, mean_eps: 0.100000\n",
      " 28405/50000: episode: 6452, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 7127.620443, mae: 548.532349, accuracy: 0.135417, mean_q: -77.137767, mean_eps: 0.100000\n",
      " 28408/50000: episode: 6453, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 7234.498372, mae: 570.316345, accuracy: 0.114583, mean_q: -60.008284, mean_eps: 0.100000\n",
      " 28411/50000: episode: 6454, duration: 0.012s, episode steps:   3, steps per second: 253, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 7374.517090, mae: 555.079692, accuracy: 0.093750, mean_q: -96.694827, mean_eps: 0.100000\n",
      " 28414/50000: episode: 6455, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 6836.197103, mae: 575.520955, accuracy: 0.114583, mean_q: -75.549131, mean_eps: 0.100000\n",
      " 28417/50000: episode: 6456, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 6706.134115, mae: 552.584330, accuracy: 0.187500, mean_q: -63.109633, mean_eps: 0.100000\n",
      " 28420/50000: episode: 6457, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 7832.642578, mae: 578.681335, accuracy: 0.114583, mean_q: -70.615059, mean_eps: 0.100000\n",
      " 28423/50000: episode: 6458, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 7367.475423, mae: 572.728577, accuracy: 0.114583, mean_q: -79.570735, mean_eps: 0.100000\n",
      " 28426/50000: episode: 6459, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 6220.220052, mae: 566.341817, accuracy: 0.072917, mean_q: -72.500579, mean_eps: 0.100000\n",
      " 28429/50000: episode: 6460, duration: 0.015s, episode steps:   3, steps per second: 195, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 5249.603190, mae: 573.920492, accuracy: 0.093750, mean_q: -87.854233, mean_eps: 0.100000\n",
      " 28432/50000: episode: 6461, duration: 0.015s, episode steps:   3, steps per second: 205, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 8195.603841, mae: 556.333883, accuracy: 0.114583, mean_q: -78.170835, mean_eps: 0.100000\n",
      " 28435/50000: episode: 6462, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 6253.815755, mae: 553.147176, accuracy: 0.114583, mean_q: -76.698046, mean_eps: 0.100000\n",
      " 28438/50000: episode: 6463, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 6888.958984, mae: 562.307821, accuracy: 0.135417, mean_q: -71.735046, mean_eps: 0.100000\n",
      " 28441/50000: episode: 6464, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 6835.213542, mae: 559.120544, accuracy: 0.197917, mean_q: -82.097655, mean_eps: 0.100000\n",
      " 28444/50000: episode: 6465, duration: 0.013s, episode steps:   3, steps per second: 227, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 6897.160482, mae: 561.650533, accuracy: 0.114583, mean_q: -95.263451, mean_eps: 0.100000\n",
      " 28447/50000: episode: 6466, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 6889.091309, mae: 573.767497, accuracy: 0.125000, mean_q: -70.415688, mean_eps: 0.100000\n",
      " 28450/50000: episode: 6467, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 7009.715332, mae: 553.285746, accuracy: 0.187500, mean_q: -88.814026, mean_eps: 0.100000\n",
      " 28453/50000: episode: 6468, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.667 [0.000, 3.000],  loss: 7897.344727, mae: 570.332174, accuracy: 0.072917, mean_q: -67.737282, mean_eps: 0.100000\n",
      " 28456/50000: episode: 6469, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 8383.218750, mae: 565.329997, accuracy: 0.104167, mean_q: -73.554790, mean_eps: 0.100000\n",
      " 28459/50000: episode: 6470, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 7484.902669, mae: 557.587565, accuracy: 0.125000, mean_q: -93.505048, mean_eps: 0.100000\n",
      " 28462/50000: episode: 6471, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 6999.876628, mae: 573.782186, accuracy: 0.125000, mean_q: -82.647580, mean_eps: 0.100000\n",
      " 28465/50000: episode: 6472, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 7241.942383, mae: 564.600810, accuracy: 0.083333, mean_q: -83.153493, mean_eps: 0.100000\n",
      " 28468/50000: episode: 6473, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 8125.703776, mae: 572.525635, accuracy: 0.125000, mean_q: -65.644299, mean_eps: 0.100000\n",
      " 28471/50000: episode: 6474, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 7634.198730, mae: 579.575907, accuracy: 0.093750, mean_q: -74.809881, mean_eps: 0.100000\n",
      " 28474/50000: episode: 6475, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 5714.984049, mae: 560.598958, accuracy: 0.104167, mean_q: -74.172943, mean_eps: 0.100000\n",
      " 28477/50000: episode: 6476, duration: 0.022s, episode steps:   3, steps per second: 134, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 6750.507487, mae: 551.209249, accuracy: 0.125000, mean_q: -85.269320, mean_eps: 0.100000\n",
      " 28480/50000: episode: 6477, duration: 0.016s, episode steps:   3, steps per second: 186, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 6206.052897, mae: 569.738749, accuracy: 0.104167, mean_q: -76.043589, mean_eps: 0.100000\n",
      " 28483/50000: episode: 6478, duration: 0.014s, episode steps:   3, steps per second: 211, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 6624.750000, mae: 586.420837, accuracy: 0.083333, mean_q: -89.192874, mean_eps: 0.100000\n",
      " 28486/50000: episode: 6479, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 7547.740560, mae: 566.967183, accuracy: 0.145833, mean_q: -75.811394, mean_eps: 0.100000\n",
      " 28489/50000: episode: 6480, duration: 0.013s, episode steps:   3, steps per second: 230, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 7151.790853, mae: 567.984558, accuracy: 0.093750, mean_q: -87.384806, mean_eps: 0.100000\n",
      " 28492/50000: episode: 6481, duration: 0.014s, episode steps:   3, steps per second: 215, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 7353.200358, mae: 588.919474, accuracy: 0.083333, mean_q: -83.757088, mean_eps: 0.100000\n",
      " 28495/50000: episode: 6482, duration: 0.014s, episode steps:   3, steps per second: 220, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 6480.576009, mae: 569.823588, accuracy: 0.104167, mean_q: -72.446864, mean_eps: 0.100000\n",
      " 28498/50000: episode: 6483, duration: 0.014s, episode steps:   3, steps per second: 212, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 7296.937500, mae: 565.930522, accuracy: 0.156250, mean_q: -69.849800, mean_eps: 0.100000\n",
      " 28501/50000: episode: 6484, duration: 0.014s, episode steps:   3, steps per second: 211, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 7716.345866, mae: 554.213420, accuracy: 0.156250, mean_q: -75.523028, mean_eps: 0.100000\n",
      " 28505/50000: episode: 6485, duration: 0.017s, episode steps:   4, steps per second: 230, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.000 [0.000, 3.000],  loss: 6749.363892, mae: 552.619583, accuracy: 0.117188, mean_q: -77.425676, mean_eps: 0.100000\n",
      " 28508/50000: episode: 6486, duration: 0.014s, episode steps:   3, steps per second: 212, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 8260.459635, mae: 578.108663, accuracy: 0.145833, mean_q: -67.438632, mean_eps: 0.100000\n",
      " 28511/50000: episode: 6487, duration: 0.014s, episode steps:   3, steps per second: 220, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 6431.641764, mae: 563.417419, accuracy: 0.104167, mean_q: -82.928978, mean_eps: 0.100000\n",
      " 28515/50000: episode: 6488, duration: 0.018s, episode steps:   4, steps per second: 222, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 5359.739746, mae: 569.546219, accuracy: 0.140625, mean_q: -61.654938, mean_eps: 0.100000\n",
      " 28518/50000: episode: 6489, duration: 0.014s, episode steps:   3, steps per second: 214, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 7897.326335, mae: 566.094076, accuracy: 0.135417, mean_q: -66.896166, mean_eps: 0.100000\n",
      " 28521/50000: episode: 6490, duration: 0.017s, episode steps:   3, steps per second: 180, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 7941.607422, mae: 563.435893, accuracy: 0.104167, mean_q: -57.732096, mean_eps: 0.100000\n",
      " 28524/50000: episode: 6491, duration: 0.016s, episode steps:   3, steps per second: 185, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 5653.591960, mae: 567.285014, accuracy: 0.093750, mean_q: -86.424385, mean_eps: 0.100000\n",
      " 28527/50000: episode: 6492, duration: 0.014s, episode steps:   3, steps per second: 212, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 7659.438151, mae: 548.586751, accuracy: 0.145833, mean_q: -83.291789, mean_eps: 0.100000\n",
      " 28531/50000: episode: 6493, duration: 0.016s, episode steps:   4, steps per second: 250, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 2.000 [0.000, 3.000],  loss: 6504.303589, mae: 561.245117, accuracy: 0.125000, mean_q: -77.750753, mean_eps: 0.100000\n",
      " 28534/50000: episode: 6494, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 6277.999023, mae: 532.470825, accuracy: 0.166667, mean_q: -67.554770, mean_eps: 0.100000\n",
      " 28537/50000: episode: 6495, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 6514.658366, mae: 556.528076, accuracy: 0.041667, mean_q: -75.103572, mean_eps: 0.100000\n",
      " 28541/50000: episode: 6496, duration: 0.015s, episode steps:   4, steps per second: 262, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 7504.484741, mae: 554.936264, accuracy: 0.117188, mean_q: -77.808351, mean_eps: 0.100000\n",
      " 28544/50000: episode: 6497, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 7586.549642, mae: 550.627421, accuracy: 0.177083, mean_q: -76.134382, mean_eps: 0.100000\n",
      " 28547/50000: episode: 6498, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 7222.279622, mae: 549.628866, accuracy: 0.166667, mean_q: -62.448990, mean_eps: 0.100000\n",
      " 28550/50000: episode: 6499, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 7265.517090, mae: 553.302368, accuracy: 0.156250, mean_q: -67.890755, mean_eps: 0.100000\n",
      " 28553/50000: episode: 6500, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 7958.228190, mae: 560.814290, accuracy: 0.166667, mean_q: -82.578501, mean_eps: 0.100000\n",
      " 28556/50000: episode: 6501, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 5726.699870, mae: 566.119344, accuracy: 0.114583, mean_q: -53.544771, mean_eps: 0.100000\n",
      " 28559/50000: episode: 6502, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 5684.458333, mae: 557.363057, accuracy: 0.145833, mean_q: -61.983421, mean_eps: 0.100000\n",
      " 28562/50000: episode: 6503, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 6348.747233, mae: 581.621969, accuracy: 0.072917, mean_q: -62.119718, mean_eps: 0.100000\n",
      " 28565/50000: episode: 6504, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 6324.762207, mae: 586.153910, accuracy: 0.135417, mean_q: -63.410529, mean_eps: 0.100000\n",
      " 28568/50000: episode: 6505, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 7825.318522, mae: 576.915385, accuracy: 0.104167, mean_q: -75.862404, mean_eps: 0.100000\n",
      " 28571/50000: episode: 6506, duration: 0.019s, episode steps:   3, steps per second: 162, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 6017.198568, mae: 546.003662, accuracy: 0.104167, mean_q: -83.552198, mean_eps: 0.100000\n",
      " 28574/50000: episode: 6507, duration: 0.017s, episode steps:   3, steps per second: 178, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 7757.522624, mae: 572.103699, accuracy: 0.104167, mean_q: -86.274251, mean_eps: 0.100000\n",
      " 28577/50000: episode: 6508, duration: 0.013s, episode steps:   3, steps per second: 227, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 6243.268066, mae: 562.999959, accuracy: 0.083333, mean_q: -58.847659, mean_eps: 0.100000\n",
      " 28581/50000: episode: 6509, duration: 0.017s, episode steps:   4, steps per second: 237, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 6951.233154, mae: 570.296066, accuracy: 0.062500, mean_q: -70.846521, mean_eps: 0.100000\n",
      " 28584/50000: episode: 6510, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 7539.967448, mae: 589.928589, accuracy: 0.166667, mean_q: -83.036804, mean_eps: 0.100000\n",
      " 28587/50000: episode: 6511, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 6418.276693, mae: 560.198364, accuracy: 0.093750, mean_q: -94.266144, mean_eps: 0.100000\n",
      " 28590/50000: episode: 6512, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 7457.986654, mae: 574.760824, accuracy: 0.104167, mean_q: -105.097473, mean_eps: 0.100000\n",
      " 28593/50000: episode: 6513, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 9561.205729, mae: 558.996338, accuracy: 0.114583, mean_q: -89.614835, mean_eps: 0.100000\n",
      " 28596/50000: episode: 6514, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 7129.676270, mae: 573.442424, accuracy: 0.083333, mean_q: -70.647699, mean_eps: 0.100000\n",
      " 28599/50000: episode: 6515, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 7278.901530, mae: 572.944804, accuracy: 0.135417, mean_q: -61.506214, mean_eps: 0.100000\n",
      " 28602/50000: episode: 6516, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 6025.477376, mae: 570.166870, accuracy: 0.072917, mean_q: -80.615448, mean_eps: 0.100000\n",
      " 28605/50000: episode: 6517, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 7715.480632, mae: 549.506836, accuracy: 0.177083, mean_q: -69.980174, mean_eps: 0.100000\n",
      " 28608/50000: episode: 6518, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 5743.597331, mae: 562.419108, accuracy: 0.125000, mean_q: -71.163371, mean_eps: 0.100000\n",
      " 28611/50000: episode: 6519, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 5698.665039, mae: 556.403097, accuracy: 0.052083, mean_q: -77.896988, mean_eps: 0.100000\n",
      " 28614/50000: episode: 6520, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 5046.077637, mae: 553.551147, accuracy: 0.125000, mean_q: -81.594007, mean_eps: 0.100000\n",
      " 28617/50000: episode: 6521, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 6876.882812, mae: 567.576660, accuracy: 0.145833, mean_q: -85.052480, mean_eps: 0.100000\n",
      " 28620/50000: episode: 6522, duration: 0.014s, episode steps:   3, steps per second: 219, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 7525.614258, mae: 541.414836, accuracy: 0.187500, mean_q: -87.290461, mean_eps: 0.100000\n",
      " 28623/50000: episode: 6523, duration: 0.014s, episode steps:   3, steps per second: 207, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 8771.850098, mae: 578.792725, accuracy: 0.135417, mean_q: -91.884104, mean_eps: 0.100000\n",
      " 28626/50000: episode: 6524, duration: 0.015s, episode steps:   3, steps per second: 207, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 6974.541504, mae: 576.345907, accuracy: 0.135417, mean_q: -81.213951, mean_eps: 0.100000\n",
      " 28629/50000: episode: 6525, duration: 0.014s, episode steps:   3, steps per second: 221, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 7859.803060, mae: 567.311808, accuracy: 0.135417, mean_q: -78.965825, mean_eps: 0.100000\n",
      " 28632/50000: episode: 6526, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 5405.434408, mae: 553.463165, accuracy: 0.145833, mean_q: -79.711082, mean_eps: 0.100000\n",
      " 28635/50000: episode: 6527, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 5934.306803, mae: 574.662699, accuracy: 0.104167, mean_q: -68.197058, mean_eps: 0.100000\n",
      " 28638/50000: episode: 6528, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 5798.615885, mae: 557.521464, accuracy: 0.125000, mean_q: -78.778599, mean_eps: 0.100000\n",
      " 28641/50000: episode: 6529, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 5991.812988, mae: 560.761983, accuracy: 0.083333, mean_q: -81.444455, mean_eps: 0.100000\n",
      " 28644/50000: episode: 6530, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 5424.868978, mae: 563.533793, accuracy: 0.093750, mean_q: -90.528763, mean_eps: 0.100000\n",
      " 28647/50000: episode: 6531, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 6409.285482, mae: 536.577555, accuracy: 0.104167, mean_q: -58.695812, mean_eps: 0.100000\n",
      " 28650/50000: episode: 6532, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 6088.059896, mae: 561.852905, accuracy: 0.104167, mean_q: -66.102386, mean_eps: 0.100000\n",
      " 28653/50000: episode: 6533, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 5984.019857, mae: 553.923869, accuracy: 0.062500, mean_q: -85.888687, mean_eps: 0.100000\n",
      " 28656/50000: episode: 6534, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 8040.201172, mae: 564.075439, accuracy: 0.187500, mean_q: -92.069290, mean_eps: 0.100000\n",
      " 28659/50000: episode: 6535, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 8126.181315, mae: 559.213949, accuracy: 0.156250, mean_q: -80.307804, mean_eps: 0.100000\n",
      " 28662/50000: episode: 6536, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 7911.026367, mae: 581.122152, accuracy: 0.104167, mean_q: -80.740311, mean_eps: 0.100000\n",
      " 28665/50000: episode: 6537, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.667 [0.000, 3.000],  loss: 8627.171875, mae: 570.866536, accuracy: 0.166667, mean_q: -75.493314, mean_eps: 0.100000\n",
      " 28669/50000: episode: 6538, duration: 0.017s, episode steps:   4, steps per second: 234, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 2.000 [0.000, 3.000],  loss: 7633.268921, mae: 562.434235, accuracy: 0.148438, mean_q: -76.718769, mean_eps: 0.100000\n",
      " 28672/50000: episode: 6539, duration: 0.020s, episode steps:   3, steps per second: 147, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 7184.441569, mae: 564.034810, accuracy: 0.083333, mean_q: -84.045593, mean_eps: 0.100000\n",
      " 28675/50000: episode: 6540, duration: 0.016s, episode steps:   3, steps per second: 193, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 6027.917155, mae: 547.889567, accuracy: 0.114583, mean_q: -102.381294, mean_eps: 0.100000\n",
      " 28678/50000: episode: 6541, duration: 0.014s, episode steps:   3, steps per second: 217, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 8309.231771, mae: 573.646708, accuracy: 0.104167, mean_q: -62.633839, mean_eps: 0.100000\n",
      " 28681/50000: episode: 6542, duration: 0.014s, episode steps:   3, steps per second: 218, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 7726.169922, mae: 558.095540, accuracy: 0.145833, mean_q: -65.728050, mean_eps: 0.100000\n",
      " 28684/50000: episode: 6543, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 5852.231689, mae: 575.370036, accuracy: 0.072917, mean_q: -84.072632, mean_eps: 0.100000\n",
      " 28687/50000: episode: 6544, duration: 0.013s, episode steps:   3, steps per second: 226, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 7168.846842, mae: 556.439677, accuracy: 0.166667, mean_q: -67.795897, mean_eps: 0.100000\n",
      " 28690/50000: episode: 6545, duration: 0.013s, episode steps:   3, steps per second: 223, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 6733.210449, mae: 578.297567, accuracy: 0.125000, mean_q: -72.729783, mean_eps: 0.100000\n",
      " 28693/50000: episode: 6546, duration: 0.014s, episode steps:   3, steps per second: 222, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 6552.735840, mae: 555.269775, accuracy: 0.125000, mean_q: -54.561946, mean_eps: 0.100000\n",
      " 28696/50000: episode: 6547, duration: 0.014s, episode steps:   3, steps per second: 221, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 7542.305176, mae: 556.192505, accuracy: 0.156250, mean_q: -78.499275, mean_eps: 0.100000\n",
      " 28700/50000: episode: 6548, duration: 0.018s, episode steps:   4, steps per second: 224, episode reward: -1238.000, mean reward: -309.500 [-999.000, -60.000], mean action: 1.500 [0.000, 3.000],  loss: 6821.097778, mae: 561.232712, accuracy: 0.117188, mean_q: -76.791326, mean_eps: 0.100000\n",
      " 28703/50000: episode: 6549, duration: 0.017s, episode steps:   3, steps per second: 173, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 6002.704427, mae: 558.262390, accuracy: 0.093750, mean_q: -79.195053, mean_eps: 0.100000\n",
      " 28707/50000: episode: 6550, duration: 0.018s, episode steps:   4, steps per second: 224, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 7035.238525, mae: 547.499023, accuracy: 0.187500, mean_q: -76.473099, mean_eps: 0.100000\n",
      " 28710/50000: episode: 6551, duration: 0.014s, episode steps:   3, steps per second: 208, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 5106.842122, mae: 554.723165, accuracy: 0.114583, mean_q: -83.330551, mean_eps: 0.100000\n",
      " 28713/50000: episode: 6552, duration: 0.015s, episode steps:   3, steps per second: 202, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 7862.216634, mae: 565.235006, accuracy: 0.135417, mean_q: -80.842899, mean_eps: 0.100000\n",
      " 28716/50000: episode: 6553, duration: 0.021s, episode steps:   3, steps per second: 144, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 7252.993327, mae: 541.923319, accuracy: 0.166667, mean_q: -89.591131, mean_eps: 0.100000\n",
      " 28719/50000: episode: 6554, duration: 0.018s, episode steps:   3, steps per second: 167, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 6004.706868, mae: 574.015076, accuracy: 0.156250, mean_q: -76.018695, mean_eps: 0.100000\n",
      " 28722/50000: episode: 6555, duration: 0.014s, episode steps:   3, steps per second: 216, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 7351.764160, mae: 580.591715, accuracy: 0.093750, mean_q: -79.773424, mean_eps: 0.100000\n",
      " 28725/50000: episode: 6556, duration: 0.013s, episode steps:   3, steps per second: 230, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 7348.265788, mae: 572.425273, accuracy: 0.166667, mean_q: -71.049310, mean_eps: 0.100000\n",
      " 28729/50000: episode: 6557, duration: 0.016s, episode steps:   4, steps per second: 246, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 7818.639893, mae: 542.948792, accuracy: 0.117188, mean_q: -87.585625, mean_eps: 0.100000\n",
      " 28734/50000: episode: 6558, duration: 0.025s, episode steps:   5, steps per second: 199, episode reward: -2193.000, mean reward: -438.600 [-999.000, -32.000], mean action: 1.600 [0.000, 3.000],  loss: 5780.879004, mae: 573.086816, accuracy: 0.118750, mean_q: -62.064169, mean_eps: 0.100000\n",
      " 28737/50000: episode: 6559, duration: 0.017s, episode steps:   3, steps per second: 181, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 8107.178548, mae: 554.881775, accuracy: 0.125000, mean_q: -78.225852, mean_eps: 0.100000\n",
      " 28740/50000: episode: 6560, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 6464.362630, mae: 550.239136, accuracy: 0.072917, mean_q: -82.208823, mean_eps: 0.100000\n",
      " 28743/50000: episode: 6561, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 7533.047201, mae: 580.515137, accuracy: 0.104167, mean_q: -93.645536, mean_eps: 0.100000\n",
      " 28746/50000: episode: 6562, duration: 0.012s, episode steps:   3, steps per second: 253, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 5702.272298, mae: 549.122518, accuracy: 0.072917, mean_q: -61.505801, mean_eps: 0.100000\n",
      " 28749/50000: episode: 6563, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 6150.244303, mae: 552.451477, accuracy: 0.072917, mean_q: -76.069942, mean_eps: 0.100000\n",
      " 28752/50000: episode: 6564, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 5558.495443, mae: 564.036682, accuracy: 0.114583, mean_q: -67.053160, mean_eps: 0.100000\n",
      " 28756/50000: episode: 6565, duration: 0.016s, episode steps:   4, steps per second: 246, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 7555.000000, mae: 567.873337, accuracy: 0.164062, mean_q: -71.620596, mean_eps: 0.100000\n",
      " 28760/50000: episode: 6566, duration: 0.016s, episode steps:   4, steps per second: 254, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 5762.869507, mae: 566.948532, accuracy: 0.109375, mean_q: -75.159967, mean_eps: 0.100000\n",
      " 28763/50000: episode: 6567, duration: 0.019s, episode steps:   3, steps per second: 155, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 7284.683268, mae: 554.784139, accuracy: 0.166667, mean_q: -86.716075, mean_eps: 0.100000\n",
      " 28766/50000: episode: 6568, duration: 0.015s, episode steps:   3, steps per second: 202, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 6097.913086, mae: 570.781148, accuracy: 0.166667, mean_q: -92.099716, mean_eps: 0.100000\n",
      " 28770/50000: episode: 6569, duration: 0.016s, episode steps:   4, steps per second: 256, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 2.250 [1.000, 3.000],  loss: 7575.538330, mae: 563.390274, accuracy: 0.125000, mean_q: -96.236540, mean_eps: 0.100000\n",
      " 28773/50000: episode: 6570, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 6409.747721, mae: 580.785339, accuracy: 0.135417, mean_q: -53.600253, mean_eps: 0.100000\n",
      " 28777/50000: episode: 6571, duration: 0.016s, episode steps:   4, steps per second: 248, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 6679.605469, mae: 557.628372, accuracy: 0.125000, mean_q: -76.019257, mean_eps: 0.100000\n",
      " 28780/50000: episode: 6572, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 7292.571452, mae: 571.049032, accuracy: 0.072917, mean_q: -85.446510, mean_eps: 0.100000\n",
      " 28783/50000: episode: 6573, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 6530.667969, mae: 547.418315, accuracy: 0.156250, mean_q: -70.112591, mean_eps: 0.100000\n",
      " 28786/50000: episode: 6574, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 7790.751139, mae: 560.396098, accuracy: 0.187500, mean_q: -83.999393, mean_eps: 0.100000\n",
      " 28789/50000: episode: 6575, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 5694.199870, mae: 580.060099, accuracy: 0.072917, mean_q: -72.640313, mean_eps: 0.100000\n",
      " 28792/50000: episode: 6576, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 7581.712565, mae: 561.417297, accuracy: 0.093750, mean_q: -85.041595, mean_eps: 0.100000\n",
      " 28795/50000: episode: 6577, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 6057.187826, mae: 552.185038, accuracy: 0.093750, mean_q: -70.510178, mean_eps: 0.100000\n",
      " 28798/50000: episode: 6578, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 5802.749837, mae: 547.158061, accuracy: 0.145833, mean_q: -76.451296, mean_eps: 0.100000\n",
      " 28801/50000: episode: 6579, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 6393.176107, mae: 559.366150, accuracy: 0.135417, mean_q: -55.550891, mean_eps: 0.100000\n",
      " 28804/50000: episode: 6580, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 6685.479980, mae: 568.299377, accuracy: 0.125000, mean_q: -67.333631, mean_eps: 0.100000\n",
      " 28808/50000: episode: 6581, duration: 0.016s, episode steps:   4, steps per second: 255, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 6784.019287, mae: 567.762497, accuracy: 0.117188, mean_q: -65.707016, mean_eps: 0.100000\n",
      " 28811/50000: episode: 6582, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 6154.358398, mae: 577.182149, accuracy: 0.083333, mean_q: -90.961856, mean_eps: 0.100000\n",
      " 28814/50000: episode: 6583, duration: 0.015s, episode steps:   3, steps per second: 200, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 8033.975098, mae: 579.773010, accuracy: 0.114583, mean_q: -80.348750, mean_eps: 0.100000\n",
      " 28817/50000: episode: 6584, duration: 0.015s, episode steps:   3, steps per second: 202, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 6269.551107, mae: 584.202494, accuracy: 0.135417, mean_q: -71.268818, mean_eps: 0.100000\n",
      " 28820/50000: episode: 6585, duration: 0.013s, episode steps:   3, steps per second: 230, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 6372.319336, mae: 566.270508, accuracy: 0.114583, mean_q: -83.468498, mean_eps: 0.100000\n",
      " 28823/50000: episode: 6586, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 6751.850586, mae: 574.835693, accuracy: 0.062500, mean_q: -73.628181, mean_eps: 0.100000\n",
      " 28827/50000: episode: 6587, duration: 0.016s, episode steps:   4, steps per second: 252, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 6470.899292, mae: 558.111404, accuracy: 0.148438, mean_q: -61.624430, mean_eps: 0.100000\n",
      " 28830/50000: episode: 6588, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 7119.659180, mae: 555.126038, accuracy: 0.125000, mean_q: -81.959305, mean_eps: 0.100000\n",
      " 28834/50000: episode: 6589, duration: 0.016s, episode steps:   4, steps per second: 256, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 6624.153198, mae: 577.044388, accuracy: 0.085938, mean_q: -69.535004, mean_eps: 0.100000\n",
      " 28838/50000: episode: 6590, duration: 0.015s, episode steps:   4, steps per second: 262, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 6482.589233, mae: 564.516296, accuracy: 0.148438, mean_q: -70.023952, mean_eps: 0.100000\n",
      " 28841/50000: episode: 6591, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 5724.444499, mae: 570.133097, accuracy: 0.093750, mean_q: -92.872587, mean_eps: 0.100000\n",
      " 28844/50000: episode: 6592, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 5954.183268, mae: 589.315918, accuracy: 0.031250, mean_q: -74.772397, mean_eps: 0.100000\n",
      " 28847/50000: episode: 6593, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 6123.218913, mae: 569.485209, accuracy: 0.093750, mean_q: -72.232803, mean_eps: 0.100000\n",
      " 28850/50000: episode: 6594, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 7352.535319, mae: 578.008891, accuracy: 0.072917, mean_q: -91.438582, mean_eps: 0.100000\n",
      " 28853/50000: episode: 6595, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 5097.413249, mae: 551.053772, accuracy: 0.156250, mean_q: -78.444135, mean_eps: 0.100000\n",
      " 28856/50000: episode: 6596, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 5875.317546, mae: 573.280273, accuracy: 0.052083, mean_q: -85.135963, mean_eps: 0.100000\n",
      " 28859/50000: episode: 6597, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 6859.855957, mae: 575.427673, accuracy: 0.177083, mean_q: -95.223737, mean_eps: 0.100000\n",
      " 28862/50000: episode: 6598, duration: 0.014s, episode steps:   3, steps per second: 218, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 6266.033366, mae: 590.467753, accuracy: 0.041667, mean_q: -80.729543, mean_eps: 0.100000\n",
      " 28866/50000: episode: 6599, duration: 0.022s, episode steps:   4, steps per second: 179, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 6378.979004, mae: 563.308655, accuracy: 0.148438, mean_q: -79.076857, mean_eps: 0.100000\n",
      " 28870/50000: episode: 6600, duration: 0.019s, episode steps:   4, steps per second: 215, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 6502.886108, mae: 542.419281, accuracy: 0.132812, mean_q: -80.486915, mean_eps: 0.100000\n",
      " 28873/50000: episode: 6601, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 6877.228027, mae: 579.073934, accuracy: 0.072917, mean_q: -87.548009, mean_eps: 0.100000\n",
      " 28876/50000: episode: 6602, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 7469.982259, mae: 566.414185, accuracy: 0.093750, mean_q: -51.271356, mean_eps: 0.100000\n",
      " 28879/50000: episode: 6603, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 5367.503092, mae: 571.036296, accuracy: 0.145833, mean_q: -64.362207, mean_eps: 0.100000\n",
      " 28882/50000: episode: 6604, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 7530.762858, mae: 563.218831, accuracy: 0.145833, mean_q: -67.733046, mean_eps: 0.100000\n",
      " 28886/50000: episode: 6605, duration: 0.015s, episode steps:   4, steps per second: 258, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 6891.085693, mae: 562.260300, accuracy: 0.078125, mean_q: -82.522479, mean_eps: 0.100000\n",
      " 28889/50000: episode: 6606, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 6548.714030, mae: 579.328410, accuracy: 0.093750, mean_q: -75.637960, mean_eps: 0.100000\n",
      " 28892/50000: episode: 6607, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 5838.662272, mae: 565.745260, accuracy: 0.145833, mean_q: -69.158897, mean_eps: 0.100000\n",
      " 28895/50000: episode: 6608, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 6494.217611, mae: 585.318624, accuracy: 0.083333, mean_q: -83.616366, mean_eps: 0.100000\n",
      " 28898/50000: episode: 6609, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 7190.009440, mae: 565.189392, accuracy: 0.083333, mean_q: -94.680990, mean_eps: 0.100000\n",
      " 28901/50000: episode: 6610, duration: 0.013s, episode steps:   3, steps per second: 230, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 6017.050456, mae: 581.783813, accuracy: 0.072917, mean_q: -96.127052, mean_eps: 0.100000\n",
      " 28904/50000: episode: 6611, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 7004.880208, mae: 573.733683, accuracy: 0.083333, mean_q: -75.104734, mean_eps: 0.100000\n",
      " 28907/50000: episode: 6612, duration: 0.013s, episode steps:   3, steps per second: 230, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 6382.875163, mae: 554.849935, accuracy: 0.145833, mean_q: -68.748159, mean_eps: 0.100000\n",
      " 28910/50000: episode: 6613, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 6112.854818, mae: 567.078674, accuracy: 0.125000, mean_q: -77.220202, mean_eps: 0.100000\n",
      " 28913/50000: episode: 6614, duration: 0.014s, episode steps:   3, steps per second: 215, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 7084.830241, mae: 569.905884, accuracy: 0.166667, mean_q: -67.200612, mean_eps: 0.100000\n",
      " 28916/50000: episode: 6615, duration: 0.016s, episode steps:   3, steps per second: 187, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 5989.394694, mae: 566.439921, accuracy: 0.114583, mean_q: -69.284027, mean_eps: 0.100000\n",
      " 28920/50000: episode: 6616, duration: 0.018s, episode steps:   4, steps per second: 224, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 6645.865356, mae: 564.170578, accuracy: 0.117188, mean_q: -86.041018, mean_eps: 0.100000\n",
      " 28923/50000: episode: 6617, duration: 0.014s, episode steps:   3, steps per second: 217, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 6229.226725, mae: 560.908691, accuracy: 0.083333, mean_q: -70.462700, mean_eps: 0.100000\n",
      " 28926/50000: episode: 6618, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 5514.178711, mae: 547.990438, accuracy: 0.114583, mean_q: -94.808759, mean_eps: 0.100000\n",
      " 28929/50000: episode: 6619, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 6088.937988, mae: 553.949443, accuracy: 0.156250, mean_q: -88.570152, mean_eps: 0.100000\n",
      " 28932/50000: episode: 6620, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 6138.126953, mae: 546.809021, accuracy: 0.145833, mean_q: -49.800209, mean_eps: 0.100000\n",
      " 28935/50000: episode: 6621, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 6162.104492, mae: 558.782430, accuracy: 0.145833, mean_q: -85.925981, mean_eps: 0.100000\n",
      " 28938/50000: episode: 6622, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 7023.541667, mae: 557.618022, accuracy: 0.125000, mean_q: -55.748767, mean_eps: 0.100000\n",
      " 28941/50000: episode: 6623, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -224.000, mean reward: -74.667 [-116.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 6608.799316, mae: 562.001607, accuracy: 0.104167, mean_q: -98.387960, mean_eps: 0.100000\n",
      " 28944/50000: episode: 6624, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 8481.828451, mae: 558.445007, accuracy: 0.135417, mean_q: -92.759491, mean_eps: 0.100000\n",
      " 28947/50000: episode: 6625, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 6022.114583, mae: 563.336202, accuracy: 0.135417, mean_q: -75.993017, mean_eps: 0.100000\n",
      " 28950/50000: episode: 6626, duration: 0.013s, episode steps:   3, steps per second: 230, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 5278.631510, mae: 561.199076, accuracy: 0.093750, mean_q: -103.606995, mean_eps: 0.100000\n",
      " 28954/50000: episode: 6627, duration: 0.016s, episode steps:   4, steps per second: 255, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 6889.237061, mae: 555.116913, accuracy: 0.085938, mean_q: -89.896891, mean_eps: 0.100000\n",
      " 28957/50000: episode: 6628, duration: 0.013s, episode steps:   3, steps per second: 227, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 7554.331217, mae: 562.065796, accuracy: 0.187500, mean_q: -78.871696, mean_eps: 0.100000\n",
      " 28961/50000: episode: 6629, duration: 0.017s, episode steps:   4, steps per second: 236, episode reward: -1238.000, mean reward: -309.500 [-999.000, -60.000], mean action: 1.750 [1.000, 3.000],  loss: 7378.545288, mae: 562.936462, accuracy: 0.140625, mean_q: -63.474865, mean_eps: 0.100000\n",
      " 28964/50000: episode: 6630, duration: 0.023s, episode steps:   3, steps per second: 133, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 7116.154134, mae: 537.125244, accuracy: 0.114583, mean_q: -78.381387, mean_eps: 0.100000\n",
      " 28967/50000: episode: 6631, duration: 0.016s, episode steps:   3, steps per second: 194, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 6352.627767, mae: 571.399943, accuracy: 0.093750, mean_q: -95.625921, mean_eps: 0.100000\n",
      " 28971/50000: episode: 6632, duration: 0.016s, episode steps:   4, steps per second: 247, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 7493.762329, mae: 548.170837, accuracy: 0.148438, mean_q: -70.957787, mean_eps: 0.100000\n",
      " 28974/50000: episode: 6633, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 6810.574870, mae: 575.068848, accuracy: 0.083333, mean_q: -73.966026, mean_eps: 0.100000\n",
      " 28977/50000: episode: 6634, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 5673.208496, mae: 557.539022, accuracy: 0.145833, mean_q: -63.194230, mean_eps: 0.100000\n",
      " 28980/50000: episode: 6635, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 5457.868652, mae: 574.444397, accuracy: 0.093750, mean_q: -64.446363, mean_eps: 0.100000\n",
      " 28983/50000: episode: 6636, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 6622.077148, mae: 558.200216, accuracy: 0.135417, mean_q: -79.636791, mean_eps: 0.100000\n",
      " 28986/50000: episode: 6637, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 6795.570312, mae: 558.463745, accuracy: 0.145833, mean_q: -85.890327, mean_eps: 0.100000\n",
      " 28989/50000: episode: 6638, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 5823.670736, mae: 579.504232, accuracy: 0.114583, mean_q: -80.054420, mean_eps: 0.100000\n",
      " 28992/50000: episode: 6639, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 6330.448242, mae: 585.629232, accuracy: 0.072917, mean_q: -61.335897, mean_eps: 0.100000\n",
      " 28995/50000: episode: 6640, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 7140.879069, mae: 558.702637, accuracy: 0.114583, mean_q: -91.165787, mean_eps: 0.100000\n",
      " 28998/50000: episode: 6641, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 5708.734701, mae: 572.809306, accuracy: 0.104167, mean_q: -82.283338, mean_eps: 0.100000\n",
      " 29001/50000: episode: 6642, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 5923.224447, mae: 556.727498, accuracy: 0.145833, mean_q: -87.281779, mean_eps: 0.100000\n",
      " 29005/50000: episode: 6643, duration: 0.016s, episode steps:   4, steps per second: 252, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 5198.291443, mae: 562.249527, accuracy: 0.140625, mean_q: -83.473757, mean_eps: 0.100000\n",
      " 29008/50000: episode: 6644, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 5128.717611, mae: 565.798828, accuracy: 0.208333, mean_q: -47.380758, mean_eps: 0.100000\n",
      " 29011/50000: episode: 6645, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 6839.169596, mae: 576.597900, accuracy: 0.093750, mean_q: -90.381170, mean_eps: 0.100000\n",
      " 29014/50000: episode: 6646, duration: 0.025s, episode steps:   3, steps per second: 118, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 7585.358073, mae: 553.693237, accuracy: 0.125000, mean_q: -83.656759, mean_eps: 0.100000\n",
      " 29017/50000: episode: 6647, duration: 0.015s, episode steps:   3, steps per second: 198, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 5584.579915, mae: 547.178650, accuracy: 0.104167, mean_q: -71.450976, mean_eps: 0.100000\n",
      " 29021/50000: episode: 6648, duration: 0.016s, episode steps:   4, steps per second: 243, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 7681.711060, mae: 545.918060, accuracy: 0.156250, mean_q: -80.005615, mean_eps: 0.100000\n",
      " 29024/50000: episode: 6649, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 7988.471191, mae: 545.331726, accuracy: 0.114583, mean_q: -63.155066, mean_eps: 0.100000\n",
      " 29027/50000: episode: 6650, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 6391.141602, mae: 562.693604, accuracy: 0.125000, mean_q: -70.757507, mean_eps: 0.100000\n",
      " 29030/50000: episode: 6651, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 6358.686523, mae: 562.523885, accuracy: 0.093750, mean_q: -61.232637, mean_eps: 0.100000\n",
      " 29033/50000: episode: 6652, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 7262.155111, mae: 543.919515, accuracy: 0.125000, mean_q: -84.254186, mean_eps: 0.100000\n",
      " 29036/50000: episode: 6653, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 6001.666992, mae: 567.582336, accuracy: 0.104167, mean_q: -73.343269, mean_eps: 0.100000\n",
      " 29039/50000: episode: 6654, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 6071.825521, mae: 575.580485, accuracy: 0.062500, mean_q: -63.268743, mean_eps: 0.100000\n",
      " 29042/50000: episode: 6655, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 7132.277669, mae: 563.188334, accuracy: 0.114583, mean_q: -63.225722, mean_eps: 0.100000\n",
      " 29046/50000: episode: 6656, duration: 0.015s, episode steps:   4, steps per second: 270, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 6453.502319, mae: 554.960861, accuracy: 0.203125, mean_q: -74.225639, mean_eps: 0.100000\n",
      " 29049/50000: episode: 6657, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 8329.003581, mae: 559.517700, accuracy: 0.145833, mean_q: -70.289415, mean_eps: 0.100000\n",
      " 29053/50000: episode: 6658, duration: 0.016s, episode steps:   4, steps per second: 255, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 5738.521362, mae: 564.509415, accuracy: 0.078125, mean_q: -73.444962, mean_eps: 0.100000\n",
      " 29056/50000: episode: 6659, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 5519.080892, mae: 560.910461, accuracy: 0.062500, mean_q: -86.118011, mean_eps: 0.100000\n",
      " 29059/50000: episode: 6660, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 4795.243327, mae: 551.194804, accuracy: 0.114583, mean_q: -80.869359, mean_eps: 0.100000\n",
      " 29062/50000: episode: 6661, duration: 0.016s, episode steps:   3, steps per second: 184, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 5872.030599, mae: 555.320109, accuracy: 0.104167, mean_q: -93.817586, mean_eps: 0.100000\n",
      " 29065/50000: episode: 6662, duration: 0.016s, episode steps:   3, steps per second: 193, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 5448.953939, mae: 552.114421, accuracy: 0.114583, mean_q: -86.989014, mean_eps: 0.100000\n",
      " 29068/50000: episode: 6663, duration: 0.015s, episode steps:   3, steps per second: 202, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 7102.467122, mae: 558.875712, accuracy: 0.104167, mean_q: -62.790860, mean_eps: 0.100000\n",
      " 29071/50000: episode: 6664, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 7548.324870, mae: 536.800456, accuracy: 0.187500, mean_q: -56.244064, mean_eps: 0.100000\n",
      " 29074/50000: episode: 6665, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 6406.771647, mae: 545.524394, accuracy: 0.177083, mean_q: -55.293213, mean_eps: 0.100000\n",
      " 29077/50000: episode: 6666, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 6527.734212, mae: 558.567749, accuracy: 0.093750, mean_q: -84.200363, mean_eps: 0.100000\n",
      " 29080/50000: episode: 6667, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -224.000, mean reward: -74.667 [-116.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 5785.757975, mae: 553.136353, accuracy: 0.093750, mean_q: -92.051427, mean_eps: 0.100000\n",
      " 29083/50000: episode: 6668, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 5569.947754, mae: 547.251363, accuracy: 0.093750, mean_q: -73.353236, mean_eps: 0.100000\n",
      " 29086/50000: episode: 6669, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 5050.579997, mae: 571.593994, accuracy: 0.135417, mean_q: -67.535164, mean_eps: 0.100000\n",
      " 29089/50000: episode: 6670, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 5583.646973, mae: 562.604146, accuracy: 0.083333, mean_q: -88.196419, mean_eps: 0.100000\n",
      " 29092/50000: episode: 6671, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 5488.190104, mae: 560.754740, accuracy: 0.156250, mean_q: -78.761332, mean_eps: 0.100000\n",
      " 29096/50000: episode: 6672, duration: 0.016s, episode steps:   4, steps per second: 252, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 5535.473877, mae: 570.229553, accuracy: 0.140625, mean_q: -55.063193, mean_eps: 0.100000\n",
      " 29099/50000: episode: 6673, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 6590.889323, mae: 577.162476, accuracy: 0.093750, mean_q: -74.169161, mean_eps: 0.100000\n",
      " 29102/50000: episode: 6674, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 7056.483398, mae: 555.616679, accuracy: 0.145833, mean_q: -95.358398, mean_eps: 0.100000\n",
      " 29105/50000: episode: 6675, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 5597.299642, mae: 567.167440, accuracy: 0.177083, mean_q: -86.212797, mean_eps: 0.100000\n",
      " 29108/50000: episode: 6676, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 6133.969564, mae: 555.802083, accuracy: 0.166667, mean_q: -68.337793, mean_eps: 0.100000\n",
      " 29111/50000: episode: 6677, duration: 0.014s, episode steps:   3, steps per second: 218, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 8061.028158, mae: 561.012024, accuracy: 0.145833, mean_q: -66.382039, mean_eps: 0.100000\n",
      " 29114/50000: episode: 6678, duration: 0.015s, episode steps:   3, steps per second: 204, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 7001.382812, mae: 563.322205, accuracy: 0.114583, mean_q: -65.045395, mean_eps: 0.100000\n",
      " 29117/50000: episode: 6679, duration: 0.015s, episode steps:   3, steps per second: 204, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 6312.626628, mae: 572.999044, accuracy: 0.125000, mean_q: -36.009832, mean_eps: 0.100000\n",
      " 29120/50000: episode: 6680, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 6552.194987, mae: 566.119832, accuracy: 0.156250, mean_q: -69.624575, mean_eps: 0.100000\n",
      " 29123/50000: episode: 6681, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 5735.791504, mae: 567.417094, accuracy: 0.104167, mean_q: -96.874842, mean_eps: 0.100000\n",
      " 29128/50000: episode: 6682, duration: 0.019s, episode steps:   5, steps per second: 264, episode reward: -2193.000, mean reward: -438.600 [-999.000, -45.000], mean action: 1.400 [0.000, 2.000],  loss: 5677.447363, mae: 551.572253, accuracy: 0.156250, mean_q: -90.009621, mean_eps: 0.100000\n",
      " 29132/50000: episode: 6683, duration: 0.015s, episode steps:   4, steps per second: 260, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 6414.780151, mae: 553.580460, accuracy: 0.132812, mean_q: -94.518074, mean_eps: 0.100000\n",
      " 29135/50000: episode: 6684, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 6254.377116, mae: 578.213359, accuracy: 0.093750, mean_q: -91.920817, mean_eps: 0.100000\n",
      " 29139/50000: episode: 6685, duration: 0.016s, episode steps:   4, steps per second: 250, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 5886.406982, mae: 575.918854, accuracy: 0.117188, mean_q: -58.079893, mean_eps: 0.100000\n",
      " 29142/50000: episode: 6686, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 7212.210286, mae: 552.596659, accuracy: 0.166667, mean_q: -76.175241, mean_eps: 0.100000\n",
      " 29145/50000: episode: 6687, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 5976.912272, mae: 567.548787, accuracy: 0.114583, mean_q: -68.988991, mean_eps: 0.100000\n",
      " 29148/50000: episode: 6688, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 6190.410156, mae: 563.682943, accuracy: 0.145833, mean_q: -87.722109, mean_eps: 0.100000\n",
      " 29151/50000: episode: 6689, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 5467.555501, mae: 565.578389, accuracy: 0.104167, mean_q: -66.508108, mean_eps: 0.100000\n",
      " 29155/50000: episode: 6690, duration: 0.016s, episode steps:   4, steps per second: 253, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 5150.539062, mae: 589.645279, accuracy: 0.078125, mean_q: -75.507015, mean_eps: 0.100000\n",
      " 29158/50000: episode: 6691, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 4441.512614, mae: 566.414673, accuracy: 0.114583, mean_q: -72.806590, mean_eps: 0.100000\n",
      " 29161/50000: episode: 6692, duration: 0.016s, episode steps:   3, steps per second: 187, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 5968.104818, mae: 560.086060, accuracy: 0.156250, mean_q: -70.987508, mean_eps: 0.100000\n",
      " 29164/50000: episode: 6693, duration: 0.014s, episode steps:   3, steps per second: 212, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 6255.458496, mae: 590.044332, accuracy: 0.052083, mean_q: -100.214414, mean_eps: 0.100000\n",
      " 29167/50000: episode: 6694, duration: 0.014s, episode steps:   3, steps per second: 211, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 5019.297038, mae: 555.394836, accuracy: 0.197917, mean_q: -78.339694, mean_eps: 0.100000\n",
      " 29170/50000: episode: 6695, duration: 0.015s, episode steps:   3, steps per second: 203, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 5937.003418, mae: 544.894938, accuracy: 0.104167, mean_q: -81.595355, mean_eps: 0.100000\n",
      " 29173/50000: episode: 6696, duration: 0.014s, episode steps:   3, steps per second: 217, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 6567.939128, mae: 586.099467, accuracy: 0.125000, mean_q: -79.278982, mean_eps: 0.100000\n",
      " 29176/50000: episode: 6697, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 5296.105794, mae: 573.654887, accuracy: 0.093750, mean_q: -74.743337, mean_eps: 0.100000\n",
      " 29179/50000: episode: 6698, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 5996.516276, mae: 549.044657, accuracy: 0.104167, mean_q: -93.132319, mean_eps: 0.100000\n",
      " 29182/50000: episode: 6699, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 6541.562988, mae: 552.143941, accuracy: 0.197917, mean_q: -73.561414, mean_eps: 0.100000\n",
      " 29185/50000: episode: 6700, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 6053.231934, mae: 554.329102, accuracy: 0.093750, mean_q: -44.885123, mean_eps: 0.100000\n",
      " 29188/50000: episode: 6701, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 5856.580892, mae: 581.158447, accuracy: 0.104167, mean_q: -75.583061, mean_eps: 0.100000\n",
      " 29191/50000: episode: 6702, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 6452.700358, mae: 560.144999, accuracy: 0.145833, mean_q: -84.017026, mean_eps: 0.100000\n",
      " 29194/50000: episode: 6703, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 7891.443359, mae: 576.251506, accuracy: 0.156250, mean_q: -77.880277, mean_eps: 0.100000\n",
      " 29197/50000: episode: 6704, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 7482.454102, mae: 570.897888, accuracy: 0.166667, mean_q: -68.909738, mean_eps: 0.100000\n",
      " 29200/50000: episode: 6705, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 7061.540853, mae: 585.715556, accuracy: 0.052083, mean_q: -70.468786, mean_eps: 0.100000\n",
      " 29203/50000: episode: 6706, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 7209.747396, mae: 565.197611, accuracy: 0.177083, mean_q: -93.074071, mean_eps: 0.100000\n",
      " 29206/50000: episode: 6707, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 5531.658691, mae: 565.246216, accuracy: 0.062500, mean_q: -80.439303, mean_eps: 0.100000\n",
      " 29209/50000: episode: 6708, duration: 0.017s, episode steps:   3, steps per second: 173, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 6033.178711, mae: 556.570679, accuracy: 0.114583, mean_q: -71.471672, mean_eps: 0.100000\n",
      " 29212/50000: episode: 6709, duration: 0.016s, episode steps:   3, steps per second: 184, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 5599.454427, mae: 555.849447, accuracy: 0.104167, mean_q: -82.729291, mean_eps: 0.100000\n",
      " 29215/50000: episode: 6710, duration: 0.015s, episode steps:   3, steps per second: 200, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 6596.178223, mae: 558.152120, accuracy: 0.114583, mean_q: -67.057194, mean_eps: 0.100000\n",
      " 29218/50000: episode: 6711, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 6109.411947, mae: 574.468852, accuracy: 0.062500, mean_q: -70.145210, mean_eps: 0.100000\n",
      " 29221/50000: episode: 6712, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 6336.225423, mae: 566.154338, accuracy: 0.104167, mean_q: -73.743673, mean_eps: 0.100000\n",
      " 29224/50000: episode: 6713, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 5784.230469, mae: 574.026001, accuracy: 0.072917, mean_q: -97.703678, mean_eps: 0.100000\n",
      " 29227/50000: episode: 6714, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 6503.776855, mae: 582.148539, accuracy: 0.135417, mean_q: -69.280988, mean_eps: 0.100000\n",
      " 29230/50000: episode: 6715, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 7258.776693, mae: 565.722921, accuracy: 0.125000, mean_q: -84.014743, mean_eps: 0.100000\n",
      " 29233/50000: episode: 6716, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 4728.812988, mae: 575.394145, accuracy: 0.135417, mean_q: -64.146763, mean_eps: 0.100000\n",
      " 29236/50000: episode: 6717, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 5256.945964, mae: 567.767476, accuracy: 0.156250, mean_q: -72.549431, mean_eps: 0.100000\n",
      " 29240/50000: episode: 6718, duration: 0.016s, episode steps:   4, steps per second: 257, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 4835.928833, mae: 557.203674, accuracy: 0.125000, mean_q: -68.121679, mean_eps: 0.100000\n",
      " 29243/50000: episode: 6719, duration: 0.013s, episode steps:   3, steps per second: 227, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 6093.475260, mae: 554.064148, accuracy: 0.125000, mean_q: -77.278214, mean_eps: 0.100000\n",
      " 29246/50000: episode: 6720, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 4741.326497, mae: 567.114868, accuracy: 0.135417, mean_q: -91.355214, mean_eps: 0.100000\n",
      " 29249/50000: episode: 6721, duration: 0.013s, episode steps:   3, steps per second: 226, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 5574.989583, mae: 579.214193, accuracy: 0.114583, mean_q: -91.061457, mean_eps: 0.100000\n",
      " 29252/50000: episode: 6722, duration: 0.014s, episode steps:   3, steps per second: 216, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 4693.515951, mae: 583.185364, accuracy: 0.125000, mean_q: -70.395126, mean_eps: 0.100000\n",
      " 29255/50000: episode: 6723, duration: 0.017s, episode steps:   3, steps per second: 178, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 6525.076009, mae: 568.852112, accuracy: 0.114583, mean_q: -106.658974, mean_eps: 0.100000\n",
      " 29258/50000: episode: 6724, duration: 0.015s, episode steps:   3, steps per second: 205, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 6580.554036, mae: 566.450358, accuracy: 0.135417, mean_q: -106.642235, mean_eps: 0.100000\n",
      " 29261/50000: episode: 6725, duration: 0.015s, episode steps:   3, steps per second: 202, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 5360.783529, mae: 573.159261, accuracy: 0.145833, mean_q: -83.032401, mean_eps: 0.100000\n",
      " 29264/50000: episode: 6726, duration: 0.015s, episode steps:   3, steps per second: 205, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 5173.457031, mae: 570.426737, accuracy: 0.114583, mean_q: -71.478521, mean_eps: 0.100000\n",
      " 29267/50000: episode: 6727, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 5697.308919, mae: 593.404236, accuracy: 0.093750, mean_q: -66.534674, mean_eps: 0.100000\n",
      " 29271/50000: episode: 6728, duration: 0.016s, episode steps:   4, steps per second: 255, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 4834.357422, mae: 570.923386, accuracy: 0.125000, mean_q: -87.822111, mean_eps: 0.100000\n",
      " 29274/50000: episode: 6729, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 5392.485840, mae: 576.880046, accuracy: 0.145833, mean_q: -73.823129, mean_eps: 0.100000\n",
      " 29277/50000: episode: 6730, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 6027.831217, mae: 563.931844, accuracy: 0.104167, mean_q: -85.016097, mean_eps: 0.100000\n",
      " 29280/50000: episode: 6731, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 4791.282715, mae: 585.210999, accuracy: 0.083333, mean_q: -83.745407, mean_eps: 0.100000\n",
      " 29284/50000: episode: 6732, duration: 0.015s, episode steps:   4, steps per second: 262, episode reward: -1223.000, mean reward: -305.750 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 7317.600342, mae: 554.153748, accuracy: 0.164062, mean_q: -102.096222, mean_eps: 0.100000\n",
      " 29287/50000: episode: 6733, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 6127.082520, mae: 560.835592, accuracy: 0.156250, mean_q: -97.195429, mean_eps: 0.100000\n",
      " 29291/50000: episode: 6734, duration: 0.015s, episode steps:   4, steps per second: 258, episode reward: -1238.000, mean reward: -309.500 [-999.000, -60.000], mean action: 2.250 [1.000, 3.000],  loss: 5563.028931, mae: 565.578735, accuracy: 0.195312, mean_q: -77.590023, mean_eps: 0.100000\n",
      " 29294/50000: episode: 6735, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 4556.906169, mae: 561.816040, accuracy: 0.072917, mean_q: -85.048149, mean_eps: 0.100000\n",
      " 29297/50000: episode: 6736, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 6556.545898, mae: 565.787984, accuracy: 0.135417, mean_q: -82.122264, mean_eps: 0.100000\n",
      " 29300/50000: episode: 6737, duration: 0.022s, episode steps:   3, steps per second: 135, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 5365.281250, mae: 555.700562, accuracy: 0.114583, mean_q: -78.241852, mean_eps: 0.100000\n",
      " 29303/50000: episode: 6738, duration: 0.017s, episode steps:   3, steps per second: 174, episode reward: -224.000, mean reward: -74.667 [-116.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 6466.616374, mae: 569.131897, accuracy: 0.114583, mean_q: -84.309311, mean_eps: 0.100000\n",
      " 29306/50000: episode: 6739, duration: 0.017s, episode steps:   3, steps per second: 175, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 6562.076335, mae: 550.151937, accuracy: 0.125000, mean_q: -84.545133, mean_eps: 0.100000\n",
      " 29309/50000: episode: 6740, duration: 0.017s, episode steps:   3, steps per second: 181, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 5601.773275, mae: 541.436198, accuracy: 0.156250, mean_q: -81.058283, mean_eps: 0.100000\n",
      " 29312/50000: episode: 6741, duration: 0.014s, episode steps:   3, steps per second: 211, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 5166.494629, mae: 549.703105, accuracy: 0.156250, mean_q: -93.814142, mean_eps: 0.100000\n",
      " 29315/50000: episode: 6742, duration: 0.015s, episode steps:   3, steps per second: 194, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 6470.274902, mae: 563.182414, accuracy: 0.156250, mean_q: -82.741104, mean_eps: 0.100000\n",
      " 29318/50000: episode: 6743, duration: 0.014s, episode steps:   3, steps per second: 217, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 4818.743652, mae: 551.092224, accuracy: 0.125000, mean_q: -81.863887, mean_eps: 0.100000\n",
      " 29321/50000: episode: 6744, duration: 0.014s, episode steps:   3, steps per second: 222, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 4713.704834, mae: 563.615763, accuracy: 0.104167, mean_q: -92.145175, mean_eps: 0.100000\n",
      " 29325/50000: episode: 6745, duration: 0.017s, episode steps:   4, steps per second: 241, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 5177.541016, mae: 574.315277, accuracy: 0.109375, mean_q: -71.690471, mean_eps: 0.100000\n",
      " 29328/50000: episode: 6746, duration: 0.013s, episode steps:   3, steps per second: 226, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 5421.155111, mae: 558.279378, accuracy: 0.062500, mean_q: -107.932933, mean_eps: 0.100000\n",
      " 29331/50000: episode: 6747, duration: 0.014s, episode steps:   3, steps per second: 216, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 5913.194824, mae: 574.731303, accuracy: 0.052083, mean_q: -73.862067, mean_eps: 0.100000\n",
      " 29336/50000: episode: 6748, duration: 0.021s, episode steps:   5, steps per second: 235, episode reward: -2237.000, mean reward: -447.400 [-999.000, -60.000], mean action: 1.800 [0.000, 3.000],  loss: 5030.787402, mae: 564.847681, accuracy: 0.131250, mean_q: -88.781397, mean_eps: 0.100000\n",
      " 29339/50000: episode: 6749, duration: 0.014s, episode steps:   3, steps per second: 210, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 6569.220866, mae: 567.893209, accuracy: 0.114583, mean_q: -75.152606, mean_eps: 0.100000\n",
      " 29342/50000: episode: 6750, duration: 0.015s, episode steps:   3, steps per second: 201, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 5537.794434, mae: 590.809224, accuracy: 0.062500, mean_q: -77.662666, mean_eps: 0.100000\n",
      " 29345/50000: episode: 6751, duration: 0.019s, episode steps:   3, steps per second: 155, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 6256.471029, mae: 571.274414, accuracy: 0.114583, mean_q: -93.093559, mean_eps: 0.100000\n",
      " 29349/50000: episode: 6752, duration: 0.021s, episode steps:   4, steps per second: 188, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 5614.731689, mae: 570.991989, accuracy: 0.117188, mean_q: -86.784128, mean_eps: 0.100000\n",
      " 29352/50000: episode: 6753, duration: 0.014s, episode steps:   3, steps per second: 208, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 4948.104167, mae: 574.838765, accuracy: 0.145833, mean_q: -91.433515, mean_eps: 0.100000\n",
      " 29355/50000: episode: 6754, duration: 0.013s, episode steps:   3, steps per second: 222, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 5405.277181, mae: 564.304932, accuracy: 0.093750, mean_q: -98.443667, mean_eps: 0.100000\n",
      " 29358/50000: episode: 6755, duration: 0.013s, episode steps:   3, steps per second: 223, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 6521.552083, mae: 569.134766, accuracy: 0.177083, mean_q: -88.252553, mean_eps: 0.100000\n",
      " 29361/50000: episode: 6756, duration: 0.014s, episode steps:   3, steps per second: 213, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 5558.779785, mae: 576.005656, accuracy: 0.135417, mean_q: -91.642787, mean_eps: 0.100000\n",
      " 29364/50000: episode: 6757, duration: 0.015s, episode steps:   3, steps per second: 204, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 5747.714193, mae: 582.875773, accuracy: 0.083333, mean_q: -97.043710, mean_eps: 0.100000\n",
      " 29367/50000: episode: 6758, duration: 0.014s, episode steps:   3, steps per second: 213, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.667 [0.000, 3.000],  loss: 4748.407715, mae: 591.678833, accuracy: 0.093750, mean_q: -80.022257, mean_eps: 0.100000\n",
      " 29370/50000: episode: 6759, duration: 0.014s, episode steps:   3, steps per second: 209, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 4621.374674, mae: 562.937459, accuracy: 0.197917, mean_q: -90.554825, mean_eps: 0.100000\n",
      " 29373/50000: episode: 6760, duration: 0.013s, episode steps:   3, steps per second: 227, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 5088.393555, mae: 550.232402, accuracy: 0.197917, mean_q: -78.474930, mean_eps: 0.100000\n",
      " 29376/50000: episode: 6761, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 5419.125326, mae: 571.168783, accuracy: 0.125000, mean_q: -89.809354, mean_eps: 0.100000\n",
      " 29379/50000: episode: 6762, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 5089.149577, mae: 566.210205, accuracy: 0.125000, mean_q: -56.668056, mean_eps: 0.100000\n",
      " 29382/50000: episode: 6763, duration: 0.014s, episode steps:   3, steps per second: 220, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 7238.020996, mae: 563.891886, accuracy: 0.083333, mean_q: -79.987251, mean_eps: 0.100000\n",
      " 29385/50000: episode: 6764, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 4732.495280, mae: 539.883972, accuracy: 0.093750, mean_q: -93.968870, mean_eps: 0.100000\n",
      " 29389/50000: episode: 6765, duration: 0.019s, episode steps:   4, steps per second: 214, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 4664.422363, mae: 590.705414, accuracy: 0.070312, mean_q: -87.079674, mean_eps: 0.100000\n",
      " 29392/50000: episode: 6766, duration: 0.015s, episode steps:   3, steps per second: 205, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 4227.555013, mae: 568.666138, accuracy: 0.197917, mean_q: -82.805033, mean_eps: 0.100000\n",
      " 29395/50000: episode: 6767, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 4695.729492, mae: 561.510234, accuracy: 0.156250, mean_q: -93.172887, mean_eps: 0.100000\n",
      " 29398/50000: episode: 6768, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 5591.460449, mae: 573.811340, accuracy: 0.156250, mean_q: -87.806628, mean_eps: 0.100000\n",
      " 29401/50000: episode: 6769, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 5037.309896, mae: 569.834717, accuracy: 0.104167, mean_q: -109.459653, mean_eps: 0.100000\n",
      " 29404/50000: episode: 6770, duration: 0.013s, episode steps:   3, steps per second: 224, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 6498.594076, mae: 582.985860, accuracy: 0.145833, mean_q: -89.929512, mean_eps: 0.100000\n",
      " 29407/50000: episode: 6771, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 4813.763672, mae: 564.996887, accuracy: 0.041667, mean_q: -105.366933, mean_eps: 0.100000\n",
      " 29411/50000: episode: 6772, duration: 0.015s, episode steps:   4, steps per second: 260, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 2.250 [1.000, 3.000],  loss: 4992.829590, mae: 571.692078, accuracy: 0.054688, mean_q: -77.023310, mean_eps: 0.100000\n",
      " 29414/50000: episode: 6773, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 5676.200033, mae: 571.062398, accuracy: 0.104167, mean_q: -87.835843, mean_eps: 0.100000\n",
      " 29417/50000: episode: 6774, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 5684.763184, mae: 567.552694, accuracy: 0.093750, mean_q: -86.241595, mean_eps: 0.100000\n",
      " 29420/50000: episode: 6775, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 5139.988607, mae: 571.646159, accuracy: 0.135417, mean_q: -83.054392, mean_eps: 0.100000\n",
      " 29423/50000: episode: 6776, duration: 0.013s, episode steps:   3, steps per second: 224, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 5856.919271, mae: 587.171204, accuracy: 0.093750, mean_q: -81.439639, mean_eps: 0.100000\n",
      " 29426/50000: episode: 6777, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 5838.485840, mae: 558.185221, accuracy: 0.104167, mean_q: -85.362752, mean_eps: 0.100000\n",
      " 29429/50000: episode: 6778, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 6043.983236, mae: 553.772095, accuracy: 0.083333, mean_q: -75.141757, mean_eps: 0.100000\n",
      " 29432/50000: episode: 6779, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 5799.346680, mae: 581.086426, accuracy: 0.145833, mean_q: -89.180834, mean_eps: 0.100000\n",
      " 29435/50000: episode: 6780, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 6265.758626, mae: 559.247742, accuracy: 0.166667, mean_q: -103.849559, mean_eps: 0.100000\n",
      " 29438/50000: episode: 6781, duration: 0.022s, episode steps:   3, steps per second: 134, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 6179.745117, mae: 560.361023, accuracy: 0.145833, mean_q: -87.179001, mean_eps: 0.100000\n",
      " 29441/50000: episode: 6782, duration: 0.017s, episode steps:   3, steps per second: 177, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 5248.461914, mae: 562.563538, accuracy: 0.197917, mean_q: -96.786395, mean_eps: 0.100000\n",
      " 29444/50000: episode: 6783, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 5245.901204, mae: 581.591085, accuracy: 0.114583, mean_q: -103.201235, mean_eps: 0.100000\n",
      " 29447/50000: episode: 6784, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 5626.902018, mae: 562.476624, accuracy: 0.166667, mean_q: -69.265503, mean_eps: 0.100000\n",
      " 29450/50000: episode: 6785, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 7456.620931, mae: 569.989909, accuracy: 0.166667, mean_q: -85.429133, mean_eps: 0.100000\n",
      " 29453/50000: episode: 6786, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 5070.626139, mae: 558.358683, accuracy: 0.187500, mean_q: -86.831111, mean_eps: 0.100000\n",
      " 29456/50000: episode: 6787, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 5102.940267, mae: 563.709493, accuracy: 0.135417, mean_q: -92.615089, mean_eps: 0.100000\n",
      " 29460/50000: episode: 6788, duration: 0.016s, episode steps:   4, steps per second: 252, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 3690.977295, mae: 564.209457, accuracy: 0.156250, mean_q: -82.746635, mean_eps: 0.100000\n",
      " 29463/50000: episode: 6789, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 5993.604818, mae: 581.196554, accuracy: 0.093750, mean_q: -85.506963, mean_eps: 0.100000\n",
      " 29466/50000: episode: 6790, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 5445.402669, mae: 577.726644, accuracy: 0.166667, mean_q: -82.171257, mean_eps: 0.100000\n",
      " 29469/50000: episode: 6791, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 4935.713867, mae: 558.800476, accuracy: 0.083333, mean_q: -78.081657, mean_eps: 0.100000\n",
      " 29472/50000: episode: 6792, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 5012.474121, mae: 564.907043, accuracy: 0.187500, mean_q: -89.557627, mean_eps: 0.100000\n",
      " 29475/50000: episode: 6793, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 4844.253255, mae: 589.399862, accuracy: 0.072917, mean_q: -76.806569, mean_eps: 0.100000\n",
      " 29478/50000: episode: 6794, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 4887.403809, mae: 579.693339, accuracy: 0.072917, mean_q: -89.053263, mean_eps: 0.100000\n",
      " 29481/50000: episode: 6795, duration: 0.014s, episode steps:   3, steps per second: 222, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 5330.037598, mae: 574.149801, accuracy: 0.104167, mean_q: -101.812564, mean_eps: 0.100000\n",
      " 29484/50000: episode: 6796, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 4891.549154, mae: 593.754456, accuracy: 0.083333, mean_q: -83.462364, mean_eps: 0.100000\n",
      " 29487/50000: episode: 6797, duration: 0.025s, episode steps:   3, steps per second: 118, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 5781.494629, mae: 571.573222, accuracy: 0.125000, mean_q: -87.150279, mean_eps: 0.100000\n",
      " 29490/50000: episode: 6798, duration: 0.016s, episode steps:   3, steps per second: 185, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 4652.029785, mae: 571.255819, accuracy: 0.135417, mean_q: -95.546125, mean_eps: 0.100000\n",
      " 29494/50000: episode: 6799, duration: 0.017s, episode steps:   4, steps per second: 241, episode reward: -1238.000, mean reward: -309.500 [-999.000, -60.000], mean action: 1.500 [0.000, 3.000],  loss: 5151.086914, mae: 573.430115, accuracy: 0.109375, mean_q: -84.956306, mean_eps: 0.100000\n",
      " 29497/50000: episode: 6800, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 5094.684733, mae: 556.794271, accuracy: 0.135417, mean_q: -94.432922, mean_eps: 0.100000\n",
      " 29500/50000: episode: 6801, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 5287.285970, mae: 543.402445, accuracy: 0.135417, mean_q: -92.262675, mean_eps: 0.100000\n",
      " 29503/50000: episode: 6802, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 5036.047689, mae: 557.684814, accuracy: 0.145833, mean_q: -76.180753, mean_eps: 0.100000\n",
      " 29506/50000: episode: 6803, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 4824.295898, mae: 557.201619, accuracy: 0.104167, mean_q: -77.240224, mean_eps: 0.100000\n",
      " 29509/50000: episode: 6804, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 4940.250163, mae: 578.822611, accuracy: 0.177083, mean_q: -91.992493, mean_eps: 0.100000\n",
      " 29512/50000: episode: 6805, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 4389.417969, mae: 586.942139, accuracy: 0.104167, mean_q: -80.452868, mean_eps: 0.100000\n",
      " 29515/50000: episode: 6806, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 4205.746745, mae: 569.445312, accuracy: 0.072917, mean_q: -105.263204, mean_eps: 0.100000\n",
      " 29518/50000: episode: 6807, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 4781.455404, mae: 578.428507, accuracy: 0.104167, mean_q: -95.764158, mean_eps: 0.100000\n",
      " 29521/50000: episode: 6808, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 5782.861654, mae: 577.641093, accuracy: 0.093750, mean_q: -97.004351, mean_eps: 0.100000\n",
      " 29525/50000: episode: 6809, duration: 0.016s, episode steps:   4, steps per second: 246, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 5494.571167, mae: 563.249481, accuracy: 0.148438, mean_q: -89.282850, mean_eps: 0.100000\n",
      " 29528/50000: episode: 6810, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 5791.940430, mae: 575.138672, accuracy: 0.156250, mean_q: -86.117076, mean_eps: 0.100000\n",
      " 29531/50000: episode: 6811, duration: 0.015s, episode steps:   3, steps per second: 199, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 5803.374186, mae: 553.531535, accuracy: 0.177083, mean_q: -82.881889, mean_eps: 0.100000\n",
      " 29534/50000: episode: 6812, duration: 0.016s, episode steps:   3, steps per second: 193, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 5892.148112, mae: 562.630900, accuracy: 0.145833, mean_q: -73.923017, mean_eps: 0.100000\n",
      " 29537/50000: episode: 6813, duration: 0.014s, episode steps:   3, steps per second: 210, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 4942.938802, mae: 563.472371, accuracy: 0.135417, mean_q: -84.418330, mean_eps: 0.100000\n",
      " 29540/50000: episode: 6814, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 4170.091960, mae: 573.307454, accuracy: 0.093750, mean_q: -62.938105, mean_eps: 0.100000\n",
      " 29543/50000: episode: 6815, duration: 0.013s, episode steps:   3, steps per second: 225, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 5379.813639, mae: 561.709106, accuracy: 0.083333, mean_q: -95.966220, mean_eps: 0.100000\n",
      " 29546/50000: episode: 6816, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 5920.718099, mae: 546.268392, accuracy: 0.114583, mean_q: -88.927073, mean_eps: 0.100000\n",
      " 29549/50000: episode: 6817, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -224.000, mean reward: -74.667 [-116.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 5155.276367, mae: 542.308472, accuracy: 0.104167, mean_q: -93.826764, mean_eps: 0.100000\n",
      " 29552/50000: episode: 6818, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 5948.506022, mae: 560.707113, accuracy: 0.145833, mean_q: -98.933426, mean_eps: 0.100000\n",
      " 29555/50000: episode: 6819, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 4276.851888, mae: 579.191976, accuracy: 0.114583, mean_q: -95.496539, mean_eps: 0.100000\n",
      " 29558/50000: episode: 6820, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 5426.437500, mae: 563.440552, accuracy: 0.156250, mean_q: -79.877294, mean_eps: 0.100000\n",
      " 29563/50000: episode: 6821, duration: 0.018s, episode steps:   5, steps per second: 272, episode reward: -2193.000, mean reward: -438.600 [-999.000, -58.000], mean action: 1.200 [0.000, 3.000],  loss: 4318.788330, mae: 575.005847, accuracy: 0.075000, mean_q: -88.976126, mean_eps: 0.100000\n",
      " 29566/50000: episode: 6822, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 5235.741536, mae: 546.555664, accuracy: 0.166667, mean_q: -89.623194, mean_eps: 0.100000\n",
      " 29569/50000: episode: 6823, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 4151.510254, mae: 571.187398, accuracy: 0.114583, mean_q: -87.567286, mean_eps: 0.100000\n",
      " 29573/50000: episode: 6824, duration: 0.015s, episode steps:   4, steps per second: 260, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 5715.437988, mae: 575.118011, accuracy: 0.117188, mean_q: -96.563265, mean_eps: 0.100000\n",
      " 29576/50000: episode: 6825, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 4592.425944, mae: 570.551371, accuracy: 0.114583, mean_q: -80.650223, mean_eps: 0.100000\n",
      " 29579/50000: episode: 6826, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 5297.004476, mae: 581.737712, accuracy: 0.166667, mean_q: -79.043861, mean_eps: 0.100000\n",
      " 29582/50000: episode: 6827, duration: 0.015s, episode steps:   3, steps per second: 206, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 6288.124674, mae: 590.592265, accuracy: 0.156250, mean_q: -87.595403, mean_eps: 0.100000\n",
      " 29585/50000: episode: 6828, duration: 0.015s, episode steps:   3, steps per second: 205, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 5359.212240, mae: 586.229533, accuracy: 0.114583, mean_q: -92.666382, mean_eps: 0.100000\n",
      " 29588/50000: episode: 6829, duration: 0.014s, episode steps:   3, steps per second: 222, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 4052.956462, mae: 568.900452, accuracy: 0.104167, mean_q: -103.622396, mean_eps: 0.100000\n",
      " 29591/50000: episode: 6830, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 6416.667155, mae: 587.107625, accuracy: 0.083333, mean_q: -89.017006, mean_eps: 0.100000\n",
      " 29594/50000: episode: 6831, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 5176.899414, mae: 576.469360, accuracy: 0.114583, mean_q: -88.835559, mean_eps: 0.100000\n",
      " 29597/50000: episode: 6832, duration: 0.014s, episode steps:   3, steps per second: 214, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 3574.003662, mae: 583.130514, accuracy: 0.135417, mean_q: -73.915972, mean_eps: 0.100000\n",
      " 29600/50000: episode: 6833, duration: 0.015s, episode steps:   3, steps per second: 201, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 4559.323893, mae: 560.461690, accuracy: 0.135417, mean_q: -99.888466, mean_eps: 0.100000\n",
      " 29603/50000: episode: 6834, duration: 0.014s, episode steps:   3, steps per second: 207, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 5352.336833, mae: 565.038757, accuracy: 0.104167, mean_q: -86.255107, mean_eps: 0.100000\n",
      " 29606/50000: episode: 6835, duration: 0.014s, episode steps:   3, steps per second: 210, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 6284.327148, mae: 564.521383, accuracy: 0.177083, mean_q: -78.874056, mean_eps: 0.100000\n",
      " 29609/50000: episode: 6836, duration: 0.014s, episode steps:   3, steps per second: 208, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 4492.730387, mae: 580.094747, accuracy: 0.104167, mean_q: -94.335777, mean_eps: 0.100000\n",
      " 29613/50000: episode: 6837, duration: 0.017s, episode steps:   4, steps per second: 231, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 5373.959595, mae: 557.711914, accuracy: 0.148438, mean_q: -85.286980, mean_eps: 0.100000\n",
      " 29616/50000: episode: 6838, duration: 0.014s, episode steps:   3, steps per second: 219, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 4841.050944, mae: 580.854065, accuracy: 0.145833, mean_q: -87.582555, mean_eps: 0.100000\n",
      " 29619/50000: episode: 6839, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 4729.181641, mae: 550.499593, accuracy: 0.135417, mean_q: -96.940018, mean_eps: 0.100000\n",
      " 29622/50000: episode: 6840, duration: 0.015s, episode steps:   3, steps per second: 198, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 5118.453288, mae: 557.685160, accuracy: 0.156250, mean_q: -98.241419, mean_eps: 0.100000\n",
      " 29625/50000: episode: 6841, duration: 0.015s, episode steps:   3, steps per second: 202, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 4861.239909, mae: 573.727356, accuracy: 0.145833, mean_q: -85.489436, mean_eps: 0.100000\n",
      " 29628/50000: episode: 6842, duration: 0.022s, episode steps:   3, steps per second: 137, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 4746.918294, mae: 579.568807, accuracy: 0.125000, mean_q: -79.733913, mean_eps: 0.100000\n",
      " 29631/50000: episode: 6843, duration: 0.016s, episode steps:   3, steps per second: 189, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 4525.013509, mae: 557.223206, accuracy: 0.072917, mean_q: -79.003632, mean_eps: 0.100000\n",
      " 29634/50000: episode: 6844, duration: 0.014s, episode steps:   3, steps per second: 214, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 5198.710612, mae: 564.660746, accuracy: 0.156250, mean_q: -88.440811, mean_eps: 0.100000\n",
      " 29638/50000: episode: 6845, duration: 0.016s, episode steps:   4, steps per second: 249, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 5987.652344, mae: 578.721954, accuracy: 0.125000, mean_q: -99.209906, mean_eps: 0.100000\n",
      " 29641/50000: episode: 6846, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 4019.005046, mae: 576.688070, accuracy: 0.166667, mean_q: -78.301842, mean_eps: 0.100000\n",
      " 29645/50000: episode: 6847, duration: 0.016s, episode steps:   4, steps per second: 257, episode reward: -1238.000, mean reward: -309.500 [-999.000, -60.000], mean action: 1.500 [0.000, 3.000],  loss: 4597.442993, mae: 568.415833, accuracy: 0.171875, mean_q: -99.092197, mean_eps: 0.100000\n",
      " 29648/50000: episode: 6848, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 5464.834473, mae: 571.429952, accuracy: 0.166667, mean_q: -99.390244, mean_eps: 0.100000\n",
      " 29651/50000: episode: 6849, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 5380.868490, mae: 557.007792, accuracy: 0.125000, mean_q: -93.048726, mean_eps: 0.100000\n",
      " 29654/50000: episode: 6850, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 3685.890544, mae: 559.076029, accuracy: 0.114583, mean_q: -94.153498, mean_eps: 0.100000\n",
      " 29658/50000: episode: 6851, duration: 0.016s, episode steps:   4, steps per second: 255, episode reward: -1238.000, mean reward: -309.500 [-999.000, -60.000], mean action: 1.500 [0.000, 3.000],  loss: 4820.835693, mae: 570.950989, accuracy: 0.164062, mean_q: -69.159544, mean_eps: 0.100000\n",
      " 29661/50000: episode: 6852, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 5114.014893, mae: 571.975972, accuracy: 0.062500, mean_q: -77.491890, mean_eps: 0.100000\n",
      " 29664/50000: episode: 6853, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 5223.605957, mae: 586.718872, accuracy: 0.135417, mean_q: -71.690475, mean_eps: 0.100000\n",
      " 29667/50000: episode: 6854, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 4631.343750, mae: 579.741394, accuracy: 0.125000, mean_q: -86.552841, mean_eps: 0.100000\n",
      " 29670/50000: episode: 6855, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 5590.588542, mae: 575.403117, accuracy: 0.166667, mean_q: -89.508362, mean_eps: 0.100000\n",
      " 29673/50000: episode: 6856, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 4512.098145, mae: 582.812826, accuracy: 0.114583, mean_q: -91.830978, mean_eps: 0.100000\n",
      " 29677/50000: episode: 6857, duration: 0.021s, episode steps:   4, steps per second: 192, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 4989.633057, mae: 588.513489, accuracy: 0.117188, mean_q: -83.667933, mean_eps: 0.100000\n",
      " 29680/50000: episode: 6858, duration: 0.018s, episode steps:   3, steps per second: 166, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 5096.211100, mae: 582.045471, accuracy: 0.125000, mean_q: -95.543429, mean_eps: 0.100000\n",
      " 29683/50000: episode: 6859, duration: 0.015s, episode steps:   3, steps per second: 200, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 4442.995850, mae: 598.889221, accuracy: 0.156250, mean_q: -90.046888, mean_eps: 0.100000\n",
      " 29686/50000: episode: 6860, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 6022.273438, mae: 601.724874, accuracy: 0.135417, mean_q: -91.055954, mean_eps: 0.100000\n",
      " 29689/50000: episode: 6861, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 4370.808024, mae: 563.301493, accuracy: 0.135417, mean_q: -91.894338, mean_eps: 0.100000\n",
      " 29692/50000: episode: 6862, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 4434.621338, mae: 574.334554, accuracy: 0.114583, mean_q: -93.380175, mean_eps: 0.100000\n",
      " 29695/50000: episode: 6863, duration: 0.012s, episode steps:   3, steps per second: 253, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 4740.925618, mae: 573.294820, accuracy: 0.114583, mean_q: -95.023783, mean_eps: 0.100000\n",
      " 29698/50000: episode: 6864, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 4863.926188, mae: 549.096110, accuracy: 0.125000, mean_q: -87.078908, mean_eps: 0.100000\n",
      " 29701/50000: episode: 6865, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 5870.542318, mae: 562.042358, accuracy: 0.114583, mean_q: -89.082258, mean_eps: 0.100000\n",
      " 29705/50000: episode: 6866, duration: 0.016s, episode steps:   4, steps per second: 253, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 6082.301270, mae: 540.150375, accuracy: 0.187500, mean_q: -98.445584, mean_eps: 0.100000\n",
      " 29708/50000: episode: 6867, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 3744.053548, mae: 554.762329, accuracy: 0.114583, mean_q: -79.722309, mean_eps: 0.100000\n",
      " 29712/50000: episode: 6868, duration: 0.016s, episode steps:   4, steps per second: 255, episode reward: -1238.000, mean reward: -309.500 [-999.000, -60.000], mean action: 2.250 [1.000, 3.000],  loss: 5422.388428, mae: 556.687088, accuracy: 0.148438, mean_q: -69.898758, mean_eps: 0.100000\n",
      " 29715/50000: episode: 6869, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 4014.518311, mae: 572.672750, accuracy: 0.145833, mean_q: -91.412641, mean_eps: 0.100000\n",
      " 29718/50000: episode: 6870, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.333 [0.000, 3.000],  loss: 4506.735433, mae: 591.010356, accuracy: 0.156250, mean_q: -91.089991, mean_eps: 0.100000\n",
      " 29721/50000: episode: 6871, duration: 0.013s, episode steps:   3, steps per second: 227, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 4910.388184, mae: 578.818624, accuracy: 0.104167, mean_q: -110.139089, mean_eps: 0.100000\n",
      " 29724/50000: episode: 6872, duration: 0.013s, episode steps:   3, steps per second: 225, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 3239.368734, mae: 567.458069, accuracy: 0.135417, mean_q: -109.889036, mean_eps: 0.100000\n",
      " 29727/50000: episode: 6873, duration: 0.018s, episode steps:   3, steps per second: 167, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 4087.856120, mae: 579.341512, accuracy: 0.104167, mean_q: -106.627983, mean_eps: 0.100000\n",
      " 29730/50000: episode: 6874, duration: 0.015s, episode steps:   3, steps per second: 206, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 5388.032145, mae: 567.414185, accuracy: 0.156250, mean_q: -87.790187, mean_eps: 0.100000\n",
      " 29733/50000: episode: 6875, duration: 0.013s, episode steps:   3, steps per second: 223, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 4836.494141, mae: 567.246724, accuracy: 0.114583, mean_q: -91.156347, mean_eps: 0.100000\n",
      " 29737/50000: episode: 6876, duration: 0.015s, episode steps:   4, steps per second: 263, episode reward: -1238.000, mean reward: -309.500 [-999.000, -60.000], mean action: 2.250 [1.000, 3.000],  loss: 5187.785889, mae: 576.444046, accuracy: 0.156250, mean_q: -78.771894, mean_eps: 0.100000\n",
      " 29740/50000: episode: 6877, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 3733.702230, mae: 555.193583, accuracy: 0.093750, mean_q: -100.116175, mean_eps: 0.100000\n",
      " 29743/50000: episode: 6878, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 4316.712240, mae: 591.574463, accuracy: 0.104167, mean_q: -87.807297, mean_eps: 0.100000\n",
      " 29746/50000: episode: 6879, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 4453.310465, mae: 571.878194, accuracy: 0.156250, mean_q: -98.847056, mean_eps: 0.100000\n",
      " 29750/50000: episode: 6880, duration: 0.016s, episode steps:   4, steps per second: 255, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 4670.115112, mae: 574.355957, accuracy: 0.164062, mean_q: -94.556761, mean_eps: 0.100000\n",
      " 29753/50000: episode: 6881, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 4604.381022, mae: 590.299479, accuracy: 0.062500, mean_q: -81.116330, mean_eps: 0.100000\n",
      " 29757/50000: episode: 6882, duration: 0.023s, episode steps:   4, steps per second: 176, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 5160.063416, mae: 577.269791, accuracy: 0.132812, mean_q: -85.187419, mean_eps: 0.100000\n",
      " 29760/50000: episode: 6883, duration: 0.014s, episode steps:   3, steps per second: 213, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 4439.267415, mae: 590.244263, accuracy: 0.187500, mean_q: -81.710345, mean_eps: 0.100000\n",
      " 29763/50000: episode: 6884, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 6174.188477, mae: 568.542562, accuracy: 0.125000, mean_q: -98.827522, mean_eps: 0.100000\n",
      " 29767/50000: episode: 6885, duration: 0.016s, episode steps:   4, steps per second: 256, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 4504.951172, mae: 579.553604, accuracy: 0.093750, mean_q: -102.432175, mean_eps: 0.100000\n",
      " 29770/50000: episode: 6886, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 4090.726074, mae: 578.202067, accuracy: 0.145833, mean_q: -78.163277, mean_eps: 0.100000\n",
      " 29773/50000: episode: 6887, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 6031.577474, mae: 570.187988, accuracy: 0.135417, mean_q: -68.319280, mean_eps: 0.100000\n",
      " 29777/50000: episode: 6888, duration: 0.019s, episode steps:   4, steps per second: 216, episode reward: -1223.000, mean reward: -305.750 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 4209.673340, mae: 564.204285, accuracy: 0.164062, mean_q: -104.108288, mean_eps: 0.100000\n",
      " 29780/50000: episode: 6889, duration: 0.015s, episode steps:   3, steps per second: 205, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 4263.727214, mae: 568.492859, accuracy: 0.145833, mean_q: -98.544484, mean_eps: 0.100000\n",
      " 29783/50000: episode: 6890, duration: 0.013s, episode steps:   3, steps per second: 230, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 4023.635010, mae: 582.344910, accuracy: 0.145833, mean_q: -88.367569, mean_eps: 0.100000\n",
      " 29786/50000: episode: 6891, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 3704.788249, mae: 576.200745, accuracy: 0.083333, mean_q: -74.456650, mean_eps: 0.100000\n",
      " 29789/50000: episode: 6892, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 4563.066406, mae: 558.835815, accuracy: 0.208333, mean_q: -84.683182, mean_eps: 0.100000\n",
      " 29792/50000: episode: 6893, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 4895.556641, mae: 567.295756, accuracy: 0.114583, mean_q: -105.555013, mean_eps: 0.100000\n",
      " 29795/50000: episode: 6894, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 3135.138102, mae: 565.101969, accuracy: 0.156250, mean_q: -90.318741, mean_eps: 0.100000\n",
      " 29798/50000: episode: 6895, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 4599.276774, mae: 577.001546, accuracy: 0.125000, mean_q: -81.173780, mean_eps: 0.100000\n",
      " 29803/50000: episode: 6896, duration: 0.019s, episode steps:   5, steps per second: 267, episode reward: -2193.000, mean reward: -438.600 [-999.000, -45.000], mean action: 1.800 [0.000, 3.000],  loss: 5481.655371, mae: 569.386157, accuracy: 0.125000, mean_q: -96.445639, mean_eps: 0.100000\n",
      " 29806/50000: episode: 6897, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 4574.184082, mae: 585.979553, accuracy: 0.072917, mean_q: -103.509684, mean_eps: 0.100000\n",
      " 29809/50000: episode: 6898, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 5659.426921, mae: 566.717997, accuracy: 0.208333, mean_q: -91.479863, mean_eps: 0.100000\n",
      " 29813/50000: episode: 6899, duration: 0.017s, episode steps:   4, steps per second: 242, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 4289.249329, mae: 571.985718, accuracy: 0.101562, mean_q: -100.835934, mean_eps: 0.100000\n",
      " 29816/50000: episode: 6900, duration: 0.015s, episode steps:   3, steps per second: 202, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 4690.859538, mae: 553.463765, accuracy: 0.156250, mean_q: -115.155556, mean_eps: 0.100000\n",
      " 29819/50000: episode: 6901, duration: 0.015s, episode steps:   3, steps per second: 197, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 4283.688151, mae: 579.443909, accuracy: 0.125000, mean_q: -89.458547, mean_eps: 0.100000\n",
      " 29822/50000: episode: 6902, duration: 0.016s, episode steps:   3, steps per second: 191, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 3913.068685, mae: 585.196940, accuracy: 0.125000, mean_q: -68.835744, mean_eps: 0.100000\n",
      " 29825/50000: episode: 6903, duration: 0.021s, episode steps:   3, steps per second: 144, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 4175.061686, mae: 565.461019, accuracy: 0.093750, mean_q: -114.319918, mean_eps: 0.100000\n",
      " 29828/50000: episode: 6904, duration: 0.016s, episode steps:   3, steps per second: 185, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 4078.225342, mae: 579.624471, accuracy: 0.145833, mean_q: -102.676895, mean_eps: 0.100000\n",
      " 29831/50000: episode: 6905, duration: 0.014s, episode steps:   3, steps per second: 212, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 5079.145508, mae: 592.726786, accuracy: 0.145833, mean_q: -84.341899, mean_eps: 0.100000\n",
      " 29834/50000: episode: 6906, duration: 0.013s, episode steps:   3, steps per second: 223, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 3730.476562, mae: 572.526835, accuracy: 0.135417, mean_q: -91.749090, mean_eps: 0.100000\n",
      " 29837/50000: episode: 6907, duration: 0.013s, episode steps:   3, steps per second: 224, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 4511.144450, mae: 569.002136, accuracy: 0.083333, mean_q: -102.237178, mean_eps: 0.100000\n",
      " 29840/50000: episode: 6908, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 4738.490885, mae: 582.974426, accuracy: 0.114583, mean_q: -79.845657, mean_eps: 0.100000\n",
      " 29843/50000: episode: 6909, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -224.000, mean reward: -74.667 [-116.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 3922.075277, mae: 556.342407, accuracy: 0.187500, mean_q: -86.663043, mean_eps: 0.100000\n",
      " 29847/50000: episode: 6910, duration: 0.016s, episode steps:   4, steps per second: 258, episode reward: -1238.000, mean reward: -309.500 [-999.000, -60.000], mean action: 1.500 [0.000, 3.000],  loss: 3879.074585, mae: 577.790634, accuracy: 0.148438, mean_q: -76.084545, mean_eps: 0.100000\n",
      " 29850/50000: episode: 6911, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 3948.322591, mae: 583.460307, accuracy: 0.062500, mean_q: -98.578328, mean_eps: 0.100000\n",
      " 29853/50000: episode: 6912, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 4108.892415, mae: 582.694438, accuracy: 0.135417, mean_q: -88.815191, mean_eps: 0.100000\n",
      " 29856/50000: episode: 6913, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 3352.434814, mae: 595.068339, accuracy: 0.125000, mean_q: -86.064303, mean_eps: 0.100000\n",
      " 29859/50000: episode: 6914, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 3982.960286, mae: 564.801493, accuracy: 0.083333, mean_q: -107.267047, mean_eps: 0.100000\n",
      " 29862/50000: episode: 6915, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 3744.392334, mae: 570.536804, accuracy: 0.114583, mean_q: -83.401281, mean_eps: 0.100000\n",
      " 29865/50000: episode: 6916, duration: 0.013s, episode steps:   3, steps per second: 227, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 4963.200439, mae: 572.812256, accuracy: 0.187500, mean_q: -86.266327, mean_eps: 0.100000\n",
      " 29868/50000: episode: 6917, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 3989.221680, mae: 574.474019, accuracy: 0.093750, mean_q: -91.576243, mean_eps: 0.100000\n",
      " 29871/50000: episode: 6918, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 4524.706380, mae: 569.766907, accuracy: 0.125000, mean_q: -95.384931, mean_eps: 0.100000\n",
      " 29875/50000: episode: 6919, duration: 0.016s, episode steps:   4, steps per second: 244, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 5051.601379, mae: 594.739792, accuracy: 0.140625, mean_q: -86.191757, mean_eps: 0.100000\n",
      " 29878/50000: episode: 6920, duration: 0.015s, episode steps:   3, steps per second: 198, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 4992.203613, mae: 568.659790, accuracy: 0.114583, mean_q: -95.976911, mean_eps: 0.100000\n",
      " 29881/50000: episode: 6921, duration: 0.014s, episode steps:   3, steps per second: 220, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 3354.568034, mae: 570.410685, accuracy: 0.145833, mean_q: -88.510394, mean_eps: 0.100000\n",
      " 29884/50000: episode: 6922, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 4021.463216, mae: 577.467244, accuracy: 0.166667, mean_q: -101.095983, mean_eps: 0.100000\n",
      " 29888/50000: episode: 6923, duration: 0.016s, episode steps:   4, steps per second: 250, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 4931.843689, mae: 586.890167, accuracy: 0.101562, mean_q: -83.183221, mean_eps: 0.100000\n",
      " 29892/50000: episode: 6924, duration: 0.016s, episode steps:   4, steps per second: 258, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 4582.419434, mae: 568.675812, accuracy: 0.132812, mean_q: -90.589338, mean_eps: 0.100000\n",
      " 29895/50000: episode: 6925, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 3968.475586, mae: 574.650330, accuracy: 0.114583, mean_q: -99.301786, mean_eps: 0.100000\n",
      " 29898/50000: episode: 6926, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 3885.273763, mae: 586.802938, accuracy: 0.125000, mean_q: -91.754730, mean_eps: 0.100000\n",
      " 29901/50000: episode: 6927, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 3915.311686, mae: 579.890747, accuracy: 0.083333, mean_q: -95.191218, mean_eps: 0.100000\n",
      " 29904/50000: episode: 6928, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 5370.777669, mae: 586.721252, accuracy: 0.177083, mean_q: -82.054466, mean_eps: 0.100000\n",
      " 29907/50000: episode: 6929, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 3803.721598, mae: 557.443298, accuracy: 0.072917, mean_q: -115.945892, mean_eps: 0.100000\n",
      " 29910/50000: episode: 6930, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 4660.565348, mae: 569.891866, accuracy: 0.104167, mean_q: -93.779610, mean_eps: 0.100000\n",
      " 29913/50000: episode: 6931, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 4865.115072, mae: 580.438578, accuracy: 0.104167, mean_q: -103.816778, mean_eps: 0.100000\n",
      " 29916/50000: episode: 6932, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 4233.217448, mae: 592.560445, accuracy: 0.166667, mean_q: -85.865501, mean_eps: 0.100000\n",
      " 29919/50000: episode: 6933, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 3335.428060, mae: 567.196309, accuracy: 0.083333, mean_q: -86.163792, mean_eps: 0.100000\n",
      " 29922/50000: episode: 6934, duration: 0.018s, episode steps:   3, steps per second: 168, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 4166.814290, mae: 598.746623, accuracy: 0.125000, mean_q: -88.066584, mean_eps: 0.100000\n",
      " 29925/50000: episode: 6935, duration: 0.019s, episode steps:   3, steps per second: 154, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 5102.068685, mae: 553.329000, accuracy: 0.135417, mean_q: -103.584053, mean_eps: 0.100000\n",
      " 29929/50000: episode: 6936, duration: 0.018s, episode steps:   4, steps per second: 226, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 4409.096252, mae: 583.761230, accuracy: 0.156250, mean_q: -86.562370, mean_eps: 0.100000\n",
      " 29932/50000: episode: 6937, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 5133.509115, mae: 564.395060, accuracy: 0.166667, mean_q: -93.105858, mean_eps: 0.100000\n",
      " 29935/50000: episode: 6938, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 4613.979167, mae: 564.776937, accuracy: 0.114583, mean_q: -105.213013, mean_eps: 0.100000\n",
      " 29938/50000: episode: 6939, duration: 0.014s, episode steps:   3, steps per second: 216, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 4995.401367, mae: 577.792155, accuracy: 0.083333, mean_q: -82.051778, mean_eps: 0.100000\n",
      " 29941/50000: episode: 6940, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 3971.707520, mae: 573.279765, accuracy: 0.093750, mean_q: -91.105789, mean_eps: 0.100000\n",
      " 29944/50000: episode: 6941, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 5104.373372, mae: 582.526449, accuracy: 0.114583, mean_q: -79.933641, mean_eps: 0.100000\n",
      " 29947/50000: episode: 6942, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 2638.967936, mae: 573.039530, accuracy: 0.093750, mean_q: -95.646166, mean_eps: 0.100000\n",
      " 29950/50000: episode: 6943, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 3767.713542, mae: 572.002563, accuracy: 0.093750, mean_q: -109.062922, mean_eps: 0.100000\n",
      " 29953/50000: episode: 6944, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 4868.137370, mae: 574.180216, accuracy: 0.125000, mean_q: -119.334864, mean_eps: 0.100000\n",
      " 29956/50000: episode: 6945, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 4186.469727, mae: 584.032694, accuracy: 0.083333, mean_q: -81.726799, mean_eps: 0.100000\n",
      " 29959/50000: episode: 6946, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 4105.423665, mae: 570.948242, accuracy: 0.218750, mean_q: -76.093731, mean_eps: 0.100000\n",
      " 29962/50000: episode: 6947, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 4395.724447, mae: 555.719849, accuracy: 0.125000, mean_q: -119.383080, mean_eps: 0.100000\n",
      " 29965/50000: episode: 6948, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 3312.835693, mae: 594.270854, accuracy: 0.166667, mean_q: -85.646416, mean_eps: 0.100000\n",
      " 29968/50000: episode: 6949, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 4108.486979, mae: 562.156718, accuracy: 0.104167, mean_q: -95.031286, mean_eps: 0.100000\n",
      " 29971/50000: episode: 6950, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 3967.214762, mae: 586.622986, accuracy: 0.093750, mean_q: -108.252520, mean_eps: 0.100000\n",
      " 29975/50000: episode: 6951, duration: 0.016s, episode steps:   4, steps per second: 248, episode reward: -1238.000, mean reward: -309.500 [-999.000, -60.000], mean action: 1.750 [1.000, 3.000],  loss: 3878.018127, mae: 589.382553, accuracy: 0.109375, mean_q: -106.618055, mean_eps: 0.100000\n",
      " 29978/50000: episode: 6952, duration: 0.014s, episode steps:   3, steps per second: 213, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 3212.505941, mae: 586.802205, accuracy: 0.062500, mean_q: -103.804688, mean_eps: 0.100000\n",
      " 29981/50000: episode: 6953, duration: 0.014s, episode steps:   3, steps per second: 211, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 4804.018880, mae: 584.430359, accuracy: 0.135417, mean_q: -97.034615, mean_eps: 0.100000\n",
      " 29984/50000: episode: 6954, duration: 0.013s, episode steps:   3, steps per second: 225, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 3465.577148, mae: 578.564697, accuracy: 0.114583, mean_q: -84.446650, mean_eps: 0.100000\n",
      " 29987/50000: episode: 6955, duration: 0.015s, episode steps:   3, steps per second: 200, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 3598.217204, mae: 584.418660, accuracy: 0.135417, mean_q: -78.841965, mean_eps: 0.100000\n",
      " 29990/50000: episode: 6956, duration: 0.019s, episode steps:   3, steps per second: 156, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 3847.690023, mae: 566.389648, accuracy: 0.145833, mean_q: -99.717089, mean_eps: 0.100000\n",
      " 29993/50000: episode: 6957, duration: 0.015s, episode steps:   3, steps per second: 199, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 3776.074626, mae: 576.778564, accuracy: 0.135417, mean_q: -110.487261, mean_eps: 0.100000\n",
      " 29996/50000: episode: 6958, duration: 0.014s, episode steps:   3, steps per second: 209, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 3085.287191, mae: 564.730387, accuracy: 0.104167, mean_q: -105.262945, mean_eps: 0.100000\n",
      " 29999/50000: episode: 6959, duration: 0.015s, episode steps:   3, steps per second: 195, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 3488.171712, mae: 560.358988, accuracy: 0.052083, mean_q: -99.244878, mean_eps: 0.100000\n",
      " 30002/50000: episode: 6960, duration: 0.013s, episode steps:   3, steps per second: 226, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 4649.288330, mae: 570.636475, accuracy: 0.062500, mean_q: -84.848831, mean_eps: 0.100000\n",
      " 30005/50000: episode: 6961, duration: 0.013s, episode steps:   3, steps per second: 226, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.333 [0.000, 3.000],  loss: 3476.676025, mae: 559.417643, accuracy: 0.093750, mean_q: -93.931307, mean_eps: 0.100000\n",
      " 30008/50000: episode: 6962, duration: 0.015s, episode steps:   3, steps per second: 195, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 3624.654785, mae: 571.012227, accuracy: 0.093750, mean_q: -102.267044, mean_eps: 0.100000\n",
      " 30011/50000: episode: 6963, duration: 0.014s, episode steps:   3, steps per second: 216, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 4231.984375, mae: 578.970276, accuracy: 0.125000, mean_q: -86.320624, mean_eps: 0.100000\n",
      " 30014/50000: episode: 6964, duration: 0.015s, episode steps:   3, steps per second: 203, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 3489.339518, mae: 564.344503, accuracy: 0.166667, mean_q: -85.080391, mean_eps: 0.100000\n",
      " 30017/50000: episode: 6965, duration: 0.021s, episode steps:   3, steps per second: 141, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 3999.025228, mae: 581.586121, accuracy: 0.083333, mean_q: -98.281092, mean_eps: 0.100000\n",
      " 30020/50000: episode: 6966, duration: 0.016s, episode steps:   3, steps per second: 189, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 4176.887044, mae: 576.112935, accuracy: 0.156250, mean_q: -95.267494, mean_eps: 0.100000\n",
      " 30023/50000: episode: 6967, duration: 0.014s, episode steps:   3, steps per second: 209, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 4071.171631, mae: 561.514526, accuracy: 0.114583, mean_q: -107.982104, mean_eps: 0.100000\n",
      " 30026/50000: episode: 6968, duration: 0.014s, episode steps:   3, steps per second: 211, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 4030.135254, mae: 571.892660, accuracy: 0.125000, mean_q: -105.180580, mean_eps: 0.100000\n",
      " 30029/50000: episode: 6969, duration: 0.013s, episode steps:   3, steps per second: 222, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 4389.662435, mae: 573.655823, accuracy: 0.177083, mean_q: -99.797195, mean_eps: 0.100000\n",
      " 30032/50000: episode: 6970, duration: 0.014s, episode steps:   3, steps per second: 212, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 2520.384725, mae: 588.569580, accuracy: 0.072917, mean_q: -91.535461, mean_eps: 0.100000\n",
      " 30035/50000: episode: 6971, duration: 0.013s, episode steps:   3, steps per second: 226, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 5166.116374, mae: 587.991618, accuracy: 0.114583, mean_q: -92.832085, mean_eps: 0.100000\n",
      " 30038/50000: episode: 6972, duration: 0.014s, episode steps:   3, steps per second: 211, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 4274.198568, mae: 585.106689, accuracy: 0.093750, mean_q: -90.695445, mean_eps: 0.100000\n",
      " 30041/50000: episode: 6973, duration: 0.014s, episode steps:   3, steps per second: 218, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 4293.046956, mae: 565.356242, accuracy: 0.166667, mean_q: -93.612508, mean_eps: 0.100000\n",
      " 30044/50000: episode: 6974, duration: 0.014s, episode steps:   3, steps per second: 211, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 4718.006348, mae: 574.444051, accuracy: 0.135417, mean_q: -97.396057, mean_eps: 0.100000\n",
      " 30048/50000: episode: 6975, duration: 0.017s, episode steps:   4, steps per second: 235, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 4010.111328, mae: 566.682693, accuracy: 0.148438, mean_q: -88.567057, mean_eps: 0.100000\n",
      " 30051/50000: episode: 6976, duration: 0.015s, episode steps:   3, steps per second: 201, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 3807.848551, mae: 564.955485, accuracy: 0.135417, mean_q: -97.853231, mean_eps: 0.100000\n",
      " 30056/50000: episode: 6977, duration: 0.021s, episode steps:   5, steps per second: 243, episode reward: -2193.000, mean reward: -438.600 [-999.000, -32.000], mean action: 1.400 [0.000, 3.000],  loss: 3887.825439, mae: 568.744263, accuracy: 0.162500, mean_q: -108.532861, mean_eps: 0.100000\n",
      " 30059/50000: episode: 6978, duration: 0.015s, episode steps:   3, steps per second: 202, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 5199.249023, mae: 597.952271, accuracy: 0.104167, mean_q: -97.240865, mean_eps: 0.100000\n",
      " 30062/50000: episode: 6979, duration: 0.017s, episode steps:   3, steps per second: 181, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 3184.938883, mae: 566.684082, accuracy: 0.125000, mean_q: -96.655591, mean_eps: 0.100000\n",
      " 30065/50000: episode: 6980, duration: 0.016s, episode steps:   3, steps per second: 186, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 3692.212321, mae: 584.776388, accuracy: 0.093750, mean_q: -115.949257, mean_eps: 0.100000\n",
      " 30068/50000: episode: 6981, duration: 0.015s, episode steps:   3, steps per second: 204, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 2593.516561, mae: 585.829651, accuracy: 0.072917, mean_q: -102.448942, mean_eps: 0.100000\n",
      " 30071/50000: episode: 6982, duration: 0.014s, episode steps:   3, steps per second: 220, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 3283.777669, mae: 545.237488, accuracy: 0.177083, mean_q: -94.033638, mean_eps: 0.100000\n",
      " 30075/50000: episode: 6983, duration: 0.017s, episode steps:   4, steps per second: 237, episode reward: -1223.000, mean reward: -305.750 [-999.000, -32.000], mean action: 1.000 [0.000, 3.000],  loss: 3896.161499, mae: 561.690826, accuracy: 0.132812, mean_q: -105.003336, mean_eps: 0.100000\n",
      " 30078/50000: episode: 6984, duration: 0.014s, episode steps:   3, steps per second: 217, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 3544.429850, mae: 587.332031, accuracy: 0.135417, mean_q: -81.132500, mean_eps: 0.100000\n",
      " 30081/50000: episode: 6985, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 3414.998454, mae: 578.063822, accuracy: 0.072917, mean_q: -95.775772, mean_eps: 0.100000\n",
      " 30084/50000: episode: 6986, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 2659.052979, mae: 578.791097, accuracy: 0.062500, mean_q: -104.379509, mean_eps: 0.100000\n",
      " 30087/50000: episode: 6987, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 3629.811361, mae: 586.833516, accuracy: 0.072917, mean_q: -94.913472, mean_eps: 0.100000\n",
      " 30090/50000: episode: 6988, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 3818.883382, mae: 562.017131, accuracy: 0.114583, mean_q: -103.210503, mean_eps: 0.100000\n",
      " 30093/50000: episode: 6989, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 3544.690837, mae: 591.699158, accuracy: 0.010417, mean_q: -105.307610, mean_eps: 0.100000\n",
      " 30096/50000: episode: 6990, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 4196.493001, mae: 578.658875, accuracy: 0.083333, mean_q: -109.055064, mean_eps: 0.100000\n",
      " 30099/50000: episode: 6991, duration: 0.014s, episode steps:   3, steps per second: 214, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 3253.757731, mae: 585.626750, accuracy: 0.114583, mean_q: -108.136108, mean_eps: 0.100000\n",
      " 30103/50000: episode: 6992, duration: 0.016s, episode steps:   4, steps per second: 253, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 4309.194580, mae: 573.390152, accuracy: 0.164062, mean_q: -90.506712, mean_eps: 0.100000\n",
      " 30107/50000: episode: 6993, duration: 0.021s, episode steps:   4, steps per second: 193, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 4772.767700, mae: 570.201660, accuracy: 0.148438, mean_q: -86.775181, mean_eps: 0.100000\n",
      " 30110/50000: episode: 6994, duration: 0.021s, episode steps:   3, steps per second: 141, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 4964.437012, mae: 567.278015, accuracy: 0.177083, mean_q: -82.566043, mean_eps: 0.100000\n",
      " 30113/50000: episode: 6995, duration: 0.013s, episode steps:   3, steps per second: 226, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 3588.432292, mae: 569.742188, accuracy: 0.135417, mean_q: -90.660929, mean_eps: 0.100000\n",
      " 30116/50000: episode: 6996, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 2934.162598, mae: 571.715515, accuracy: 0.093750, mean_q: -103.787921, mean_eps: 0.100000\n",
      " 30119/50000: episode: 6997, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 3803.462728, mae: 588.055298, accuracy: 0.114583, mean_q: -94.950053, mean_eps: 0.100000\n",
      " 30122/50000: episode: 6998, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 4106.968913, mae: 574.615153, accuracy: 0.135417, mean_q: -107.166056, mean_eps: 0.100000\n",
      " 30125/50000: episode: 6999, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 3933.996826, mae: 594.446167, accuracy: 0.104167, mean_q: -105.007551, mean_eps: 0.100000\n",
      " 30128/50000: episode: 7000, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 4225.065430, mae: 560.944499, accuracy: 0.218750, mean_q: -106.530551, mean_eps: 0.100000\n",
      " 30131/50000: episode: 7001, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 4231.276204, mae: 561.251994, accuracy: 0.135417, mean_q: -95.770149, mean_eps: 0.100000\n",
      " 30134/50000: episode: 7002, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 4008.864583, mae: 583.685933, accuracy: 0.135417, mean_q: -102.990532, mean_eps: 0.100000\n",
      " 30137/50000: episode: 7003, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 4391.956217, mae: 579.846598, accuracy: 0.145833, mean_q: -95.966207, mean_eps: 0.100000\n",
      " 30140/50000: episode: 7004, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 3929.308024, mae: 602.644409, accuracy: 0.083333, mean_q: -100.927671, mean_eps: 0.100000\n",
      " 30143/50000: episode: 7005, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 3078.366455, mae: 572.810710, accuracy: 0.114583, mean_q: -113.390340, mean_eps: 0.100000\n",
      " 30146/50000: episode: 7006, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 3818.540690, mae: 561.254557, accuracy: 0.156250, mean_q: -84.100382, mean_eps: 0.100000\n",
      " 30149/50000: episode: 7007, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 3412.580729, mae: 574.521545, accuracy: 0.114583, mean_q: -104.130559, mean_eps: 0.100000\n",
      " 30152/50000: episode: 7008, duration: 0.013s, episode steps:   3, steps per second: 225, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 3888.382161, mae: 567.642517, accuracy: 0.104167, mean_q: -104.115779, mean_eps: 0.100000\n",
      " 30155/50000: episode: 7009, duration: 0.015s, episode steps:   3, steps per second: 200, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 4205.102376, mae: 576.198527, accuracy: 0.197917, mean_q: -91.237966, mean_eps: 0.100000\n",
      " 30159/50000: episode: 7010, duration: 0.018s, episode steps:   4, steps per second: 228, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 3098.022461, mae: 566.372910, accuracy: 0.117188, mean_q: -99.157400, mean_eps: 0.100000\n",
      " 30162/50000: episode: 7011, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 3425.935791, mae: 559.633565, accuracy: 0.125000, mean_q: -95.700335, mean_eps: 0.100000\n",
      " 30165/50000: episode: 7012, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 4019.711914, mae: 587.812276, accuracy: 0.197917, mean_q: -88.808847, mean_eps: 0.100000\n",
      " 30169/50000: episode: 7013, duration: 0.016s, episode steps:   4, steps per second: 248, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 4527.292969, mae: 579.928802, accuracy: 0.125000, mean_q: -97.904476, mean_eps: 0.100000\n",
      " 30172/50000: episode: 7014, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 3438.230062, mae: 577.548055, accuracy: 0.145833, mean_q: -94.233849, mean_eps: 0.100000\n",
      " 30175/50000: episode: 7015, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 3943.573730, mae: 561.864299, accuracy: 0.145833, mean_q: -110.277206, mean_eps: 0.100000\n",
      " 30178/50000: episode: 7016, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 3614.852865, mae: 575.363261, accuracy: 0.166667, mean_q: -104.987111, mean_eps: 0.100000\n",
      " 30181/50000: episode: 7017, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 3403.997803, mae: 574.502218, accuracy: 0.177083, mean_q: -102.221090, mean_eps: 0.100000\n",
      " 30187/50000: episode: 7018, duration: 0.021s, episode steps:   6, steps per second: 281, episode reward: -3192.000, mean reward: -532.000 [-999.000, -32.000], mean action: 1.167 [0.000, 3.000],  loss: 3725.538859, mae: 572.364848, accuracy: 0.098958, mean_q: -91.278931, mean_eps: 0.100000\n",
      " 30190/50000: episode: 7019, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 3745.183024, mae: 558.220785, accuracy: 0.093750, mean_q: -95.201813, mean_eps: 0.100000\n",
      " 30194/50000: episode: 7020, duration: 0.016s, episode steps:   4, steps per second: 254, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 4097.061462, mae: 567.109421, accuracy: 0.085938, mean_q: -99.538095, mean_eps: 0.100000\n",
      " 30197/50000: episode: 7021, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 4120.093424, mae: 573.592387, accuracy: 0.156250, mean_q: -103.535378, mean_eps: 0.100000\n",
      " 30200/50000: episode: 7022, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 2436.666138, mae: 572.905538, accuracy: 0.135417, mean_q: -101.874313, mean_eps: 0.100000\n",
      " 30203/50000: episode: 7023, duration: 0.013s, episode steps:   3, steps per second: 226, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 3793.044108, mae: 577.920939, accuracy: 0.125000, mean_q: -98.423014, mean_eps: 0.100000\n",
      " 30207/50000: episode: 7024, duration: 0.021s, episode steps:   4, steps per second: 190, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 3950.571899, mae: 578.716202, accuracy: 0.156250, mean_q: -97.968515, mean_eps: 0.100000\n",
      " 30210/50000: episode: 7025, duration: 0.015s, episode steps:   3, steps per second: 206, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 4209.693359, mae: 568.130656, accuracy: 0.135417, mean_q: -94.310331, mean_eps: 0.100000\n",
      " 30213/50000: episode: 7026, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 4236.808919, mae: 558.432373, accuracy: 0.093750, mean_q: -102.619514, mean_eps: 0.100000\n",
      " 30216/50000: episode: 7027, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 4410.233561, mae: 566.098348, accuracy: 0.145833, mean_q: -92.898079, mean_eps: 0.100000\n",
      " 30219/50000: episode: 7028, duration: 0.013s, episode steps:   3, steps per second: 226, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 3700.859538, mae: 571.842814, accuracy: 0.083333, mean_q: -91.647532, mean_eps: 0.100000\n",
      " 30222/50000: episode: 7029, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 3839.077393, mae: 552.210531, accuracy: 0.125000, mean_q: -99.384740, mean_eps: 0.100000\n",
      " 30226/50000: episode: 7030, duration: 0.016s, episode steps:   4, steps per second: 249, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 3024.062134, mae: 580.368515, accuracy: 0.164062, mean_q: -88.597229, mean_eps: 0.100000\n",
      " 30229/50000: episode: 7031, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 3078.338949, mae: 578.361654, accuracy: 0.125000, mean_q: -115.387276, mean_eps: 0.100000\n",
      " 30234/50000: episode: 7032, duration: 0.019s, episode steps:   5, steps per second: 265, episode reward: -2193.000, mean reward: -438.600 [-999.000, -45.000], mean action: 1.600 [0.000, 3.000],  loss: 2901.473291, mae: 585.350488, accuracy: 0.093750, mean_q: -97.398537, mean_eps: 0.100000\n",
      " 30238/50000: episode: 7033, duration: 0.015s, episode steps:   4, steps per second: 258, episode reward: -1238.000, mean reward: -309.500 [-999.000, -60.000], mean action: 1.500 [0.000, 3.000],  loss: 3013.546021, mae: 579.438583, accuracy: 0.109375, mean_q: -102.668890, mean_eps: 0.100000\n",
      " 30242/50000: episode: 7034, duration: 0.016s, episode steps:   4, steps per second: 257, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 3501.885254, mae: 569.798401, accuracy: 0.140625, mean_q: -96.404263, mean_eps: 0.100000\n",
      " 30245/50000: episode: 7035, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 3905.717611, mae: 575.373739, accuracy: 0.156250, mean_q: -97.046928, mean_eps: 0.100000\n",
      " 30248/50000: episode: 7036, duration: 0.013s, episode steps:   3, steps per second: 230, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 3286.764974, mae: 598.064982, accuracy: 0.083333, mean_q: -94.033259, mean_eps: 0.100000\n",
      " 30251/50000: episode: 7037, duration: 0.024s, episode steps:   3, steps per second: 127, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 4245.599365, mae: 574.086955, accuracy: 0.166667, mean_q: -103.706088, mean_eps: 0.100000\n",
      " 30254/50000: episode: 7038, duration: 0.018s, episode steps:   3, steps per second: 168, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 3770.693604, mae: 579.791219, accuracy: 0.093750, mean_q: -112.092573, mean_eps: 0.100000\n",
      " 30257/50000: episode: 7039, duration: 0.013s, episode steps:   3, steps per second: 222, episode reward: -224.000, mean reward: -74.667 [-116.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 4020.250326, mae: 565.792338, accuracy: 0.062500, mean_q: -116.822464, mean_eps: 0.100000\n",
      " 30260/50000: episode: 7040, duration: 0.013s, episode steps:   3, steps per second: 230, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 3095.563232, mae: 558.395752, accuracy: 0.145833, mean_q: -94.996155, mean_eps: 0.100000\n",
      " 30263/50000: episode: 7041, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 4342.824219, mae: 574.194031, accuracy: 0.104167, mean_q: -101.062976, mean_eps: 0.100000\n",
      " 30268/50000: episode: 7042, duration: 0.020s, episode steps:   5, steps per second: 251, episode reward: -2193.000, mean reward: -438.600 [-999.000, -32.000], mean action: 1.600 [0.000, 3.000],  loss: 2780.190430, mae: 577.791394, accuracy: 0.081250, mean_q: -108.705473, mean_eps: 0.100000\n",
      " 30271/50000: episode: 7043, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 2501.703695, mae: 568.436523, accuracy: 0.156250, mean_q: -92.483630, mean_eps: 0.100000\n",
      " 30274/50000: episode: 7044, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 3784.144857, mae: 563.281962, accuracy: 0.177083, mean_q: -106.457311, mean_eps: 0.100000\n",
      " 30277/50000: episode: 7045, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 3125.511149, mae: 570.593831, accuracy: 0.104167, mean_q: -97.490715, mean_eps: 0.100000\n",
      " 30283/50000: episode: 7046, duration: 0.022s, episode steps:   6, steps per second: 278, episode reward: -3192.000, mean reward: -532.000 [-999.000, -45.000], mean action: 1.833 [0.000, 3.000],  loss: 3792.432292, mae: 565.477793, accuracy: 0.109375, mean_q: -108.215195, mean_eps: 0.100000\n",
      " 30286/50000: episode: 7047, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 2869.132731, mae: 576.121053, accuracy: 0.104167, mean_q: -101.242304, mean_eps: 0.100000\n",
      " 30289/50000: episode: 7048, duration: 0.013s, episode steps:   3, steps per second: 230, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 3653.247559, mae: 583.198547, accuracy: 0.104167, mean_q: -95.552798, mean_eps: 0.100000\n",
      " 30292/50000: episode: 7049, duration: 0.015s, episode steps:   3, steps per second: 195, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 3075.903890, mae: 585.620850, accuracy: 0.125000, mean_q: -97.864375, mean_eps: 0.100000\n",
      " 30295/50000: episode: 7050, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 2347.605876, mae: 577.749817, accuracy: 0.072917, mean_q: -97.838694, mean_eps: 0.100000\n",
      " 30298/50000: episode: 7051, duration: 0.014s, episode steps:   3, steps per second: 215, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 3029.820475, mae: 600.789042, accuracy: 0.052083, mean_q: -104.009425, mean_eps: 0.100000\n",
      " 30301/50000: episode: 7052, duration: 0.014s, episode steps:   3, steps per second: 210, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 3803.639323, mae: 579.981974, accuracy: 0.093750, mean_q: -109.433062, mean_eps: 0.100000\n",
      " 30305/50000: episode: 7053, duration: 0.027s, episode steps:   4, steps per second: 147, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 2940.298462, mae: 560.129242, accuracy: 0.132812, mean_q: -101.766273, mean_eps: 0.100000\n",
      " 30310/50000: episode: 7054, duration: 0.021s, episode steps:   5, steps per second: 243, episode reward: -2193.000, mean reward: -438.600 [-999.000, -45.000], mean action: 1.200 [0.000, 2.000],  loss: 2543.595044, mae: 601.524158, accuracy: 0.075000, mean_q: -106.417023, mean_eps: 0.100000\n",
      " 30313/50000: episode: 7055, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 3013.594889, mae: 571.228984, accuracy: 0.208333, mean_q: -93.606789, mean_eps: 0.100000\n",
      " 30316/50000: episode: 7056, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 3717.819987, mae: 584.584574, accuracy: 0.062500, mean_q: -109.800613, mean_eps: 0.100000\n",
      " 30319/50000: episode: 7057, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 3284.713460, mae: 576.135701, accuracy: 0.125000, mean_q: -98.846667, mean_eps: 0.100000\n",
      " 30322/50000: episode: 7058, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 3778.698649, mae: 590.395650, accuracy: 0.125000, mean_q: -105.461116, mean_eps: 0.100000\n",
      " 30325/50000: episode: 7059, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 4045.423665, mae: 549.401591, accuracy: 0.145833, mean_q: -111.688993, mean_eps: 0.100000\n",
      " 30328/50000: episode: 7060, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 3229.619792, mae: 579.327840, accuracy: 0.145833, mean_q: -81.769239, mean_eps: 0.100000\n",
      " 30331/50000: episode: 7061, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 3669.516683, mae: 579.585673, accuracy: 0.145833, mean_q: -84.420703, mean_eps: 0.100000\n",
      " 30335/50000: episode: 7062, duration: 0.016s, episode steps:   4, steps per second: 248, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 2952.079956, mae: 577.745529, accuracy: 0.117188, mean_q: -104.691458, mean_eps: 0.100000\n",
      " 30338/50000: episode: 7063, duration: 0.013s, episode steps:   3, steps per second: 227, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 3320.982666, mae: 579.140279, accuracy: 0.104167, mean_q: -103.181691, mean_eps: 0.100000\n",
      " 30341/50000: episode: 7064, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 2841.868652, mae: 601.934611, accuracy: 0.156250, mean_q: -109.897512, mean_eps: 0.100000\n",
      " 30345/50000: episode: 7065, duration: 0.016s, episode steps:   4, steps per second: 255, episode reward: -1238.000, mean reward: -309.500 [-999.000, -60.000], mean action: 2.250 [1.000, 3.000],  loss: 3850.587036, mae: 578.600006, accuracy: 0.156250, mean_q: -110.393488, mean_eps: 0.100000\n",
      " 30348/50000: episode: 7066, duration: 0.015s, episode steps:   3, steps per second: 205, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 2470.964111, mae: 561.049052, accuracy: 0.156250, mean_q: -94.000547, mean_eps: 0.100000\n",
      " 30351/50000: episode: 7067, duration: 0.015s, episode steps:   3, steps per second: 207, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 3200.568115, mae: 570.767090, accuracy: 0.187500, mean_q: -98.102331, mean_eps: 0.100000\n",
      " 30354/50000: episode: 7068, duration: 0.014s, episode steps:   3, steps per second: 214, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 2584.598551, mae: 576.786967, accuracy: 0.145833, mean_q: -108.029498, mean_eps: 0.100000\n",
      " 30357/50000: episode: 7069, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 3195.434570, mae: 562.027690, accuracy: 0.145833, mean_q: -99.327924, mean_eps: 0.100000\n",
      " 30360/50000: episode: 7070, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 4052.968262, mae: 579.786275, accuracy: 0.135417, mean_q: -94.782275, mean_eps: 0.100000\n",
      " 30363/50000: episode: 7071, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 3773.237712, mae: 562.522013, accuracy: 0.156250, mean_q: -98.914645, mean_eps: 0.100000\n",
      " 30366/50000: episode: 7072, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 3240.128092, mae: 575.794230, accuracy: 0.125000, mean_q: -111.574727, mean_eps: 0.100000\n",
      " 30369/50000: episode: 7073, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 3168.723958, mae: 590.074300, accuracy: 0.125000, mean_q: -109.796987, mean_eps: 0.100000\n",
      " 30373/50000: episode: 7074, duration: 0.015s, episode steps:   4, steps per second: 260, episode reward: -1238.000, mean reward: -309.500 [-999.000, -60.000], mean action: 1.500 [0.000, 3.000],  loss: 3342.588318, mae: 578.768921, accuracy: 0.140625, mean_q: -111.368429, mean_eps: 0.100000\n",
      " 30376/50000: episode: 7075, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 2923.086914, mae: 570.949565, accuracy: 0.104167, mean_q: -108.886759, mean_eps: 0.100000\n",
      " 30379/50000: episode: 7076, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 2716.340617, mae: 570.232910, accuracy: 0.125000, mean_q: -102.092743, mean_eps: 0.100000\n",
      " 30382/50000: episode: 7077, duration: 0.014s, episode steps:   3, steps per second: 221, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 3230.255615, mae: 589.488403, accuracy: 0.104167, mean_q: -108.003370, mean_eps: 0.100000\n",
      " 30385/50000: episode: 7078, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 2813.927734, mae: 578.998332, accuracy: 0.166667, mean_q: -121.188723, mean_eps: 0.100000\n",
      " 30389/50000: episode: 7079, duration: 0.016s, episode steps:   4, steps per second: 249, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 3834.461548, mae: 584.128723, accuracy: 0.125000, mean_q: -118.030155, mean_eps: 0.100000\n",
      " 30392/50000: episode: 7080, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 2969.689290, mae: 581.923218, accuracy: 0.166667, mean_q: -99.698916, mean_eps: 0.100000\n",
      " 30395/50000: episode: 7081, duration: 0.017s, episode steps:   3, steps per second: 179, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.333 [0.000, 3.000],  loss: 2751.064046, mae: 564.564596, accuracy: 0.177083, mean_q: -109.721873, mean_eps: 0.100000\n",
      " 30398/50000: episode: 7082, duration: 0.016s, episode steps:   3, steps per second: 188, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 2747.573771, mae: 600.628805, accuracy: 0.125000, mean_q: -98.349927, mean_eps: 0.100000\n",
      " 30402/50000: episode: 7083, duration: 0.020s, episode steps:   4, steps per second: 197, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 3110.540527, mae: 582.562225, accuracy: 0.109375, mean_q: -108.633373, mean_eps: 0.100000\n",
      " 30405/50000: episode: 7084, duration: 0.015s, episode steps:   3, steps per second: 201, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 2408.118327, mae: 568.448039, accuracy: 0.114583, mean_q: -93.264379, mean_eps: 0.100000\n",
      " 30408/50000: episode: 7085, duration: 0.014s, episode steps:   3, steps per second: 211, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 3151.030111, mae: 588.807251, accuracy: 0.093750, mean_q: -96.856585, mean_eps: 0.100000\n",
      " 30411/50000: episode: 7086, duration: 0.014s, episode steps:   3, steps per second: 212, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 2916.854167, mae: 576.613647, accuracy: 0.125000, mean_q: -110.721064, mean_eps: 0.100000\n",
      " 30414/50000: episode: 7087, duration: 0.014s, episode steps:   3, steps per second: 217, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 3422.676025, mae: 562.800496, accuracy: 0.125000, mean_q: -103.569260, mean_eps: 0.100000\n",
      " 30417/50000: episode: 7088, duration: 0.014s, episode steps:   3, steps per second: 215, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 3040.457601, mae: 582.846720, accuracy: 0.104167, mean_q: -102.784800, mean_eps: 0.100000\n",
      " 30420/50000: episode: 7089, duration: 0.014s, episode steps:   3, steps per second: 218, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 2496.529378, mae: 602.586670, accuracy: 0.125000, mean_q: -91.392403, mean_eps: 0.100000\n",
      " 30423/50000: episode: 7090, duration: 0.013s, episode steps:   3, steps per second: 230, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 2912.726969, mae: 585.304382, accuracy: 0.093750, mean_q: -130.173991, mean_eps: 0.100000\n",
      " 30426/50000: episode: 7091, duration: 0.014s, episode steps:   3, steps per second: 214, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 2818.189697, mae: 574.450968, accuracy: 0.166667, mean_q: -113.543785, mean_eps: 0.100000\n",
      " 30429/50000: episode: 7092, duration: 0.015s, episode steps:   3, steps per second: 202, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 3454.500814, mae: 580.782939, accuracy: 0.177083, mean_q: -100.507212, mean_eps: 0.100000\n",
      " 30432/50000: episode: 7093, duration: 0.014s, episode steps:   3, steps per second: 207, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 2299.063436, mae: 575.311890, accuracy: 0.093750, mean_q: -102.901037, mean_eps: 0.100000\n",
      " 30437/50000: episode: 7094, duration: 0.021s, episode steps:   5, steps per second: 239, episode reward: -2193.000, mean reward: -438.600 [-999.000, -32.000], mean action: 1.400 [0.000, 3.000],  loss: 3158.046094, mae: 578.087988, accuracy: 0.081250, mean_q: -115.637668, mean_eps: 0.100000\n",
      " 30441/50000: episode: 7095, duration: 0.022s, episode steps:   4, steps per second: 178, episode reward: -1238.000, mean reward: -309.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 3974.252960, mae: 585.054932, accuracy: 0.085938, mean_q: -93.255743, mean_eps: 0.100000\n",
      " 30444/50000: episode: 7096, duration: 0.015s, episode steps:   3, steps per second: 203, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 3337.375041, mae: 585.104024, accuracy: 0.135417, mean_q: -98.456258, mean_eps: 0.100000\n",
      " 30447/50000: episode: 7097, duration: 0.015s, episode steps:   3, steps per second: 200, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 3512.387207, mae: 562.637105, accuracy: 0.093750, mean_q: -109.670476, mean_eps: 0.100000\n",
      " 30450/50000: episode: 7098, duration: 0.020s, episode steps:   3, steps per second: 148, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 2967.458333, mae: 576.200928, accuracy: 0.156250, mean_q: -106.089307, mean_eps: 0.100000\n",
      " 30453/50000: episode: 7099, duration: 0.018s, episode steps:   3, steps per second: 167, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 3517.628092, mae: 581.787577, accuracy: 0.104167, mean_q: -106.248817, mean_eps: 0.100000\n",
      " 30456/50000: episode: 7100, duration: 0.014s, episode steps:   3, steps per second: 216, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 3312.579753, mae: 601.051839, accuracy: 0.093750, mean_q: -92.500626, mean_eps: 0.100000\n",
      " 30459/50000: episode: 7101, duration: 0.014s, episode steps:   3, steps per second: 219, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 3030.067220, mae: 570.381592, accuracy: 0.125000, mean_q: -90.922208, mean_eps: 0.100000\n",
      " 30462/50000: episode: 7102, duration: 0.014s, episode steps:   3, steps per second: 217, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 3467.003988, mae: 578.455098, accuracy: 0.072917, mean_q: -99.657140, mean_eps: 0.100000\n",
      " 30465/50000: episode: 7103, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 2568.036784, mae: 573.516561, accuracy: 0.093750, mean_q: -116.936071, mean_eps: 0.100000\n",
      " 30468/50000: episode: 7104, duration: 0.015s, episode steps:   3, steps per second: 196, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 2885.758057, mae: 589.216654, accuracy: 0.093750, mean_q: -107.949600, mean_eps: 0.100000\n",
      " 30471/50000: episode: 7105, duration: 0.014s, episode steps:   3, steps per second: 222, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 1800.005941, mae: 597.237773, accuracy: 0.135417, mean_q: -100.883596, mean_eps: 0.100000\n",
      " 30474/50000: episode: 7106, duration: 0.013s, episode steps:   3, steps per second: 230, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 3220.454753, mae: 572.885254, accuracy: 0.135417, mean_q: -101.112274, mean_eps: 0.100000\n",
      " 30477/50000: episode: 7107, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 2644.979980, mae: 574.363892, accuracy: 0.135417, mean_q: -94.085981, mean_eps: 0.100000\n",
      " 30480/50000: episode: 7108, duration: 0.019s, episode steps:   3, steps per second: 156, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 2400.235677, mae: 579.155660, accuracy: 0.104167, mean_q: -117.252652, mean_eps: 0.100000\n",
      " 30483/50000: episode: 7109, duration: 0.020s, episode steps:   3, steps per second: 148, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 4386.068359, mae: 571.303162, accuracy: 0.145833, mean_q: -106.209831, mean_eps: 0.100000\n",
      " 30486/50000: episode: 7110, duration: 0.015s, episode steps:   3, steps per second: 195, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 2221.671875, mae: 593.070882, accuracy: 0.135417, mean_q: -98.518840, mean_eps: 0.100000\n",
      " 30489/50000: episode: 7111, duration: 0.013s, episode steps:   3, steps per second: 225, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 2834.091634, mae: 576.284220, accuracy: 0.145833, mean_q: -106.964709, mean_eps: 0.100000\n",
      " 30492/50000: episode: 7112, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 2945.706299, mae: 560.431335, accuracy: 0.156250, mean_q: -103.890144, mean_eps: 0.100000\n",
      " 30495/50000: episode: 7113, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 3203.163086, mae: 581.680969, accuracy: 0.135417, mean_q: -107.970957, mean_eps: 0.100000\n",
      " 30498/50000: episode: 7114, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 3056.299967, mae: 603.370178, accuracy: 0.114583, mean_q: -105.661621, mean_eps: 0.100000\n",
      " 30502/50000: episode: 7115, duration: 0.016s, episode steps:   4, steps per second: 254, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 2888.266113, mae: 578.386230, accuracy: 0.093750, mean_q: -105.185368, mean_eps: 0.100000\n",
      " 30505/50000: episode: 7116, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 3493.167562, mae: 583.817790, accuracy: 0.156250, mean_q: -99.721591, mean_eps: 0.100000\n",
      " 30508/50000: episode: 7117, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 2695.291667, mae: 565.481608, accuracy: 0.083333, mean_q: -110.925229, mean_eps: 0.100000\n",
      " 30511/50000: episode: 7118, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 4353.326009, mae: 595.253337, accuracy: 0.083333, mean_q: -106.432610, mean_eps: 0.100000\n",
      " 30514/50000: episode: 7119, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 3262.053792, mae: 560.032878, accuracy: 0.145833, mean_q: -110.544192, mean_eps: 0.100000\n",
      " 30517/50000: episode: 7120, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 3212.922404, mae: 587.718018, accuracy: 0.135417, mean_q: -108.259900, mean_eps: 0.100000\n",
      " 30520/50000: episode: 7121, duration: 0.013s, episode steps:   3, steps per second: 222, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 2349.172363, mae: 558.657206, accuracy: 0.135417, mean_q: -114.030062, mean_eps: 0.100000\n",
      " 30523/50000: episode: 7122, duration: 0.013s, episode steps:   3, steps per second: 223, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 4148.716878, mae: 568.609416, accuracy: 0.145833, mean_q: -109.538752, mean_eps: 0.100000\n",
      " 30527/50000: episode: 7123, duration: 0.018s, episode steps:   4, steps per second: 218, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 2711.522278, mae: 580.548920, accuracy: 0.117188, mean_q: -101.261177, mean_eps: 0.100000\n",
      " 30530/50000: episode: 7124, duration: 0.015s, episode steps:   3, steps per second: 204, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 2620.918660, mae: 552.694784, accuracy: 0.187500, mean_q: -103.841527, mean_eps: 0.100000\n",
      " 30533/50000: episode: 7125, duration: 0.014s, episode steps:   3, steps per second: 216, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 2282.964681, mae: 590.476603, accuracy: 0.125000, mean_q: -87.443932, mean_eps: 0.100000\n",
      " 30536/50000: episode: 7126, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 3311.334717, mae: 613.222046, accuracy: 0.083333, mean_q: -121.539973, mean_eps: 0.100000\n",
      " 30539/50000: episode: 7127, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 2546.788289, mae: 602.649862, accuracy: 0.093750, mean_q: -119.125946, mean_eps: 0.100000\n",
      " 30542/50000: episode: 7128, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 2813.005371, mae: 597.435120, accuracy: 0.072917, mean_q: -121.804382, mean_eps: 0.100000\n",
      " 30547/50000: episode: 7129, duration: 0.019s, episode steps:   5, steps per second: 262, episode reward: -2193.000, mean reward: -438.600 [-999.000, -32.000], mean action: 1.000 [0.000, 3.000],  loss: 2763.221338, mae: 568.767432, accuracy: 0.150000, mean_q: -108.750981, mean_eps: 0.100000\n",
      " 30551/50000: episode: 7130, duration: 0.016s, episode steps:   4, steps per second: 255, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 2357.801086, mae: 579.468658, accuracy: 0.085938, mean_q: -105.065979, mean_eps: 0.100000\n",
      " 30554/50000: episode: 7131, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 3184.446899, mae: 580.876078, accuracy: 0.093750, mean_q: -111.459035, mean_eps: 0.100000\n",
      " 30557/50000: episode: 7132, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 3220.898112, mae: 569.771200, accuracy: 0.177083, mean_q: -98.033880, mean_eps: 0.100000\n",
      " 30560/50000: episode: 7133, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 2656.178630, mae: 585.104492, accuracy: 0.104167, mean_q: -104.128100, mean_eps: 0.100000\n",
      " 30563/50000: episode: 7134, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 2088.264201, mae: 568.664062, accuracy: 0.072917, mean_q: -109.627368, mean_eps: 0.100000\n",
      " 30566/50000: episode: 7135, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 1776.546224, mae: 588.281677, accuracy: 0.072917, mean_q: -113.076706, mean_eps: 0.100000\n",
      " 30569/50000: episode: 7136, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 2954.178630, mae: 586.434875, accuracy: 0.114583, mean_q: -122.883817, mean_eps: 0.100000\n",
      " 30572/50000: episode: 7137, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 4098.956787, mae: 585.400085, accuracy: 0.135417, mean_q: -111.158488, mean_eps: 0.100000\n",
      " 30575/50000: episode: 7138, duration: 0.017s, episode steps:   3, steps per second: 180, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 2856.597819, mae: 572.739624, accuracy: 0.145833, mean_q: -122.404180, mean_eps: 0.100000\n",
      " 30578/50000: episode: 7139, duration: 0.017s, episode steps:   3, steps per second: 174, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 2880.295736, mae: 557.327820, accuracy: 0.166667, mean_q: -111.378629, mean_eps: 0.100000\n",
      " 30581/50000: episode: 7140, duration: 0.015s, episode steps:   3, steps per second: 194, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 3726.586589, mae: 569.635173, accuracy: 0.104167, mean_q: -112.723117, mean_eps: 0.100000\n",
      " 30584/50000: episode: 7141, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 2397.099365, mae: 576.426921, accuracy: 0.145833, mean_q: -93.163460, mean_eps: 0.100000\n",
      " 30588/50000: episode: 7142, duration: 0.016s, episode steps:   4, steps per second: 252, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 2689.072693, mae: 555.862152, accuracy: 0.085938, mean_q: -110.517035, mean_eps: 0.100000\n",
      " 30592/50000: episode: 7143, duration: 0.016s, episode steps:   4, steps per second: 252, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 2816.682495, mae: 593.510681, accuracy: 0.046875, mean_q: -100.813913, mean_eps: 0.100000\n",
      " 30595/50000: episode: 7144, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 2526.901530, mae: 581.133341, accuracy: 0.083333, mean_q: -111.058085, mean_eps: 0.100000\n",
      " 30598/50000: episode: 7145, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 2819.132161, mae: 613.290710, accuracy: 0.104167, mean_q: -103.350939, mean_eps: 0.100000\n",
      " 30601/50000: episode: 7146, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 2870.401286, mae: 598.746195, accuracy: 0.083333, mean_q: -120.341901, mean_eps: 0.100000\n",
      " 30604/50000: episode: 7147, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 2376.144368, mae: 582.699117, accuracy: 0.072917, mean_q: -118.743744, mean_eps: 0.100000\n",
      " 30607/50000: episode: 7148, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 2915.832845, mae: 556.299520, accuracy: 0.177083, mean_q: -112.553800, mean_eps: 0.100000\n",
      " 30610/50000: episode: 7149, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 2017.020386, mae: 578.902140, accuracy: 0.125000, mean_q: -98.958460, mean_eps: 0.100000\n",
      " 30613/50000: episode: 7150, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 2691.089518, mae: 574.220927, accuracy: 0.104167, mean_q: -105.285703, mean_eps: 0.100000\n",
      " 30616/50000: episode: 7151, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 2604.127604, mae: 573.435465, accuracy: 0.125000, mean_q: -102.643547, mean_eps: 0.100000\n",
      " 30619/50000: episode: 7152, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 3506.509359, mae: 585.556132, accuracy: 0.156250, mean_q: -101.824013, mean_eps: 0.100000\n",
      " 30622/50000: episode: 7153, duration: 0.013s, episode steps:   3, steps per second: 224, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 1892.940186, mae: 597.913940, accuracy: 0.114583, mean_q: -106.837448, mean_eps: 0.100000\n",
      " 30625/50000: episode: 7154, duration: 0.016s, episode steps:   3, steps per second: 190, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 3204.366699, mae: 576.165446, accuracy: 0.145833, mean_q: -122.227292, mean_eps: 0.100000\n",
      " 30628/50000: episode: 7155, duration: 0.016s, episode steps:   3, steps per second: 190, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 2614.449788, mae: 598.816081, accuracy: 0.166667, mean_q: -116.520673, mean_eps: 0.100000\n",
      " 30631/50000: episode: 7156, duration: 0.013s, episode steps:   3, steps per second: 225, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 2281.290120, mae: 587.967550, accuracy: 0.156250, mean_q: -116.831599, mean_eps: 0.100000\n",
      " 30635/50000: episode: 7157, duration: 0.016s, episode steps:   4, steps per second: 248, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 2.000 [0.000, 3.000],  loss: 1835.076233, mae: 568.520538, accuracy: 0.117188, mean_q: -116.991089, mean_eps: 0.100000\n",
      " 30638/50000: episode: 7158, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 3111.020345, mae: 592.902039, accuracy: 0.114583, mean_q: -103.166359, mean_eps: 0.100000\n",
      " 30641/50000: episode: 7159, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 2593.246338, mae: 577.759644, accuracy: 0.187500, mean_q: -105.070813, mean_eps: 0.100000\n",
      " 30644/50000: episode: 7160, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 2613.392985, mae: 570.156901, accuracy: 0.104167, mean_q: -114.514371, mean_eps: 0.100000\n",
      " 30647/50000: episode: 7161, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 3532.237630, mae: 566.981262, accuracy: 0.114583, mean_q: -105.711868, mean_eps: 0.100000\n",
      " 30650/50000: episode: 7162, duration: 0.016s, episode steps:   3, steps per second: 186, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 2106.271973, mae: 570.880656, accuracy: 0.166667, mean_q: -111.714208, mean_eps: 0.100000\n",
      " 30653/50000: episode: 7163, duration: 0.019s, episode steps:   3, steps per second: 160, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 3605.652425, mae: 580.985189, accuracy: 0.125000, mean_q: -119.518715, mean_eps: 0.100000\n",
      " 30656/50000: episode: 7164, duration: 0.015s, episode steps:   3, steps per second: 196, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 2.000 [1.000, 3.000],  loss: 2674.181885, mae: 608.706299, accuracy: 0.145833, mean_q: -107.539597, mean_eps: 0.100000\n",
      " 30660/50000: episode: 7165, duration: 0.016s, episode steps:   4, steps per second: 255, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 2766.398071, mae: 559.056915, accuracy: 0.171875, mean_q: -116.651188, mean_eps: 0.100000\n",
      " 30663/50000: episode: 7166, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 2075.077962, mae: 584.063354, accuracy: 0.083333, mean_q: -106.235191, mean_eps: 0.100000\n",
      " 30666/50000: episode: 7167, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 2130.415771, mae: 603.064026, accuracy: 0.135417, mean_q: -103.699585, mean_eps: 0.100000\n",
      " 30669/50000: episode: 7168, duration: 0.023s, episode steps:   3, steps per second: 132, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 2428.731771, mae: 577.048564, accuracy: 0.125000, mean_q: -119.587639, mean_eps: 0.100000\n",
      " 30672/50000: episode: 7169, duration: 0.018s, episode steps:   3, steps per second: 169, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 2001.664307, mae: 594.042867, accuracy: 0.083333, mean_q: -110.247531, mean_eps: 0.100000\n",
      " 30677/50000: episode: 7170, duration: 0.019s, episode steps:   5, steps per second: 260, episode reward: -2193.000, mean reward: -438.600 [-999.000, -32.000], mean action: 1.400 [0.000, 3.000],  loss: 2465.144214, mae: 604.183606, accuracy: 0.125000, mean_q: -105.076202, mean_eps: 0.100000\n",
      " 30681/50000: episode: 7171, duration: 0.015s, episode steps:   4, steps per second: 261, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 2211.356750, mae: 565.058594, accuracy: 0.148438, mean_q: -114.073555, mean_eps: 0.100000\n",
      " 30684/50000: episode: 7172, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 2052.333130, mae: 599.438029, accuracy: 0.104167, mean_q: -98.997622, mean_eps: 0.100000\n",
      " 30687/50000: episode: 7173, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 2767.391073, mae: 584.107971, accuracy: 0.135417, mean_q: -128.532069, mean_eps: 0.100000\n",
      " 30691/50000: episode: 7174, duration: 0.017s, episode steps:   4, steps per second: 236, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 2531.577728, mae: 576.061157, accuracy: 0.109375, mean_q: -121.064234, mean_eps: 0.100000\n",
      " 30695/50000: episode: 7175, duration: 0.019s, episode steps:   4, steps per second: 211, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 3185.578491, mae: 588.249985, accuracy: 0.132812, mean_q: -112.590082, mean_eps: 0.100000\n",
      " 30698/50000: episode: 7176, duration: 0.014s, episode steps:   3, steps per second: 211, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 3249.616862, mae: 573.536580, accuracy: 0.145833, mean_q: -100.124550, mean_eps: 0.100000\n",
      " 30702/50000: episode: 7177, duration: 0.018s, episode steps:   4, steps per second: 216, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 2519.404053, mae: 568.401291, accuracy: 0.109375, mean_q: -115.172947, mean_eps: 0.100000\n",
      " 30705/50000: episode: 7178, duration: 0.014s, episode steps:   3, steps per second: 211, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 2614.692790, mae: 579.092285, accuracy: 0.135417, mean_q: -105.059443, mean_eps: 0.100000\n",
      " 30708/50000: episode: 7179, duration: 0.014s, episode steps:   3, steps per second: 219, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 2511.193848, mae: 589.974874, accuracy: 0.062500, mean_q: -119.630646, mean_eps: 0.100000\n",
      " 30711/50000: episode: 7180, duration: 0.013s, episode steps:   3, steps per second: 226, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 2683.745280, mae: 580.325195, accuracy: 0.104167, mean_q: -125.866613, mean_eps: 0.100000\n",
      " 30714/50000: episode: 7181, duration: 0.013s, episode steps:   3, steps per second: 223, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 2883.055664, mae: 597.487142, accuracy: 0.093750, mean_q: -123.408236, mean_eps: 0.100000\n",
      " 30717/50000: episode: 7182, duration: 0.017s, episode steps:   3, steps per second: 173, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 2253.141276, mae: 574.081441, accuracy: 0.083333, mean_q: -117.563227, mean_eps: 0.100000\n",
      " 30720/50000: episode: 7183, duration: 0.020s, episode steps:   3, steps per second: 154, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 3453.923584, mae: 588.256144, accuracy: 0.125000, mean_q: -110.148872, mean_eps: 0.100000\n",
      " 30723/50000: episode: 7184, duration: 0.016s, episode steps:   3, steps per second: 188, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 1697.254069, mae: 587.052429, accuracy: 0.104167, mean_q: -119.791626, mean_eps: 0.100000\n",
      " 30726/50000: episode: 7185, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.333 [0.000, 3.000],  loss: 1971.319824, mae: 577.214111, accuracy: 0.145833, mean_q: -120.990707, mean_eps: 0.100000\n",
      " 30729/50000: episode: 7186, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 2570.466349, mae: 601.144572, accuracy: 0.187500, mean_q: -102.776744, mean_eps: 0.100000\n",
      " 30732/50000: episode: 7187, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 2893.745524, mae: 586.148336, accuracy: 0.177083, mean_q: -117.536608, mean_eps: 0.100000\n",
      " 30736/50000: episode: 7188, duration: 0.016s, episode steps:   4, steps per second: 254, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 3225.419189, mae: 574.923874, accuracy: 0.140625, mean_q: -116.522182, mean_eps: 0.100000\n",
      " 30739/50000: episode: 7189, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 3495.074870, mae: 575.837708, accuracy: 0.177083, mean_q: -118.430786, mean_eps: 0.100000\n",
      " 30742/50000: episode: 7190, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 2145.915934, mae: 569.246948, accuracy: 0.135417, mean_q: -103.591866, mean_eps: 0.100000\n",
      " 30745/50000: episode: 7191, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 2538.318848, mae: 563.441732, accuracy: 0.145833, mean_q: -118.774134, mean_eps: 0.100000\n",
      " 30749/50000: episode: 7192, duration: 0.015s, episode steps:   4, steps per second: 261, episode reward: -1238.000, mean reward: -309.500 [-999.000, -60.000], mean action: 1.500 [0.000, 3.000],  loss: 2208.946381, mae: 575.004883, accuracy: 0.125000, mean_q: -110.373514, mean_eps: 0.100000\n",
      " 30752/50000: episode: 7193, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 2503.107992, mae: 577.966085, accuracy: 0.104167, mean_q: -113.458031, mean_eps: 0.100000\n",
      " 30755/50000: episode: 7194, duration: 0.013s, episode steps:   3, steps per second: 230, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 2866.447103, mae: 572.121867, accuracy: 0.104167, mean_q: -107.909912, mean_eps: 0.100000\n",
      " 30758/50000: episode: 7195, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 2034.147786, mae: 589.912862, accuracy: 0.093750, mean_q: -111.143748, mean_eps: 0.100000\n",
      " 30762/50000: episode: 7196, duration: 0.017s, episode steps:   4, steps per second: 241, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 1682.538666, mae: 576.441147, accuracy: 0.171875, mean_q: -111.478615, mean_eps: 0.100000\n",
      " 30765/50000: episode: 7197, duration: 0.015s, episode steps:   3, steps per second: 195, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 1217.341777, mae: 594.273621, accuracy: 0.104167, mean_q: -107.082204, mean_eps: 0.100000\n",
      " 30769/50000: episode: 7198, duration: 0.018s, episode steps:   4, steps per second: 225, episode reward: -1238.000, mean reward: -309.500 [-999.000, -60.000], mean action: 1.500 [0.000, 3.000],  loss: 2200.190674, mae: 550.065201, accuracy: 0.171875, mean_q: -119.891638, mean_eps: 0.100000\n",
      " 30772/50000: episode: 7199, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 3061.013143, mae: 586.787882, accuracy: 0.062500, mean_q: -120.820257, mean_eps: 0.100000\n",
      " 30775/50000: episode: 7200, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 2331.337769, mae: 605.196208, accuracy: 0.072917, mean_q: -107.019007, mean_eps: 0.100000\n",
      " 30778/50000: episode: 7201, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 2438.650187, mae: 582.122213, accuracy: 0.093750, mean_q: -106.469648, mean_eps: 0.100000\n",
      " 30781/50000: episode: 7202, duration: 0.012s, episode steps:   3, steps per second: 252, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 2485.864258, mae: 563.548381, accuracy: 0.166667, mean_q: -104.815956, mean_eps: 0.100000\n",
      " 30785/50000: episode: 7203, duration: 0.016s, episode steps:   4, steps per second: 250, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 1618.359650, mae: 580.819702, accuracy: 0.101562, mean_q: -113.597824, mean_eps: 0.100000\n",
      " 30788/50000: episode: 7204, duration: 0.013s, episode steps:   3, steps per second: 230, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 2133.199544, mae: 582.558472, accuracy: 0.125000, mean_q: -111.434703, mean_eps: 0.100000\n",
      " 30791/50000: episode: 7205, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 2235.937297, mae: 577.771281, accuracy: 0.135417, mean_q: -124.896856, mean_eps: 0.100000\n",
      " 30794/50000: episode: 7206, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 2232.823812, mae: 574.588745, accuracy: 0.083333, mean_q: -122.129514, mean_eps: 0.100000\n",
      " 30797/50000: episode: 7207, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 3675.575684, mae: 610.825887, accuracy: 0.114583, mean_q: -106.383247, mean_eps: 0.100000\n",
      " 30800/50000: episode: 7208, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 2008.467692, mae: 583.355591, accuracy: 0.104167, mean_q: -112.681508, mean_eps: 0.100000\n",
      " 30803/50000: episode: 7209, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 2344.164225, mae: 585.721741, accuracy: 0.093750, mean_q: -118.576612, mean_eps: 0.100000\n",
      " 30806/50000: episode: 7210, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 1933.066081, mae: 546.636068, accuracy: 0.083333, mean_q: -124.865560, mean_eps: 0.100000\n",
      " 30810/50000: episode: 7211, duration: 0.017s, episode steps:   4, steps per second: 233, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 3243.768677, mae: 578.324142, accuracy: 0.117188, mean_q: -105.272825, mean_eps: 0.100000\n",
      " 30813/50000: episode: 7212, duration: 0.018s, episode steps:   3, steps per second: 169, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 2338.295980, mae: 584.250061, accuracy: 0.125000, mean_q: -119.829954, mean_eps: 0.100000\n",
      " 30816/50000: episode: 7213, duration: 0.015s, episode steps:   3, steps per second: 205, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 2319.129517, mae: 590.687154, accuracy: 0.104167, mean_q: -118.958183, mean_eps: 0.100000\n",
      " 30819/50000: episode: 7214, duration: 0.013s, episode steps:   3, steps per second: 225, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 3067.787842, mae: 562.093363, accuracy: 0.177083, mean_q: -117.737378, mean_eps: 0.100000\n",
      " 30822/50000: episode: 7215, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 2110.054362, mae: 582.197611, accuracy: 0.135417, mean_q: -115.235519, mean_eps: 0.100000\n",
      " 30825/50000: episode: 7216, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 2096.557129, mae: 589.566162, accuracy: 0.104167, mean_q: -111.263565, mean_eps: 0.100000\n",
      " 30828/50000: episode: 7217, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 1711.727214, mae: 582.203613, accuracy: 0.104167, mean_q: -121.700302, mean_eps: 0.100000\n",
      " 30831/50000: episode: 7218, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 2086.871948, mae: 554.560262, accuracy: 0.072917, mean_q: -122.105372, mean_eps: 0.100000\n",
      " 30834/50000: episode: 7219, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 2040.121419, mae: 589.768412, accuracy: 0.104167, mean_q: -96.987218, mean_eps: 0.100000\n",
      " 30837/50000: episode: 7220, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 1839.490479, mae: 582.938517, accuracy: 0.135417, mean_q: -109.344421, mean_eps: 0.100000\n",
      " 30840/50000: episode: 7221, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 3048.795410, mae: 594.452535, accuracy: 0.125000, mean_q: -116.870143, mean_eps: 0.100000\n",
      " 30844/50000: episode: 7222, duration: 0.015s, episode steps:   4, steps per second: 259, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 2106.890717, mae: 605.766037, accuracy: 0.101562, mean_q: -124.525043, mean_eps: 0.100000\n",
      " 30847/50000: episode: 7223, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 2022.603068, mae: 588.061625, accuracy: 0.104167, mean_q: -131.307943, mean_eps: 0.100000\n",
      " 30850/50000: episode: 7224, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 2011.465454, mae: 592.092977, accuracy: 0.062500, mean_q: -123.687607, mean_eps: 0.100000\n",
      " 30854/50000: episode: 7225, duration: 0.019s, episode steps:   4, steps per second: 210, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 2245.808685, mae: 579.022797, accuracy: 0.132812, mean_q: -104.537741, mean_eps: 0.100000\n",
      " 30857/50000: episode: 7226, duration: 0.018s, episode steps:   3, steps per second: 171, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 3044.575439, mae: 585.742594, accuracy: 0.114583, mean_q: -121.712997, mean_eps: 0.100000\n",
      " 30861/50000: episode: 7227, duration: 0.023s, episode steps:   4, steps per second: 173, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 1868.809998, mae: 588.270660, accuracy: 0.093750, mean_q: -111.754646, mean_eps: 0.100000\n",
      " 30864/50000: episode: 7228, duration: 0.017s, episode steps:   3, steps per second: 179, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 2243.978923, mae: 579.268250, accuracy: 0.104167, mean_q: -117.329638, mean_eps: 0.100000\n",
      " 30867/50000: episode: 7229, duration: 0.015s, episode steps:   3, steps per second: 205, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 1913.785156, mae: 554.661194, accuracy: 0.114583, mean_q: -115.218498, mean_eps: 0.100000\n",
      " 30870/50000: episode: 7230, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 2423.597656, mae: 586.386210, accuracy: 0.093750, mean_q: -104.184146, mean_eps: 0.100000\n",
      " 30873/50000: episode: 7231, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 1884.461670, mae: 593.822266, accuracy: 0.114583, mean_q: -114.513484, mean_eps: 0.100000\n",
      " 30876/50000: episode: 7232, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 2587.639282, mae: 582.976217, accuracy: 0.177083, mean_q: -111.186935, mean_eps: 0.100000\n",
      " 30879/50000: episode: 7233, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 2036.527588, mae: 601.657979, accuracy: 0.093750, mean_q: -131.208649, mean_eps: 0.100000\n",
      " 30882/50000: episode: 7234, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 2463.109456, mae: 609.115743, accuracy: 0.156250, mean_q: -121.619695, mean_eps: 0.100000\n",
      " 30886/50000: episode: 7235, duration: 0.016s, episode steps:   4, steps per second: 251, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 2064.196899, mae: 603.662811, accuracy: 0.101562, mean_q: -114.721142, mean_eps: 0.100000\n",
      " 30889/50000: episode: 7236, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 2698.562093, mae: 581.977478, accuracy: 0.093750, mean_q: -124.729065, mean_eps: 0.100000\n",
      " 30892/50000: episode: 7237, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 1920.161947, mae: 586.018473, accuracy: 0.093750, mean_q: -114.321487, mean_eps: 0.100000\n",
      " 30895/50000: episode: 7238, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 2146.513021, mae: 582.261719, accuracy: 0.145833, mean_q: -97.567273, mean_eps: 0.100000\n",
      " 30898/50000: episode: 7239, duration: 0.013s, episode steps:   3, steps per second: 227, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 1780.858887, mae: 581.413778, accuracy: 0.093750, mean_q: -111.893308, mean_eps: 0.100000\n",
      " 30901/50000: episode: 7240, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 1546.112061, mae: 594.143636, accuracy: 0.135417, mean_q: -120.349943, mean_eps: 0.100000\n",
      " 30905/50000: episode: 7241, duration: 0.022s, episode steps:   4, steps per second: 181, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.000 [0.000, 3.000],  loss: 2273.532440, mae: 592.307983, accuracy: 0.078125, mean_q: -118.204208, mean_eps: 0.100000\n",
      " 30909/50000: episode: 7242, duration: 0.019s, episode steps:   4, steps per second: 207, episode reward: -1238.000, mean reward: -309.500 [-999.000, -60.000], mean action: 1.500 [0.000, 3.000],  loss: 1827.628906, mae: 591.111084, accuracy: 0.156250, mean_q: -115.670986, mean_eps: 0.100000\n",
      " 30912/50000: episode: 7243, duration: 0.014s, episode steps:   3, steps per second: 209, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 1438.069661, mae: 575.567607, accuracy: 0.093750, mean_q: -116.282743, mean_eps: 0.100000\n",
      " 30915/50000: episode: 7244, duration: 0.015s, episode steps:   3, steps per second: 198, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 2083.298665, mae: 571.707682, accuracy: 0.041667, mean_q: -109.480069, mean_eps: 0.100000\n",
      " 30918/50000: episode: 7245, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 1388.655273, mae: 596.006287, accuracy: 0.052083, mean_q: -111.247879, mean_eps: 0.100000\n",
      " 30921/50000: episode: 7246, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 2569.407756, mae: 609.663167, accuracy: 0.093750, mean_q: -103.386187, mean_eps: 0.100000\n",
      " 30925/50000: episode: 7247, duration: 0.016s, episode steps:   4, steps per second: 257, episode reward: -1223.000, mean reward: -305.750 [-999.000, -58.000], mean action: 1.750 [0.000, 3.000],  loss: 1700.785431, mae: 607.564011, accuracy: 0.101562, mean_q: -121.733023, mean_eps: 0.100000\n",
      " 30928/50000: episode: 7248, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 1894.810506, mae: 582.629313, accuracy: 0.072917, mean_q: -124.467219, mean_eps: 0.100000\n",
      " 30932/50000: episode: 7249, duration: 0.015s, episode steps:   4, steps per second: 259, episode reward: -1238.000, mean reward: -309.500 [-999.000, -60.000], mean action: 1.500 [0.000, 3.000],  loss: 1945.255310, mae: 590.775009, accuracy: 0.085938, mean_q: -112.941851, mean_eps: 0.100000\n",
      " 30936/50000: episode: 7250, duration: 0.016s, episode steps:   4, steps per second: 254, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 2042.669067, mae: 589.278397, accuracy: 0.125000, mean_q: -109.468534, mean_eps: 0.100000\n",
      " 30939/50000: episode: 7251, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 1399.046794, mae: 616.861532, accuracy: 0.104167, mean_q: -123.339198, mean_eps: 0.100000\n",
      " 30942/50000: episode: 7252, duration: 0.014s, episode steps:   3, steps per second: 214, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 2336.465861, mae: 566.361491, accuracy: 0.177083, mean_q: -114.774261, mean_eps: 0.100000\n",
      " 30945/50000: episode: 7253, duration: 0.015s, episode steps:   3, steps per second: 201, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 2092.712891, mae: 598.011353, accuracy: 0.062500, mean_q: -112.986448, mean_eps: 0.100000\n",
      " 30948/50000: episode: 7254, duration: 0.014s, episode steps:   3, steps per second: 210, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 1781.706746, mae: 588.609578, accuracy: 0.125000, mean_q: -115.845492, mean_eps: 0.100000\n",
      " 30951/50000: episode: 7255, duration: 0.016s, episode steps:   3, steps per second: 188, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 1631.055908, mae: 571.280619, accuracy: 0.145833, mean_q: -119.686696, mean_eps: 0.100000\n",
      " 30954/50000: episode: 7256, duration: 0.016s, episode steps:   3, steps per second: 189, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 1881.416789, mae: 593.621277, accuracy: 0.114583, mean_q: -108.898008, mean_eps: 0.100000\n",
      " 30957/50000: episode: 7257, duration: 0.017s, episode steps:   3, steps per second: 182, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 2519.675578, mae: 584.711121, accuracy: 0.083333, mean_q: -125.603617, mean_eps: 0.100000\n",
      " 30960/50000: episode: 7258, duration: 0.015s, episode steps:   3, steps per second: 204, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 2125.310872, mae: 604.369181, accuracy: 0.114583, mean_q: -113.861018, mean_eps: 0.100000\n",
      " 30963/50000: episode: 7259, duration: 0.014s, episode steps:   3, steps per second: 209, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 3202.216960, mae: 575.841329, accuracy: 0.135417, mean_q: -113.987727, mean_eps: 0.100000\n",
      " 30966/50000: episode: 7260, duration: 0.014s, episode steps:   3, steps per second: 217, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 1961.235881, mae: 618.988810, accuracy: 0.093750, mean_q: -110.965487, mean_eps: 0.100000\n",
      " 30969/50000: episode: 7261, duration: 0.013s, episode steps:   3, steps per second: 225, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 1359.221924, mae: 581.208516, accuracy: 0.093750, mean_q: -115.402229, mean_eps: 0.100000\n",
      " 30972/50000: episode: 7262, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 1614.191650, mae: 581.347290, accuracy: 0.229167, mean_q: -106.850433, mean_eps: 0.100000\n",
      " 30975/50000: episode: 7263, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 2382.082153, mae: 584.923157, accuracy: 0.156250, mean_q: -103.212041, mean_eps: 0.100000\n",
      " 30978/50000: episode: 7264, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 1940.462484, mae: 574.585897, accuracy: 0.114583, mean_q: -111.183935, mean_eps: 0.100000\n",
      " 30982/50000: episode: 7265, duration: 0.016s, episode steps:   4, steps per second: 250, episode reward: -1238.000, mean reward: -309.500 [-999.000, -45.000], mean action: 1.250 [0.000, 3.000],  loss: 1520.742554, mae: 583.133301, accuracy: 0.132812, mean_q: -116.610693, mean_eps: 0.100000\n",
      " 30985/50000: episode: 7266, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 2108.419352, mae: 579.782715, accuracy: 0.114583, mean_q: -118.158689, mean_eps: 0.100000\n",
      " 30989/50000: episode: 7267, duration: 0.016s, episode steps:   4, steps per second: 252, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 1734.130005, mae: 584.312164, accuracy: 0.125000, mean_q: -115.090723, mean_eps: 0.100000\n",
      " 30992/50000: episode: 7268, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 1312.111247, mae: 570.527466, accuracy: 0.083333, mean_q: -117.140350, mean_eps: 0.100000\n",
      " 30996/50000: episode: 7269, duration: 0.017s, episode steps:   4, steps per second: 229, episode reward: -1238.000, mean reward: -309.500 [-999.000, -60.000], mean action: 2.250 [1.000, 3.000],  loss: 1757.263336, mae: 580.790848, accuracy: 0.156250, mean_q: -117.224691, mean_eps: 0.100000\n",
      " 30999/50000: episode: 7270, duration: 0.017s, episode steps:   3, steps per second: 179, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 1284.782389, mae: 600.040141, accuracy: 0.145833, mean_q: -116.756246, mean_eps: 0.100000\n",
      " 31003/50000: episode: 7271, duration: 0.018s, episode steps:   4, steps per second: 219, episode reward: -1238.000, mean reward: -309.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 1792.316177, mae: 574.757019, accuracy: 0.125000, mean_q: -122.999609, mean_eps: 0.100000\n",
      " 31006/50000: episode: 7272, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 1839.380819, mae: 605.888448, accuracy: 0.072917, mean_q: -116.682030, mean_eps: 0.100000\n",
      " 31009/50000: episode: 7273, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 1740.536702, mae: 591.508016, accuracy: 0.072917, mean_q: -110.039769, mean_eps: 0.100000\n",
      " 31012/50000: episode: 7274, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 1995.840454, mae: 576.436910, accuracy: 0.072917, mean_q: -126.212293, mean_eps: 0.100000\n",
      " 31015/50000: episode: 7275, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 1984.447917, mae: 609.859273, accuracy: 0.125000, mean_q: -115.637352, mean_eps: 0.100000\n",
      " 31019/50000: episode: 7276, duration: 0.016s, episode steps:   4, steps per second: 255, episode reward: -1223.000, mean reward: -305.750 [-999.000, -58.000], mean action: 1.750 [0.000, 3.000],  loss: 1453.676880, mae: 586.815765, accuracy: 0.078125, mean_q: -123.544846, mean_eps: 0.100000\n",
      " 31022/50000: episode: 7277, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 2088.226685, mae: 584.422180, accuracy: 0.104167, mean_q: -130.009127, mean_eps: 0.100000\n",
      " 31025/50000: episode: 7278, duration: 0.013s, episode steps:   3, steps per second: 227, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 2353.630127, mae: 573.896708, accuracy: 0.114583, mean_q: -117.284973, mean_eps: 0.100000\n",
      " 31028/50000: episode: 7279, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 1608.357829, mae: 596.166402, accuracy: 0.135417, mean_q: -113.868279, mean_eps: 0.100000\n",
      " 31031/50000: episode: 7280, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 2031.153239, mae: 602.209595, accuracy: 0.114583, mean_q: -111.142385, mean_eps: 0.100000\n",
      " 31034/50000: episode: 7281, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 1406.044840, mae: 605.477356, accuracy: 0.166667, mean_q: -119.862282, mean_eps: 0.100000\n",
      " 31037/50000: episode: 7282, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 1868.283895, mae: 607.203837, accuracy: 0.125000, mean_q: -109.430229, mean_eps: 0.100000\n",
      " 31040/50000: episode: 7283, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 1900.956991, mae: 601.136027, accuracy: 0.083333, mean_q: -114.137624, mean_eps: 0.100000\n",
      " 31043/50000: episode: 7284, duration: 0.023s, episode steps:   3, steps per second: 132, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 1669.435588, mae: 596.547628, accuracy: 0.104167, mean_q: -115.860608, mean_eps: 0.100000\n",
      " 31046/50000: episode: 7285, duration: 0.022s, episode steps:   3, steps per second: 139, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 1775.870585, mae: 579.176900, accuracy: 0.156250, mean_q: -118.026171, mean_eps: 0.100000\n",
      " 31049/50000: episode: 7286, duration: 0.015s, episode steps:   3, steps per second: 200, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 2364.343953, mae: 626.225769, accuracy: 0.062500, mean_q: -98.569578, mean_eps: 0.100000\n",
      " 31052/50000: episode: 7287, duration: 0.013s, episode steps:   3, steps per second: 226, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 1771.043945, mae: 595.894775, accuracy: 0.125000, mean_q: -123.914780, mean_eps: 0.100000\n",
      " 31055/50000: episode: 7288, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 1361.033447, mae: 628.791911, accuracy: 0.072917, mean_q: -116.083669, mean_eps: 0.100000\n",
      " 31059/50000: episode: 7289, duration: 0.016s, episode steps:   4, steps per second: 255, episode reward: -1223.000, mean reward: -305.750 [-999.000, -58.000], mean action: 1.750 [0.000, 3.000],  loss: 1756.878906, mae: 582.041061, accuracy: 0.085938, mean_q: -123.613445, mean_eps: 0.100000\n",
      " 31062/50000: episode: 7290, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 1407.545471, mae: 607.062663, accuracy: 0.093750, mean_q: -114.896065, mean_eps: 0.100000\n",
      " 31065/50000: episode: 7291, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 1384.786825, mae: 584.399862, accuracy: 0.072917, mean_q: -114.056142, mean_eps: 0.100000\n",
      " 31068/50000: episode: 7292, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 1542.395874, mae: 584.340861, accuracy: 0.156250, mean_q: -113.715864, mean_eps: 0.100000\n",
      " 31071/50000: episode: 7293, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 1979.004232, mae: 595.332621, accuracy: 0.125000, mean_q: -111.436361, mean_eps: 0.100000\n",
      " 31074/50000: episode: 7294, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 1605.419881, mae: 597.513021, accuracy: 0.135417, mean_q: -122.575661, mean_eps: 0.100000\n",
      " 31077/50000: episode: 7295, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 2244.424479, mae: 573.121297, accuracy: 0.104167, mean_q: -132.600416, mean_eps: 0.100000\n",
      " 31080/50000: episode: 7296, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 1740.290731, mae: 616.196228, accuracy: 0.104167, mean_q: -104.807396, mean_eps: 0.100000\n",
      " 31083/50000: episode: 7297, duration: 0.014s, episode steps:   3, steps per second: 222, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 2380.229004, mae: 573.283813, accuracy: 0.072917, mean_q: -123.512505, mean_eps: 0.100000\n",
      " 31088/50000: episode: 7298, duration: 0.020s, episode steps:   5, steps per second: 249, episode reward: -2237.000, mean reward: -447.400 [-999.000, -45.000], mean action: 1.400 [0.000, 3.000],  loss: 2408.785864, mae: 578.326270, accuracy: 0.118750, mean_q: -116.068176, mean_eps: 0.100000\n",
      " 31091/50000: episode: 7299, duration: 0.019s, episode steps:   3, steps per second: 161, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 2164.648112, mae: 601.862122, accuracy: 0.135417, mean_q: -106.580676, mean_eps: 0.100000\n",
      " 31094/50000: episode: 7300, duration: 0.015s, episode steps:   3, steps per second: 194, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 2061.313009, mae: 586.829610, accuracy: 0.156250, mean_q: -116.279976, mean_eps: 0.100000\n",
      " 31097/50000: episode: 7301, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 1670.507446, mae: 588.771057, accuracy: 0.093750, mean_q: -120.639826, mean_eps: 0.100000\n",
      " 31100/50000: episode: 7302, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 2281.267253, mae: 580.805664, accuracy: 0.072917, mean_q: -122.033829, mean_eps: 0.100000\n",
      " 31103/50000: episode: 7303, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 1655.362895, mae: 595.081665, accuracy: 0.093750, mean_q: -122.904510, mean_eps: 0.100000\n",
      " 31106/50000: episode: 7304, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 1338.204346, mae: 600.022583, accuracy: 0.125000, mean_q: -129.367915, mean_eps: 0.100000\n",
      " 31109/50000: episode: 7305, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 1723.047445, mae: 579.077311, accuracy: 0.093750, mean_q: -125.271825, mean_eps: 0.100000\n",
      " 31113/50000: episode: 7306, duration: 0.016s, episode steps:   4, steps per second: 255, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 1388.602203, mae: 575.636978, accuracy: 0.085938, mean_q: -119.293957, mean_eps: 0.100000\n",
      " 31116/50000: episode: 7307, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 1553.840088, mae: 594.051941, accuracy: 0.104167, mean_q: -118.694911, mean_eps: 0.100000\n",
      " 31119/50000: episode: 7308, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 2285.210368, mae: 569.193014, accuracy: 0.093750, mean_q: -119.108177, mean_eps: 0.100000\n",
      " 31122/50000: episode: 7309, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 1967.975098, mae: 566.839274, accuracy: 0.125000, mean_q: -122.783460, mean_eps: 0.100000\n",
      " 31125/50000: episode: 7310, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 1434.888713, mae: 578.309163, accuracy: 0.072917, mean_q: -129.473317, mean_eps: 0.100000\n",
      " 31128/50000: episode: 7311, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 1843.861328, mae: 565.280172, accuracy: 0.156250, mean_q: -115.563176, mean_eps: 0.100000\n",
      " 31131/50000: episode: 7312, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 1967.082601, mae: 573.187398, accuracy: 0.072917, mean_q: -119.644343, mean_eps: 0.100000\n",
      " 31135/50000: episode: 7313, duration: 0.016s, episode steps:   4, steps per second: 250, episode reward: -1238.000, mean reward: -309.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 1536.204285, mae: 575.161514, accuracy: 0.085938, mean_q: -125.749538, mean_eps: 0.100000\n",
      " 31138/50000: episode: 7314, duration: 0.014s, episode steps:   3, steps per second: 218, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 1635.870544, mae: 592.341899, accuracy: 0.145833, mean_q: -123.485357, mean_eps: 0.100000\n",
      " 31141/50000: episode: 7315, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 1806.586487, mae: 587.382507, accuracy: 0.093750, mean_q: -124.135935, mean_eps: 0.100000\n",
      " 31144/50000: episode: 7316, duration: 0.015s, episode steps:   3, steps per second: 196, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 981.033854, mae: 580.720418, accuracy: 0.093750, mean_q: -115.539103, mean_eps: 0.100000\n",
      " 31147/50000: episode: 7317, duration: 0.015s, episode steps:   3, steps per second: 201, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 1968.263997, mae: 596.475098, accuracy: 0.093750, mean_q: -110.596865, mean_eps: 0.100000\n",
      " 31150/50000: episode: 7318, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 2138.381388, mae: 600.814372, accuracy: 0.104167, mean_q: -119.337056, mean_eps: 0.100000\n",
      " 31153/50000: episode: 7319, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 2044.468119, mae: 574.928569, accuracy: 0.166667, mean_q: -116.383362, mean_eps: 0.100000\n",
      " 31157/50000: episode: 7320, duration: 0.016s, episode steps:   4, steps per second: 250, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 1373.869934, mae: 577.426590, accuracy: 0.148438, mean_q: -118.397160, mean_eps: 0.100000\n",
      " 31161/50000: episode: 7321, duration: 0.016s, episode steps:   4, steps per second: 254, episode reward: -1238.000, mean reward: -309.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 1557.317932, mae: 584.988159, accuracy: 0.093750, mean_q: -116.649212, mean_eps: 0.100000\n",
      " 31164/50000: episode: 7322, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 1471.517069, mae: 565.935282, accuracy: 0.041667, mean_q: -124.112831, mean_eps: 0.100000\n",
      " 31167/50000: episode: 7323, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 1661.467122, mae: 567.669027, accuracy: 0.125000, mean_q: -130.945353, mean_eps: 0.100000\n",
      " 31170/50000: episode: 7324, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 1336.599386, mae: 591.726278, accuracy: 0.125000, mean_q: -123.225650, mean_eps: 0.100000\n",
      " 31175/50000: episode: 7325, duration: 0.019s, episode steps:   5, steps per second: 269, episode reward: -2193.000, mean reward: -438.600 [-999.000, -45.000], mean action: 1.800 [0.000, 3.000],  loss: 2193.017603, mae: 568.314587, accuracy: 0.106250, mean_q: -121.173543, mean_eps: 0.100000\n",
      " 31178/50000: episode: 7326, duration: 0.013s, episode steps:   3, steps per second: 226, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 1714.045573, mae: 563.712260, accuracy: 0.093750, mean_q: -116.596954, mean_eps: 0.100000\n",
      " 31182/50000: episode: 7327, duration: 0.016s, episode steps:   4, steps per second: 244, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 1512.362823, mae: 574.400711, accuracy: 0.171875, mean_q: -106.979181, mean_eps: 0.100000\n",
      " 31185/50000: episode: 7328, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 1750.434652, mae: 597.655436, accuracy: 0.125000, mean_q: -124.072772, mean_eps: 0.100000\n",
      " 31188/50000: episode: 7329, duration: 0.014s, episode steps:   3, steps per second: 215, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 1749.751790, mae: 604.912577, accuracy: 0.177083, mean_q: -120.546270, mean_eps: 0.100000\n",
      " 31191/50000: episode: 7330, duration: 0.019s, episode steps:   3, steps per second: 158, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 1537.805176, mae: 579.710225, accuracy: 0.125000, mean_q: -134.170761, mean_eps: 0.100000\n",
      " 31194/50000: episode: 7331, duration: 0.014s, episode steps:   3, steps per second: 216, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 1439.037659, mae: 621.572795, accuracy: 0.125000, mean_q: -122.127467, mean_eps: 0.100000\n",
      " 31197/50000: episode: 7332, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 1620.549316, mae: 588.277649, accuracy: 0.072917, mean_q: -131.911947, mean_eps: 0.100000\n",
      " 31200/50000: episode: 7333, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 1505.216268, mae: 572.926514, accuracy: 0.145833, mean_q: -128.511058, mean_eps: 0.100000\n",
      " 31203/50000: episode: 7334, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 1936.861491, mae: 591.016052, accuracy: 0.062500, mean_q: -122.865206, mean_eps: 0.100000\n",
      " 31206/50000: episode: 7335, duration: 0.013s, episode steps:   3, steps per second: 226, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 1759.282959, mae: 577.611043, accuracy: 0.166667, mean_q: -108.580589, mean_eps: 0.100000\n",
      " 31210/50000: episode: 7336, duration: 0.016s, episode steps:   4, steps per second: 255, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 2.000 [0.000, 3.000],  loss: 1226.893677, mae: 567.245605, accuracy: 0.093750, mean_q: -127.634243, mean_eps: 0.100000\n",
      " 31213/50000: episode: 7337, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 1113.005086, mae: 587.193705, accuracy: 0.114583, mean_q: -125.317505, mean_eps: 0.100000\n",
      " 31216/50000: episode: 7338, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 976.342448, mae: 567.858805, accuracy: 0.125000, mean_q: -129.041847, mean_eps: 0.100000\n",
      " 31219/50000: episode: 7339, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 1168.701904, mae: 607.914388, accuracy: 0.104167, mean_q: -117.374959, mean_eps: 0.100000\n",
      " 31222/50000: episode: 7340, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.333 [0.000, 3.000],  loss: 1861.614237, mae: 570.860942, accuracy: 0.093750, mean_q: -128.983604, mean_eps: 0.100000\n",
      " 31225/50000: episode: 7341, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 1303.275309, mae: 595.281982, accuracy: 0.145833, mean_q: -121.826879, mean_eps: 0.100000\n",
      " 31228/50000: episode: 7342, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 1690.352173, mae: 580.436666, accuracy: 0.166667, mean_q: -125.888680, mean_eps: 0.100000\n",
      " 31232/50000: episode: 7343, duration: 0.018s, episode steps:   4, steps per second: 227, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 1382.206253, mae: 583.126190, accuracy: 0.140625, mean_q: -121.065472, mean_eps: 0.100000\n",
      " 31235/50000: episode: 7344, duration: 0.017s, episode steps:   3, steps per second: 173, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 1879.001750, mae: 563.379313, accuracy: 0.114583, mean_q: -124.156031, mean_eps: 0.100000\n",
      " 31238/50000: episode: 7345, duration: 0.027s, episode steps:   3, steps per second: 111, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 1415.523315, mae: 597.508036, accuracy: 0.093750, mean_q: -124.510292, mean_eps: 0.100000\n",
      " 31242/50000: episode: 7346, duration: 0.019s, episode steps:   4, steps per second: 209, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 2122.816559, mae: 595.821457, accuracy: 0.125000, mean_q: -118.891808, mean_eps: 0.100000\n",
      " 31245/50000: episode: 7347, duration: 0.013s, episode steps:   3, steps per second: 224, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 1582.259928, mae: 587.691325, accuracy: 0.072917, mean_q: -132.874461, mean_eps: 0.100000\n",
      " 31248/50000: episode: 7348, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 1704.336955, mae: 573.515625, accuracy: 0.114583, mean_q: -130.905998, mean_eps: 0.100000\n",
      " 31251/50000: episode: 7349, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 1590.720805, mae: 565.333049, accuracy: 0.166667, mean_q: -118.734047, mean_eps: 0.100000\n",
      " 31254/50000: episode: 7350, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 1348.203166, mae: 580.318624, accuracy: 0.114583, mean_q: -125.060555, mean_eps: 0.100000\n",
      " 31257/50000: episode: 7351, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 1606.002665, mae: 581.927287, accuracy: 0.135417, mean_q: -123.803134, mean_eps: 0.100000\n",
      " 31260/50000: episode: 7352, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 1438.170166, mae: 595.102153, accuracy: 0.072917, mean_q: -116.704137, mean_eps: 0.100000\n",
      " 31263/50000: episode: 7353, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 1055.738932, mae: 567.233602, accuracy: 0.104167, mean_q: -119.077039, mean_eps: 0.100000\n",
      " 31267/50000: episode: 7354, duration: 0.015s, episode steps:   4, steps per second: 262, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 2.000 [0.000, 3.000],  loss: 1862.469299, mae: 570.074738, accuracy: 0.078125, mean_q: -125.545338, mean_eps: 0.100000\n",
      " 31271/50000: episode: 7355, duration: 0.016s, episode steps:   4, steps per second: 249, episode reward: -1223.000, mean reward: -305.750 [-999.000, -58.000], mean action: 1.750 [0.000, 3.000],  loss: 1527.640503, mae: 585.978165, accuracy: 0.085938, mean_q: -128.376566, mean_eps: 0.100000\n",
      " 31274/50000: episode: 7356, duration: 0.013s, episode steps:   3, steps per second: 226, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 1037.528951, mae: 585.232727, accuracy: 0.114583, mean_q: -122.108897, mean_eps: 0.100000\n",
      " 31277/50000: episode: 7357, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 1568.404989, mae: 605.194132, accuracy: 0.052083, mean_q: -102.713659, mean_eps: 0.100000\n",
      " 31280/50000: episode: 7358, duration: 0.016s, episode steps:   3, steps per second: 187, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 2270.466715, mae: 585.049479, accuracy: 0.093750, mean_q: -114.906370, mean_eps: 0.100000\n",
      " 31283/50000: episode: 7359, duration: 0.016s, episode steps:   3, steps per second: 183, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 1832.044434, mae: 578.634481, accuracy: 0.083333, mean_q: -123.167381, mean_eps: 0.100000\n",
      " 31286/50000: episode: 7360, duration: 0.015s, episode steps:   3, steps per second: 202, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 985.689596, mae: 573.255107, accuracy: 0.083333, mean_q: -127.682126, mean_eps: 0.100000\n",
      " 31289/50000: episode: 7361, duration: 0.013s, episode steps:   3, steps per second: 225, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 1437.926737, mae: 586.749858, accuracy: 0.104167, mean_q: -128.490194, mean_eps: 0.100000\n",
      " 31293/50000: episode: 7362, duration: 0.016s, episode steps:   4, steps per second: 248, episode reward: -1238.000, mean reward: -309.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 1775.864594, mae: 569.553696, accuracy: 0.101562, mean_q: -123.170612, mean_eps: 0.100000\n",
      " 31296/50000: episode: 7363, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 1520.382812, mae: 561.783997, accuracy: 0.114583, mean_q: -127.641612, mean_eps: 0.100000\n",
      " 31299/50000: episode: 7364, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 1413.602295, mae: 540.667816, accuracy: 0.083333, mean_q: -128.281998, mean_eps: 0.100000\n",
      " 31302/50000: episode: 7365, duration: 0.012s, episode steps:   3, steps per second: 253, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 1667.944417, mae: 583.253255, accuracy: 0.062500, mean_q: -111.437594, mean_eps: 0.100000\n",
      " 31306/50000: episode: 7366, duration: 0.015s, episode steps:   4, steps per second: 265, episode reward: -1223.000, mean reward: -305.750 [-999.000, -58.000], mean action: 1.750 [0.000, 3.000],  loss: 1725.132233, mae: 573.528183, accuracy: 0.132812, mean_q: -119.720449, mean_eps: 0.100000\n",
      " 31309/50000: episode: 7367, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 1116.958740, mae: 585.872843, accuracy: 0.062500, mean_q: -128.038844, mean_eps: 0.100000\n",
      " 31313/50000: episode: 7368, duration: 0.015s, episode steps:   4, steps per second: 261, episode reward: -1223.000, mean reward: -305.750 [-999.000, -58.000], mean action: 1.750 [0.000, 3.000],  loss: 1347.465637, mae: 602.783096, accuracy: 0.054688, mean_q: -134.400053, mean_eps: 0.100000\n",
      " 31316/50000: episode: 7369, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 971.945557, mae: 588.021627, accuracy: 0.072917, mean_q: -129.202454, mean_eps: 0.100000\n",
      " 31319/50000: episode: 7370, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 1635.893412, mae: 548.769653, accuracy: 0.052083, mean_q: -133.935944, mean_eps: 0.100000\n",
      " 31322/50000: episode: 7371, duration: 0.014s, episode steps:   3, steps per second: 207, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 1261.285502, mae: 600.891907, accuracy: 0.104167, mean_q: -108.134766, mean_eps: 0.100000\n",
      " 31325/50000: episode: 7372, duration: 0.017s, episode steps:   3, steps per second: 176, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 1187.975301, mae: 588.708374, accuracy: 0.114583, mean_q: -114.863342, mean_eps: 0.100000\n",
      " 31328/50000: episode: 7373, duration: 0.014s, episode steps:   3, steps per second: 218, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 1541.723165, mae: 580.858358, accuracy: 0.156250, mean_q: -113.426353, mean_eps: 0.100000\n",
      " 31331/50000: episode: 7374, duration: 0.015s, episode steps:   3, steps per second: 194, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 1509.316366, mae: 579.426819, accuracy: 0.104167, mean_q: -133.209061, mean_eps: 0.100000\n",
      " 31334/50000: episode: 7375, duration: 0.016s, episode steps:   3, steps per second: 189, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 1672.375692, mae: 597.013123, accuracy: 0.041667, mean_q: -130.015432, mean_eps: 0.100000\n",
      " 31337/50000: episode: 7376, duration: 0.015s, episode steps:   3, steps per second: 198, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 1277.931905, mae: 574.383667, accuracy: 0.052083, mean_q: -138.586075, mean_eps: 0.100000\n",
      " 31340/50000: episode: 7377, duration: 0.014s, episode steps:   3, steps per second: 220, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 1045.740601, mae: 592.013306, accuracy: 0.083333, mean_q: -118.936373, mean_eps: 0.100000\n",
      " 31343/50000: episode: 7378, duration: 0.013s, episode steps:   3, steps per second: 224, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 1381.528117, mae: 588.265055, accuracy: 0.125000, mean_q: -114.555979, mean_eps: 0.100000\n",
      " 31346/50000: episode: 7379, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 1321.322754, mae: 597.953959, accuracy: 0.093750, mean_q: -121.234968, mean_eps: 0.100000\n",
      " 31350/50000: episode: 7380, duration: 0.016s, episode steps:   4, steps per second: 257, episode reward: -1238.000, mean reward: -309.500 [-999.000, -60.000], mean action: 1.500 [0.000, 3.000],  loss: 1389.179596, mae: 589.372482, accuracy: 0.093750, mean_q: -119.679567, mean_eps: 0.100000\n",
      " 31353/50000: episode: 7381, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 1400.561564, mae: 605.533915, accuracy: 0.072917, mean_q: -138.153524, mean_eps: 0.100000\n",
      " 31356/50000: episode: 7382, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 1040.206299, mae: 607.794128, accuracy: 0.062500, mean_q: -128.187892, mean_eps: 0.100000\n",
      " 31359/50000: episode: 7383, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 1059.127218, mae: 592.075968, accuracy: 0.062500, mean_q: -114.227503, mean_eps: 0.100000\n",
      " 31362/50000: episode: 7384, duration: 0.013s, episode steps:   3, steps per second: 224, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 1498.959188, mae: 595.332438, accuracy: 0.093750, mean_q: -119.411835, mean_eps: 0.100000\n",
      " 31366/50000: episode: 7385, duration: 0.016s, episode steps:   4, steps per second: 250, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 1340.983932, mae: 596.773880, accuracy: 0.093750, mean_q: -121.912968, mean_eps: 0.100000\n",
      " 31369/50000: episode: 7386, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 811.691101, mae: 599.950073, accuracy: 0.072917, mean_q: -121.163119, mean_eps: 0.100000\n",
      " 31372/50000: episode: 7387, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 1330.443929, mae: 563.121765, accuracy: 0.072917, mean_q: -128.881866, mean_eps: 0.100000\n",
      " 31375/50000: episode: 7388, duration: 0.013s, episode steps:   3, steps per second: 223, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 1793.014974, mae: 575.111572, accuracy: 0.083333, mean_q: -129.011381, mean_eps: 0.100000\n",
      " 31378/50000: episode: 7389, duration: 0.019s, episode steps:   3, steps per second: 161, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 1283.524882, mae: 580.416809, accuracy: 0.062500, mean_q: -123.042081, mean_eps: 0.100000\n",
      " 31381/50000: episode: 7390, duration: 0.015s, episode steps:   3, steps per second: 203, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 1080.182007, mae: 603.530721, accuracy: 0.093750, mean_q: -127.149320, mean_eps: 0.100000\n",
      " 31384/50000: episode: 7391, duration: 0.013s, episode steps:   3, steps per second: 225, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 1075.628540, mae: 601.262797, accuracy: 0.145833, mean_q: -120.846603, mean_eps: 0.100000\n",
      " 31387/50000: episode: 7392, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 1369.921712, mae: 584.867249, accuracy: 0.145833, mean_q: -106.101494, mean_eps: 0.100000\n",
      " 31390/50000: episode: 7393, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 1454.589274, mae: 572.537211, accuracy: 0.125000, mean_q: -110.118151, mean_eps: 0.100000\n",
      " 31393/50000: episode: 7394, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 1609.584188, mae: 564.915792, accuracy: 0.083333, mean_q: -122.033305, mean_eps: 0.100000\n",
      " 31396/50000: episode: 7395, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 1248.300741, mae: 577.376831, accuracy: 0.093750, mean_q: -126.152267, mean_eps: 0.100000\n",
      " 31399/50000: episode: 7396, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 877.783752, mae: 599.832438, accuracy: 0.072917, mean_q: -127.633575, mean_eps: 0.100000\n",
      " 31402/50000: episode: 7397, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 1187.497192, mae: 589.742065, accuracy: 0.052083, mean_q: -127.981565, mean_eps: 0.100000\n",
      " 31405/50000: episode: 7398, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 1810.394735, mae: 573.056030, accuracy: 0.083333, mean_q: -125.777316, mean_eps: 0.100000\n",
      " 31408/50000: episode: 7399, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 1433.178162, mae: 573.986959, accuracy: 0.083333, mean_q: -121.401197, mean_eps: 0.100000\n",
      " 31411/50000: episode: 7400, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 1236.735962, mae: 570.149760, accuracy: 0.104167, mean_q: -111.884410, mean_eps: 0.100000\n",
      " 31414/50000: episode: 7401, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 1289.908122, mae: 559.204773, accuracy: 0.072917, mean_q: -127.638028, mean_eps: 0.100000\n",
      " 31417/50000: episode: 7402, duration: 0.013s, episode steps:   3, steps per second: 230, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 1208.283610, mae: 585.682149, accuracy: 0.125000, mean_q: -120.123436, mean_eps: 0.100000\n",
      " 31420/50000: episode: 7403, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 1244.420146, mae: 566.179321, accuracy: 0.114583, mean_q: -128.125613, mean_eps: 0.100000\n",
      " 31423/50000: episode: 7404, duration: 0.017s, episode steps:   3, steps per second: 176, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 1290.111023, mae: 593.268677, accuracy: 0.083333, mean_q: -130.043574, mean_eps: 0.100000\n",
      " 31426/50000: episode: 7405, duration: 0.027s, episode steps:   3, steps per second: 111, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 1203.568542, mae: 577.894613, accuracy: 0.072917, mean_q: -127.078128, mean_eps: 0.100000\n",
      " 31429/50000: episode: 7406, duration: 0.017s, episode steps:   3, steps per second: 175, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 1017.764323, mae: 598.071269, accuracy: 0.104167, mean_q: -113.022336, mean_eps: 0.100000\n",
      " 31432/50000: episode: 7407, duration: 0.014s, episode steps:   3, steps per second: 216, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 873.237610, mae: 576.252665, accuracy: 0.041667, mean_q: -129.499898, mean_eps: 0.100000\n",
      " 31435/50000: episode: 7408, duration: 0.013s, episode steps:   3, steps per second: 225, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 1500.849040, mae: 588.758952, accuracy: 0.072917, mean_q: -121.227646, mean_eps: 0.100000\n",
      " 31438/50000: episode: 7409, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 809.793396, mae: 583.721273, accuracy: 0.083333, mean_q: -122.593643, mean_eps: 0.100000\n",
      " 31441/50000: episode: 7410, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 1222.974955, mae: 592.061137, accuracy: 0.104167, mean_q: -117.958921, mean_eps: 0.100000\n",
      " 31444/50000: episode: 7411, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 1325.522909, mae: 582.693746, accuracy: 0.062500, mean_q: -127.223831, mean_eps: 0.100000\n",
      " 31447/50000: episode: 7412, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 1140.409220, mae: 625.811747, accuracy: 0.072917, mean_q: -118.589630, mean_eps: 0.100000\n",
      " 31450/50000: episode: 7413, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 1274.882975, mae: 589.764038, accuracy: 0.104167, mean_q: -129.266490, mean_eps: 0.100000\n",
      " 31453/50000: episode: 7414, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 861.071798, mae: 621.472819, accuracy: 0.104167, mean_q: -118.965808, mean_eps: 0.100000\n",
      " 31456/50000: episode: 7415, duration: 0.013s, episode steps:   3, steps per second: 223, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 1209.401449, mae: 600.999390, accuracy: 0.083333, mean_q: -125.512182, mean_eps: 0.100000\n",
      " 31459/50000: episode: 7416, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 979.944458, mae: 585.383708, accuracy: 0.072917, mean_q: -119.826767, mean_eps: 0.100000\n",
      " 31462/50000: episode: 7417, duration: 0.013s, episode steps:   3, steps per second: 227, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 1162.811056, mae: 580.367025, accuracy: 0.072917, mean_q: -123.628894, mean_eps: 0.100000\n",
      " 31465/50000: episode: 7418, duration: 0.018s, episode steps:   3, steps per second: 166, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 915.779928, mae: 573.137309, accuracy: 0.114583, mean_q: -120.308317, mean_eps: 0.100000\n",
      " 31469/50000: episode: 7419, duration: 0.018s, episode steps:   4, steps per second: 217, episode reward: -1223.000, mean reward: -305.750 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 1381.260117, mae: 585.942764, accuracy: 0.132812, mean_q: -124.721117, mean_eps: 0.100000\n",
      " 31473/50000: episode: 7420, duration: 0.017s, episode steps:   4, steps per second: 231, episode reward: -1223.000, mean reward: -305.750 [-999.000, -58.000], mean action: 1.750 [0.000, 3.000],  loss: 1178.594986, mae: 580.430313, accuracy: 0.054688, mean_q: -129.753931, mean_eps: 0.100000\n",
      " 31477/50000: episode: 7421, duration: 0.016s, episode steps:   4, steps per second: 246, episode reward: -1223.000, mean reward: -305.750 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 1210.439697, mae: 580.138962, accuracy: 0.101562, mean_q: -122.635906, mean_eps: 0.100000\n",
      " 31480/50000: episode: 7422, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 1226.771729, mae: 559.016256, accuracy: 0.114583, mean_q: -131.495153, mean_eps: 0.100000\n",
      " 31483/50000: episode: 7423, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 1391.910075, mae: 604.750488, accuracy: 0.083333, mean_q: -116.764086, mean_eps: 0.100000\n",
      " 31487/50000: episode: 7424, duration: 0.016s, episode steps:   4, steps per second: 258, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 1276.158066, mae: 589.071625, accuracy: 0.117188, mean_q: -122.211798, mean_eps: 0.100000\n",
      " 31490/50000: episode: 7425, duration: 0.013s, episode steps:   3, steps per second: 223, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 1181.062927, mae: 594.975789, accuracy: 0.093750, mean_q: -107.441577, mean_eps: 0.100000\n",
      " 31493/50000: episode: 7426, duration: 0.014s, episode steps:   3, steps per second: 210, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 1540.175212, mae: 569.643311, accuracy: 0.114583, mean_q: -126.995033, mean_eps: 0.100000\n",
      " 31496/50000: episode: 7427, duration: 0.014s, episode steps:   3, steps per second: 220, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 1283.905436, mae: 594.955811, accuracy: 0.187500, mean_q: -116.299830, mean_eps: 0.100000\n",
      " 31499/50000: episode: 7428, duration: 0.014s, episode steps:   3, steps per second: 212, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 1099.298523, mae: 584.193868, accuracy: 0.135417, mean_q: -120.506091, mean_eps: 0.100000\n",
      " 31502/50000: episode: 7429, duration: 0.015s, episode steps:   3, steps per second: 207, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 1356.807007, mae: 605.078979, accuracy: 0.093750, mean_q: -119.536593, mean_eps: 0.100000\n",
      " 31505/50000: episode: 7430, duration: 0.014s, episode steps:   3, steps per second: 216, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 1411.871358, mae: 552.531738, accuracy: 0.114583, mean_q: -125.823860, mean_eps: 0.100000\n",
      " 31508/50000: episode: 7431, duration: 0.016s, episode steps:   3, steps per second: 193, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 627.271718, mae: 580.835673, accuracy: 0.083333, mean_q: -125.235601, mean_eps: 0.100000\n",
      " 31511/50000: episode: 7432, duration: 0.016s, episode steps:   3, steps per second: 191, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 720.299988, mae: 590.987162, accuracy: 0.104167, mean_q: -124.121175, mean_eps: 0.100000\n",
      " 31514/50000: episode: 7433, duration: 0.015s, episode steps:   3, steps per second: 194, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 766.540120, mae: 583.286743, accuracy: 0.052083, mean_q: -133.256516, mean_eps: 0.100000\n",
      " 31517/50000: episode: 7434, duration: 0.014s, episode steps:   3, steps per second: 211, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 1114.161621, mae: 589.214274, accuracy: 0.114583, mean_q: -138.946716, mean_eps: 0.100000\n",
      " 31520/50000: episode: 7435, duration: 0.015s, episode steps:   3, steps per second: 199, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 1301.785400, mae: 588.274272, accuracy: 0.072917, mean_q: -126.837555, mean_eps: 0.100000\n",
      " 31523/50000: episode: 7436, duration: 0.015s, episode steps:   3, steps per second: 197, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 871.283203, mae: 591.940165, accuracy: 0.083333, mean_q: -117.879450, mean_eps: 0.100000\n",
      " 31526/50000: episode: 7437, duration: 0.015s, episode steps:   3, steps per second: 207, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 820.299540, mae: 602.726807, accuracy: 0.072917, mean_q: -121.098473, mean_eps: 0.100000\n",
      " 31529/50000: episode: 7438, duration: 0.014s, episode steps:   3, steps per second: 211, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 790.948446, mae: 582.034953, accuracy: 0.062500, mean_q: -121.641212, mean_eps: 0.100000\n",
      " 31532/50000: episode: 7439, duration: 0.014s, episode steps:   3, steps per second: 210, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 795.550639, mae: 611.011414, accuracy: 0.083333, mean_q: -121.677414, mean_eps: 0.100000\n",
      " 31535/50000: episode: 7440, duration: 0.014s, episode steps:   3, steps per second: 214, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 1388.599650, mae: 582.322266, accuracy: 0.093750, mean_q: -127.298065, mean_eps: 0.100000\n",
      " 31538/50000: episode: 7441, duration: 0.014s, episode steps:   3, steps per second: 220, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 653.145182, mae: 610.503438, accuracy: 0.083333, mean_q: -125.200048, mean_eps: 0.100000\n",
      " 31541/50000: episode: 7442, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 876.411326, mae: 597.065002, accuracy: 0.062500, mean_q: -122.298480, mean_eps: 0.100000\n",
      " 31544/50000: episode: 7443, duration: 0.014s, episode steps:   3, steps per second: 216, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 1064.733093, mae: 551.427673, accuracy: 0.072917, mean_q: -127.646937, mean_eps: 0.100000\n",
      " 31547/50000: episode: 7444, duration: 0.015s, episode steps:   3, steps per second: 195, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 1278.135742, mae: 580.555196, accuracy: 0.114583, mean_q: -115.544248, mean_eps: 0.100000\n",
      " 31550/50000: episode: 7445, duration: 0.016s, episode steps:   3, steps per second: 191, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 617.536825, mae: 558.939738, accuracy: 0.093750, mean_q: -130.095856, mean_eps: 0.100000\n",
      " 31553/50000: episode: 7446, duration: 0.020s, episode steps:   3, steps per second: 147, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 960.976888, mae: 574.710571, accuracy: 0.083333, mean_q: -129.494336, mean_eps: 0.100000\n",
      " 31556/50000: episode: 7447, duration: 0.017s, episode steps:   3, steps per second: 182, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 716.659973, mae: 579.606689, accuracy: 0.083333, mean_q: -129.661962, mean_eps: 0.100000\n",
      " 31559/50000: episode: 7448, duration: 0.014s, episode steps:   3, steps per second: 217, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 1008.573334, mae: 596.322550, accuracy: 0.062500, mean_q: -128.084897, mean_eps: 0.100000\n",
      " 31562/50000: episode: 7449, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 755.848796, mae: 598.188843, accuracy: 0.114583, mean_q: -129.983991, mean_eps: 0.100000\n",
      " 31565/50000: episode: 7450, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 1467.715841, mae: 582.934123, accuracy: 0.083333, mean_q: -131.201312, mean_eps: 0.100000\n",
      " 31568/50000: episode: 7451, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 1005.889608, mae: 578.831441, accuracy: 0.093750, mean_q: -135.960271, mean_eps: 0.100000\n",
      " 31571/50000: episode: 7452, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 1041.789937, mae: 608.813985, accuracy: 0.093750, mean_q: -124.410594, mean_eps: 0.100000\n",
      " 31574/50000: episode: 7453, duration: 0.013s, episode steps:   3, steps per second: 230, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 659.789775, mae: 595.199280, accuracy: 0.052083, mean_q: -129.299123, mean_eps: 0.100000\n",
      " 31577/50000: episode: 7454, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 1008.021383, mae: 589.422160, accuracy: 0.083333, mean_q: -124.798058, mean_eps: 0.100000\n",
      " 31580/50000: episode: 7455, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 1021.144023, mae: 602.754069, accuracy: 0.062500, mean_q: -126.255379, mean_eps: 0.100000\n",
      " 31583/50000: episode: 7456, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 908.799479, mae: 594.803060, accuracy: 0.135417, mean_q: -122.097341, mean_eps: 0.100000\n",
      " 31586/50000: episode: 7457, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 1192.130737, mae: 597.325012, accuracy: 0.072917, mean_q: -125.374453, mean_eps: 0.100000\n",
      " 31589/50000: episode: 7458, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 1021.088603, mae: 605.412313, accuracy: 0.135417, mean_q: -116.508197, mean_eps: 0.100000\n",
      " 31592/50000: episode: 7459, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 758.486135, mae: 597.594543, accuracy: 0.135417, mean_q: -123.675412, mean_eps: 0.100000\n",
      " 31595/50000: episode: 7460, duration: 0.013s, episode steps:   3, steps per second: 226, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 1267.671773, mae: 592.943481, accuracy: 0.114583, mean_q: -125.770884, mean_eps: 0.100000\n",
      " 31598/50000: episode: 7461, duration: 0.019s, episode steps:   3, steps per second: 160, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 928.878540, mae: 602.436178, accuracy: 0.093750, mean_q: -129.382373, mean_eps: 0.100000\n",
      " 31602/50000: episode: 7462, duration: 0.020s, episode steps:   4, steps per second: 197, episode reward: -1223.000, mean reward: -305.750 [-999.000, -58.000], mean action: 1.750 [0.000, 3.000],  loss: 700.248154, mae: 588.570831, accuracy: 0.093750, mean_q: -124.237764, mean_eps: 0.100000\n",
      " 31605/50000: episode: 7463, duration: 0.013s, episode steps:   3, steps per second: 222, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 931.061584, mae: 606.692261, accuracy: 0.072917, mean_q: -127.735092, mean_eps: 0.100000\n",
      " 31608/50000: episode: 7464, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 874.593282, mae: 604.499980, accuracy: 0.062500, mean_q: -135.858795, mean_eps: 0.100000\n",
      " 31611/50000: episode: 7465, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 868.975301, mae: 587.205404, accuracy: 0.083333, mean_q: -127.869871, mean_eps: 0.100000\n",
      " 31614/50000: episode: 7466, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 1174.579183, mae: 557.783305, accuracy: 0.125000, mean_q: -121.030599, mean_eps: 0.100000\n",
      " 31617/50000: episode: 7467, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 1182.621134, mae: 585.447835, accuracy: 0.125000, mean_q: -114.568863, mean_eps: 0.100000\n",
      " 31620/50000: episode: 7468, duration: 0.013s, episode steps:   3, steps per second: 224, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 1148.840535, mae: 552.255147, accuracy: 0.104167, mean_q: -135.542191, mean_eps: 0.100000\n",
      " 31623/50000: episode: 7469, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 1243.588969, mae: 601.783854, accuracy: 0.093750, mean_q: -128.224854, mean_eps: 0.100000\n",
      " 31626/50000: episode: 7470, duration: 0.013s, episode steps:   3, steps per second: 225, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 859.021627, mae: 561.803101, accuracy: 0.166667, mean_q: -122.865250, mean_eps: 0.100000\n",
      " 31630/50000: episode: 7471, duration: 0.017s, episode steps:   4, steps per second: 230, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 1035.843735, mae: 562.647873, accuracy: 0.125000, mean_q: -125.782539, mean_eps: 0.100000\n",
      " 31633/50000: episode: 7472, duration: 0.013s, episode steps:   3, steps per second: 225, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 1393.757955, mae: 581.528646, accuracy: 0.125000, mean_q: -118.898354, mean_eps: 0.100000\n",
      " 31636/50000: episode: 7473, duration: 0.014s, episode steps:   3, steps per second: 217, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 760.204793, mae: 586.159159, accuracy: 0.135417, mean_q: -124.889872, mean_eps: 0.100000\n",
      " 31639/50000: episode: 7474, duration: 0.014s, episode steps:   3, steps per second: 214, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 1019.539754, mae: 599.221415, accuracy: 0.083333, mean_q: -131.465169, mean_eps: 0.100000\n",
      " 31642/50000: episode: 7475, duration: 0.013s, episode steps:   3, steps per second: 223, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 866.398132, mae: 602.671122, accuracy: 0.041667, mean_q: -138.440084, mean_eps: 0.100000\n",
      " 31645/50000: episode: 7476, duration: 0.028s, episode steps:   3, steps per second: 106, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 504.555969, mae: 577.985006, accuracy: 0.135417, mean_q: -122.658618, mean_eps: 0.100000\n",
      " 31648/50000: episode: 7477, duration: 0.022s, episode steps:   3, steps per second: 136, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 1438.787313, mae: 580.745097, accuracy: 0.083333, mean_q: -118.077769, mean_eps: 0.100000\n",
      " 31651/50000: episode: 7478, duration: 0.014s, episode steps:   3, steps per second: 208, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 921.716878, mae: 551.774129, accuracy: 0.093750, mean_q: -127.018122, mean_eps: 0.100000\n",
      " 31654/50000: episode: 7479, duration: 0.014s, episode steps:   3, steps per second: 210, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 2.000 [1.000, 3.000],  loss: 814.049906, mae: 591.977926, accuracy: 0.104167, mean_q: -120.782186, mean_eps: 0.100000\n",
      " 31657/50000: episode: 7480, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 960.938497, mae: 585.843221, accuracy: 0.114583, mean_q: -120.256358, mean_eps: 0.100000\n",
      " 31660/50000: episode: 7481, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 848.004964, mae: 598.674744, accuracy: 0.072917, mean_q: -131.137227, mean_eps: 0.100000\n",
      " 31664/50000: episode: 7482, duration: 0.016s, episode steps:   4, steps per second: 253, episode reward: -1223.000, mean reward: -305.750 [-999.000, -58.000], mean action: 1.750 [0.000, 3.000],  loss: 1068.097610, mae: 618.994888, accuracy: 0.078125, mean_q: -129.348045, mean_eps: 0.100000\n",
      " 31667/50000: episode: 7483, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 1002.205037, mae: 590.352763, accuracy: 0.062500, mean_q: -128.785756, mean_eps: 0.100000\n",
      " 31670/50000: episode: 7484, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 772.754476, mae: 598.523905, accuracy: 0.104167, mean_q: -122.238472, mean_eps: 0.100000\n",
      " 31673/50000: episode: 7485, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 1140.022786, mae: 595.554362, accuracy: 0.177083, mean_q: -123.250341, mean_eps: 0.100000\n",
      " 31676/50000: episode: 7486, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 798.157471, mae: 588.772237, accuracy: 0.104167, mean_q: -121.518883, mean_eps: 0.100000\n",
      " 31679/50000: episode: 7487, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 1246.033529, mae: 587.371663, accuracy: 0.083333, mean_q: -126.838671, mean_eps: 0.100000\n",
      " 31682/50000: episode: 7488, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 765.741720, mae: 579.949565, accuracy: 0.093750, mean_q: -125.271601, mean_eps: 0.100000\n",
      " 31685/50000: episode: 7489, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 942.647237, mae: 616.909220, accuracy: 0.083333, mean_q: -121.318507, mean_eps: 0.100000\n",
      " 31688/50000: episode: 7490, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 928.633972, mae: 567.797160, accuracy: 0.114583, mean_q: -131.055527, mean_eps: 0.100000\n",
      " 31691/50000: episode: 7491, duration: 0.015s, episode steps:   3, steps per second: 201, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 1252.344706, mae: 589.500753, accuracy: 0.135417, mean_q: -125.828857, mean_eps: 0.100000\n",
      " 31694/50000: episode: 7492, duration: 0.015s, episode steps:   3, steps per second: 197, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 968.234660, mae: 576.197286, accuracy: 0.135417, mean_q: -116.426450, mean_eps: 0.100000\n",
      " 31697/50000: episode: 7493, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 767.501882, mae: 608.400106, accuracy: 0.093750, mean_q: -119.962479, mean_eps: 0.100000\n",
      " 31700/50000: episode: 7494, duration: 0.013s, episode steps:   3, steps per second: 230, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 1410.305603, mae: 574.843648, accuracy: 0.093750, mean_q: -131.944041, mean_eps: 0.100000\n",
      " 31703/50000: episode: 7495, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 613.568034, mae: 583.588216, accuracy: 0.072917, mean_q: -131.456192, mean_eps: 0.100000\n",
      " 31706/50000: episode: 7496, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 681.639028, mae: 594.877665, accuracy: 0.083333, mean_q: -134.176356, mean_eps: 0.100000\n",
      " 31709/50000: episode: 7497, duration: 0.013s, episode steps:   3, steps per second: 227, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 947.761475, mae: 585.446431, accuracy: 0.052083, mean_q: -134.930901, mean_eps: 0.100000\n",
      " 31712/50000: episode: 7498, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 877.993245, mae: 585.639119, accuracy: 0.125000, mean_q: -120.956375, mean_eps: 0.100000\n",
      " 31715/50000: episode: 7499, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 647.126770, mae: 584.715617, accuracy: 0.093750, mean_q: -120.259168, mean_eps: 0.100000\n",
      " 31718/50000: episode: 7500, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 993.915446, mae: 596.275045, accuracy: 0.072917, mean_q: -129.130178, mean_eps: 0.100000\n",
      " 31721/50000: episode: 7501, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 908.404704, mae: 576.697489, accuracy: 0.135417, mean_q: -131.157166, mean_eps: 0.100000\n",
      " 31724/50000: episode: 7502, duration: 0.013s, episode steps:   3, steps per second: 226, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 682.656657, mae: 575.065918, accuracy: 0.083333, mean_q: -137.269699, mean_eps: 0.100000\n",
      " 31727/50000: episode: 7503, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 921.011536, mae: 616.079569, accuracy: 0.072917, mean_q: -126.619703, mean_eps: 0.100000\n",
      " 31731/50000: episode: 7504, duration: 0.016s, episode steps:   4, steps per second: 250, episode reward: -1238.000, mean reward: -309.500 [-999.000, -45.000], mean action: 1.750 [0.000, 3.000],  loss: 599.283066, mae: 610.821167, accuracy: 0.046875, mean_q: -121.849911, mean_eps: 0.100000\n",
      " 31734/50000: episode: 7505, duration: 0.013s, episode steps:   3, steps per second: 225, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 981.262207, mae: 595.778280, accuracy: 0.125000, mean_q: -124.111773, mean_eps: 0.100000\n",
      " 31737/50000: episode: 7506, duration: 0.014s, episode steps:   3, steps per second: 214, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 623.052897, mae: 584.000854, accuracy: 0.062500, mean_q: -128.364990, mean_eps: 0.100000\n",
      " 31740/50000: episode: 7507, duration: 0.019s, episode steps:   3, steps per second: 158, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 653.708537, mae: 571.513713, accuracy: 0.114583, mean_q: -128.325030, mean_eps: 0.100000\n",
      " 31743/50000: episode: 7508, duration: 0.014s, episode steps:   3, steps per second: 217, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 692.671865, mae: 570.507304, accuracy: 0.125000, mean_q: -116.910637, mean_eps: 0.100000\n",
      " 31747/50000: episode: 7509, duration: 0.017s, episode steps:   4, steps per second: 241, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 2.000 [0.000, 3.000],  loss: 628.694077, mae: 627.049454, accuracy: 0.062500, mean_q: -116.392744, mean_eps: 0.100000\n",
      " 31750/50000: episode: 7510, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 614.055318, mae: 604.280680, accuracy: 0.104167, mean_q: -118.965164, mean_eps: 0.100000\n",
      " 31753/50000: episode: 7511, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 669.444010, mae: 607.219869, accuracy: 0.041667, mean_q: -133.144445, mean_eps: 0.100000\n",
      " 31756/50000: episode: 7512, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 511.637604, mae: 597.611654, accuracy: 0.041667, mean_q: -137.098455, mean_eps: 0.100000\n",
      " 31761/50000: episode: 7513, duration: 0.019s, episode steps:   5, steps per second: 266, episode reward: -2222.000, mean reward: -444.400 [-999.000, -58.000], mean action: 2.000 [0.000, 3.000],  loss: 866.760608, mae: 595.326025, accuracy: 0.118750, mean_q: -130.023978, mean_eps: 0.100000\n",
      " 31764/50000: episode: 7514, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 2.000 [1.000, 3.000],  loss: 639.460337, mae: 590.873230, accuracy: 0.114583, mean_q: -125.599988, mean_eps: 0.100000\n",
      " 31767/50000: episode: 7515, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 1053.202738, mae: 588.923665, accuracy: 0.125000, mean_q: -131.105123, mean_eps: 0.100000\n",
      " 31770/50000: episode: 7516, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 544.742126, mae: 582.700521, accuracy: 0.114583, mean_q: -128.790807, mean_eps: 0.100000\n",
      " 31773/50000: episode: 7517, duration: 0.013s, episode steps:   3, steps per second: 230, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 464.228643, mae: 604.216736, accuracy: 0.104167, mean_q: -134.711355, mean_eps: 0.100000\n",
      " 31777/50000: episode: 7518, duration: 0.016s, episode steps:   4, steps per second: 253, episode reward: -1238.000, mean reward: -309.500 [-999.000, -60.000], mean action: 1.500 [0.000, 3.000],  loss: 1017.445618, mae: 584.484650, accuracy: 0.046875, mean_q: -129.948292, mean_eps: 0.100000\n",
      " 31780/50000: episode: 7519, duration: 0.014s, episode steps:   3, steps per second: 208, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 1098.704651, mae: 603.127035, accuracy: 0.072917, mean_q: -119.496900, mean_eps: 0.100000\n",
      " 31783/50000: episode: 7520, duration: 0.016s, episode steps:   3, steps per second: 189, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 651.052653, mae: 586.709127, accuracy: 0.114583, mean_q: -125.889826, mean_eps: 0.100000\n",
      " 31786/50000: episode: 7521, duration: 0.022s, episode steps:   3, steps per second: 135, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 986.779338, mae: 602.206604, accuracy: 0.093750, mean_q: -127.611913, mean_eps: 0.100000\n",
      " 31789/50000: episode: 7522, duration: 0.020s, episode steps:   3, steps per second: 152, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 908.962433, mae: 600.178955, accuracy: 0.083333, mean_q: -129.527715, mean_eps: 0.100000\n",
      " 31792/50000: episode: 7523, duration: 0.015s, episode steps:   3, steps per second: 200, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 1003.689819, mae: 608.856913, accuracy: 0.083333, mean_q: -124.618385, mean_eps: 0.100000\n",
      " 31795/50000: episode: 7524, duration: 0.014s, episode steps:   3, steps per second: 209, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 783.895060, mae: 615.624959, accuracy: 0.062500, mean_q: -124.916687, mean_eps: 0.100000\n",
      " 31798/50000: episode: 7525, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 906.918365, mae: 584.169637, accuracy: 0.052083, mean_q: -137.986407, mean_eps: 0.100000\n",
      " 31801/50000: episode: 7526, duration: 0.015s, episode steps:   3, steps per second: 204, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 902.132751, mae: 550.796163, accuracy: 0.156250, mean_q: -134.506566, mean_eps: 0.100000\n",
      " 31804/50000: episode: 7527, duration: 0.014s, episode steps:   3, steps per second: 220, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 448.394221, mae: 593.195028, accuracy: 0.104167, mean_q: -124.747309, mean_eps: 0.100000\n",
      " 31807/50000: episode: 7528, duration: 0.014s, episode steps:   3, steps per second: 215, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 1222.713074, mae: 587.076396, accuracy: 0.072917, mean_q: -126.807264, mean_eps: 0.100000\n",
      " 31810/50000: episode: 7529, duration: 0.013s, episode steps:   3, steps per second: 225, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 581.873515, mae: 597.888204, accuracy: 0.093750, mean_q: -132.324287, mean_eps: 0.100000\n",
      " 31813/50000: episode: 7530, duration: 0.013s, episode steps:   3, steps per second: 223, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 905.926687, mae: 605.806335, accuracy: 0.072917, mean_q: -138.099167, mean_eps: 0.100000\n",
      " 31816/50000: episode: 7531, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 691.426778, mae: 566.407084, accuracy: 0.135417, mean_q: -131.382093, mean_eps: 0.100000\n",
      " 31819/50000: episode: 7532, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 664.380493, mae: 573.473267, accuracy: 0.083333, mean_q: -134.768883, mean_eps: 0.100000\n",
      " 31823/50000: episode: 7533, duration: 0.015s, episode steps:   4, steps per second: 259, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 754.622177, mae: 599.672318, accuracy: 0.078125, mean_q: -130.299820, mean_eps: 0.100000\n",
      " 31827/50000: episode: 7534, duration: 0.016s, episode steps:   4, steps per second: 255, episode reward: -1238.000, mean reward: -309.500 [-999.000, -60.000], mean action: 1.500 [0.000, 3.000],  loss: 502.789581, mae: 591.824097, accuracy: 0.085938, mean_q: -130.617451, mean_eps: 0.100000\n",
      " 31831/50000: episode: 7535, duration: 0.019s, episode steps:   4, steps per second: 206, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 801.590843, mae: 605.181686, accuracy: 0.078125, mean_q: -137.546432, mean_eps: 0.100000\n",
      " 31834/50000: episode: 7536, duration: 0.018s, episode steps:   3, steps per second: 168, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 1035.497986, mae: 547.886251, accuracy: 0.104167, mean_q: -139.210068, mean_eps: 0.100000\n",
      " 31837/50000: episode: 7537, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 613.776693, mae: 586.851074, accuracy: 0.114583, mean_q: -116.113762, mean_eps: 0.100000\n",
      " 31840/50000: episode: 7538, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 1162.899577, mae: 595.603882, accuracy: 0.062500, mean_q: -120.139203, mean_eps: 0.100000\n",
      " 31843/50000: episode: 7539, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 721.511820, mae: 596.386373, accuracy: 0.083333, mean_q: -125.547971, mean_eps: 0.100000\n",
      " 31846/50000: episode: 7540, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 987.891500, mae: 578.269104, accuracy: 0.156250, mean_q: -129.124687, mean_eps: 0.100000\n",
      " 31849/50000: episode: 7541, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 620.818522, mae: 590.501058, accuracy: 0.052083, mean_q: -136.829402, mean_eps: 0.100000\n",
      " 31852/50000: episode: 7542, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 648.227681, mae: 599.930522, accuracy: 0.135417, mean_q: -130.310425, mean_eps: 0.100000\n",
      " 31856/50000: episode: 7543, duration: 0.016s, episode steps:   4, steps per second: 253, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 1200.235687, mae: 588.714172, accuracy: 0.117188, mean_q: -126.002844, mean_eps: 0.100000\n",
      " 31859/50000: episode: 7544, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 410.606527, mae: 596.082316, accuracy: 0.104167, mean_q: -127.960648, mean_eps: 0.100000\n",
      " 31862/50000: episode: 7545, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 609.824910, mae: 591.308451, accuracy: 0.145833, mean_q: -127.847562, mean_eps: 0.100000\n",
      " 31865/50000: episode: 7546, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 1111.569458, mae: 577.754985, accuracy: 0.104167, mean_q: -137.619603, mean_eps: 0.100000\n",
      " 31868/50000: episode: 7547, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 790.570638, mae: 587.041646, accuracy: 0.083333, mean_q: -131.886243, mean_eps: 0.100000\n",
      " 31871/50000: episode: 7548, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 762.233541, mae: 598.615560, accuracy: 0.114583, mean_q: -122.159658, mean_eps: 0.100000\n",
      " 31874/50000: episode: 7549, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 597.390371, mae: 584.398661, accuracy: 0.114583, mean_q: -126.865822, mean_eps: 0.100000\n",
      " 31879/50000: episode: 7550, duration: 0.019s, episode steps:   5, steps per second: 260, episode reward: -2222.000, mean reward: -444.400 [-999.000, -58.000], mean action: 1.800 [0.000, 3.000],  loss: 796.915344, mae: 592.248389, accuracy: 0.081250, mean_q: -129.054280, mean_eps: 0.100000\n",
      " 31882/50000: episode: 7551, duration: 0.013s, episode steps:   3, steps per second: 225, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 675.923258, mae: 595.192830, accuracy: 0.114583, mean_q: -127.397550, mean_eps: 0.100000\n",
      " 31885/50000: episode: 7552, duration: 0.014s, episode steps:   3, steps per second: 218, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 483.063446, mae: 592.434285, accuracy: 0.104167, mean_q: -125.508766, mean_eps: 0.100000\n",
      " 31888/50000: episode: 7553, duration: 0.015s, episode steps:   3, steps per second: 201, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 708.573039, mae: 585.168742, accuracy: 0.166667, mean_q: -124.116391, mean_eps: 0.100000\n",
      " 31891/50000: episode: 7554, duration: 0.015s, episode steps:   3, steps per second: 206, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 630.283783, mae: 571.942179, accuracy: 0.104167, mean_q: -130.189735, mean_eps: 0.100000\n",
      " 31894/50000: episode: 7555, duration: 0.013s, episode steps:   3, steps per second: 223, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 837.316813, mae: 595.233378, accuracy: 0.114583, mean_q: -129.087133, mean_eps: 0.100000\n",
      " 31897/50000: episode: 7556, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 1134.839884, mae: 596.674662, accuracy: 0.062500, mean_q: -133.929448, mean_eps: 0.100000\n",
      " 31900/50000: episode: 7557, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 924.073975, mae: 561.157715, accuracy: 0.114583, mean_q: -133.519811, mean_eps: 0.100000\n",
      " 31903/50000: episode: 7558, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 507.133403, mae: 599.332947, accuracy: 0.125000, mean_q: -119.236928, mean_eps: 0.100000\n",
      " 31906/50000: episode: 7559, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 624.061473, mae: 593.558105, accuracy: 0.114583, mean_q: -117.566592, mean_eps: 0.100000\n",
      " 31910/50000: episode: 7560, duration: 0.016s, episode steps:   4, steps per second: 258, episode reward: -1223.000, mean reward: -305.750 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 740.060333, mae: 600.379623, accuracy: 0.109375, mean_q: -132.497211, mean_eps: 0.100000\n",
      " 31913/50000: episode: 7561, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 583.776245, mae: 602.369385, accuracy: 0.125000, mean_q: -136.080083, mean_eps: 0.100000\n",
      " 31917/50000: episode: 7562, duration: 0.016s, episode steps:   4, steps per second: 257, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 795.440048, mae: 586.608536, accuracy: 0.109375, mean_q: -138.347744, mean_eps: 0.100000\n",
      " 31920/50000: episode: 7563, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 1180.783285, mae: 546.671021, accuracy: 0.125000, mean_q: -132.775223, mean_eps: 0.100000\n",
      " 31923/50000: episode: 7564, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 780.464762, mae: 570.707397, accuracy: 0.083333, mean_q: -116.544993, mean_eps: 0.100000\n",
      " 31927/50000: episode: 7565, duration: 0.016s, episode steps:   4, steps per second: 246, episode reward: -1238.000, mean reward: -309.500 [-999.000, -45.000], mean action: 1.750 [0.000, 3.000],  loss: 660.936768, mae: 579.911392, accuracy: 0.132812, mean_q: -122.622782, mean_eps: 0.100000\n",
      " 31930/50000: episode: 7566, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 721.401896, mae: 554.470357, accuracy: 0.083333, mean_q: -138.405370, mean_eps: 0.100000\n",
      " 31933/50000: episode: 7567, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 746.771535, mae: 586.368408, accuracy: 0.062500, mean_q: -133.613597, mean_eps: 0.100000\n",
      " 31937/50000: episode: 7568, duration: 0.022s, episode steps:   4, steps per second: 181, episode reward: -1238.000, mean reward: -309.500 [-999.000, -45.000], mean action: 1.750 [0.000, 3.000],  loss: 815.207031, mae: 613.664368, accuracy: 0.078125, mean_q: -124.935417, mean_eps: 0.100000\n",
      " 31940/50000: episode: 7569, duration: 0.014s, episode steps:   3, steps per second: 221, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 1027.732218, mae: 614.081970, accuracy: 0.114583, mean_q: -128.964223, mean_eps: 0.100000\n",
      " 31944/50000: episode: 7570, duration: 0.018s, episode steps:   4, steps per second: 220, episode reward: -1238.000, mean reward: -309.500 [-999.000, -60.000], mean action: 2.250 [1.000, 3.000],  loss: 694.691849, mae: 569.309509, accuracy: 0.093750, mean_q: -132.736605, mean_eps: 0.100000\n",
      " 31947/50000: episode: 7571, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 512.481394, mae: 595.960592, accuracy: 0.031250, mean_q: -128.559118, mean_eps: 0.100000\n",
      " 31950/50000: episode: 7572, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 802.646861, mae: 589.177002, accuracy: 0.114583, mean_q: -131.238650, mean_eps: 0.100000\n",
      " 31953/50000: episode: 7573, duration: 0.020s, episode steps:   3, steps per second: 147, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 466.850505, mae: 571.263387, accuracy: 0.125000, mean_q: -132.431636, mean_eps: 0.100000\n",
      " 31956/50000: episode: 7574, duration: 0.016s, episode steps:   3, steps per second: 182, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 545.934825, mae: 590.792236, accuracy: 0.145833, mean_q: -136.473887, mean_eps: 0.100000\n",
      " 31960/50000: episode: 7575, duration: 0.016s, episode steps:   4, steps per second: 251, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 685.674606, mae: 571.587921, accuracy: 0.132812, mean_q: -138.749199, mean_eps: 0.100000\n",
      " 31963/50000: episode: 7576, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 573.501383, mae: 565.618306, accuracy: 0.062500, mean_q: -135.220665, mean_eps: 0.100000\n",
      " 31966/50000: episode: 7577, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 587.347534, mae: 589.814819, accuracy: 0.145833, mean_q: -138.602834, mean_eps: 0.100000\n",
      " 31970/50000: episode: 7578, duration: 0.015s, episode steps:   4, steps per second: 258, episode reward: -1238.000, mean reward: -309.500 [-999.000, -60.000], mean action: 1.750 [1.000, 3.000],  loss: 983.188492, mae: 585.891632, accuracy: 0.093750, mean_q: -136.100754, mean_eps: 0.100000\n",
      " 31973/50000: episode: 7579, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 472.438599, mae: 573.372131, accuracy: 0.156250, mean_q: -130.779622, mean_eps: 0.100000\n",
      " 31976/50000: episode: 7580, duration: 0.013s, episode steps:   3, steps per second: 225, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 557.421672, mae: 608.106791, accuracy: 0.083333, mean_q: -127.455915, mean_eps: 0.100000\n",
      " 31979/50000: episode: 7581, duration: 0.016s, episode steps:   3, steps per second: 193, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 569.052343, mae: 561.745667, accuracy: 0.083333, mean_q: -136.448171, mean_eps: 0.100000\n",
      " 31982/50000: episode: 7582, duration: 0.016s, episode steps:   3, steps per second: 186, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 741.790568, mae: 591.874959, accuracy: 0.114583, mean_q: -131.197932, mean_eps: 0.100000\n",
      " 31985/50000: episode: 7583, duration: 0.015s, episode steps:   3, steps per second: 203, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 544.724599, mae: 592.720723, accuracy: 0.093750, mean_q: -131.811996, mean_eps: 0.100000\n",
      " 31988/50000: episode: 7584, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 400.364309, mae: 578.158712, accuracy: 0.072917, mean_q: -143.688950, mean_eps: 0.100000\n",
      " 31991/50000: episode: 7585, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 1011.394511, mae: 580.560994, accuracy: 0.104167, mean_q: -138.875473, mean_eps: 0.100000\n",
      " 31994/50000: episode: 7586, duration: 0.014s, episode steps:   3, steps per second: 210, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 561.335337, mae: 576.163086, accuracy: 0.145833, mean_q: -135.366842, mean_eps: 0.100000\n",
      " 31997/50000: episode: 7587, duration: 0.014s, episode steps:   3, steps per second: 211, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 807.694092, mae: 559.908671, accuracy: 0.125000, mean_q: -127.646584, mean_eps: 0.100000\n",
      " 32000/50000: episode: 7588, duration: 0.013s, episode steps:   3, steps per second: 227, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 388.454824, mae: 564.949524, accuracy: 0.083333, mean_q: -133.712880, mean_eps: 0.100000\n",
      " 32003/50000: episode: 7589, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 935.532694, mae: 595.144816, accuracy: 0.083333, mean_q: -142.906494, mean_eps: 0.100000\n",
      " 32006/50000: episode: 7590, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 1111.507650, mae: 574.681661, accuracy: 0.114583, mean_q: -142.444702, mean_eps: 0.100000\n",
      " 32009/50000: episode: 7591, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 588.521769, mae: 578.033997, accuracy: 0.083333, mean_q: -127.600883, mean_eps: 0.100000\n",
      " 32012/50000: episode: 7592, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 931.083811, mae: 565.192362, accuracy: 0.166667, mean_q: -127.565201, mean_eps: 0.100000\n",
      " 32015/50000: episode: 7593, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 618.446238, mae: 584.783346, accuracy: 0.145833, mean_q: -129.429128, mean_eps: 0.100000\n",
      " 32018/50000: episode: 7594, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 595.999725, mae: 577.040487, accuracy: 0.062500, mean_q: -141.386709, mean_eps: 0.100000\n",
      " 32021/50000: episode: 7595, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 773.783539, mae: 583.215088, accuracy: 0.104167, mean_q: -132.096227, mean_eps: 0.100000\n",
      " 32024/50000: episode: 7596, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 562.596608, mae: 576.404989, accuracy: 0.104167, mean_q: -133.486099, mean_eps: 0.100000\n",
      " 32027/50000: episode: 7597, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 494.836365, mae: 605.368286, accuracy: 0.083333, mean_q: -133.915655, mean_eps: 0.100000\n",
      " 32030/50000: episode: 7598, duration: 0.014s, episode steps:   3, steps per second: 214, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 603.439606, mae: 584.851685, accuracy: 0.177083, mean_q: -131.016177, mean_eps: 0.100000\n",
      " 32033/50000: episode: 7599, duration: 0.019s, episode steps:   3, steps per second: 158, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 952.040385, mae: 599.592834, accuracy: 0.125000, mean_q: -128.790232, mean_eps: 0.100000\n",
      " 32036/50000: episode: 7600, duration: 0.014s, episode steps:   3, steps per second: 214, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 517.644124, mae: 603.777893, accuracy: 0.125000, mean_q: -129.519076, mean_eps: 0.100000\n",
      " 32039/50000: episode: 7601, duration: 0.014s, episode steps:   3, steps per second: 215, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 903.832967, mae: 590.122925, accuracy: 0.114583, mean_q: -133.732450, mean_eps: 0.100000\n",
      " 32042/50000: episode: 7602, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 858.039103, mae: 589.071716, accuracy: 0.072917, mean_q: -133.021515, mean_eps: 0.100000\n",
      " 32045/50000: episode: 7603, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 837.187337, mae: 549.502981, accuracy: 0.062500, mean_q: -134.446147, mean_eps: 0.100000\n",
      " 32048/50000: episode: 7604, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 658.658590, mae: 592.611043, accuracy: 0.093750, mean_q: -130.420977, mean_eps: 0.100000\n",
      " 32051/50000: episode: 7605, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 679.817729, mae: 588.382935, accuracy: 0.104167, mean_q: -129.092690, mean_eps: 0.100000\n",
      " 32054/50000: episode: 7606, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 771.096425, mae: 567.860555, accuracy: 0.145833, mean_q: -128.309619, mean_eps: 0.100000\n",
      " 32058/50000: episode: 7607, duration: 0.015s, episode steps:   4, steps per second: 264, episode reward: -1238.000, mean reward: -309.500 [-999.000, -60.000], mean action: 1.500 [0.000, 3.000],  loss: 762.832977, mae: 571.185806, accuracy: 0.140625, mean_q: -134.391705, mean_eps: 0.100000\n",
      " 32061/50000: episode: 7608, duration: 0.014s, episode steps:   3, steps per second: 212, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 882.337484, mae: 599.528890, accuracy: 0.083333, mean_q: -136.641439, mean_eps: 0.100000\n",
      " 32065/50000: episode: 7609, duration: 0.017s, episode steps:   4, steps per second: 235, episode reward: -1238.000, mean reward: -309.500 [-999.000, -60.000], mean action: 1.500 [0.000, 3.000],  loss: 622.135365, mae: 599.205902, accuracy: 0.046875, mean_q: -142.639278, mean_eps: 0.100000\n",
      " 32069/50000: episode: 7610, duration: 0.017s, episode steps:   4, steps per second: 229, episode reward: -1238.000, mean reward: -309.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 652.441483, mae: 616.920288, accuracy: 0.062500, mean_q: -132.764280, mean_eps: 0.100000\n",
      " 32072/50000: episode: 7611, duration: 0.014s, episode steps:   3, steps per second: 215, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 429.674154, mae: 599.943929, accuracy: 0.125000, mean_q: -130.875554, mean_eps: 0.100000\n",
      " 32075/50000: episode: 7612, duration: 0.015s, episode steps:   3, steps per second: 204, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 682.475464, mae: 592.417908, accuracy: 0.145833, mean_q: -134.311859, mean_eps: 0.100000\n",
      " 32078/50000: episode: 7613, duration: 0.014s, episode steps:   3, steps per second: 208, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 556.667236, mae: 571.659566, accuracy: 0.125000, mean_q: -131.064494, mean_eps: 0.100000\n",
      " 32081/50000: episode: 7614, duration: 0.014s, episode steps:   3, steps per second: 211, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.333 [0.000, 3.000],  loss: 1134.565104, mae: 591.832947, accuracy: 0.052083, mean_q: -135.127640, mean_eps: 0.100000\n",
      " 32084/50000: episode: 7615, duration: 0.015s, episode steps:   3, steps per second: 196, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 789.909403, mae: 599.174052, accuracy: 0.083333, mean_q: -137.493958, mean_eps: 0.100000\n",
      " 32087/50000: episode: 7616, duration: 0.014s, episode steps:   3, steps per second: 214, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 1048.645355, mae: 588.619710, accuracy: 0.062500, mean_q: -143.592524, mean_eps: 0.100000\n",
      " 32090/50000: episode: 7617, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 768.338114, mae: 578.756836, accuracy: 0.145833, mean_q: -130.421193, mean_eps: 0.100000\n",
      " 32093/50000: episode: 7618, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 972.235758, mae: 581.922201, accuracy: 0.114583, mean_q: -116.739759, mean_eps: 0.100000\n",
      " 32096/50000: episode: 7619, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 719.410177, mae: 587.643188, accuracy: 0.104167, mean_q: -127.987734, mean_eps: 0.100000\n",
      " 32099/50000: episode: 7620, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 694.278422, mae: 588.476318, accuracy: 0.104167, mean_q: -131.463745, mean_eps: 0.100000\n",
      " 32102/50000: episode: 7621, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 735.986013, mae: 617.135376, accuracy: 0.072917, mean_q: -134.723460, mean_eps: 0.100000\n",
      " 32105/50000: episode: 7622, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -224.000, mean reward: -74.667 [-116.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 571.516469, mae: 566.454061, accuracy: 0.104167, mean_q: -137.853694, mean_eps: 0.100000\n",
      " 32108/50000: episode: 7623, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 509.558380, mae: 590.460999, accuracy: 0.114583, mean_q: -128.600655, mean_eps: 0.100000\n",
      " 32112/50000: episode: 7624, duration: 0.016s, episode steps:   4, steps per second: 257, episode reward: -1238.000, mean reward: -309.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 583.271332, mae: 596.792038, accuracy: 0.117188, mean_q: -127.618889, mean_eps: 0.100000\n",
      " 32115/50000: episode: 7625, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 766.017578, mae: 590.087484, accuracy: 0.052083, mean_q: -134.581177, mean_eps: 0.100000\n",
      " 32118/50000: episode: 7626, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 518.801300, mae: 585.582520, accuracy: 0.052083, mean_q: -143.987000, mean_eps: 0.100000\n",
      " 32121/50000: episode: 7627, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 641.485453, mae: 588.009318, accuracy: 0.041667, mean_q: -138.144445, mean_eps: 0.100000\n",
      " 32124/50000: episode: 7628, duration: 0.014s, episode steps:   3, steps per second: 219, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 753.337097, mae: 601.966003, accuracy: 0.093750, mean_q: -132.298370, mean_eps: 0.100000\n",
      " 32127/50000: episode: 7629, duration: 0.017s, episode steps:   3, steps per second: 177, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 525.658936, mae: 573.281250, accuracy: 0.104167, mean_q: -128.351054, mean_eps: 0.100000\n",
      " 32130/50000: episode: 7630, duration: 0.016s, episode steps:   3, steps per second: 190, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 510.927032, mae: 602.369303, accuracy: 0.093750, mean_q: -132.032450, mean_eps: 0.100000\n",
      " 32133/50000: episode: 7631, duration: 0.014s, episode steps:   3, steps per second: 221, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 611.819377, mae: 599.064799, accuracy: 0.145833, mean_q: -133.400889, mean_eps: 0.100000\n",
      " 32137/50000: episode: 7632, duration: 0.016s, episode steps:   4, steps per second: 249, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 614.822083, mae: 583.653122, accuracy: 0.070312, mean_q: -135.986818, mean_eps: 0.100000\n",
      " 32140/50000: episode: 7633, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 624.923462, mae: 600.355225, accuracy: 0.093750, mean_q: -132.916199, mean_eps: 0.100000\n",
      " 32143/50000: episode: 7634, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 732.265910, mae: 606.579041, accuracy: 0.104167, mean_q: -126.780945, mean_eps: 0.100000\n",
      " 32146/50000: episode: 7635, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 648.333323, mae: 564.062948, accuracy: 0.145833, mean_q: -139.906723, mean_eps: 0.100000\n",
      " 32149/50000: episode: 7636, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 513.984014, mae: 587.607605, accuracy: 0.125000, mean_q: -132.528300, mean_eps: 0.100000\n",
      " 32152/50000: episode: 7637, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 506.827647, mae: 608.944824, accuracy: 0.083333, mean_q: -131.029714, mean_eps: 0.100000\n",
      " 32155/50000: episode: 7638, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 614.515025, mae: 606.852722, accuracy: 0.125000, mean_q: -132.921850, mean_eps: 0.100000\n",
      " 32158/50000: episode: 7639, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 649.686605, mae: 578.038920, accuracy: 0.135417, mean_q: -134.704631, mean_eps: 0.100000\n",
      " 32161/50000: episode: 7640, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 409.725505, mae: 596.775859, accuracy: 0.052083, mean_q: -141.114436, mean_eps: 0.100000\n",
      " 32164/50000: episode: 7641, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 497.805745, mae: 587.707804, accuracy: 0.062500, mean_q: -132.875142, mean_eps: 0.100000\n",
      " 32167/50000: episode: 7642, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 480.544230, mae: 596.388652, accuracy: 0.052083, mean_q: -138.536275, mean_eps: 0.100000\n",
      " 32170/50000: episode: 7643, duration: 0.016s, episode steps:   3, steps per second: 184, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 394.168772, mae: 578.909831, accuracy: 0.031250, mean_q: -144.547058, mean_eps: 0.100000\n",
      " 32173/50000: episode: 7644, duration: 0.017s, episode steps:   3, steps per second: 176, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 565.812174, mae: 600.750956, accuracy: 0.125000, mean_q: -133.221125, mean_eps: 0.100000\n",
      " 32176/50000: episode: 7645, duration: 0.015s, episode steps:   3, steps per second: 205, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 494.575134, mae: 614.915181, accuracy: 0.083333, mean_q: -125.867999, mean_eps: 0.100000\n",
      " 32179/50000: episode: 7646, duration: 0.020s, episode steps:   3, steps per second: 147, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 464.547038, mae: 550.202677, accuracy: 0.104167, mean_q: -139.566355, mean_eps: 0.100000\n",
      " 32182/50000: episode: 7647, duration: 0.017s, episode steps:   3, steps per second: 177, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 346.498088, mae: 593.892375, accuracy: 0.156250, mean_q: -124.206223, mean_eps: 0.100000\n",
      " 32185/50000: episode: 7648, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 449.173269, mae: 610.574504, accuracy: 0.135417, mean_q: -132.930308, mean_eps: 0.100000\n",
      " 32188/50000: episode: 7649, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 490.062012, mae: 557.372518, accuracy: 0.083333, mean_q: -136.378988, mean_eps: 0.100000\n",
      " 32191/50000: episode: 7650, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 995.133077, mae: 578.996989, accuracy: 0.104167, mean_q: -135.721283, mean_eps: 0.100000\n",
      " 32195/50000: episode: 7651, duration: 0.016s, episode steps:   4, steps per second: 253, episode reward: -1238.000, mean reward: -309.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 835.165016, mae: 593.705109, accuracy: 0.046875, mean_q: -138.223976, mean_eps: 0.100000\n",
      " 32198/50000: episode: 7652, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 316.332286, mae: 574.948486, accuracy: 0.072917, mean_q: -134.742081, mean_eps: 0.100000\n",
      " 32202/50000: episode: 7653, duration: 0.016s, episode steps:   4, steps per second: 257, episode reward: -1238.000, mean reward: -309.500 [-999.000, -60.000], mean action: 1.500 [0.000, 3.000],  loss: 435.404533, mae: 592.854889, accuracy: 0.070312, mean_q: -131.354237, mean_eps: 0.100000\n",
      " 32205/50000: episode: 7654, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 476.539505, mae: 617.928507, accuracy: 0.052083, mean_q: -127.910090, mean_eps: 0.100000\n",
      " 32208/50000: episode: 7655, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 498.959757, mae: 612.602132, accuracy: 0.156250, mean_q: -135.757319, mean_eps: 0.100000\n",
      " 32212/50000: episode: 7656, duration: 0.016s, episode steps:   4, steps per second: 244, episode reward: -1238.000, mean reward: -309.500 [-999.000, -45.000], mean action: 1.250 [0.000, 3.000],  loss: 616.339931, mae: 588.873871, accuracy: 0.062500, mean_q: -138.535805, mean_eps: 0.100000\n",
      " 32215/50000: episode: 7657, duration: 0.014s, episode steps:   3, steps per second: 218, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 795.607402, mae: 553.641439, accuracy: 0.104167, mean_q: -141.921412, mean_eps: 0.100000\n",
      " 32218/50000: episode: 7658, duration: 0.018s, episode steps:   3, steps per second: 164, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 549.803528, mae: 606.442383, accuracy: 0.104167, mean_q: -131.265673, mean_eps: 0.100000\n",
      " 32221/50000: episode: 7659, duration: 0.017s, episode steps:   3, steps per second: 175, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 573.068705, mae: 559.781331, accuracy: 0.156250, mean_q: -130.763349, mean_eps: 0.100000\n",
      " 32224/50000: episode: 7660, duration: 0.013s, episode steps:   3, steps per second: 222, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 586.855306, mae: 575.219218, accuracy: 0.072917, mean_q: -129.397349, mean_eps: 0.100000\n",
      " 32227/50000: episode: 7661, duration: 0.014s, episode steps:   3, steps per second: 220, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 491.625203, mae: 599.197998, accuracy: 0.062500, mean_q: -136.256269, mean_eps: 0.100000\n",
      " 32230/50000: episode: 7662, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 701.926666, mae: 568.558634, accuracy: 0.197917, mean_q: -138.392064, mean_eps: 0.100000\n",
      " 32233/50000: episode: 7663, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 216.564529, mae: 600.664164, accuracy: 0.083333, mean_q: -130.188563, mean_eps: 0.100000\n",
      " 32237/50000: episode: 7664, duration: 0.016s, episode steps:   4, steps per second: 258, episode reward: -1238.000, mean reward: -309.500 [-999.000, -60.000], mean action: 1.500 [0.000, 3.000],  loss: 932.035751, mae: 595.363052, accuracy: 0.062500, mean_q: -130.081118, mean_eps: 0.100000\n",
      " 32240/50000: episode: 7665, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 347.498037, mae: 582.471619, accuracy: 0.041667, mean_q: -132.962718, mean_eps: 0.100000\n",
      " 32243/50000: episode: 7666, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 449.086060, mae: 597.866842, accuracy: 0.114583, mean_q: -130.671748, mean_eps: 0.100000\n",
      " 32246/50000: episode: 7667, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 370.543009, mae: 566.926290, accuracy: 0.135417, mean_q: -137.544785, mean_eps: 0.100000\n",
      " 32249/50000: episode: 7668, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 817.581258, mae: 584.896423, accuracy: 0.125000, mean_q: -130.256963, mean_eps: 0.100000\n",
      " 32252/50000: episode: 7669, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 306.824778, mae: 586.352641, accuracy: 0.125000, mean_q: -130.556806, mean_eps: 0.100000\n",
      " 32255/50000: episode: 7670, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 614.612386, mae: 620.127584, accuracy: 0.104167, mean_q: -129.023384, mean_eps: 0.100000\n",
      " 32258/50000: episode: 7671, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 585.863770, mae: 578.821940, accuracy: 0.020833, mean_q: -132.306160, mean_eps: 0.100000\n",
      " 32262/50000: episode: 7672, duration: 0.016s, episode steps:   4, steps per second: 251, episode reward: -1238.000, mean reward: -309.500 [-999.000, -60.000], mean action: 1.500 [0.000, 3.000],  loss: 506.754681, mae: 599.379074, accuracy: 0.070312, mean_q: -131.113934, mean_eps: 0.100000\n",
      " 32265/50000: episode: 7673, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 610.586344, mae: 570.419759, accuracy: 0.104167, mean_q: -134.741862, mean_eps: 0.100000\n",
      " 32268/50000: episode: 7674, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 949.987305, mae: 598.605713, accuracy: 0.114583, mean_q: -128.162092, mean_eps: 0.100000\n",
      " 32272/50000: episode: 7675, duration: 0.018s, episode steps:   4, steps per second: 221, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 695.728508, mae: 571.212250, accuracy: 0.109375, mean_q: -127.160929, mean_eps: 0.100000\n",
      " 32275/50000: episode: 7676, duration: 0.015s, episode steps:   3, steps per second: 204, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 543.566132, mae: 583.875814, accuracy: 0.104167, mean_q: -126.853577, mean_eps: 0.100000\n",
      " 32279/50000: episode: 7677, duration: 0.016s, episode steps:   4, steps per second: 248, episode reward: -1238.000, mean reward: -309.500 [-999.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 495.514183, mae: 571.506958, accuracy: 0.101562, mean_q: -136.170441, mean_eps: 0.100000\n",
      " 32282/50000: episode: 7678, duration: 0.013s, episode steps:   3, steps per second: 227, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 444.469259, mae: 609.035278, accuracy: 0.031250, mean_q: -132.217138, mean_eps: 0.100000\n",
      " 32286/50000: episode: 7679, duration: 0.016s, episode steps:   4, steps per second: 255, episode reward: -1238.000, mean reward: -309.500 [-999.000, -60.000], mean action: 1.750 [1.000, 3.000],  loss: 560.343086, mae: 587.414276, accuracy: 0.078125, mean_q: -135.119678, mean_eps: 0.100000\n",
      " 32289/50000: episode: 7680, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 608.836690, mae: 598.198669, accuracy: 0.093750, mean_q: -134.479584, mean_eps: 0.100000\n",
      " 32292/50000: episode: 7681, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 420.031657, mae: 597.499430, accuracy: 0.177083, mean_q: -132.852557, mean_eps: 0.100000\n",
      " 32295/50000: episode: 7682, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 413.905701, mae: 591.575317, accuracy: 0.072917, mean_q: -132.839528, mean_eps: 0.100000\n",
      " 32298/50000: episode: 7683, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 378.081395, mae: 560.594686, accuracy: 0.083333, mean_q: -143.365916, mean_eps: 0.100000\n",
      " 32301/50000: episode: 7684, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 497.313558, mae: 579.293538, accuracy: 0.114583, mean_q: -131.794383, mean_eps: 0.100000\n",
      " 32304/50000: episode: 7685, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 515.431127, mae: 583.337443, accuracy: 0.072917, mean_q: -136.367188, mean_eps: 0.100000\n",
      " 32307/50000: episode: 7686, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 357.780614, mae: 597.769694, accuracy: 0.083333, mean_q: -140.524307, mean_eps: 0.100000\n",
      " 32311/50000: episode: 7687, duration: 0.016s, episode steps:   4, steps per second: 244, episode reward: -1223.000, mean reward: -305.750 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 490.987202, mae: 568.446213, accuracy: 0.070312, mean_q: -136.788517, mean_eps: 0.100000\n",
      " 32314/50000: episode: 7688, duration: 0.015s, episode steps:   3, steps per second: 202, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 501.930288, mae: 603.431722, accuracy: 0.052083, mean_q: -126.389005, mean_eps: 0.100000\n",
      " 32317/50000: episode: 7689, duration: 0.018s, episode steps:   3, steps per second: 170, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 448.210286, mae: 578.704834, accuracy: 0.093750, mean_q: -134.171529, mean_eps: 0.100000\n",
      " 32320/50000: episode: 7690, duration: 0.014s, episode steps:   3, steps per second: 211, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 390.735779, mae: 566.835612, accuracy: 0.166667, mean_q: -135.426849, mean_eps: 0.100000\n",
      " 32323/50000: episode: 7691, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 445.419098, mae: 569.158305, accuracy: 0.093750, mean_q: -138.165176, mean_eps: 0.100000\n",
      " 32326/50000: episode: 7692, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 694.704997, mae: 573.464396, accuracy: 0.104167, mean_q: -134.329686, mean_eps: 0.100000\n",
      " 32329/50000: episode: 7693, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 273.965942, mae: 575.658407, accuracy: 0.114583, mean_q: -129.338013, mean_eps: 0.100000\n",
      " 32332/50000: episode: 7694, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 396.230988, mae: 600.475952, accuracy: 0.125000, mean_q: -131.397224, mean_eps: 0.100000\n",
      " 32335/50000: episode: 7695, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 295.374329, mae: 576.549805, accuracy: 0.093750, mean_q: -133.964559, mean_eps: 0.100000\n",
      " 32338/50000: episode: 7696, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 364.541738, mae: 588.152812, accuracy: 0.104167, mean_q: -135.197571, mean_eps: 0.100000\n",
      " 32341/50000: episode: 7697, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 334.475677, mae: 576.845439, accuracy: 0.114583, mean_q: -135.417816, mean_eps: 0.100000\n",
      " 32344/50000: episode: 7698, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 428.794627, mae: 602.553813, accuracy: 0.135417, mean_q: -122.146517, mean_eps: 0.100000\n",
      " 32347/50000: episode: 7699, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 261.279999, mae: 586.066386, accuracy: 0.093750, mean_q: -139.074371, mean_eps: 0.100000\n",
      " 32350/50000: episode: 7700, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 573.343516, mae: 602.874288, accuracy: 0.125000, mean_q: -138.430735, mean_eps: 0.100000\n",
      " 32353/50000: episode: 7701, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 746.256551, mae: 601.252319, accuracy: 0.145833, mean_q: -131.072187, mean_eps: 0.100000\n",
      " 32356/50000: episode: 7702, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 406.103475, mae: 612.859965, accuracy: 0.072917, mean_q: -121.108983, mean_eps: 0.100000\n",
      " 32359/50000: episode: 7703, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 489.581451, mae: 555.159627, accuracy: 0.104167, mean_q: -128.945134, mean_eps: 0.100000\n",
      " 32362/50000: episode: 7704, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 623.835520, mae: 614.629171, accuracy: 0.114583, mean_q: -119.468422, mean_eps: 0.100000\n",
      " 32365/50000: episode: 7705, duration: 0.017s, episode steps:   3, steps per second: 176, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 483.246928, mae: 561.533854, accuracy: 0.062500, mean_q: -147.282883, mean_eps: 0.100000\n",
      " 32368/50000: episode: 7706, duration: 0.040s, episode steps:   3, steps per second:  76, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 477.367462, mae: 580.928935, accuracy: 0.093750, mean_q: -140.084366, mean_eps: 0.100000\n",
      " 32371/50000: episode: 7707, duration: 0.014s, episode steps:   3, steps per second: 211, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 263.399089, mae: 605.916239, accuracy: 0.062500, mean_q: -127.752907, mean_eps: 0.100000\n",
      " 32374/50000: episode: 7708, duration: 0.013s, episode steps:   3, steps per second: 230, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 319.271871, mae: 563.530782, accuracy: 0.125000, mean_q: -136.534419, mean_eps: 0.100000\n",
      " 32377/50000: episode: 7709, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 448.654495, mae: 561.486471, accuracy: 0.093750, mean_q: -139.373489, mean_eps: 0.100000\n",
      " 32380/50000: episode: 7710, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 217.227926, mae: 591.474386, accuracy: 0.072917, mean_q: -134.476791, mean_eps: 0.100000\n",
      " 32383/50000: episode: 7711, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 406.123647, mae: 597.004049, accuracy: 0.156250, mean_q: -130.729645, mean_eps: 0.100000\n",
      " 32386/50000: episode: 7712, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 312.862793, mae: 617.149028, accuracy: 0.072917, mean_q: -133.513234, mean_eps: 0.100000\n",
      " 32389/50000: episode: 7713, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 684.605815, mae: 570.950378, accuracy: 0.166667, mean_q: -130.290855, mean_eps: 0.100000\n",
      " 32392/50000: episode: 7714, duration: 0.013s, episode steps:   3, steps per second: 230, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 477.362264, mae: 590.595296, accuracy: 0.145833, mean_q: -135.454748, mean_eps: 0.100000\n",
      " 32395/50000: episode: 7715, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 444.116496, mae: 595.848714, accuracy: 0.093750, mean_q: -130.489955, mean_eps: 0.100000\n",
      " 32398/50000: episode: 7716, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 369.797272, mae: 604.794067, accuracy: 0.135417, mean_q: -135.604024, mean_eps: 0.100000\n",
      " 32401/50000: episode: 7717, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 353.133433, mae: 609.089905, accuracy: 0.104167, mean_q: -127.458964, mean_eps: 0.100000\n",
      " 32404/50000: episode: 7718, duration: 0.016s, episode steps:   3, steps per second: 187, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 449.109996, mae: 576.329508, accuracy: 0.083333, mean_q: -137.065653, mean_eps: 0.100000\n",
      " 32407/50000: episode: 7719, duration: 0.016s, episode steps:   3, steps per second: 192, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 550.999257, mae: 573.192464, accuracy: 0.062500, mean_q: -138.362361, mean_eps: 0.100000\n",
      " 32410/50000: episode: 7720, duration: 0.015s, episode steps:   3, steps per second: 198, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 365.808731, mae: 596.075541, accuracy: 0.083333, mean_q: -136.980509, mean_eps: 0.100000\n",
      " 32413/50000: episode: 7721, duration: 0.014s, episode steps:   3, steps per second: 219, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 579.789653, mae: 577.467326, accuracy: 0.145833, mean_q: -136.338211, mean_eps: 0.100000\n",
      " 32416/50000: episode: 7722, duration: 0.013s, episode steps:   3, steps per second: 230, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 459.995667, mae: 556.629333, accuracy: 0.135417, mean_q: -139.648727, mean_eps: 0.100000\n",
      " 32419/50000: episode: 7723, duration: 0.013s, episode steps:   3, steps per second: 227, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 758.327393, mae: 595.979207, accuracy: 0.093750, mean_q: -124.064952, mean_eps: 0.100000\n",
      " 32422/50000: episode: 7724, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 634.605916, mae: 582.778809, accuracy: 0.197917, mean_q: -131.363892, mean_eps: 0.100000\n",
      " 32425/50000: episode: 7725, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 443.391083, mae: 603.508850, accuracy: 0.072917, mean_q: -134.019160, mean_eps: 0.100000\n",
      " 32429/50000: episode: 7726, duration: 0.016s, episode steps:   4, steps per second: 257, episode reward: -1238.000, mean reward: -309.500 [-999.000, -60.000], mean action: 1.500 [0.000, 3.000],  loss: 337.784584, mae: 609.409607, accuracy: 0.117188, mean_q: -127.511555, mean_eps: 0.100000\n",
      " 32432/50000: episode: 7727, duration: 0.013s, episode steps:   3, steps per second: 225, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 389.149536, mae: 616.805481, accuracy: 0.093750, mean_q: -126.032669, mean_eps: 0.100000\n",
      " 32435/50000: episode: 7728, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 2.000 [1.000, 3.000],  loss: 410.084117, mae: 577.673767, accuracy: 0.072917, mean_q: -131.897003, mean_eps: 0.100000\n",
      " 32438/50000: episode: 7729, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 467.435089, mae: 595.844686, accuracy: 0.062500, mean_q: -141.589920, mean_eps: 0.100000\n",
      " 32441/50000: episode: 7730, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 235.449102, mae: 596.559916, accuracy: 0.104167, mean_q: -135.039706, mean_eps: 0.100000\n",
      " 32444/50000: episode: 7731, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 606.137665, mae: 566.061056, accuracy: 0.104167, mean_q: -133.599846, mean_eps: 0.100000\n",
      " 32447/50000: episode: 7732, duration: 0.014s, episode steps:   3, steps per second: 208, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 270.528381, mae: 601.380086, accuracy: 0.145833, mean_q: -119.284129, mean_eps: 0.100000\n",
      " 32451/50000: episode: 7733, duration: 0.022s, episode steps:   4, steps per second: 181, episode reward: -1223.000, mean reward: -305.750 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 301.854568, mae: 571.473465, accuracy: 0.070312, mean_q: -133.969139, mean_eps: 0.100000\n",
      " 32454/50000: episode: 7734, duration: 0.016s, episode steps:   3, steps per second: 189, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 307.576721, mae: 578.111979, accuracy: 0.083333, mean_q: -134.969564, mean_eps: 0.100000\n",
      " 32457/50000: episode: 7735, duration: 0.013s, episode steps:   3, steps per second: 230, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 265.847641, mae: 584.180664, accuracy: 0.093750, mean_q: -128.827255, mean_eps: 0.100000\n",
      " 32460/50000: episode: 7736, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 415.387736, mae: 583.955119, accuracy: 0.156250, mean_q: -129.916173, mean_eps: 0.100000\n",
      " 32463/50000: episode: 7737, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 294.172913, mae: 589.502157, accuracy: 0.093750, mean_q: -128.503647, mean_eps: 0.100000\n",
      " 32466/50000: episode: 7738, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 403.147527, mae: 610.995809, accuracy: 0.104167, mean_q: -130.754161, mean_eps: 0.100000\n",
      " 32469/50000: episode: 7739, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 341.829041, mae: 614.731181, accuracy: 0.093750, mean_q: -136.386202, mean_eps: 0.100000\n",
      " 32472/50000: episode: 7740, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 314.580582, mae: 596.577169, accuracy: 0.104167, mean_q: -138.378479, mean_eps: 0.100000\n",
      " 32475/50000: episode: 7741, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 412.087575, mae: 557.863464, accuracy: 0.114583, mean_q: -138.496933, mean_eps: 0.100000\n",
      " 32479/50000: episode: 7742, duration: 0.016s, episode steps:   4, steps per second: 250, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 366.317722, mae: 573.498474, accuracy: 0.062500, mean_q: -131.605543, mean_eps: 0.100000\n",
      " 32482/50000: episode: 7743, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 390.327011, mae: 590.556722, accuracy: 0.072917, mean_q: -129.725525, mean_eps: 0.100000\n",
      " 32485/50000: episode: 7744, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 373.622986, mae: 578.055522, accuracy: 0.125000, mean_q: -131.099505, mean_eps: 0.100000\n",
      " 32490/50000: episode: 7745, duration: 0.020s, episode steps:   5, steps per second: 253, episode reward: -2237.000, mean reward: -447.400 [-999.000, -60.000], mean action: 1.200 [0.000, 3.000],  loss: 249.525992, mae: 593.001746, accuracy: 0.112500, mean_q: -129.094951, mean_eps: 0.100000\n",
      " 32493/50000: episode: 7746, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 232.831233, mae: 565.677836, accuracy: 0.104167, mean_q: -142.727269, mean_eps: 0.100000\n",
      " 32497/50000: episode: 7747, duration: 0.016s, episode steps:   4, steps per second: 257, episode reward: -1238.000, mean reward: -309.500 [-999.000, -60.000], mean action: 1.500 [0.000, 3.000],  loss: 322.384636, mae: 575.256531, accuracy: 0.078125, mean_q: -137.528576, mean_eps: 0.100000\n",
      " 32500/50000: episode: 7748, duration: 0.014s, episode steps:   3, steps per second: 208, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 321.613958, mae: 548.272868, accuracy: 0.104167, mean_q: -134.192362, mean_eps: 0.100000\n",
      " 32503/50000: episode: 7749, duration: 0.020s, episode steps:   3, steps per second: 150, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 303.458435, mae: 572.791931, accuracy: 0.145833, mean_q: -131.681058, mean_eps: 0.100000\n",
      " 32506/50000: episode: 7750, duration: 0.014s, episode steps:   3, steps per second: 207, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 360.518031, mae: 593.599223, accuracy: 0.093750, mean_q: -129.802180, mean_eps: 0.100000\n",
      " 32510/50000: episode: 7751, duration: 0.016s, episode steps:   4, steps per second: 244, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 345.136196, mae: 595.224243, accuracy: 0.062500, mean_q: -133.893890, mean_eps: 0.100000\n",
      " 32513/50000: episode: 7752, duration: 0.013s, episode steps:   3, steps per second: 226, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 296.298874, mae: 601.351664, accuracy: 0.041667, mean_q: -133.038869, mean_eps: 0.100000\n",
      " 32516/50000: episode: 7753, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 486.262258, mae: 572.509583, accuracy: 0.208333, mean_q: -136.468315, mean_eps: 0.100000\n",
      " 32519/50000: episode: 7754, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 361.802078, mae: 575.005514, accuracy: 0.104167, mean_q: -130.608833, mean_eps: 0.100000\n",
      " 32522/50000: episode: 7755, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 441.622793, mae: 597.852010, accuracy: 0.031250, mean_q: -133.271434, mean_eps: 0.100000\n",
      " 32525/50000: episode: 7756, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 250.614543, mae: 559.065308, accuracy: 0.114583, mean_q: -135.920064, mean_eps: 0.100000\n",
      " 32528/50000: episode: 7757, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 356.071508, mae: 592.506877, accuracy: 0.145833, mean_q: -125.502319, mean_eps: 0.100000\n",
      " 32531/50000: episode: 7758, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 264.014755, mae: 569.088867, accuracy: 0.125000, mean_q: -134.773036, mean_eps: 0.100000\n",
      " 32534/50000: episode: 7759, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 155.212204, mae: 566.607198, accuracy: 0.052083, mean_q: -134.351496, mean_eps: 0.100000\n",
      " 32537/50000: episode: 7760, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 160.404928, mae: 563.706421, accuracy: 0.093750, mean_q: -137.413177, mean_eps: 0.100000\n",
      " 32540/50000: episode: 7761, duration: 0.013s, episode steps:   3, steps per second: 230, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 350.611374, mae: 604.921590, accuracy: 0.020833, mean_q: -130.987335, mean_eps: 0.100000\n",
      " 32543/50000: episode: 7762, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 363.224253, mae: 608.618815, accuracy: 0.114583, mean_q: -133.061839, mean_eps: 0.100000\n",
      " 32546/50000: episode: 7763, duration: 0.024s, episode steps:   3, steps per second: 124, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 334.034536, mae: 554.987773, accuracy: 0.135417, mean_q: -145.413544, mean_eps: 0.100000\n",
      " 32550/50000: episode: 7764, duration: 0.026s, episode steps:   4, steps per second: 152, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 172.693886, mae: 585.524017, accuracy: 0.109375, mean_q: -135.228401, mean_eps: 0.100000\n",
      " 32553/50000: episode: 7765, duration: 0.016s, episode steps:   3, steps per second: 186, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 461.258219, mae: 595.746663, accuracy: 0.083333, mean_q: -129.774221, mean_eps: 0.100000\n",
      " 32557/50000: episode: 7766, duration: 0.016s, episode steps:   4, steps per second: 251, episode reward: -1223.000, mean reward: -305.750 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 301.758202, mae: 606.035690, accuracy: 0.062500, mean_q: -134.098688, mean_eps: 0.100000\n",
      " 32560/50000: episode: 7767, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 267.742605, mae: 567.034749, accuracy: 0.145833, mean_q: -134.553421, mean_eps: 0.100000\n",
      " 32563/50000: episode: 7768, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 297.258087, mae: 610.702047, accuracy: 0.156250, mean_q: -130.686529, mean_eps: 0.100000\n",
      " 32566/50000: episode: 7769, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 241.774445, mae: 607.395874, accuracy: 0.135417, mean_q: -124.902196, mean_eps: 0.100000\n",
      " 32569/50000: episode: 7770, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 485.342534, mae: 581.138062, accuracy: 0.135417, mean_q: -132.582153, mean_eps: 0.100000\n",
      " 32572/50000: episode: 7771, duration: 0.014s, episode steps:   3, steps per second: 221, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 261.994029, mae: 576.218852, accuracy: 0.041667, mean_q: -141.859512, mean_eps: 0.100000\n",
      " 32575/50000: episode: 7772, duration: 0.014s, episode steps:   3, steps per second: 212, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 233.113401, mae: 569.733805, accuracy: 0.083333, mean_q: -138.725418, mean_eps: 0.100000\n",
      " 32578/50000: episode: 7773, duration: 0.013s, episode steps:   3, steps per second: 225, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 187.582748, mae: 559.402812, accuracy: 0.093750, mean_q: -135.253553, mean_eps: 0.100000\n",
      " 32581/50000: episode: 7774, duration: 0.014s, episode steps:   3, steps per second: 207, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 557.572876, mae: 574.519714, accuracy: 0.083333, mean_q: -130.825841, mean_eps: 0.100000\n",
      " 32584/50000: episode: 7775, duration: 0.014s, episode steps:   3, steps per second: 217, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 328.006154, mae: 591.946391, accuracy: 0.104167, mean_q: -127.718381, mean_eps: 0.100000\n",
      " 32588/50000: episode: 7776, duration: 0.025s, episode steps:   4, steps per second: 159, episode reward: -1223.000, mean reward: -305.750 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 400.203053, mae: 561.609161, accuracy: 0.070312, mean_q: -139.117607, mean_eps: 0.100000\n",
      " 32592/50000: episode: 7777, duration: 0.019s, episode steps:   4, steps per second: 213, episode reward: -1238.000, mean reward: -309.500 [-999.000, -45.000], mean action: 1.750 [0.000, 3.000],  loss: 261.672298, mae: 575.528900, accuracy: 0.109375, mean_q: -131.549557, mean_eps: 0.100000\n",
      " 32595/50000: episode: 7778, duration: 0.015s, episode steps:   3, steps per second: 201, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 2.000 [1.000, 3.000],  loss: 280.850972, mae: 592.865865, accuracy: 0.093750, mean_q: -131.565503, mean_eps: 0.100000\n",
      " 32598/50000: episode: 7779, duration: 0.013s, episode steps:   3, steps per second: 223, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 237.660212, mae: 617.153951, accuracy: 0.083333, mean_q: -130.123942, mean_eps: 0.100000\n",
      " 32601/50000: episode: 7780, duration: 0.013s, episode steps:   3, steps per second: 226, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 394.435974, mae: 584.786865, accuracy: 0.072917, mean_q: -136.866923, mean_eps: 0.100000\n",
      " 32604/50000: episode: 7781, duration: 0.014s, episode steps:   3, steps per second: 211, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 300.384913, mae: 584.364644, accuracy: 0.104167, mean_q: -135.723429, mean_eps: 0.100000\n",
      " 32607/50000: episode: 7782, duration: 0.016s, episode steps:   3, steps per second: 193, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 297.457303, mae: 577.015808, accuracy: 0.114583, mean_q: -134.687927, mean_eps: 0.100000\n",
      " 32610/50000: episode: 7783, duration: 0.014s, episode steps:   3, steps per second: 209, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 307.083349, mae: 607.680257, accuracy: 0.093750, mean_q: -136.116262, mean_eps: 0.100000\n",
      " 32613/50000: episode: 7784, duration: 0.014s, episode steps:   3, steps per second: 216, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 314.797424, mae: 569.281820, accuracy: 0.125000, mean_q: -135.494476, mean_eps: 0.100000\n",
      " 32616/50000: episode: 7785, duration: 0.014s, episode steps:   3, steps per second: 217, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 210.328186, mae: 564.185567, accuracy: 0.083333, mean_q: -136.313929, mean_eps: 0.100000\n",
      " 32619/50000: episode: 7786, duration: 0.014s, episode steps:   3, steps per second: 214, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 200.725042, mae: 596.943726, accuracy: 0.083333, mean_q: -132.957926, mean_eps: 0.100000\n",
      " 32622/50000: episode: 7787, duration: 0.013s, episode steps:   3, steps per second: 226, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 258.262329, mae: 563.797262, accuracy: 0.083333, mean_q: -141.292674, mean_eps: 0.100000\n",
      " 32625/50000: episode: 7788, duration: 0.014s, episode steps:   3, steps per second: 212, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 212.935404, mae: 577.333903, accuracy: 0.072917, mean_q: -131.599269, mean_eps: 0.100000\n",
      " 32628/50000: episode: 7789, duration: 0.014s, episode steps:   3, steps per second: 210, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 401.565704, mae: 574.030233, accuracy: 0.125000, mean_q: -128.136907, mean_eps: 0.100000\n",
      " 32631/50000: episode: 7790, duration: 0.019s, episode steps:   3, steps per second: 160, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 369.768107, mae: 570.915161, accuracy: 0.083333, mean_q: -128.931605, mean_eps: 0.100000\n",
      " 32634/50000: episode: 7791, duration: 0.018s, episode steps:   3, steps per second: 165, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 268.576233, mae: 579.173096, accuracy: 0.114583, mean_q: -137.036196, mean_eps: 0.100000\n",
      " 32638/50000: episode: 7792, duration: 0.023s, episode steps:   4, steps per second: 177, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 434.494123, mae: 585.487366, accuracy: 0.093750, mean_q: -141.165913, mean_eps: 0.100000\n",
      " 32641/50000: episode: 7793, duration: 0.014s, episode steps:   3, steps per second: 213, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 154.228577, mae: 563.742086, accuracy: 0.135417, mean_q: -136.001500, mean_eps: 0.100000\n",
      " 32644/50000: episode: 7794, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 232.146988, mae: 588.188944, accuracy: 0.166667, mean_q: -124.679810, mean_eps: 0.100000\n",
      " 32647/50000: episode: 7795, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 298.182988, mae: 576.326029, accuracy: 0.125000, mean_q: -135.608185, mean_eps: 0.100000\n",
      " 32650/50000: episode: 7796, duration: 0.014s, episode steps:   3, steps per second: 212, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 275.623174, mae: 585.458883, accuracy: 0.125000, mean_q: -136.304331, mean_eps: 0.100000\n",
      " 32654/50000: episode: 7797, duration: 0.016s, episode steps:   4, steps per second: 256, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 245.893929, mae: 590.793762, accuracy: 0.132812, mean_q: -133.762901, mean_eps: 0.100000\n",
      " 32657/50000: episode: 7798, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 475.005809, mae: 580.457642, accuracy: 0.114583, mean_q: -135.556798, mean_eps: 0.100000\n",
      " 32660/50000: episode: 7799, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 269.875793, mae: 584.944356, accuracy: 0.166667, mean_q: -132.791883, mean_eps: 0.100000\n",
      " 32663/50000: episode: 7800, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 173.659063, mae: 562.960897, accuracy: 0.166667, mean_q: -138.090790, mean_eps: 0.100000\n",
      " 32666/50000: episode: 7801, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 258.932765, mae: 572.599528, accuracy: 0.104167, mean_q: -131.854446, mean_eps: 0.100000\n",
      " 32669/50000: episode: 7802, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 309.095357, mae: 572.337016, accuracy: 0.072917, mean_q: -137.376536, mean_eps: 0.100000\n",
      " 32672/50000: episode: 7803, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 215.499385, mae: 588.488057, accuracy: 0.114583, mean_q: -134.211901, mean_eps: 0.100000\n",
      " 32675/50000: episode: 7804, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 167.348841, mae: 577.053263, accuracy: 0.093750, mean_q: -136.849411, mean_eps: 0.100000\n",
      " 32678/50000: episode: 7805, duration: 0.015s, episode steps:   3, steps per second: 207, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 400.401891, mae: 580.042419, accuracy: 0.114583, mean_q: -133.728394, mean_eps: 0.100000\n",
      " 32682/50000: episode: 7806, duration: 0.022s, episode steps:   4, steps per second: 184, episode reward: -1238.000, mean reward: -309.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 275.584183, mae: 599.590042, accuracy: 0.125000, mean_q: -132.532953, mean_eps: 0.100000\n",
      " 32685/50000: episode: 7807, duration: 0.013s, episode steps:   3, steps per second: 224, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 319.164266, mae: 580.410746, accuracy: 0.104167, mean_q: -136.602488, mean_eps: 0.100000\n",
      " 32688/50000: episode: 7808, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 144.016083, mae: 576.326111, accuracy: 0.145833, mean_q: -138.399078, mean_eps: 0.100000\n",
      " 32691/50000: episode: 7809, duration: 0.013s, episode steps:   3, steps per second: 227, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 218.898880, mae: 583.663493, accuracy: 0.020833, mean_q: -136.264989, mean_eps: 0.100000\n",
      " 32694/50000: episode: 7810, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 180.512990, mae: 588.954875, accuracy: 0.072917, mean_q: -134.605906, mean_eps: 0.100000\n",
      " 32697/50000: episode: 7811, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 212.930522, mae: 579.758708, accuracy: 0.125000, mean_q: -137.740407, mean_eps: 0.100000\n",
      " 32700/50000: episode: 7812, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 415.914042, mae: 563.174113, accuracy: 0.114583, mean_q: -141.425924, mean_eps: 0.100000\n",
      " 32704/50000: episode: 7813, duration: 0.016s, episode steps:   4, steps per second: 257, episode reward: -1223.000, mean reward: -305.750 [-999.000, -58.000], mean action: 1.250 [0.000, 3.000],  loss: 296.567974, mae: 601.139908, accuracy: 0.101562, mean_q: -130.525146, mean_eps: 0.100000\n",
      " 32707/50000: episode: 7814, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 233.221924, mae: 558.504150, accuracy: 0.083333, mean_q: -136.422223, mean_eps: 0.100000\n",
      " 32710/50000: episode: 7815, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 388.459920, mae: 607.533813, accuracy: 0.083333, mean_q: -136.451986, mean_eps: 0.100000\n",
      " 32713/50000: episode: 7816, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 269.793111, mae: 577.550293, accuracy: 0.104167, mean_q: -143.380122, mean_eps: 0.100000\n",
      " 32716/50000: episode: 7817, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 186.376470, mae: 572.307515, accuracy: 0.093750, mean_q: -136.737228, mean_eps: 0.100000\n",
      " 32719/50000: episode: 7818, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 219.145134, mae: 559.494934, accuracy: 0.062500, mean_q: -143.471720, mean_eps: 0.100000\n",
      " 32722/50000: episode: 7819, duration: 0.014s, episode steps:   3, steps per second: 216, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 244.619680, mae: 564.790405, accuracy: 0.125000, mean_q: -137.346781, mean_eps: 0.100000\n",
      " 32725/50000: episode: 7820, duration: 0.013s, episode steps:   3, steps per second: 223, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 285.263336, mae: 559.572876, accuracy: 0.104167, mean_q: -141.544876, mean_eps: 0.100000\n",
      " 32728/50000: episode: 7821, duration: 0.018s, episode steps:   3, steps per second: 165, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 197.840708, mae: 568.669067, accuracy: 0.041667, mean_q: -144.714971, mean_eps: 0.100000\n",
      " 32732/50000: episode: 7822, duration: 0.021s, episode steps:   4, steps per second: 194, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 206.656210, mae: 575.180527, accuracy: 0.109375, mean_q: -140.152458, mean_eps: 0.100000\n",
      " 32735/50000: episode: 7823, duration: 0.014s, episode steps:   3, steps per second: 208, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 217.716255, mae: 575.722046, accuracy: 0.125000, mean_q: -136.073680, mean_eps: 0.100000\n",
      " 32738/50000: episode: 7824, duration: 0.014s, episode steps:   3, steps per second: 222, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 180.158264, mae: 569.816895, accuracy: 0.114583, mean_q: -145.481959, mean_eps: 0.100000\n",
      " 32741/50000: episode: 7825, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 191.724159, mae: 570.150716, accuracy: 0.114583, mean_q: -138.176503, mean_eps: 0.100000\n",
      " 32745/50000: episode: 7826, duration: 0.016s, episode steps:   4, steps per second: 248, episode reward: -1238.000, mean reward: -309.500 [-999.000, -60.000], mean action: 2.250 [1.000, 3.000],  loss: 209.221821, mae: 578.946396, accuracy: 0.062500, mean_q: -142.164986, mean_eps: 0.100000\n",
      " 32748/50000: episode: 7827, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 230.491277, mae: 609.963643, accuracy: 0.125000, mean_q: -135.831156, mean_eps: 0.100000\n",
      " 32751/50000: episode: 7828, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 310.801997, mae: 596.969299, accuracy: 0.166667, mean_q: -130.474813, mean_eps: 0.100000\n",
      " 32755/50000: episode: 7829, duration: 0.015s, episode steps:   4, steps per second: 259, episode reward: -1223.000, mean reward: -305.750 [-999.000, -32.000], mean action: 1.000 [0.000, 3.000],  loss: 223.378590, mae: 601.136307, accuracy: 0.078125, mean_q: -131.843679, mean_eps: 0.100000\n",
      " 32758/50000: episode: 7830, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 253.203176, mae: 570.657186, accuracy: 0.062500, mean_q: -146.337128, mean_eps: 0.100000\n",
      " 32761/50000: episode: 7831, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 326.255778, mae: 574.001872, accuracy: 0.052083, mean_q: -147.610814, mean_eps: 0.100000\n",
      " 32764/50000: episode: 7832, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 205.018600, mae: 562.953267, accuracy: 0.145833, mean_q: -140.040507, mean_eps: 0.100000\n",
      " 32767/50000: episode: 7833, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 176.040202, mae: 564.450195, accuracy: 0.125000, mean_q: -138.935374, mean_eps: 0.100000\n",
      " 32770/50000: episode: 7834, duration: 0.013s, episode steps:   3, steps per second: 227, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 281.652537, mae: 585.000346, accuracy: 0.135417, mean_q: -133.575002, mean_eps: 0.100000\n",
      " 32773/50000: episode: 7835, duration: 0.015s, episode steps:   3, steps per second: 205, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 115.510677, mae: 599.532247, accuracy: 0.062500, mean_q: -138.591604, mean_eps: 0.100000\n",
      " 32776/50000: episode: 7836, duration: 0.022s, episode steps:   3, steps per second: 138, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 157.865508, mae: 602.179301, accuracy: 0.125000, mean_q: -131.737017, mean_eps: 0.100000\n",
      " 32779/50000: episode: 7837, duration: 0.022s, episode steps:   3, steps per second: 138, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 247.233877, mae: 561.907023, accuracy: 0.125000, mean_q: -140.336100, mean_eps: 0.100000\n",
      " 32782/50000: episode: 7838, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 251.991282, mae: 595.977722, accuracy: 0.145833, mean_q: -134.933462, mean_eps: 0.100000\n",
      " 32785/50000: episode: 7839, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 267.813029, mae: 591.301941, accuracy: 0.104167, mean_q: -134.737742, mean_eps: 0.100000\n",
      " 32788/50000: episode: 7840, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 255.126689, mae: 561.410665, accuracy: 0.125000, mean_q: -141.378743, mean_eps: 0.100000\n",
      " 32791/50000: episode: 7841, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 166.495272, mae: 580.510925, accuracy: 0.104167, mean_q: -137.449025, mean_eps: 0.100000\n",
      " 32794/50000: episode: 7842, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 123.364311, mae: 558.435445, accuracy: 0.114583, mean_q: -147.773911, mean_eps: 0.100000\n",
      " 32797/50000: episode: 7843, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 188.236872, mae: 601.527812, accuracy: 0.177083, mean_q: -133.112661, mean_eps: 0.100000\n",
      " 32800/50000: episode: 7844, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 240.222346, mae: 600.516378, accuracy: 0.114583, mean_q: -131.816554, mean_eps: 0.100000\n",
      " 32803/50000: episode: 7845, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 171.371478, mae: 576.577087, accuracy: 0.083333, mean_q: -135.588303, mean_eps: 0.100000\n",
      " 32806/50000: episode: 7846, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 237.605184, mae: 591.701945, accuracy: 0.145833, mean_q: -136.644348, mean_eps: 0.100000\n",
      " 32809/50000: episode: 7847, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 115.112195, mae: 588.697998, accuracy: 0.125000, mean_q: -142.725581, mean_eps: 0.100000\n",
      " 32812/50000: episode: 7848, duration: 0.013s, episode steps:   3, steps per second: 226, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 230.036064, mae: 575.010295, accuracy: 0.177083, mean_q: -139.208282, mean_eps: 0.100000\n",
      " 32815/50000: episode: 7849, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 184.448715, mae: 574.856445, accuracy: 0.156250, mean_q: -138.636225, mean_eps: 0.100000\n",
      " 32818/50000: episode: 7850, duration: 0.015s, episode steps:   3, steps per second: 200, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 180.750585, mae: 598.686279, accuracy: 0.104167, mean_q: -137.727356, mean_eps: 0.100000\n",
      " 32821/50000: episode: 7851, duration: 0.016s, episode steps:   3, steps per second: 191, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 140.207255, mae: 607.091390, accuracy: 0.062500, mean_q: -140.120682, mean_eps: 0.100000\n",
      " 32824/50000: episode: 7852, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 146.182912, mae: 602.789225, accuracy: 0.156250, mean_q: -144.369985, mean_eps: 0.100000\n",
      " 32827/50000: episode: 7853, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 155.984750, mae: 567.730062, accuracy: 0.187500, mean_q: -140.063324, mean_eps: 0.100000\n",
      " 32830/50000: episode: 7854, duration: 0.013s, episode steps:   3, steps per second: 227, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 203.029648, mae: 580.278666, accuracy: 0.145833, mean_q: -138.010666, mean_eps: 0.100000\n",
      " 32833/50000: episode: 7855, duration: 0.013s, episode steps:   3, steps per second: 224, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 432.300975, mae: 545.466797, accuracy: 0.135417, mean_q: -142.011119, mean_eps: 0.100000\n",
      " 32836/50000: episode: 7856, duration: 0.014s, episode steps:   3, steps per second: 219, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 212.033554, mae: 619.998718, accuracy: 0.072917, mean_q: -131.479777, mean_eps: 0.100000\n",
      " 32839/50000: episode: 7857, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 109.375507, mae: 585.144084, accuracy: 0.177083, mean_q: -138.132136, mean_eps: 0.100000\n",
      " 32842/50000: episode: 7858, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 101.966811, mae: 590.374471, accuracy: 0.208333, mean_q: -132.842038, mean_eps: 0.100000\n",
      " 32845/50000: episode: 7859, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 262.843272, mae: 601.178467, accuracy: 0.187500, mean_q: -135.599533, mean_eps: 0.100000\n",
      " 32848/50000: episode: 7860, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 172.660126, mae: 576.534119, accuracy: 0.208333, mean_q: -141.513809, mean_eps: 0.100000\n",
      " 32851/50000: episode: 7861, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 146.762138, mae: 573.049703, accuracy: 0.260417, mean_q: -134.744235, mean_eps: 0.100000\n",
      " 32855/50000: episode: 7862, duration: 0.016s, episode steps:   4, steps per second: 250, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 148.446823, mae: 591.083969, accuracy: 0.140625, mean_q: -131.121088, mean_eps: 0.100000\n",
      " 32859/50000: episode: 7863, duration: 0.016s, episode steps:   4, steps per second: 245, episode reward: -1223.000, mean reward: -305.750 [-999.000, -32.000], mean action: 1.750 [0.000, 3.000],  loss: 261.623497, mae: 581.036545, accuracy: 0.148438, mean_q: -137.321259, mean_eps: 0.100000\n",
      " 32862/50000: episode: 7864, duration: 0.015s, episode steps:   3, steps per second: 203, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 284.391113, mae: 583.641052, accuracy: 0.166667, mean_q: -140.035843, mean_eps: 0.100000\n",
      " 32865/50000: episode: 7865, duration: 0.021s, episode steps:   3, steps per second: 142, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 229.475683, mae: 601.027018, accuracy: 0.156250, mean_q: -140.976252, mean_eps: 0.100000\n",
      " 32869/50000: episode: 7866, duration: 0.020s, episode steps:   4, steps per second: 199, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 158.948427, mae: 586.255249, accuracy: 0.195312, mean_q: -133.318680, mean_eps: 0.100000\n",
      " 32872/50000: episode: 7867, duration: 0.016s, episode steps:   3, steps per second: 190, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 263.242249, mae: 559.422312, accuracy: 0.197917, mean_q: -133.518824, mean_eps: 0.100000\n",
      " 32875/50000: episode: 7868, duration: 0.015s, episode steps:   3, steps per second: 200, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 200.774048, mae: 573.237101, accuracy: 0.197917, mean_q: -139.808156, mean_eps: 0.100000\n",
      " 32879/50000: episode: 7869, duration: 0.017s, episode steps:   4, steps per second: 235, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 189.023795, mae: 598.873337, accuracy: 0.164062, mean_q: -130.397678, mean_eps: 0.100000\n",
      " 32882/50000: episode: 7870, duration: 0.014s, episode steps:   3, steps per second: 220, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 185.078181, mae: 592.919556, accuracy: 0.229167, mean_q: -136.172165, mean_eps: 0.100000\n",
      " 32886/50000: episode: 7871, duration: 0.016s, episode steps:   4, steps per second: 246, episode reward: -1238.000, mean reward: -309.500 [-999.000, -60.000], mean action: 2.250 [1.000, 3.000],  loss: 119.857038, mae: 602.415207, accuracy: 0.109375, mean_q: -142.578732, mean_eps: 0.100000\n",
      " 32889/50000: episode: 7872, duration: 0.014s, episode steps:   3, steps per second: 211, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 198.586418, mae: 570.011332, accuracy: 0.135417, mean_q: -141.062180, mean_eps: 0.100000\n",
      " 32892/50000: episode: 7873, duration: 0.014s, episode steps:   3, steps per second: 218, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 151.700345, mae: 563.568888, accuracy: 0.218750, mean_q: -139.334056, mean_eps: 0.100000\n",
      " 32895/50000: episode: 7874, duration: 0.014s, episode steps:   3, steps per second: 215, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 176.898677, mae: 600.929525, accuracy: 0.072917, mean_q: -135.336273, mean_eps: 0.100000\n",
      " 32898/50000: episode: 7875, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 224.884293, mae: 566.454631, accuracy: 0.104167, mean_q: -143.099487, mean_eps: 0.100000\n",
      " 32901/50000: episode: 7876, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 190.599818, mae: 549.444743, accuracy: 0.229167, mean_q: -142.537196, mean_eps: 0.100000\n",
      " 32904/50000: episode: 7877, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 193.834585, mae: 566.784953, accuracy: 0.177083, mean_q: -140.523417, mean_eps: 0.100000\n",
      " 32907/50000: episode: 7878, duration: 0.014s, episode steps:   3, steps per second: 219, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 336.181259, mae: 590.966532, accuracy: 0.177083, mean_q: -131.426193, mean_eps: 0.100000\n",
      " 32910/50000: episode: 7879, duration: 0.023s, episode steps:   3, steps per second: 128, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 220.896540, mae: 566.574188, accuracy: 0.145833, mean_q: -143.936595, mean_eps: 0.100000\n",
      " 32913/50000: episode: 7880, duration: 0.015s, episode steps:   3, steps per second: 197, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 183.054606, mae: 607.846883, accuracy: 0.166667, mean_q: -135.060476, mean_eps: 0.100000\n",
      " 32916/50000: episode: 7881, duration: 0.013s, episode steps:   3, steps per second: 224, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 260.320012, mae: 562.684408, accuracy: 0.218750, mean_q: -143.615651, mean_eps: 0.100000\n",
      " 32920/50000: episode: 7882, duration: 0.017s, episode steps:   4, steps per second: 242, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 132.190702, mae: 593.712112, accuracy: 0.125000, mean_q: -143.882057, mean_eps: 0.100000\n",
      " 32923/50000: episode: 7883, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 237.946447, mae: 615.550090, accuracy: 0.166667, mean_q: -132.186829, mean_eps: 0.100000\n",
      " 32927/50000: episode: 7884, duration: 0.016s, episode steps:   4, steps per second: 251, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 171.817724, mae: 590.186508, accuracy: 0.210938, mean_q: -139.069534, mean_eps: 0.100000\n",
      " 32931/50000: episode: 7885, duration: 0.016s, episode steps:   4, steps per second: 254, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 2.000 [0.000, 3.000],  loss: 219.049072, mae: 577.246338, accuracy: 0.218750, mean_q: -138.114311, mean_eps: 0.100000\n",
      " 32934/50000: episode: 7886, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 164.728060, mae: 582.232218, accuracy: 0.104167, mean_q: -141.512568, mean_eps: 0.100000\n",
      " 32937/50000: episode: 7887, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 146.080289, mae: 553.221049, accuracy: 0.145833, mean_q: -144.117823, mean_eps: 0.100000\n",
      " 32940/50000: episode: 7888, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 262.732137, mae: 589.147583, accuracy: 0.093750, mean_q: -144.041606, mean_eps: 0.100000\n",
      " 32943/50000: episode: 7889, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 97.677139, mae: 586.200155, accuracy: 0.156250, mean_q: -139.896383, mean_eps: 0.100000\n",
      " 32946/50000: episode: 7890, duration: 0.014s, episode steps:   3, steps per second: 217, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 132.170458, mae: 629.924927, accuracy: 0.145833, mean_q: -132.016510, mean_eps: 0.100000\n",
      " 32949/50000: episode: 7891, duration: 0.014s, episode steps:   3, steps per second: 214, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 163.927450, mae: 590.527262, accuracy: 0.145833, mean_q: -139.162898, mean_eps: 0.100000\n",
      " 32952/50000: episode: 7892, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 191.158737, mae: 553.535075, accuracy: 0.197917, mean_q: -146.706304, mean_eps: 0.100000\n",
      " 32956/50000: episode: 7893, duration: 0.016s, episode steps:   4, steps per second: 247, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 201.444718, mae: 576.984604, accuracy: 0.234375, mean_q: -136.160025, mean_eps: 0.100000\n",
      " 32959/50000: episode: 7894, duration: 0.014s, episode steps:   3, steps per second: 219, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 204.350037, mae: 586.356242, accuracy: 0.229167, mean_q: -127.977661, mean_eps: 0.100000\n",
      " 32962/50000: episode: 7895, duration: 0.019s, episode steps:   3, steps per second: 154, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 188.493123, mae: 598.941121, accuracy: 0.229167, mean_q: -132.451630, mean_eps: 0.100000\n",
      " 32965/50000: episode: 7896, duration: 0.015s, episode steps:   3, steps per second: 199, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 227.544495, mae: 554.797729, accuracy: 0.218750, mean_q: -140.106232, mean_eps: 0.100000\n",
      " 32968/50000: episode: 7897, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 112.527191, mae: 613.676432, accuracy: 0.145833, mean_q: -137.463674, mean_eps: 0.100000\n",
      " 32971/50000: episode: 7898, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 191.971909, mae: 600.448873, accuracy: 0.083333, mean_q: -137.547699, mean_eps: 0.100000\n",
      " 32974/50000: episode: 7899, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 161.392283, mae: 600.618327, accuracy: 0.083333, mean_q: -134.755569, mean_eps: 0.100000\n",
      " 32977/50000: episode: 7900, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 125.367605, mae: 586.194295, accuracy: 0.250000, mean_q: -139.421539, mean_eps: 0.100000\n",
      " 32980/50000: episode: 7901, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 186.651248, mae: 609.539205, accuracy: 0.093750, mean_q: -137.229538, mean_eps: 0.100000\n",
      " 32983/50000: episode: 7902, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 130.454802, mae: 610.603312, accuracy: 0.083333, mean_q: -135.020691, mean_eps: 0.100000\n",
      " 32986/50000: episode: 7903, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 142.644165, mae: 560.406514, accuracy: 0.083333, mean_q: -137.799919, mean_eps: 0.100000\n",
      " 32989/50000: episode: 7904, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 258.356069, mae: 555.869527, accuracy: 0.187500, mean_q: -140.312419, mean_eps: 0.100000\n",
      " 32992/50000: episode: 7905, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 174.714717, mae: 567.593669, accuracy: 0.187500, mean_q: -145.489563, mean_eps: 0.100000\n",
      " 32995/50000: episode: 7906, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 132.931178, mae: 589.903483, accuracy: 0.156250, mean_q: -138.101532, mean_eps: 0.100000\n",
      " 32998/50000: episode: 7907, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 90.601695, mae: 590.873861, accuracy: 0.187500, mean_q: -135.213816, mean_eps: 0.100000\n",
      " 33001/50000: episode: 7908, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 147.182350, mae: 597.057597, accuracy: 0.114583, mean_q: -137.398763, mean_eps: 0.100000\n",
      " 33004/50000: episode: 7909, duration: 0.021s, episode steps:   3, steps per second: 146, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 233.142380, mae: 594.783447, accuracy: 0.083333, mean_q: -141.020905, mean_eps: 0.100000\n",
      " 33007/50000: episode: 7910, duration: 0.013s, episode steps:   3, steps per second: 224, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 127.235697, mae: 569.036438, accuracy: 0.177083, mean_q: -144.279093, mean_eps: 0.100000\n",
      " 33010/50000: episode: 7911, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 192.191493, mae: 640.951416, accuracy: 0.125000, mean_q: -132.803828, mean_eps: 0.100000\n",
      " 33013/50000: episode: 7912, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 159.768183, mae: 567.117839, accuracy: 0.270833, mean_q: -139.445704, mean_eps: 0.100000\n",
      " 33016/50000: episode: 7913, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 113.886754, mae: 552.758931, accuracy: 0.187500, mean_q: -149.962545, mean_eps: 0.100000\n",
      " 33019/50000: episode: 7914, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 89.766154, mae: 583.756470, accuracy: 0.125000, mean_q: -142.576853, mean_eps: 0.100000\n",
      " 33022/50000: episode: 7915, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 118.464692, mae: 547.854716, accuracy: 0.177083, mean_q: -149.704976, mean_eps: 0.100000\n",
      " 33025/50000: episode: 7916, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 78.432742, mae: 589.970418, accuracy: 0.187500, mean_q: -142.194341, mean_eps: 0.100000\n",
      " 33028/50000: episode: 7917, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 233.142321, mae: 578.933594, accuracy: 0.156250, mean_q: -135.895986, mean_eps: 0.100000\n",
      " 33031/50000: episode: 7918, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 128.232781, mae: 568.082092, accuracy: 0.166667, mean_q: -143.118434, mean_eps: 0.100000\n",
      " 33034/50000: episode: 7919, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 159.164225, mae: 575.934570, accuracy: 0.208333, mean_q: -144.001740, mean_eps: 0.100000\n",
      " 33037/50000: episode: 7920, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 180.264023, mae: 592.692993, accuracy: 0.187500, mean_q: -134.906591, mean_eps: 0.100000\n",
      " 33040/50000: episode: 7921, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 157.086380, mae: 600.530497, accuracy: 0.197917, mean_q: -137.208745, mean_eps: 0.100000\n",
      " 33043/50000: episode: 7922, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 99.670128, mae: 583.268453, accuracy: 0.145833, mean_q: -145.024424, mean_eps: 0.100000\n",
      " 33046/50000: episode: 7923, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 109.769577, mae: 565.860372, accuracy: 0.187500, mean_q: -144.818909, mean_eps: 0.100000\n",
      " 33049/50000: episode: 7924, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 94.182826, mae: 584.628113, accuracy: 0.197917, mean_q: -136.482150, mean_eps: 0.100000\n",
      " 33052/50000: episode: 7925, duration: 0.016s, episode steps:   3, steps per second: 191, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 253.544118, mae: 573.473246, accuracy: 0.145833, mean_q: -142.477880, mean_eps: 0.100000\n",
      " 33055/50000: episode: 7926, duration: 0.016s, episode steps:   3, steps per second: 190, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.667 [0.000, 3.000],  loss: 122.199553, mae: 562.104879, accuracy: 0.229167, mean_q: -137.787898, mean_eps: 0.100000\n",
      " 33058/50000: episode: 7927, duration: 0.015s, episode steps:   3, steps per second: 200, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 163.537369, mae: 576.222758, accuracy: 0.145833, mean_q: -146.286021, mean_eps: 0.100000\n",
      " 33061/50000: episode: 7928, duration: 0.013s, episode steps:   3, steps per second: 230, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 95.854940, mae: 593.928345, accuracy: 0.145833, mean_q: -134.593282, mean_eps: 0.100000\n",
      " 33064/50000: episode: 7929, duration: 0.016s, episode steps:   3, steps per second: 187, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 130.048159, mae: 604.833069, accuracy: 0.187500, mean_q: -139.486974, mean_eps: 0.100000\n",
      " 33068/50000: episode: 7930, duration: 0.023s, episode steps:   4, steps per second: 175, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 113.250057, mae: 586.367035, accuracy: 0.148438, mean_q: -139.251938, mean_eps: 0.100000\n",
      " 33071/50000: episode: 7931, duration: 0.014s, episode steps:   3, steps per second: 216, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 137.396999, mae: 578.340434, accuracy: 0.208333, mean_q: -140.139501, mean_eps: 0.100000\n",
      " 33074/50000: episode: 7932, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 117.932963, mae: 583.345072, accuracy: 0.229167, mean_q: -148.630473, mean_eps: 0.100000\n",
      " 33077/50000: episode: 7933, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 187.246524, mae: 589.558512, accuracy: 0.135417, mean_q: -137.806722, mean_eps: 0.100000\n",
      " 33080/50000: episode: 7934, duration: 0.014s, episode steps:   3, steps per second: 216, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 123.534108, mae: 572.606771, accuracy: 0.145833, mean_q: -147.217682, mean_eps: 0.100000\n",
      " 33083/50000: episode: 7935, duration: 0.014s, episode steps:   3, steps per second: 212, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 120.046158, mae: 570.301819, accuracy: 0.104167, mean_q: -142.620514, mean_eps: 0.100000\n",
      " 33087/50000: episode: 7936, duration: 0.018s, episode steps:   4, steps per second: 225, episode reward: -1223.000, mean reward: -305.750 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 170.716187, mae: 564.804688, accuracy: 0.117188, mean_q: -140.794357, mean_eps: 0.100000\n",
      " 33090/50000: episode: 7937, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 134.888713, mae: 550.577189, accuracy: 0.114583, mean_q: -147.308080, mean_eps: 0.100000\n",
      " 33093/50000: episode: 7938, duration: 0.013s, episode steps:   3, steps per second: 230, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 150.533366, mae: 561.994954, accuracy: 0.166667, mean_q: -147.248764, mean_eps: 0.100000\n",
      " 33096/50000: episode: 7939, duration: 0.015s, episode steps:   3, steps per second: 202, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 164.999977, mae: 570.297221, accuracy: 0.187500, mean_q: -136.778575, mean_eps: 0.100000\n",
      " 33100/50000: episode: 7940, duration: 0.028s, episode steps:   4, steps per second: 142, episode reward: -1223.000, mean reward: -305.750 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 108.302197, mae: 581.768143, accuracy: 0.265625, mean_q: -140.661407, mean_eps: 0.100000\n",
      " 33103/50000: episode: 7941, duration: 0.014s, episode steps:   3, steps per second: 211, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 157.481364, mae: 597.566528, accuracy: 0.135417, mean_q: -140.976273, mean_eps: 0.100000\n",
      " 33106/50000: episode: 7942, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 160.668221, mae: 599.785787, accuracy: 0.156250, mean_q: -140.157338, mean_eps: 0.100000\n",
      " 33109/50000: episode: 7943, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 137.820124, mae: 586.305949, accuracy: 0.197917, mean_q: -135.319539, mean_eps: 0.100000\n",
      " 33112/50000: episode: 7944, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 140.971553, mae: 600.340800, accuracy: 0.197917, mean_q: -134.824300, mean_eps: 0.100000\n",
      " 33115/50000: episode: 7945, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 70.397039, mae: 607.745097, accuracy: 0.145833, mean_q: -137.892199, mean_eps: 0.100000\n",
      " 33118/50000: episode: 7946, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 120.535517, mae: 596.810242, accuracy: 0.208333, mean_q: -139.200775, mean_eps: 0.100000\n",
      " 33122/50000: episode: 7947, duration: 0.015s, episode steps:   4, steps per second: 260, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 154.391824, mae: 599.410675, accuracy: 0.140625, mean_q: -137.756729, mean_eps: 0.100000\n",
      " 33125/50000: episode: 7948, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 144.284241, mae: 583.697144, accuracy: 0.197917, mean_q: -137.480082, mean_eps: 0.100000\n",
      " 33128/50000: episode: 7949, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 114.610260, mae: 580.579692, accuracy: 0.208333, mean_q: -139.611857, mean_eps: 0.100000\n",
      " 33131/50000: episode: 7950, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 132.117746, mae: 588.676249, accuracy: 0.177083, mean_q: -139.299464, mean_eps: 0.100000\n",
      " 33134/50000: episode: 7951, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 105.410172, mae: 581.964030, accuracy: 0.239583, mean_q: -138.658188, mean_eps: 0.100000\n",
      " 33137/50000: episode: 7952, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 78.637601, mae: 595.728434, accuracy: 0.083333, mean_q: -141.640701, mean_eps: 0.100000\n",
      " 33140/50000: episode: 7953, duration: 0.014s, episode steps:   3, steps per second: 221, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 116.475315, mae: 574.475138, accuracy: 0.270833, mean_q: -141.077199, mean_eps: 0.100000\n",
      " 33143/50000: episode: 7954, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 121.763344, mae: 573.899455, accuracy: 0.156250, mean_q: -146.166092, mean_eps: 0.100000\n",
      " 33146/50000: episode: 7955, duration: 0.013s, episode steps:   3, steps per second: 224, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 146.469065, mae: 588.117757, accuracy: 0.166667, mean_q: -139.877874, mean_eps: 0.100000\n",
      " 33149/50000: episode: 7956, duration: 0.019s, episode steps:   3, steps per second: 161, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 165.245092, mae: 577.258423, accuracy: 0.197917, mean_q: -136.157522, mean_eps: 0.100000\n",
      " 33152/50000: episode: 7957, duration: 0.017s, episode steps:   3, steps per second: 179, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 106.928819, mae: 590.093506, accuracy: 0.125000, mean_q: -143.568441, mean_eps: 0.100000\n",
      " 33155/50000: episode: 7958, duration: 0.014s, episode steps:   3, steps per second: 213, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 121.554845, mae: 593.769572, accuracy: 0.218750, mean_q: -137.861801, mean_eps: 0.100000\n",
      " 33158/50000: episode: 7959, duration: 0.013s, episode steps:   3, steps per second: 222, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 122.938711, mae: 583.078105, accuracy: 0.208333, mean_q: -136.887578, mean_eps: 0.100000\n",
      " 33161/50000: episode: 7960, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 230.911616, mae: 596.714294, accuracy: 0.208333, mean_q: -137.222727, mean_eps: 0.100000\n",
      " 33164/50000: episode: 7961, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 188.529226, mae: 558.067505, accuracy: 0.229167, mean_q: -139.185644, mean_eps: 0.100000\n",
      " 33167/50000: episode: 7962, duration: 0.013s, episode steps:   3, steps per second: 223, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 78.879361, mae: 590.832581, accuracy: 0.166667, mean_q: -136.415207, mean_eps: 0.100000\n",
      " 33170/50000: episode: 7963, duration: 0.013s, episode steps:   3, steps per second: 230, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 94.263133, mae: 577.960225, accuracy: 0.187500, mean_q: -140.972183, mean_eps: 0.100000\n",
      " 33173/50000: episode: 7964, duration: 0.014s, episode steps:   3, steps per second: 215, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 126.325129, mae: 596.478312, accuracy: 0.083333, mean_q: -136.965968, mean_eps: 0.100000\n",
      " 33176/50000: episode: 7965, duration: 0.014s, episode steps:   3, steps per second: 208, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 140.801575, mae: 574.832499, accuracy: 0.166667, mean_q: -139.415334, mean_eps: 0.100000\n",
      " 33180/50000: episode: 7966, duration: 0.018s, episode steps:   4, steps per second: 217, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 146.533665, mae: 570.548859, accuracy: 0.187500, mean_q: -136.024208, mean_eps: 0.100000\n",
      " 33183/50000: episode: 7967, duration: 0.014s, episode steps:   3, steps per second: 217, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 109.802862, mae: 582.279785, accuracy: 0.156250, mean_q: -143.323364, mean_eps: 0.100000\n",
      " 33186/50000: episode: 7968, duration: 0.015s, episode steps:   3, steps per second: 203, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 182.946485, mae: 586.715658, accuracy: 0.135417, mean_q: -139.667674, mean_eps: 0.100000\n",
      " 33189/50000: episode: 7969, duration: 0.016s, episode steps:   3, steps per second: 185, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 118.779976, mae: 579.150920, accuracy: 0.114583, mean_q: -142.461685, mean_eps: 0.100000\n",
      " 33192/50000: episode: 7970, duration: 0.022s, episode steps:   3, steps per second: 134, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 120.675761, mae: 587.973511, accuracy: 0.187500, mean_q: -136.978119, mean_eps: 0.100000\n",
      " 33195/50000: episode: 7971, duration: 0.015s, episode steps:   3, steps per second: 200, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 77.529134, mae: 576.998942, accuracy: 0.156250, mean_q: -146.119125, mean_eps: 0.100000\n",
      " 33198/50000: episode: 7972, duration: 0.014s, episode steps:   3, steps per second: 221, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 138.513084, mae: 552.386983, accuracy: 0.197917, mean_q: -143.569860, mean_eps: 0.100000\n",
      " 33201/50000: episode: 7973, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 79.754410, mae: 555.967550, accuracy: 0.156250, mean_q: -144.361384, mean_eps: 0.100000\n",
      " 33204/50000: episode: 7974, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -239.000, mean reward: -79.667 [-103.000, -60.000], mean action: 2.000 [1.000, 3.000],  loss: 73.274640, mae: 589.045247, accuracy: 0.239583, mean_q: -142.485448, mean_eps: 0.100000\n",
      " 33208/50000: episode: 7975, duration: 0.016s, episode steps:   4, steps per second: 256, episode reward: -1238.000, mean reward: -309.500 [-999.000, -60.000], mean action: 1.500 [0.000, 3.000],  loss: 124.258366, mae: 602.188461, accuracy: 0.171875, mean_q: -138.539598, mean_eps: 0.100000\n",
      " 33211/50000: episode: 7976, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 149.074702, mae: 583.167948, accuracy: 0.187500, mean_q: -141.218277, mean_eps: 0.100000\n",
      " 33215/50000: episode: 7977, duration: 0.016s, episode steps:   4, steps per second: 251, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 118.951241, mae: 604.720825, accuracy: 0.171875, mean_q: -135.961781, mean_eps: 0.100000\n",
      " 33218/50000: episode: 7978, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 91.140653, mae: 573.069519, accuracy: 0.177083, mean_q: -139.887461, mean_eps: 0.100000\n",
      " 33221/50000: episode: 7979, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 163.887807, mae: 571.011678, accuracy: 0.177083, mean_q: -142.228628, mean_eps: 0.100000\n",
      " 33224/50000: episode: 7980, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 52.991826, mae: 584.262370, accuracy: 0.260417, mean_q: -139.031153, mean_eps: 0.100000\n",
      " 33227/50000: episode: 7981, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 63.355446, mae: 618.365824, accuracy: 0.156250, mean_q: -132.577021, mean_eps: 0.100000\n",
      " 33230/50000: episode: 7982, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 220.324804, mae: 598.555257, accuracy: 0.135417, mean_q: -131.948374, mean_eps: 0.100000\n",
      " 33233/50000: episode: 7983, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 175.212931, mae: 572.315857, accuracy: 0.208333, mean_q: -144.025258, mean_eps: 0.100000\n",
      " 33236/50000: episode: 7984, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 145.212130, mae: 595.810832, accuracy: 0.166667, mean_q: -133.504349, mean_eps: 0.100000\n",
      " 33239/50000: episode: 7985, duration: 0.016s, episode steps:   3, steps per second: 184, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 98.302919, mae: 568.479289, accuracy: 0.218750, mean_q: -137.652288, mean_eps: 0.100000\n",
      " 33242/50000: episode: 7986, duration: 0.016s, episode steps:   3, steps per second: 183, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 60.365096, mae: 604.914876, accuracy: 0.177083, mean_q: -135.744110, mean_eps: 0.100000\n",
      " 33245/50000: episode: 7987, duration: 0.015s, episode steps:   3, steps per second: 199, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 134.030482, mae: 570.862264, accuracy: 0.208333, mean_q: -146.504654, mean_eps: 0.100000\n",
      " 33248/50000: episode: 7988, duration: 0.013s, episode steps:   3, steps per second: 227, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 80.592602, mae: 548.117839, accuracy: 0.239583, mean_q: -140.395345, mean_eps: 0.100000\n",
      " 33251/50000: episode: 7989, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 76.235346, mae: 589.789327, accuracy: 0.104167, mean_q: -134.645976, mean_eps: 0.100000\n",
      " 33254/50000: episode: 7990, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 61.472828, mae: 598.115580, accuracy: 0.145833, mean_q: -138.229060, mean_eps: 0.100000\n",
      " 33257/50000: episode: 7991, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 133.282356, mae: 581.545532, accuracy: 0.208333, mean_q: -143.855581, mean_eps: 0.100000\n",
      " 33260/50000: episode: 7992, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 153.433578, mae: 585.561422, accuracy: 0.177083, mean_q: -135.605357, mean_eps: 0.100000\n",
      " 33263/50000: episode: 7993, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 119.638957, mae: 574.100728, accuracy: 0.125000, mean_q: -139.387863, mean_eps: 0.100000\n",
      " 33266/50000: episode: 7994, duration: 0.013s, episode steps:   3, steps per second: 230, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 62.547428, mae: 616.789124, accuracy: 0.114583, mean_q: -135.168640, mean_eps: 0.100000\n",
      " 33269/50000: episode: 7995, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 150.297651, mae: 550.762309, accuracy: 0.187500, mean_q: -147.334412, mean_eps: 0.100000\n",
      " 33272/50000: episode: 7996, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 177.144480, mae: 573.649109, accuracy: 0.197917, mean_q: -139.169896, mean_eps: 0.100000\n",
      " 33275/50000: episode: 7997, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 202.857249, mae: 582.184285, accuracy: 0.145833, mean_q: -142.398616, mean_eps: 0.100000\n",
      " 33279/50000: episode: 7998, duration: 0.015s, episode steps:   4, steps per second: 259, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 124.897017, mae: 573.829132, accuracy: 0.210938, mean_q: -143.783783, mean_eps: 0.100000\n",
      " 33282/50000: episode: 7999, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 105.299918, mae: 616.154460, accuracy: 0.208333, mean_q: -133.657722, mean_eps: 0.100000\n",
      " 33285/50000: episode: 8000, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 53.100826, mae: 599.600444, accuracy: 0.135417, mean_q: -136.805216, mean_eps: 0.100000\n",
      " 33290/50000: episode: 8001, duration: 0.032s, episode steps:   5, steps per second: 159, episode reward: -2193.000, mean reward: -438.600 [-999.000, -58.000], mean action: 2.400 [1.000, 3.000],  loss: 155.197658, mae: 569.508972, accuracy: 0.168750, mean_q: -144.964481, mean_eps: 0.100000\n",
      " 33293/50000: episode: 8002, duration: 0.016s, episode steps:   3, steps per second: 186, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 111.967120, mae: 577.028259, accuracy: 0.229167, mean_q: -138.861465, mean_eps: 0.100000\n",
      " 33296/50000: episode: 8003, duration: 0.013s, episode steps:   3, steps per second: 223, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 117.988485, mae: 598.974609, accuracy: 0.135417, mean_q: -137.251465, mean_eps: 0.100000\n",
      " 33299/50000: episode: 8004, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 108.131093, mae: 566.676127, accuracy: 0.229167, mean_q: -138.384872, mean_eps: 0.100000\n",
      " 33302/50000: episode: 8005, duration: 0.013s, episode steps:   3, steps per second: 230, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 68.446658, mae: 591.935588, accuracy: 0.239583, mean_q: -136.464828, mean_eps: 0.100000\n",
      " 33305/50000: episode: 8006, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 86.998365, mae: 612.045125, accuracy: 0.166667, mean_q: -138.961456, mean_eps: 0.100000\n",
      " 33309/50000: episode: 8007, duration: 0.025s, episode steps:   4, steps per second: 160, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 115.124067, mae: 569.224899, accuracy: 0.187500, mean_q: -149.084461, mean_eps: 0.100000\n",
      " 33312/50000: episode: 8008, duration: 0.016s, episode steps:   3, steps per second: 182, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 87.367221, mae: 579.069071, accuracy: 0.197917, mean_q: -137.134028, mean_eps: 0.100000\n",
      " 33315/50000: episode: 8009, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 117.949966, mae: 609.712280, accuracy: 0.135417, mean_q: -130.453690, mean_eps: 0.100000\n",
      " 33318/50000: episode: 8010, duration: 0.012s, episode steps:   3, steps per second: 248, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 87.891721, mae: 578.522034, accuracy: 0.187500, mean_q: -144.245733, mean_eps: 0.100000\n",
      " 33321/50000: episode: 8011, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 196.400848, mae: 570.462016, accuracy: 0.218750, mean_q: -142.463562, mean_eps: 0.100000\n",
      " 33324/50000: episode: 8012, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 122.249031, mae: 563.571838, accuracy: 0.156250, mean_q: -139.390600, mean_eps: 0.100000\n",
      " 33328/50000: episode: 8013, duration: 0.016s, episode steps:   4, steps per second: 248, episode reward: -1223.000, mean reward: -305.750 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 125.508944, mae: 562.777344, accuracy: 0.289062, mean_q: -145.741737, mean_eps: 0.100000\n",
      " 33331/50000: episode: 8014, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 133.755613, mae: 571.258525, accuracy: 0.218750, mean_q: -137.576202, mean_eps: 0.100000\n",
      " 33334/50000: episode: 8015, duration: 0.014s, episode steps:   3, steps per second: 213, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 111.265780, mae: 570.465576, accuracy: 0.156250, mean_q: -144.220657, mean_eps: 0.100000\n",
      " 33337/50000: episode: 8016, duration: 0.018s, episode steps:   3, steps per second: 167, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 129.582995, mae: 584.074992, accuracy: 0.177083, mean_q: -141.593140, mean_eps: 0.100000\n",
      " 33340/50000: episode: 8017, duration: 0.015s, episode steps:   3, steps per second: 206, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 78.662440, mae: 603.868815, accuracy: 0.125000, mean_q: -137.766973, mean_eps: 0.100000\n",
      " 33343/50000: episode: 8018, duration: 0.014s, episode steps:   3, steps per second: 216, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 89.019750, mae: 564.818054, accuracy: 0.229167, mean_q: -139.429337, mean_eps: 0.100000\n",
      " 33346/50000: episode: 8019, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 119.458150, mae: 607.281779, accuracy: 0.208333, mean_q: -137.462479, mean_eps: 0.100000\n",
      " 33349/50000: episode: 8020, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 98.389706, mae: 603.375875, accuracy: 0.197917, mean_q: -135.049942, mean_eps: 0.100000\n",
      " 33352/50000: episode: 8021, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 154.671080, mae: 568.555766, accuracy: 0.093750, mean_q: -144.588069, mean_eps: 0.100000\n",
      " 33356/50000: episode: 8022, duration: 0.016s, episode steps:   4, steps per second: 252, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 71.164217, mae: 577.834488, accuracy: 0.210938, mean_q: -138.249348, mean_eps: 0.100000\n",
      " 33359/50000: episode: 8023, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 80.214979, mae: 594.650248, accuracy: 0.135417, mean_q: -137.933177, mean_eps: 0.100000\n",
      " 33362/50000: episode: 8024, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 155.080668, mae: 552.132182, accuracy: 0.270833, mean_q: -143.635213, mean_eps: 0.100000\n",
      " 33365/50000: episode: 8025, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 142.797086, mae: 570.112020, accuracy: 0.239583, mean_q: -133.694707, mean_eps: 0.100000\n",
      " 33368/50000: episode: 8026, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 86.146174, mae: 564.676961, accuracy: 0.156250, mean_q: -143.957102, mean_eps: 0.100000\n",
      " 33371/50000: episode: 8027, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 108.419268, mae: 604.692912, accuracy: 0.166667, mean_q: -146.621134, mean_eps: 0.100000\n",
      " 33374/50000: episode: 8028, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 165.667834, mae: 595.326375, accuracy: 0.166667, mean_q: -140.033513, mean_eps: 0.100000\n",
      " 33377/50000: episode: 8029, duration: 0.013s, episode steps:   3, steps per second: 223, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 125.698120, mae: 569.052327, accuracy: 0.187500, mean_q: -134.318069, mean_eps: 0.100000\n",
      " 33380/50000: episode: 8030, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 78.322144, mae: 604.973999, accuracy: 0.177083, mean_q: -138.836421, mean_eps: 0.100000\n",
      " 33383/50000: episode: 8031, duration: 0.017s, episode steps:   3, steps per second: 177, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 105.074856, mae: 571.265971, accuracy: 0.260417, mean_q: -145.106817, mean_eps: 0.100000\n",
      " 33386/50000: episode: 8032, duration: 0.015s, episode steps:   3, steps per second: 194, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 118.951797, mae: 599.547445, accuracy: 0.166667, mean_q: -137.832469, mean_eps: 0.100000\n",
      " 33389/50000: episode: 8033, duration: 0.013s, episode steps:   3, steps per second: 223, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 165.118006, mae: 598.613892, accuracy: 0.187500, mean_q: -136.287359, mean_eps: 0.100000\n",
      " 33393/50000: episode: 8034, duration: 0.016s, episode steps:   4, steps per second: 243, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 101.818806, mae: 612.118805, accuracy: 0.156250, mean_q: -132.819458, mean_eps: 0.100000\n",
      " 33396/50000: episode: 8035, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 81.975632, mae: 611.731425, accuracy: 0.177083, mean_q: -141.048772, mean_eps: 0.100000\n",
      " 33399/50000: episode: 8036, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 86.894614, mae: 583.444661, accuracy: 0.187500, mean_q: -146.960231, mean_eps: 0.100000\n",
      " 33402/50000: episode: 8037, duration: 0.013s, episode steps:   3, steps per second: 226, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 79.265060, mae: 559.985697, accuracy: 0.239583, mean_q: -143.800440, mean_eps: 0.100000\n",
      " 33405/50000: episode: 8038, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 148.483668, mae: 555.615377, accuracy: 0.145833, mean_q: -143.840734, mean_eps: 0.100000\n",
      " 33408/50000: episode: 8039, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 111.033970, mae: 591.945129, accuracy: 0.125000, mean_q: -138.195948, mean_eps: 0.100000\n",
      " 33411/50000: episode: 8040, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 114.976084, mae: 578.380412, accuracy: 0.239583, mean_q: -142.748566, mean_eps: 0.100000\n",
      " 33414/50000: episode: 8041, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 45.089341, mae: 594.292664, accuracy: 0.187500, mean_q: -140.404139, mean_eps: 0.100000\n",
      " 33417/50000: episode: 8042, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 93.933372, mae: 612.900106, accuracy: 0.166667, mean_q: -135.430084, mean_eps: 0.100000\n",
      " 33420/50000: episode: 8043, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 105.343397, mae: 597.642822, accuracy: 0.187500, mean_q: -140.822906, mean_eps: 0.100000\n",
      " 33423/50000: episode: 8044, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 54.694372, mae: 593.951681, accuracy: 0.177083, mean_q: -142.480835, mean_eps: 0.100000\n",
      " 33426/50000: episode: 8045, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 225.202438, mae: 584.073466, accuracy: 0.229167, mean_q: -141.691783, mean_eps: 0.100000\n",
      " 33429/50000: episode: 8046, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 68.623498, mae: 599.325094, accuracy: 0.177083, mean_q: -134.513372, mean_eps: 0.100000\n",
      " 33432/50000: episode: 8047, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 78.834422, mae: 586.759135, accuracy: 0.218750, mean_q: -142.976074, mean_eps: 0.100000\n",
      " 33435/50000: episode: 8048, duration: 0.019s, episode steps:   3, steps per second: 157, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 157.995341, mae: 599.902018, accuracy: 0.177083, mean_q: -137.494008, mean_eps: 0.100000\n",
      " 33438/50000: episode: 8049, duration: 0.014s, episode steps:   3, steps per second: 207, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 66.091478, mae: 574.629395, accuracy: 0.177083, mean_q: -147.832199, mean_eps: 0.100000\n",
      " 33441/50000: episode: 8050, duration: 0.013s, episode steps:   3, steps per second: 225, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 103.853831, mae: 588.438273, accuracy: 0.187500, mean_q: -135.372477, mean_eps: 0.100000\n",
      " 33444/50000: episode: 8051, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 92.293980, mae: 578.445190, accuracy: 0.177083, mean_q: -138.100403, mean_eps: 0.100000\n",
      " 33448/50000: episode: 8052, duration: 0.016s, episode steps:   4, steps per second: 251, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 97.620674, mae: 596.531052, accuracy: 0.218750, mean_q: -140.649937, mean_eps: 0.100000\n",
      " 33451/50000: episode: 8053, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 105.682683, mae: 596.043579, accuracy: 0.229167, mean_q: -143.569402, mean_eps: 0.100000\n",
      " 33454/50000: episode: 8054, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 164.282277, mae: 561.911316, accuracy: 0.197917, mean_q: -137.755020, mean_eps: 0.100000\n",
      " 33457/50000: episode: 8055, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 81.018183, mae: 558.240377, accuracy: 0.291667, mean_q: -143.020513, mean_eps: 0.100000\n",
      " 33460/50000: episode: 8056, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 92.407148, mae: 580.028076, accuracy: 0.208333, mean_q: -139.569143, mean_eps: 0.100000\n",
      " 33463/50000: episode: 8057, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 84.978864, mae: 595.173828, accuracy: 0.187500, mean_q: -136.545283, mean_eps: 0.100000\n",
      " 33466/50000: episode: 8058, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 105.052355, mae: 592.865417, accuracy: 0.156250, mean_q: -135.224269, mean_eps: 0.100000\n",
      " 33469/50000: episode: 8059, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 118.544235, mae: 558.143433, accuracy: 0.239583, mean_q: -141.500600, mean_eps: 0.100000\n",
      " 33472/50000: episode: 8060, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 101.361125, mae: 561.446208, accuracy: 0.208333, mean_q: -148.063487, mean_eps: 0.100000\n",
      " 33475/50000: episode: 8061, duration: 0.014s, episode steps:   3, steps per second: 222, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 134.851863, mae: 589.276306, accuracy: 0.177083, mean_q: -141.500112, mean_eps: 0.100000\n",
      " 33478/50000: episode: 8062, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 111.580526, mae: 595.713196, accuracy: 0.239583, mean_q: -133.841721, mean_eps: 0.100000\n",
      " 33481/50000: episode: 8063, duration: 0.013s, episode steps:   3, steps per second: 223, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 118.154959, mae: 610.136780, accuracy: 0.218750, mean_q: -133.346339, mean_eps: 0.100000\n",
      " 33484/50000: episode: 8064, duration: 0.022s, episode steps:   3, steps per second: 135, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 80.520180, mae: 573.080546, accuracy: 0.187500, mean_q: -140.752040, mean_eps: 0.100000\n",
      " 33487/50000: episode: 8065, duration: 0.025s, episode steps:   3, steps per second: 121, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 48.107554, mae: 554.311829, accuracy: 0.135417, mean_q: -149.195541, mean_eps: 0.100000\n",
      " 33491/50000: episode: 8066, duration: 0.020s, episode steps:   4, steps per second: 203, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 128.736723, mae: 562.898972, accuracy: 0.234375, mean_q: -140.463039, mean_eps: 0.100000\n",
      " 33495/50000: episode: 8067, duration: 0.016s, episode steps:   4, steps per second: 253, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 91.912135, mae: 580.474838, accuracy: 0.156250, mean_q: -143.048351, mean_eps: 0.100000\n",
      " 33498/50000: episode: 8068, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 53.744408, mae: 595.750590, accuracy: 0.197917, mean_q: -139.296275, mean_eps: 0.100000\n",
      " 33501/50000: episode: 8069, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 90.283980, mae: 552.549764, accuracy: 0.270833, mean_q: -143.840266, mean_eps: 0.100000\n",
      " 33504/50000: episode: 8070, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 100.120962, mae: 628.683187, accuracy: 0.052083, mean_q: -131.867472, mean_eps: 0.100000\n",
      " 33507/50000: episode: 8071, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 102.346476, mae: 591.621969, accuracy: 0.145833, mean_q: -140.625254, mean_eps: 0.100000\n",
      " 33510/50000: episode: 8072, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 74.519521, mae: 586.302083, accuracy: 0.135417, mean_q: -139.559809, mean_eps: 0.100000\n",
      " 33513/50000: episode: 8073, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 91.598646, mae: 566.904704, accuracy: 0.208333, mean_q: -137.775899, mean_eps: 0.100000\n",
      " 33516/50000: episode: 8074, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 101.539711, mae: 597.066467, accuracy: 0.177083, mean_q: -137.246562, mean_eps: 0.100000\n",
      " 33520/50000: episode: 8075, duration: 0.018s, episode steps:   4, steps per second: 224, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 69.077558, mae: 598.686111, accuracy: 0.234375, mean_q: -137.918461, mean_eps: 0.100000\n",
      " 33523/50000: episode: 8076, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 106.413315, mae: 605.909098, accuracy: 0.166667, mean_q: -132.003794, mean_eps: 0.100000\n",
      " 33526/50000: episode: 8077, duration: 0.013s, episode steps:   3, steps per second: 223, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 47.479031, mae: 603.490499, accuracy: 0.270833, mean_q: -139.143514, mean_eps: 0.100000\n",
      " 33530/50000: episode: 8078, duration: 0.025s, episode steps:   4, steps per second: 163, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 116.859962, mae: 597.803696, accuracy: 0.203125, mean_q: -138.221119, mean_eps: 0.100000\n",
      " 33533/50000: episode: 8079, duration: 0.014s, episode steps:   3, steps per second: 221, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 97.102961, mae: 573.018778, accuracy: 0.145833, mean_q: -141.638621, mean_eps: 0.100000\n",
      " 33536/50000: episode: 8080, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 89.650180, mae: 590.646851, accuracy: 0.093750, mean_q: -138.840912, mean_eps: 0.100000\n",
      " 33539/50000: episode: 8081, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 136.060641, mae: 588.852091, accuracy: 0.145833, mean_q: -138.714681, mean_eps: 0.100000\n",
      " 33542/50000: episode: 8082, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 105.720760, mae: 614.645976, accuracy: 0.208333, mean_q: -134.322439, mean_eps: 0.100000\n",
      " 33545/50000: episode: 8083, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 2.000 [1.000, 3.000],  loss: 103.376892, mae: 579.885701, accuracy: 0.197917, mean_q: -143.305791, mean_eps: 0.100000\n",
      " 33548/50000: episode: 8084, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 89.174355, mae: 561.332031, accuracy: 0.208333, mean_q: -141.949127, mean_eps: 0.100000\n",
      " 33551/50000: episode: 8085, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 117.475668, mae: 579.669169, accuracy: 0.135417, mean_q: -137.177241, mean_eps: 0.100000\n",
      " 33555/50000: episode: 8086, duration: 0.016s, episode steps:   4, steps per second: 251, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.000 [0.000, 3.000],  loss: 54.646017, mae: 598.234161, accuracy: 0.132812, mean_q: -136.404366, mean_eps: 0.100000\n",
      " 33558/50000: episode: 8087, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 68.554761, mae: 583.076335, accuracy: 0.177083, mean_q: -137.177071, mean_eps: 0.100000\n",
      " 33561/50000: episode: 8088, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 106.134880, mae: 572.462016, accuracy: 0.187500, mean_q: -141.624466, mean_eps: 0.100000\n",
      " 33564/50000: episode: 8089, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 122.006177, mae: 585.080322, accuracy: 0.229167, mean_q: -139.774526, mean_eps: 0.100000\n",
      " 33567/50000: episode: 8090, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 161.939489, mae: 589.936646, accuracy: 0.197917, mean_q: -136.010267, mean_eps: 0.100000\n",
      " 33571/50000: episode: 8091, duration: 0.016s, episode steps:   4, steps per second: 246, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 0.750 [0.000, 2.000],  loss: 89.925634, mae: 585.445160, accuracy: 0.140625, mean_q: -142.049362, mean_eps: 0.100000\n",
      " 33574/50000: episode: 8092, duration: 0.015s, episode steps:   3, steps per second: 196, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 41.712507, mae: 613.056539, accuracy: 0.104167, mean_q: -138.141754, mean_eps: 0.100000\n",
      " 33577/50000: episode: 8093, duration: 0.016s, episode steps:   3, steps per second: 186, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 72.071239, mae: 596.660380, accuracy: 0.093750, mean_q: -137.785034, mean_eps: 0.100000\n",
      " 33580/50000: episode: 8094, duration: 0.014s, episode steps:   3, steps per second: 207, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 53.206515, mae: 583.468424, accuracy: 0.166667, mean_q: -142.488805, mean_eps: 0.100000\n",
      " 33583/50000: episode: 8095, duration: 0.013s, episode steps:   3, steps per second: 225, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 73.905417, mae: 559.680745, accuracy: 0.177083, mean_q: -147.921392, mean_eps: 0.100000\n",
      " 33586/50000: episode: 8096, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 26.173203, mae: 589.700073, accuracy: 0.166667, mean_q: -140.032633, mean_eps: 0.100000\n",
      " 33589/50000: episode: 8097, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 137.645851, mae: 603.181335, accuracy: 0.187500, mean_q: -139.032379, mean_eps: 0.100000\n",
      " 33592/50000: episode: 8098, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 71.047648, mae: 573.933044, accuracy: 0.166667, mean_q: -148.001775, mean_eps: 0.100000\n",
      " 33596/50000: episode: 8099, duration: 0.017s, episode steps:   4, steps per second: 231, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 2.250 [1.000, 3.000],  loss: 90.833478, mae: 614.488297, accuracy: 0.171875, mean_q: -132.687756, mean_eps: 0.100000\n",
      " 33599/50000: episode: 8100, duration: 0.015s, episode steps:   3, steps per second: 200, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 85.052177, mae: 587.067708, accuracy: 0.197917, mean_q: -136.061844, mean_eps: 0.100000\n",
      " 33602/50000: episode: 8101, duration: 0.014s, episode steps:   3, steps per second: 219, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.667 [0.000, 3.000],  loss: 98.888283, mae: 606.839437, accuracy: 0.125000, mean_q: -141.563522, mean_eps: 0.100000\n",
      " 33605/50000: episode: 8102, duration: 0.013s, episode steps:   3, steps per second: 227, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 69.011753, mae: 579.613525, accuracy: 0.208333, mean_q: -138.420954, mean_eps: 0.100000\n",
      " 33608/50000: episode: 8103, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 95.180206, mae: 595.997192, accuracy: 0.145833, mean_q: -135.580866, mean_eps: 0.100000\n",
      " 33611/50000: episode: 8104, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 82.681451, mae: 571.849101, accuracy: 0.187500, mean_q: -141.126007, mean_eps: 0.100000\n",
      " 33614/50000: episode: 8105, duration: 0.014s, episode steps:   3, steps per second: 219, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 106.413564, mae: 588.650818, accuracy: 0.218750, mean_q: -132.495552, mean_eps: 0.100000\n",
      " 33617/50000: episode: 8106, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 66.057386, mae: 582.671102, accuracy: 0.270833, mean_q: -139.076040, mean_eps: 0.100000\n",
      " 33620/50000: episode: 8107, duration: 0.014s, episode steps:   3, steps per second: 221, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 90.480654, mae: 595.364787, accuracy: 0.187500, mean_q: -135.077596, mean_eps: 0.100000\n",
      " 33623/50000: episode: 8108, duration: 0.018s, episode steps:   3, steps per second: 164, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 77.195309, mae: 583.838969, accuracy: 0.208333, mean_q: -144.181244, mean_eps: 0.100000\n",
      " 33626/50000: episode: 8109, duration: 0.015s, episode steps:   3, steps per second: 194, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 91.560838, mae: 595.034587, accuracy: 0.125000, mean_q: -137.701843, mean_eps: 0.100000\n",
      " 33629/50000: episode: 8110, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 81.897430, mae: 601.857157, accuracy: 0.135417, mean_q: -138.345973, mean_eps: 0.100000\n",
      " 33632/50000: episode: 8111, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 169.191602, mae: 570.970744, accuracy: 0.156250, mean_q: -146.337941, mean_eps: 0.100000\n",
      " 33635/50000: episode: 8112, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 106.395897, mae: 595.376994, accuracy: 0.166667, mean_q: -135.026581, mean_eps: 0.100000\n",
      " 33638/50000: episode: 8113, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 70.890275, mae: 579.088155, accuracy: 0.218750, mean_q: -139.378738, mean_eps: 0.100000\n",
      " 33642/50000: episode: 8114, duration: 0.017s, episode steps:   4, steps per second: 237, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 161.576614, mae: 587.593811, accuracy: 0.179688, mean_q: -139.770695, mean_eps: 0.100000\n",
      " 33645/50000: episode: 8115, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 53.787117, mae: 556.928589, accuracy: 0.218750, mean_q: -147.105738, mean_eps: 0.100000\n",
      " 33649/50000: episode: 8116, duration: 0.016s, episode steps:   4, steps per second: 257, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.750 [1.000, 3.000],  loss: 54.272361, mae: 564.267365, accuracy: 0.203125, mean_q: -138.761623, mean_eps: 0.100000\n",
      " 33652/50000: episode: 8117, duration: 0.013s, episode steps:   3, steps per second: 226, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 71.058721, mae: 593.300212, accuracy: 0.187500, mean_q: -134.973099, mean_eps: 0.100000\n",
      " 33655/50000: episode: 8118, duration: 0.014s, episode steps:   3, steps per second: 217, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 89.864290, mae: 581.135925, accuracy: 0.197917, mean_q: -141.672943, mean_eps: 0.100000\n",
      " 33658/50000: episode: 8119, duration: 0.014s, episode steps:   3, steps per second: 207, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 81.233871, mae: 579.608032, accuracy: 0.135417, mean_q: -142.738724, mean_eps: 0.100000\n",
      " 33662/50000: episode: 8120, duration: 0.019s, episode steps:   4, steps per second: 211, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 111.778073, mae: 582.565475, accuracy: 0.242188, mean_q: -138.956833, mean_eps: 0.100000\n",
      " 33665/50000: episode: 8121, duration: 0.015s, episode steps:   3, steps per second: 194, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 122.318515, mae: 572.416260, accuracy: 0.218750, mean_q: -140.555613, mean_eps: 0.100000\n",
      " 33668/50000: episode: 8122, duration: 0.025s, episode steps:   3, steps per second: 122, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 81.535791, mae: 581.392619, accuracy: 0.166667, mean_q: -142.654556, mean_eps: 0.100000\n",
      " 33671/50000: episode: 8123, duration: 0.017s, episode steps:   3, steps per second: 178, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 64.603814, mae: 563.580770, accuracy: 0.208333, mean_q: -146.354858, mean_eps: 0.100000\n",
      " 33675/50000: episode: 8124, duration: 0.018s, episode steps:   4, steps per second: 227, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 105.686957, mae: 598.558853, accuracy: 0.210938, mean_q: -133.467678, mean_eps: 0.100000\n",
      " 33679/50000: episode: 8125, duration: 0.019s, episode steps:   4, steps per second: 215, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 2.000 [0.000, 3.000],  loss: 68.296040, mae: 556.781799, accuracy: 0.273438, mean_q: -150.266560, mean_eps: 0.100000\n",
      " 33682/50000: episode: 8126, duration: 0.014s, episode steps:   3, steps per second: 214, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 54.516368, mae: 556.648254, accuracy: 0.135417, mean_q: -143.192566, mean_eps: 0.100000\n",
      " 33685/50000: episode: 8127, duration: 0.015s, episode steps:   3, steps per second: 198, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 109.295097, mae: 607.212301, accuracy: 0.145833, mean_q: -136.221756, mean_eps: 0.100000\n",
      " 33688/50000: episode: 8128, duration: 0.014s, episode steps:   3, steps per second: 211, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 95.182589, mae: 591.982157, accuracy: 0.166667, mean_q: -137.573964, mean_eps: 0.100000\n",
      " 33691/50000: episode: 8129, duration: 0.014s, episode steps:   3, steps per second: 221, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 96.039027, mae: 564.787516, accuracy: 0.208333, mean_q: -138.817434, mean_eps: 0.100000\n",
      " 33694/50000: episode: 8130, duration: 0.014s, episode steps:   3, steps per second: 217, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 74.997271, mae: 570.731995, accuracy: 0.229167, mean_q: -140.064819, mean_eps: 0.100000\n",
      " 33697/50000: episode: 8131, duration: 0.014s, episode steps:   3, steps per second: 221, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 44.643800, mae: 596.911723, accuracy: 0.187500, mean_q: -140.569570, mean_eps: 0.100000\n",
      " 33700/50000: episode: 8132, duration: 0.014s, episode steps:   3, steps per second: 215, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 107.697962, mae: 580.206970, accuracy: 0.156250, mean_q: -143.239120, mean_eps: 0.100000\n",
      " 33703/50000: episode: 8133, duration: 0.014s, episode steps:   3, steps per second: 221, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 95.095652, mae: 597.997599, accuracy: 0.187500, mean_q: -136.694427, mean_eps: 0.100000\n",
      " 33706/50000: episode: 8134, duration: 0.014s, episode steps:   3, steps per second: 220, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 85.000060, mae: 582.799744, accuracy: 0.218750, mean_q: -139.850449, mean_eps: 0.100000\n",
      " 33709/50000: episode: 8135, duration: 0.016s, episode steps:   3, steps per second: 188, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 64.793708, mae: 591.143860, accuracy: 0.135417, mean_q: -141.911799, mean_eps: 0.100000\n",
      " 33712/50000: episode: 8136, duration: 0.021s, episode steps:   3, steps per second: 145, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 85.195005, mae: 597.516988, accuracy: 0.177083, mean_q: -142.442225, mean_eps: 0.100000\n",
      " 33715/50000: episode: 8137, duration: 0.017s, episode steps:   3, steps per second: 173, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 63.950109, mae: 580.294678, accuracy: 0.177083, mean_q: -142.616409, mean_eps: 0.100000\n",
      " 33719/50000: episode: 8138, duration: 0.017s, episode steps:   4, steps per second: 229, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 78.580502, mae: 579.287491, accuracy: 0.148438, mean_q: -141.233345, mean_eps: 0.100000\n",
      " 33722/50000: episode: 8139, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 73.742502, mae: 604.468058, accuracy: 0.093750, mean_q: -136.094294, mean_eps: 0.100000\n",
      " 33725/50000: episode: 8140, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 93.779093, mae: 580.480794, accuracy: 0.302083, mean_q: -136.630585, mean_eps: 0.100000\n",
      " 33728/50000: episode: 8141, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 88.589622, mae: 582.761271, accuracy: 0.197917, mean_q: -140.260020, mean_eps: 0.100000\n",
      " 33731/50000: episode: 8142, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 93.027248, mae: 600.162415, accuracy: 0.208333, mean_q: -144.809443, mean_eps: 0.100000\n",
      " 33734/50000: episode: 8143, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 66.953094, mae: 582.028605, accuracy: 0.125000, mean_q: -136.873912, mean_eps: 0.100000\n",
      " 33737/50000: episode: 8144, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 82.304188, mae: 569.951518, accuracy: 0.083333, mean_q: -143.287435, mean_eps: 0.100000\n",
      " 33741/50000: episode: 8145, duration: 0.016s, episode steps:   4, steps per second: 249, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 167.504280, mae: 576.635345, accuracy: 0.179688, mean_q: -146.457031, mean_eps: 0.100000\n",
      " 33744/50000: episode: 8146, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 82.205070, mae: 581.456238, accuracy: 0.177083, mean_q: -141.691208, mean_eps: 0.100000\n",
      " 33747/50000: episode: 8147, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 50.927059, mae: 587.283203, accuracy: 0.260417, mean_q: -135.053569, mean_eps: 0.100000\n",
      " 33750/50000: episode: 8148, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 58.033326, mae: 584.405253, accuracy: 0.114583, mean_q: -141.870539, mean_eps: 0.100000\n",
      " 33753/50000: episode: 8149, duration: 0.013s, episode steps:   3, steps per second: 222, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.333 [0.000, 3.000],  loss: 55.758568, mae: 626.652323, accuracy: 0.135417, mean_q: -132.829519, mean_eps: 0.100000\n",
      " 33756/50000: episode: 8150, duration: 0.015s, episode steps:   3, steps per second: 199, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 123.591420, mae: 571.045308, accuracy: 0.197917, mean_q: -142.062408, mean_eps: 0.100000\n",
      " 33759/50000: episode: 8151, duration: 0.014s, episode steps:   3, steps per second: 207, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 131.406560, mae: 554.798930, accuracy: 0.218750, mean_q: -148.796229, mean_eps: 0.100000\n",
      " 33762/50000: episode: 8152, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 57.374443, mae: 588.639079, accuracy: 0.229167, mean_q: -142.270508, mean_eps: 0.100000\n",
      " 33765/50000: episode: 8153, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 48.038382, mae: 570.300028, accuracy: 0.177083, mean_q: -141.103912, mean_eps: 0.100000\n",
      " 33768/50000: episode: 8154, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 79.574005, mae: 612.323934, accuracy: 0.156250, mean_q: -134.098852, mean_eps: 0.100000\n",
      " 33771/50000: episode: 8155, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 84.551684, mae: 581.378866, accuracy: 0.135417, mean_q: -141.997330, mean_eps: 0.100000\n",
      " 33774/50000: episode: 8156, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 70.969564, mae: 567.162211, accuracy: 0.166667, mean_q: -148.148122, mean_eps: 0.100000\n",
      " 33777/50000: episode: 8157, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 64.013653, mae: 573.049642, accuracy: 0.177083, mean_q: -140.954046, mean_eps: 0.100000\n",
      " 33780/50000: episode: 8158, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 93.244548, mae: 572.655192, accuracy: 0.125000, mean_q: -141.674062, mean_eps: 0.100000\n",
      " 33783/50000: episode: 8159, duration: 0.013s, episode steps:   3, steps per second: 230, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 74.151680, mae: 572.035502, accuracy: 0.104167, mean_q: -147.869670, mean_eps: 0.100000\n",
      " 33788/50000: episode: 8160, duration: 0.019s, episode steps:   5, steps per second: 269, episode reward: -2193.000, mean reward: -438.600 [-999.000, -45.000], mean action: 1.600 [0.000, 3.000],  loss: 63.089446, mae: 580.029968, accuracy: 0.206250, mean_q: -141.717752, mean_eps: 0.100000\n",
      " 33791/50000: episode: 8161, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 56.350705, mae: 592.839762, accuracy: 0.187500, mean_q: -138.540197, mean_eps: 0.100000\n",
      " 33794/50000: episode: 8162, duration: 0.016s, episode steps:   3, steps per second: 187, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 117.391492, mae: 583.371195, accuracy: 0.218750, mean_q: -142.981725, mean_eps: 0.100000\n",
      " 33797/50000: episode: 8163, duration: 0.019s, episode steps:   3, steps per second: 158, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 96.502312, mae: 575.479248, accuracy: 0.156250, mean_q: -139.518692, mean_eps: 0.100000\n",
      " 33801/50000: episode: 8164, duration: 0.017s, episode steps:   4, steps per second: 236, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 93.600695, mae: 560.123566, accuracy: 0.148438, mean_q: -140.391895, mean_eps: 0.100000\n",
      " 33804/50000: episode: 8165, duration: 0.017s, episode steps:   3, steps per second: 180, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 83.742673, mae: 574.645406, accuracy: 0.156250, mean_q: -148.763855, mean_eps: 0.100000\n",
      " 33807/50000: episode: 8166, duration: 0.017s, episode steps:   3, steps per second: 179, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 99.197344, mae: 594.686442, accuracy: 0.260417, mean_q: -138.067238, mean_eps: 0.100000\n",
      " 33810/50000: episode: 8167, duration: 0.015s, episode steps:   3, steps per second: 200, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 54.984047, mae: 599.161153, accuracy: 0.229167, mean_q: -131.814957, mean_eps: 0.100000\n",
      " 33813/50000: episode: 8168, duration: 0.014s, episode steps:   3, steps per second: 212, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 101.882423, mae: 579.933838, accuracy: 0.197917, mean_q: -135.144897, mean_eps: 0.100000\n",
      " 33816/50000: episode: 8169, duration: 0.013s, episode steps:   3, steps per second: 227, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 72.258413, mae: 585.046611, accuracy: 0.072917, mean_q: -145.728053, mean_eps: 0.100000\n",
      " 33819/50000: episode: 8170, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 59.123734, mae: 601.779297, accuracy: 0.166667, mean_q: -143.128733, mean_eps: 0.100000\n",
      " 33822/50000: episode: 8171, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 85.448702, mae: 614.150675, accuracy: 0.135417, mean_q: -132.945053, mean_eps: 0.100000\n",
      " 33826/50000: episode: 8172, duration: 0.015s, episode steps:   4, steps per second: 260, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.750 [1.000, 3.000],  loss: 54.635359, mae: 594.714050, accuracy: 0.226562, mean_q: -139.977310, mean_eps: 0.100000\n",
      " 33829/50000: episode: 8173, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 65.337453, mae: 572.441711, accuracy: 0.218750, mean_q: -147.381363, mean_eps: 0.100000\n",
      " 33832/50000: episode: 8174, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 51.796532, mae: 594.616679, accuracy: 0.135417, mean_q: -136.153829, mean_eps: 0.100000\n",
      " 33836/50000: episode: 8175, duration: 0.016s, episode steps:   4, steps per second: 256, episode reward: -1223.000, mean reward: -305.750 [-999.000, -32.000], mean action: 1.000 [0.000, 3.000],  loss: 58.822124, mae: 594.013458, accuracy: 0.187500, mean_q: -138.050369, mean_eps: 0.100000\n",
      " 33839/50000: episode: 8176, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 75.660008, mae: 565.167908, accuracy: 0.135417, mean_q: -144.436152, mean_eps: 0.100000\n",
      " 33843/50000: episode: 8177, duration: 0.016s, episode steps:   4, steps per second: 247, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 134.441866, mae: 577.189468, accuracy: 0.164062, mean_q: -136.288387, mean_eps: 0.100000\n",
      " 33846/50000: episode: 8178, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 106.559615, mae: 587.866455, accuracy: 0.239583, mean_q: -138.566528, mean_eps: 0.100000\n",
      " 33849/50000: episode: 8179, duration: 0.014s, episode steps:   3, steps per second: 212, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 111.388514, mae: 581.836202, accuracy: 0.093750, mean_q: -140.934077, mean_eps: 0.100000\n",
      " 33852/50000: episode: 8180, duration: 0.015s, episode steps:   3, steps per second: 195, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 132.035464, mae: 614.790914, accuracy: 0.093750, mean_q: -134.218063, mean_eps: 0.100000\n",
      " 33855/50000: episode: 8181, duration: 0.016s, episode steps:   3, steps per second: 187, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 64.190353, mae: 570.230652, accuracy: 0.166667, mean_q: -145.671031, mean_eps: 0.100000\n",
      " 33858/50000: episode: 8182, duration: 0.015s, episode steps:   3, steps per second: 198, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 70.605963, mae: 588.468526, accuracy: 0.218750, mean_q: -136.112971, mean_eps: 0.100000\n",
      " 33861/50000: episode: 8183, duration: 0.014s, episode steps:   3, steps per second: 221, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 48.009591, mae: 609.652201, accuracy: 0.093750, mean_q: -139.025238, mean_eps: 0.100000\n",
      " 33864/50000: episode: 8184, duration: 0.014s, episode steps:   3, steps per second: 218, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 84.714008, mae: 592.529765, accuracy: 0.156250, mean_q: -142.116063, mean_eps: 0.100000\n",
      " 33867/50000: episode: 8185, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 45.054653, mae: 593.751078, accuracy: 0.166667, mean_q: -133.630285, mean_eps: 0.100000\n",
      " 33870/50000: episode: 8186, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 50.385854, mae: 596.765381, accuracy: 0.229167, mean_q: -137.603780, mean_eps: 0.100000\n",
      " 33873/50000: episode: 8187, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 69.999326, mae: 580.535929, accuracy: 0.218750, mean_q: -136.588491, mean_eps: 0.100000\n",
      " 33876/50000: episode: 8188, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 130.670308, mae: 596.285380, accuracy: 0.187500, mean_q: -134.440994, mean_eps: 0.100000\n",
      " 33879/50000: episode: 8189, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 61.166354, mae: 584.080953, accuracy: 0.166667, mean_q: -141.306071, mean_eps: 0.100000\n",
      " 33882/50000: episode: 8190, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 89.118978, mae: 580.621663, accuracy: 0.218750, mean_q: -139.420873, mean_eps: 0.100000\n",
      " 33885/50000: episode: 8191, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 96.992299, mae: 591.773438, accuracy: 0.291667, mean_q: -131.351133, mean_eps: 0.100000\n",
      " 33890/50000: episode: 8192, duration: 0.019s, episode steps:   5, steps per second: 261, episode reward: -2193.000, mean reward: -438.600 [-999.000, -32.000], mean action: 1.200 [0.000, 3.000],  loss: 102.870122, mae: 576.925757, accuracy: 0.250000, mean_q: -138.921774, mean_eps: 0.100000\n",
      " 33894/50000: episode: 8193, duration: 0.016s, episode steps:   4, steps per second: 244, episode reward: -1238.000, mean reward: -309.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 45.428859, mae: 580.884644, accuracy: 0.171875, mean_q: -141.881981, mean_eps: 0.100000\n",
      " 33897/50000: episode: 8194, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 37.964994, mae: 575.650350, accuracy: 0.135417, mean_q: -141.566757, mean_eps: 0.100000\n",
      " 33900/50000: episode: 8195, duration: 0.017s, episode steps:   3, steps per second: 180, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 98.478093, mae: 595.390299, accuracy: 0.187500, mean_q: -140.784582, mean_eps: 0.100000\n",
      " 33903/50000: episode: 8196, duration: 0.016s, episode steps:   3, steps per second: 187, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 63.095104, mae: 593.939331, accuracy: 0.177083, mean_q: -136.742798, mean_eps: 0.100000\n",
      " 33906/50000: episode: 8197, duration: 0.016s, episode steps:   3, steps per second: 188, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 81.126956, mae: 586.584147, accuracy: 0.239583, mean_q: -135.108932, mean_eps: 0.100000\n",
      " 33909/50000: episode: 8198, duration: 0.014s, episode steps:   3, steps per second: 222, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 47.801979, mae: 597.506714, accuracy: 0.239583, mean_q: -136.814748, mean_eps: 0.100000\n",
      " 33912/50000: episode: 8199, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 94.593873, mae: 573.555461, accuracy: 0.291667, mean_q: -145.903442, mean_eps: 0.100000\n",
      " 33915/50000: episode: 8200, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 86.601524, mae: 575.837891, accuracy: 0.166667, mean_q: -139.223694, mean_eps: 0.100000\n",
      " 33918/50000: episode: 8201, duration: 0.012s, episode steps:   3, steps per second: 249, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 47.877809, mae: 570.535299, accuracy: 0.187500, mean_q: -146.153000, mean_eps: 0.100000\n",
      " 33921/50000: episode: 8202, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 46.975123, mae: 572.983378, accuracy: 0.166667, mean_q: -144.943151, mean_eps: 0.100000\n",
      " 33925/50000: episode: 8203, duration: 0.016s, episode steps:   4, steps per second: 257, episode reward: -1223.000, mean reward: -305.750 [-999.000, -32.000], mean action: 1.000 [0.000, 3.000],  loss: 44.502703, mae: 568.325729, accuracy: 0.242188, mean_q: -141.180901, mean_eps: 0.100000\n",
      " 33929/50000: episode: 8204, duration: 0.016s, episode steps:   4, steps per second: 254, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 77.715978, mae: 591.241089, accuracy: 0.218750, mean_q: -142.666210, mean_eps: 0.100000\n",
      " 33932/50000: episode: 8205, duration: 0.013s, episode steps:   3, steps per second: 227, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 70.125935, mae: 558.857544, accuracy: 0.145833, mean_q: -136.504573, mean_eps: 0.100000\n",
      " 33935/50000: episode: 8206, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 92.382093, mae: 621.307414, accuracy: 0.104167, mean_q: -136.117259, mean_eps: 0.100000\n",
      " 33938/50000: episode: 8207, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 77.577218, mae: 592.619242, accuracy: 0.177083, mean_q: -146.919729, mean_eps: 0.100000\n",
      " 33941/50000: episode: 8208, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 63.747473, mae: 600.625448, accuracy: 0.156250, mean_q: -135.983152, mean_eps: 0.100000\n",
      " 33944/50000: episode: 8209, duration: 0.014s, episode steps:   3, steps per second: 210, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 122.471132, mae: 584.114054, accuracy: 0.177083, mean_q: -133.044449, mean_eps: 0.100000\n",
      " 33947/50000: episode: 8210, duration: 0.014s, episode steps:   3, steps per second: 214, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 65.015345, mae: 568.281413, accuracy: 0.114583, mean_q: -151.367126, mean_eps: 0.100000\n",
      " 33950/50000: episode: 8211, duration: 0.017s, episode steps:   3, steps per second: 177, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 75.029566, mae: 604.824402, accuracy: 0.208333, mean_q: -141.957921, mean_eps: 0.100000\n",
      " 33953/50000: episode: 8212, duration: 0.019s, episode steps:   3, steps per second: 162, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 98.253813, mae: 575.430562, accuracy: 0.187500, mean_q: -145.072611, mean_eps: 0.100000\n",
      " 33956/50000: episode: 8213, duration: 0.017s, episode steps:   3, steps per second: 173, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 61.353583, mae: 595.394450, accuracy: 0.083333, mean_q: -135.639048, mean_eps: 0.100000\n",
      " 33959/50000: episode: 8214, duration: 0.015s, episode steps:   3, steps per second: 197, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 66.138008, mae: 585.458069, accuracy: 0.197917, mean_q: -142.744400, mean_eps: 0.100000\n",
      " 33963/50000: episode: 8215, duration: 0.017s, episode steps:   4, steps per second: 237, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 51.095005, mae: 607.347321, accuracy: 0.117188, mean_q: -133.578369, mean_eps: 0.100000\n",
      " 33966/50000: episode: 8216, duration: 0.014s, episode steps:   3, steps per second: 217, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 58.145874, mae: 598.852051, accuracy: 0.083333, mean_q: -139.921636, mean_eps: 0.100000\n",
      " 33970/50000: episode: 8217, duration: 0.018s, episode steps:   4, steps per second: 225, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 2.250 [1.000, 3.000],  loss: 60.769121, mae: 594.760483, accuracy: 0.210938, mean_q: -139.412563, mean_eps: 0.100000\n",
      " 33973/50000: episode: 8218, duration: 0.020s, episode steps:   3, steps per second: 154, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 41.607670, mae: 587.398438, accuracy: 0.187500, mean_q: -140.978602, mean_eps: 0.100000\n",
      " 33976/50000: episode: 8219, duration: 0.020s, episode steps:   3, steps per second: 150, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 66.235469, mae: 601.793050, accuracy: 0.197917, mean_q: -134.518489, mean_eps: 0.100000\n",
      " 33979/50000: episode: 8220, duration: 0.014s, episode steps:   3, steps per second: 219, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 63.682337, mae: 609.852112, accuracy: 0.208333, mean_q: -139.772313, mean_eps: 0.100000\n",
      " 33982/50000: episode: 8221, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 70.175009, mae: 588.587240, accuracy: 0.197917, mean_q: -139.018875, mean_eps: 0.100000\n",
      " 33985/50000: episode: 8222, duration: 0.013s, episode steps:   3, steps per second: 226, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 69.028322, mae: 576.637858, accuracy: 0.239583, mean_q: -138.663864, mean_eps: 0.100000\n",
      " 33988/50000: episode: 8223, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 47.719549, mae: 586.851379, accuracy: 0.114583, mean_q: -135.752828, mean_eps: 0.100000\n",
      " 33991/50000: episode: 8224, duration: 0.017s, episode steps:   3, steps per second: 177, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 67.022954, mae: 538.133199, accuracy: 0.250000, mean_q: -150.562042, mean_eps: 0.100000\n",
      " 33994/50000: episode: 8225, duration: 0.017s, episode steps:   3, steps per second: 174, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 57.693705, mae: 587.520304, accuracy: 0.156250, mean_q: -137.288610, mean_eps: 0.100000\n",
      " 33998/50000: episode: 8226, duration: 0.017s, episode steps:   4, steps per second: 232, episode reward: -1223.000, mean reward: -305.750 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 44.922467, mae: 596.320038, accuracy: 0.164062, mean_q: -139.412304, mean_eps: 0.100000\n",
      " 34001/50000: episode: 8227, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 101.114090, mae: 600.036764, accuracy: 0.239583, mean_q: -143.488861, mean_eps: 0.100000\n",
      " 34004/50000: episode: 8228, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 86.323961, mae: 550.941020, accuracy: 0.208333, mean_q: -142.484248, mean_eps: 0.100000\n",
      " 34007/50000: episode: 8229, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 84.455062, mae: 578.158854, accuracy: 0.114583, mean_q: -141.367859, mean_eps: 0.100000\n",
      " 34010/50000: episode: 8230, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 68.412665, mae: 609.773722, accuracy: 0.250000, mean_q: -136.916364, mean_eps: 0.100000\n",
      " 34013/50000: episode: 8231, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 36.004046, mae: 571.104329, accuracy: 0.250000, mean_q: -140.412613, mean_eps: 0.100000\n",
      " 34016/50000: episode: 8232, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 59.277640, mae: 568.388590, accuracy: 0.145833, mean_q: -139.065257, mean_eps: 0.100000\n",
      " 34019/50000: episode: 8233, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 53.647132, mae: 573.943319, accuracy: 0.197917, mean_q: -140.694000, mean_eps: 0.100000\n",
      " 34022/50000: episode: 8234, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 77.407930, mae: 602.480164, accuracy: 0.250000, mean_q: -138.446487, mean_eps: 0.100000\n",
      " 34025/50000: episode: 8235, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 80.215563, mae: 588.835876, accuracy: 0.177083, mean_q: -136.711670, mean_eps: 0.100000\n",
      " 34028/50000: episode: 8236, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 63.580053, mae: 583.674947, accuracy: 0.218750, mean_q: -143.595957, mean_eps: 0.100000\n",
      " 34031/50000: episode: 8237, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 38.108593, mae: 581.231669, accuracy: 0.229167, mean_q: -146.266581, mean_eps: 0.100000\n",
      " 34034/50000: episode: 8238, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 55.701938, mae: 589.561971, accuracy: 0.125000, mean_q: -140.119380, mean_eps: 0.100000\n",
      " 34038/50000: episode: 8239, duration: 0.017s, episode steps:   4, steps per second: 242, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 62.277809, mae: 583.715347, accuracy: 0.117188, mean_q: -139.992455, mean_eps: 0.100000\n",
      " 34041/50000: episode: 8240, duration: 0.020s, episode steps:   3, steps per second: 148, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 35.023443, mae: 564.713114, accuracy: 0.093750, mean_q: -146.414113, mean_eps: 0.100000\n",
      " 34044/50000: episode: 8241, duration: 0.019s, episode steps:   3, steps per second: 155, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 90.734863, mae: 572.856567, accuracy: 0.114583, mean_q: -141.014074, mean_eps: 0.100000\n",
      " 34047/50000: episode: 8242, duration: 0.014s, episode steps:   3, steps per second: 218, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 41.913916, mae: 561.677775, accuracy: 0.218750, mean_q: -141.205444, mean_eps: 0.100000\n",
      " 34050/50000: episode: 8243, duration: 0.013s, episode steps:   3, steps per second: 226, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 40.889516, mae: 554.990784, accuracy: 0.145833, mean_q: -148.879369, mean_eps: 0.100000\n",
      " 34053/50000: episode: 8244, duration: 0.013s, episode steps:   3, steps per second: 227, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 104.726918, mae: 545.816040, accuracy: 0.312500, mean_q: -143.208649, mean_eps: 0.100000\n",
      " 34056/50000: episode: 8245, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 62.913386, mae: 572.888733, accuracy: 0.229167, mean_q: -136.155167, mean_eps: 0.100000\n",
      " 34059/50000: episode: 8246, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 116.560249, mae: 603.376973, accuracy: 0.093750, mean_q: -140.080317, mean_eps: 0.100000\n",
      " 34062/50000: episode: 8247, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 51.188857, mae: 581.827555, accuracy: 0.250000, mean_q: -139.990112, mean_eps: 0.100000\n",
      " 34065/50000: episode: 8248, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 81.316531, mae: 565.864014, accuracy: 0.177083, mean_q: -141.600922, mean_eps: 0.100000\n",
      " 34068/50000: episode: 8249, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 77.159838, mae: 590.355408, accuracy: 0.145833, mean_q: -137.638036, mean_eps: 0.100000\n",
      " 34071/50000: episode: 8250, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 41.077807, mae: 593.723775, accuracy: 0.218750, mean_q: -141.320292, mean_eps: 0.100000\n",
      " 34074/50000: episode: 8251, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -239.000, mean reward: -79.667 [-118.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 43.531325, mae: 592.523438, accuracy: 0.145833, mean_q: -133.786280, mean_eps: 0.100000\n",
      " 34078/50000: episode: 8252, duration: 0.015s, episode steps:   4, steps per second: 259, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 36.645716, mae: 582.425873, accuracy: 0.156250, mean_q: -142.652969, mean_eps: 0.100000\n",
      " 34081/50000: episode: 8253, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 95.399928, mae: 609.115417, accuracy: 0.208333, mean_q: -132.488859, mean_eps: 0.100000\n",
      " 34084/50000: episode: 8254, duration: 0.013s, episode steps:   3, steps per second: 225, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 28.985996, mae: 605.387594, accuracy: 0.125000, mean_q: -138.688975, mean_eps: 0.100000\n",
      " 34088/50000: episode: 8255, duration: 0.019s, episode steps:   4, steps per second: 210, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 35.716819, mae: 609.228363, accuracy: 0.156250, mean_q: -136.471443, mean_eps: 0.100000\n",
      " 34091/50000: episode: 8256, duration: 0.017s, episode steps:   3, steps per second: 176, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 29.417754, mae: 625.954488, accuracy: 0.166667, mean_q: -137.783936, mean_eps: 0.100000\n",
      " 34094/50000: episode: 8257, duration: 0.015s, episode steps:   3, steps per second: 195, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 46.029720, mae: 578.174784, accuracy: 0.135417, mean_q: -139.358187, mean_eps: 0.100000\n",
      " 34097/50000: episode: 8258, duration: 0.014s, episode steps:   3, steps per second: 221, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 75.989293, mae: 571.849752, accuracy: 0.218750, mean_q: -140.514226, mean_eps: 0.100000\n",
      " 34100/50000: episode: 8259, duration: 0.013s, episode steps:   3, steps per second: 230, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 45.041471, mae: 574.020304, accuracy: 0.281250, mean_q: -143.303177, mean_eps: 0.100000\n",
      " 34104/50000: episode: 8260, duration: 0.016s, episode steps:   4, steps per second: 254, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.750 [0.000, 3.000],  loss: 78.570061, mae: 593.114532, accuracy: 0.148438, mean_q: -136.824474, mean_eps: 0.100000\n",
      " 34107/50000: episode: 8261, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 55.869394, mae: 590.974040, accuracy: 0.208333, mean_q: -142.747945, mean_eps: 0.100000\n",
      " 34110/50000: episode: 8262, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -224.000, mean reward: -74.667 [-116.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 91.437586, mae: 593.714193, accuracy: 0.197917, mean_q: -137.670273, mean_eps: 0.100000\n",
      " 34113/50000: episode: 8263, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 54.300742, mae: 606.915080, accuracy: 0.135417, mean_q: -142.603978, mean_eps: 0.100000\n",
      " 34116/50000: episode: 8264, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 81.327632, mae: 613.822774, accuracy: 0.218750, mean_q: -138.118673, mean_eps: 0.100000\n",
      " 34119/50000: episode: 8265, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 69.808065, mae: 553.460693, accuracy: 0.166667, mean_q: -144.543701, mean_eps: 0.100000\n",
      " 34122/50000: episode: 8266, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 63.716902, mae: 546.017843, accuracy: 0.239583, mean_q: -151.039424, mean_eps: 0.100000\n",
      " 34125/50000: episode: 8267, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 38.243441, mae: 579.356445, accuracy: 0.208333, mean_q: -139.161092, mean_eps: 0.100000\n",
      " 34128/50000: episode: 8268, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 39.632932, mae: 582.936381, accuracy: 0.125000, mean_q: -136.081004, mean_eps: 0.100000\n",
      " 34131/50000: episode: 8269, duration: 0.014s, episode steps:   3, steps per second: 218, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 52.842401, mae: 581.034485, accuracy: 0.156250, mean_q: -145.356628, mean_eps: 0.100000\n",
      " 34135/50000: episode: 8270, duration: 0.017s, episode steps:   4, steps per second: 235, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 71.238672, mae: 563.382416, accuracy: 0.210938, mean_q: -143.709412, mean_eps: 0.100000\n",
      " 34138/50000: episode: 8271, duration: 0.020s, episode steps:   3, steps per second: 154, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 87.849464, mae: 614.363342, accuracy: 0.156250, mean_q: -134.926860, mean_eps: 0.100000\n",
      " 34141/50000: episode: 8272, duration: 0.016s, episode steps:   3, steps per second: 192, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 114.677650, mae: 587.126017, accuracy: 0.229167, mean_q: -139.650625, mean_eps: 0.100000\n",
      " 34144/50000: episode: 8273, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 54.645373, mae: 623.858175, accuracy: 0.125000, mean_q: -138.124222, mean_eps: 0.100000\n",
      " 34147/50000: episode: 8274, duration: 0.013s, episode steps:   3, steps per second: 230, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 35.159022, mae: 601.169454, accuracy: 0.208333, mean_q: -139.498723, mean_eps: 0.100000\n",
      " 34150/50000: episode: 8275, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 69.090828, mae: 582.617411, accuracy: 0.125000, mean_q: -141.226425, mean_eps: 0.100000\n",
      " 34153/50000: episode: 8276, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 44.638959, mae: 551.775930, accuracy: 0.166667, mean_q: -146.098063, mean_eps: 0.100000\n",
      " 34156/50000: episode: 8277, duration: 0.013s, episode steps:   3, steps per second: 227, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 48.566186, mae: 568.549438, accuracy: 0.250000, mean_q: -144.649180, mean_eps: 0.100000\n",
      " 34159/50000: episode: 8278, duration: 0.014s, episode steps:   3, steps per second: 216, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 60.895580, mae: 586.385417, accuracy: 0.208333, mean_q: -137.506536, mean_eps: 0.100000\n",
      " 34162/50000: episode: 8279, duration: 0.015s, episode steps:   3, steps per second: 206, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 70.777560, mae: 582.630086, accuracy: 0.166667, mean_q: -141.766220, mean_eps: 0.100000\n",
      " 34165/50000: episode: 8280, duration: 0.014s, episode steps:   3, steps per second: 214, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 74.279401, mae: 604.673218, accuracy: 0.135417, mean_q: -138.689651, mean_eps: 0.100000\n",
      " 34168/50000: episode: 8281, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 51.413757, mae: 589.239258, accuracy: 0.156250, mean_q: -138.815613, mean_eps: 0.100000\n",
      " 34171/50000: episode: 8282, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 49.102093, mae: 583.158061, accuracy: 0.166667, mean_q: -137.531418, mean_eps: 0.100000\n",
      " 34174/50000: episode: 8283, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 62.190568, mae: 577.784465, accuracy: 0.197917, mean_q: -144.436498, mean_eps: 0.100000\n",
      " 34177/50000: episode: 8284, duration: 0.013s, episode steps:   3, steps per second: 223, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 40.125159, mae: 581.028727, accuracy: 0.166667, mean_q: -141.869548, mean_eps: 0.100000\n",
      " 34180/50000: episode: 8285, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 50.394404, mae: 590.673726, accuracy: 0.156250, mean_q: -137.131266, mean_eps: 0.100000\n",
      " 34183/50000: episode: 8286, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 34.366125, mae: 592.705668, accuracy: 0.062500, mean_q: -142.277374, mean_eps: 0.100000\n",
      " 34186/50000: episode: 8287, duration: 0.030s, episode steps:   3, steps per second: 101, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 42.496085, mae: 607.811686, accuracy: 0.125000, mean_q: -136.009420, mean_eps: 0.100000\n",
      " 34189/50000: episode: 8288, duration: 0.017s, episode steps:   3, steps per second: 182, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 42.071873, mae: 593.410278, accuracy: 0.052083, mean_q: -137.530869, mean_eps: 0.100000\n",
      " 34193/50000: episode: 8289, duration: 0.016s, episode steps:   4, steps per second: 242, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 54.547988, mae: 590.281494, accuracy: 0.109375, mean_q: -139.820923, mean_eps: 0.100000\n",
      " 34197/50000: episode: 8290, duration: 0.016s, episode steps:   4, steps per second: 252, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 96.025618, mae: 563.813522, accuracy: 0.125000, mean_q: -146.848717, mean_eps: 0.100000\n",
      " 34200/50000: episode: 8291, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -224.000, mean reward: -74.667 [-116.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 37.049099, mae: 588.645081, accuracy: 0.083333, mean_q: -142.125921, mean_eps: 0.100000\n",
      " 34203/50000: episode: 8292, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 36.496751, mae: 584.363831, accuracy: 0.260417, mean_q: -135.585103, mean_eps: 0.100000\n",
      " 34206/50000: episode: 8293, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 39.293494, mae: 616.371847, accuracy: 0.125000, mean_q: -140.557302, mean_eps: 0.100000\n",
      " 34209/50000: episode: 8294, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 41.942360, mae: 587.452291, accuracy: 0.197917, mean_q: -135.302989, mean_eps: 0.100000\n",
      " 34212/50000: episode: 8295, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 67.142763, mae: 565.923706, accuracy: 0.218750, mean_q: -146.941442, mean_eps: 0.100000\n",
      " 34215/50000: episode: 8296, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 33.397460, mae: 588.096476, accuracy: 0.114583, mean_q: -147.020350, mean_eps: 0.100000\n",
      " 34218/50000: episode: 8297, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 87.885686, mae: 576.237956, accuracy: 0.156250, mean_q: -142.952031, mean_eps: 0.100000\n",
      " 34221/50000: episode: 8298, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 56.063754, mae: 598.380514, accuracy: 0.177083, mean_q: -138.940974, mean_eps: 0.100000\n",
      " 34224/50000: episode: 8299, duration: 0.014s, episode steps:   3, steps per second: 219, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 72.528295, mae: 578.946594, accuracy: 0.208333, mean_q: -138.131475, mean_eps: 0.100000\n",
      " 34227/50000: episode: 8300, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 45.961657, mae: 570.687826, accuracy: 0.114583, mean_q: -142.455811, mean_eps: 0.100000\n",
      " 34230/50000: episode: 8301, duration: 0.013s, episode steps:   3, steps per second: 225, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 35.424174, mae: 604.075724, accuracy: 0.135417, mean_q: -138.584401, mean_eps: 0.100000\n",
      " 34233/50000: episode: 8302, duration: 0.016s, episode steps:   3, steps per second: 188, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 39.643774, mae: 596.049438, accuracy: 0.229167, mean_q: -140.501317, mean_eps: 0.100000\n",
      " 34237/50000: episode: 8303, duration: 0.026s, episode steps:   4, steps per second: 151, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 60.387544, mae: 581.563522, accuracy: 0.078125, mean_q: -141.508656, mean_eps: 0.100000\n",
      " 34240/50000: episode: 8304, duration: 0.014s, episode steps:   3, steps per second: 222, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 62.514234, mae: 590.479065, accuracy: 0.166667, mean_q: -137.577616, mean_eps: 0.100000\n",
      " 34243/50000: episode: 8305, duration: 0.014s, episode steps:   3, steps per second: 219, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 61.302093, mae: 595.357910, accuracy: 0.229167, mean_q: -138.373693, mean_eps: 0.100000\n",
      " 34247/50000: episode: 8306, duration: 0.016s, episode steps:   4, steps per second: 250, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 53.065356, mae: 579.780090, accuracy: 0.093750, mean_q: -141.023411, mean_eps: 0.100000\n",
      " 34250/50000: episode: 8307, duration: 0.013s, episode steps:   3, steps per second: 230, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 71.474818, mae: 579.893229, accuracy: 0.062500, mean_q: -144.572103, mean_eps: 0.100000\n",
      " 34254/50000: episode: 8308, duration: 0.016s, episode steps:   4, steps per second: 249, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 45.551671, mae: 583.383987, accuracy: 0.125000, mean_q: -139.406673, mean_eps: 0.100000\n",
      " 34257/50000: episode: 8309, duration: 0.012s, episode steps:   3, steps per second: 250, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 58.013622, mae: 579.990499, accuracy: 0.145833, mean_q: -145.308787, mean_eps: 0.100000\n",
      " 34260/50000: episode: 8310, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 64.381978, mae: 574.430481, accuracy: 0.166667, mean_q: -151.542964, mean_eps: 0.100000\n",
      " 34263/50000: episode: 8311, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 56.874947, mae: 587.302348, accuracy: 0.177083, mean_q: -136.275548, mean_eps: 0.100000\n",
      " 34268/50000: episode: 8312, duration: 0.019s, episode steps:   5, steps per second: 262, episode reward: -2193.000, mean reward: -438.600 [-999.000, -32.000], mean action: 1.400 [0.000, 3.000],  loss: 44.525676, mae: 585.133948, accuracy: 0.237500, mean_q: -140.652438, mean_eps: 0.100000\n",
      " 34271/50000: episode: 8313, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 57.893021, mae: 593.158956, accuracy: 0.270833, mean_q: -146.730158, mean_eps: 0.100000\n",
      " 34274/50000: episode: 8314, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 54.328953, mae: 593.175090, accuracy: 0.135417, mean_q: -134.159208, mean_eps: 0.100000\n",
      " 34277/50000: episode: 8315, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 36.836385, mae: 557.341125, accuracy: 0.114583, mean_q: -145.448990, mean_eps: 0.100000\n",
      " 34280/50000: episode: 8316, duration: 0.018s, episode steps:   3, steps per second: 169, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 46.158171, mae: 596.177165, accuracy: 0.208333, mean_q: -138.844416, mean_eps: 0.100000\n",
      " 34283/50000: episode: 8317, duration: 0.017s, episode steps:   3, steps per second: 180, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 68.771810, mae: 581.129272, accuracy: 0.187500, mean_q: -137.263885, mean_eps: 0.100000\n",
      " 34286/50000: episode: 8318, duration: 0.015s, episode steps:   3, steps per second: 200, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 70.937149, mae: 576.693461, accuracy: 0.135417, mean_q: -144.925771, mean_eps: 0.100000\n",
      " 34290/50000: episode: 8319, duration: 0.019s, episode steps:   4, steps per second: 209, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 56.295617, mae: 599.692322, accuracy: 0.101562, mean_q: -141.211811, mean_eps: 0.100000\n",
      " 34293/50000: episode: 8320, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 52.762293, mae: 582.159119, accuracy: 0.218750, mean_q: -141.921722, mean_eps: 0.100000\n",
      " 34296/50000: episode: 8321, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 57.748985, mae: 604.546061, accuracy: 0.208333, mean_q: -140.749797, mean_eps: 0.100000\n",
      " 34299/50000: episode: 8322, duration: 0.013s, episode steps:   3, steps per second: 226, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 69.593566, mae: 585.827454, accuracy: 0.114583, mean_q: -141.696879, mean_eps: 0.100000\n",
      " 34302/50000: episode: 8323, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 74.659837, mae: 581.885885, accuracy: 0.177083, mean_q: -134.368805, mean_eps: 0.100000\n",
      " 34305/50000: episode: 8324, duration: 0.013s, episode steps:   3, steps per second: 223, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 53.651431, mae: 589.046590, accuracy: 0.145833, mean_q: -139.200099, mean_eps: 0.100000\n",
      " 34308/50000: episode: 8325, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 68.733513, mae: 587.449015, accuracy: 0.208333, mean_q: -145.522146, mean_eps: 0.100000\n",
      " 34311/50000: episode: 8326, duration: 0.012s, episode steps:   3, steps per second: 245, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 65.604024, mae: 591.741557, accuracy: 0.302083, mean_q: -137.374115, mean_eps: 0.100000\n",
      " 34314/50000: episode: 8327, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 69.871357, mae: 582.911723, accuracy: 0.166667, mean_q: -133.906367, mean_eps: 0.100000\n",
      " 34317/50000: episode: 8328, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 84.354519, mae: 581.864176, accuracy: 0.166667, mean_q: -139.608597, mean_eps: 0.100000\n",
      " 34320/50000: episode: 8329, duration: 0.013s, episode steps:   3, steps per second: 226, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 44.466983, mae: 583.774414, accuracy: 0.239583, mean_q: -138.631083, mean_eps: 0.100000\n",
      " 34323/50000: episode: 8330, duration: 0.019s, episode steps:   3, steps per second: 160, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 43.295100, mae: 596.539103, accuracy: 0.177083, mean_q: -137.359868, mean_eps: 0.100000\n",
      " 34326/50000: episode: 8331, duration: 0.016s, episode steps:   3, steps per second: 192, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 52.908902, mae: 607.315267, accuracy: 0.156250, mean_q: -133.182287, mean_eps: 0.100000\n",
      " 34330/50000: episode: 8332, duration: 0.016s, episode steps:   4, steps per second: 245, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 2.000 [0.000, 3.000],  loss: 45.343554, mae: 577.171432, accuracy: 0.195312, mean_q: -146.957352, mean_eps: 0.100000\n",
      " 34333/50000: episode: 8333, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 65.260054, mae: 581.645915, accuracy: 0.166667, mean_q: -143.327209, mean_eps: 0.100000\n",
      " 34336/50000: episode: 8334, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 64.084221, mae: 590.891418, accuracy: 0.135417, mean_q: -136.320094, mean_eps: 0.100000\n",
      " 34340/50000: episode: 8335, duration: 0.016s, episode steps:   4, steps per second: 257, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 27.420624, mae: 576.570389, accuracy: 0.164062, mean_q: -142.942638, mean_eps: 0.100000\n",
      " 34343/50000: episode: 8336, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 44.947542, mae: 605.617106, accuracy: 0.135417, mean_q: -135.678589, mean_eps: 0.100000\n",
      " 34347/50000: episode: 8337, duration: 0.016s, episode steps:   4, steps per second: 244, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 47.503045, mae: 572.491699, accuracy: 0.148438, mean_q: -140.924927, mean_eps: 0.100000\n",
      " 34350/50000: episode: 8338, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 55.331846, mae: 615.048482, accuracy: 0.114583, mean_q: -138.304357, mean_eps: 0.100000\n",
      " 34353/50000: episode: 8339, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 63.230723, mae: 582.133260, accuracy: 0.135417, mean_q: -139.761968, mean_eps: 0.100000\n",
      " 34356/50000: episode: 8340, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 70.835862, mae: 562.399943, accuracy: 0.156250, mean_q: -141.934382, mean_eps: 0.100000\n",
      " 34360/50000: episode: 8341, duration: 0.016s, episode steps:   4, steps per second: 251, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 82.781240, mae: 596.554764, accuracy: 0.218750, mean_q: -136.491043, mean_eps: 0.100000\n",
      " 34363/50000: episode: 8342, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 56.816552, mae: 585.880758, accuracy: 0.187500, mean_q: -141.302907, mean_eps: 0.100000\n",
      " 34366/50000: episode: 8343, duration: 0.013s, episode steps:   3, steps per second: 223, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 60.279860, mae: 572.735474, accuracy: 0.135417, mean_q: -144.051331, mean_eps: 0.100000\n",
      " 34369/50000: episode: 8344, duration: 0.018s, episode steps:   3, steps per second: 163, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 50.645051, mae: 574.527364, accuracy: 0.135417, mean_q: -146.753515, mean_eps: 0.100000\n",
      " 34372/50000: episode: 8345, duration: 0.016s, episode steps:   3, steps per second: 193, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 64.587485, mae: 599.053569, accuracy: 0.093750, mean_q: -137.203288, mean_eps: 0.100000\n",
      " 34375/50000: episode: 8346, duration: 0.013s, episode steps:   3, steps per second: 227, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 64.057658, mae: 587.129598, accuracy: 0.083333, mean_q: -134.147125, mean_eps: 0.100000\n",
      " 34378/50000: episode: 8347, duration: 0.014s, episode steps:   3, steps per second: 222, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 72.507308, mae: 613.296549, accuracy: 0.104167, mean_q: -140.092748, mean_eps: 0.100000\n",
      " 34381/50000: episode: 8348, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 84.274813, mae: 573.349548, accuracy: 0.166667, mean_q: -141.654292, mean_eps: 0.100000\n",
      " 34385/50000: episode: 8349, duration: 0.016s, episode steps:   4, steps per second: 249, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 51.609626, mae: 552.512741, accuracy: 0.164062, mean_q: -144.014557, mean_eps: 0.100000\n",
      " 34389/50000: episode: 8350, duration: 0.016s, episode steps:   4, steps per second: 246, episode reward: -1223.000, mean reward: -305.750 [-999.000, -32.000], mean action: 0.750 [0.000, 2.000],  loss: 64.229963, mae: 595.893600, accuracy: 0.140625, mean_q: -140.364494, mean_eps: 0.100000\n",
      " 34392/50000: episode: 8351, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 56.884660, mae: 554.898458, accuracy: 0.218750, mean_q: -141.201172, mean_eps: 0.100000\n",
      " 34395/50000: episode: 8352, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 39.215319, mae: 570.797302, accuracy: 0.104167, mean_q: -140.925629, mean_eps: 0.100000\n",
      " 34400/50000: episode: 8353, duration: 0.019s, episode steps:   5, steps per second: 266, episode reward: -2237.000, mean reward: -447.400 [-999.000, -45.000], mean action: 1.200 [0.000, 3.000],  loss: 51.375556, mae: 588.501868, accuracy: 0.206250, mean_q: -141.651172, mean_eps: 0.100000\n",
      " 34403/50000: episode: 8354, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 36.729235, mae: 578.291687, accuracy: 0.083333, mean_q: -140.374776, mean_eps: 0.100000\n",
      " 34406/50000: episode: 8355, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 59.003441, mae: 561.380920, accuracy: 0.135417, mean_q: -141.087825, mean_eps: 0.100000\n",
      " 34409/50000: episode: 8356, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 58.731452, mae: 600.339457, accuracy: 0.250000, mean_q: -139.777751, mean_eps: 0.100000\n",
      " 34412/50000: episode: 8357, duration: 0.022s, episode steps:   3, steps per second: 136, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 67.295069, mae: 570.421407, accuracy: 0.125000, mean_q: -140.219096, mean_eps: 0.100000\n",
      " 34415/50000: episode: 8358, duration: 0.017s, episode steps:   3, steps per second: 174, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 42.403939, mae: 600.214722, accuracy: 0.156250, mean_q: -136.480550, mean_eps: 0.100000\n",
      " 34418/50000: episode: 8359, duration: 0.022s, episode steps:   3, steps per second: 137, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 57.257702, mae: 574.754517, accuracy: 0.177083, mean_q: -147.048833, mean_eps: 0.100000\n",
      " 34421/50000: episode: 8360, duration: 0.016s, episode steps:   3, steps per second: 191, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 28.818643, mae: 575.458293, accuracy: 0.114583, mean_q: -147.627085, mean_eps: 0.100000\n",
      " 34424/50000: episode: 8361, duration: 0.014s, episode steps:   3, steps per second: 220, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 58.665775, mae: 564.494507, accuracy: 0.156250, mean_q: -144.189306, mean_eps: 0.100000\n",
      " 34427/50000: episode: 8362, duration: 0.013s, episode steps:   3, steps per second: 227, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 34.043449, mae: 587.359395, accuracy: 0.166667, mean_q: -139.291468, mean_eps: 0.100000\n",
      " 34430/50000: episode: 8363, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 32.014384, mae: 581.203837, accuracy: 0.135417, mean_q: -137.894577, mean_eps: 0.100000\n",
      " 34433/50000: episode: 8364, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 36.687965, mae: 593.647542, accuracy: 0.156250, mean_q: -136.412679, mean_eps: 0.100000\n",
      " 34436/50000: episode: 8365, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 43.881372, mae: 595.385030, accuracy: 0.125000, mean_q: -138.817881, mean_eps: 0.100000\n",
      " 34439/50000: episode: 8366, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 54.729764, mae: 579.784505, accuracy: 0.156250, mean_q: -140.075373, mean_eps: 0.100000\n",
      " 34444/50000: episode: 8367, duration: 0.019s, episode steps:   5, steps per second: 270, episode reward: -2193.000, mean reward: -438.600 [-999.000, -45.000], mean action: 1.600 [0.000, 3.000],  loss: 28.118634, mae: 593.457349, accuracy: 0.087500, mean_q: -135.243082, mean_eps: 0.100000\n",
      " 34447/50000: episode: 8368, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 49.390212, mae: 602.746216, accuracy: 0.197917, mean_q: -136.653142, mean_eps: 0.100000\n",
      " 34450/50000: episode: 8369, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 38.917370, mae: 567.688232, accuracy: 0.083333, mean_q: -148.618739, mean_eps: 0.100000\n",
      " 34453/50000: episode: 8370, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 58.203972, mae: 601.362244, accuracy: 0.145833, mean_q: -140.018397, mean_eps: 0.100000\n",
      " 34456/50000: episode: 8371, duration: 0.016s, episode steps:   3, steps per second: 186, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 28.352794, mae: 583.343933, accuracy: 0.125000, mean_q: -140.045158, mean_eps: 0.100000\n",
      " 34459/50000: episode: 8372, duration: 0.017s, episode steps:   3, steps per second: 175, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.667 [0.000, 3.000],  loss: 76.256898, mae: 624.860636, accuracy: 0.145833, mean_q: -133.191559, mean_eps: 0.100000\n",
      " 34462/50000: episode: 8373, duration: 0.015s, episode steps:   3, steps per second: 199, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 19.462429, mae: 603.112366, accuracy: 0.072917, mean_q: -137.257024, mean_eps: 0.100000\n",
      " 34465/50000: episode: 8374, duration: 0.014s, episode steps:   3, steps per second: 216, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 46.829809, mae: 590.249797, accuracy: 0.114583, mean_q: -136.675898, mean_eps: 0.100000\n",
      " 34468/50000: episode: 8375, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 42.186239, mae: 571.946167, accuracy: 0.156250, mean_q: -142.235992, mean_eps: 0.100000\n",
      " 34471/50000: episode: 8376, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 86.786105, mae: 558.227498, accuracy: 0.125000, mean_q: -148.385773, mean_eps: 0.100000\n",
      " 34474/50000: episode: 8377, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 41.612310, mae: 575.140442, accuracy: 0.145833, mean_q: -140.900055, mean_eps: 0.100000\n",
      " 34477/50000: episode: 8378, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 41.647753, mae: 584.507548, accuracy: 0.125000, mean_q: -140.484695, mean_eps: 0.100000\n",
      " 34480/50000: episode: 8379, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 38.524492, mae: 579.013184, accuracy: 0.166667, mean_q: -144.059509, mean_eps: 0.100000\n",
      " 34484/50000: episode: 8380, duration: 0.016s, episode steps:   4, steps per second: 248, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 35.046647, mae: 577.516602, accuracy: 0.109375, mean_q: -137.661743, mean_eps: 0.100000\n",
      " 34487/50000: episode: 8381, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 68.205404, mae: 572.976237, accuracy: 0.156250, mean_q: -144.613688, mean_eps: 0.100000\n",
      " 34490/50000: episode: 8382, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 35.343224, mae: 578.837728, accuracy: 0.114583, mean_q: -138.837682, mean_eps: 0.100000\n",
      " 34493/50000: episode: 8383, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 35.454105, mae: 589.099162, accuracy: 0.093750, mean_q: -138.068899, mean_eps: 0.100000\n",
      " 34496/50000: episode: 8384, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 78.382893, mae: 607.119792, accuracy: 0.135417, mean_q: -135.091995, mean_eps: 0.100000\n",
      " 34499/50000: episode: 8385, duration: 0.013s, episode steps:   3, steps per second: 226, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 37.489723, mae: 585.940002, accuracy: 0.135417, mean_q: -141.053838, mean_eps: 0.100000\n",
      " 34502/50000: episode: 8386, duration: 0.014s, episode steps:   3, steps per second: 216, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 41.349282, mae: 606.695068, accuracy: 0.145833, mean_q: -133.886269, mean_eps: 0.100000\n",
      " 34505/50000: episode: 8387, duration: 0.018s, episode steps:   3, steps per second: 163, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 48.916719, mae: 562.624227, accuracy: 0.135417, mean_q: -144.604675, mean_eps: 0.100000\n",
      " 34511/50000: episode: 8388, duration: 0.025s, episode steps:   6, steps per second: 237, episode reward: -3192.000, mean reward: -532.000 [-999.000, -58.000], mean action: 1.000 [0.000, 3.000],  loss: 29.669676, mae: 573.757670, accuracy: 0.098958, mean_q: -142.789574, mean_eps: 0.100000\n",
      " 34515/50000: episode: 8389, duration: 0.016s, episode steps:   4, steps per second: 245, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 58.403551, mae: 567.436157, accuracy: 0.140625, mean_q: -142.322018, mean_eps: 0.100000\n",
      " 34518/50000: episode: 8390, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 48.500121, mae: 605.701884, accuracy: 0.125000, mean_q: -134.446777, mean_eps: 0.100000\n",
      " 34521/50000: episode: 8391, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 42.406680, mae: 601.394430, accuracy: 0.114583, mean_q: -137.565384, mean_eps: 0.100000\n",
      " 34524/50000: episode: 8392, duration: 0.012s, episode steps:   3, steps per second: 242, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 43.839355, mae: 594.149312, accuracy: 0.177083, mean_q: -138.528183, mean_eps: 0.100000\n",
      " 34528/50000: episode: 8393, duration: 0.015s, episode steps:   4, steps per second: 262, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.750 [0.000, 3.000],  loss: 38.518976, mae: 586.157440, accuracy: 0.179688, mean_q: -133.388018, mean_eps: 0.100000\n",
      " 34533/50000: episode: 8394, duration: 0.019s, episode steps:   5, steps per second: 265, episode reward: -2222.000, mean reward: -444.400 [-999.000, -58.000], mean action: 1.800 [0.000, 3.000],  loss: 59.017202, mae: 588.829846, accuracy: 0.131250, mean_q: -139.127670, mean_eps: 0.100000\n",
      " 34537/50000: episode: 8395, duration: 0.016s, episode steps:   4, steps per second: 251, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 31.236303, mae: 598.209396, accuracy: 0.093750, mean_q: -141.869949, mean_eps: 0.100000\n",
      " 34540/50000: episode: 8396, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 64.114667, mae: 604.210897, accuracy: 0.135417, mean_q: -135.320491, mean_eps: 0.100000\n",
      " 34545/50000: episode: 8397, duration: 0.019s, episode steps:   5, steps per second: 266, episode reward: -2222.000, mean reward: -444.400 [-999.000, -58.000], mean action: 2.200 [0.000, 3.000],  loss: 32.855541, mae: 568.096375, accuracy: 0.137500, mean_q: -144.802896, mean_eps: 0.100000\n",
      " 34548/50000: episode: 8398, duration: 0.013s, episode steps:   3, steps per second: 230, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 33.758163, mae: 574.011129, accuracy: 0.145833, mean_q: -143.098470, mean_eps: 0.100000\n",
      " 34551/50000: episode: 8399, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 59.076257, mae: 597.686808, accuracy: 0.135417, mean_q: -143.472656, mean_eps: 0.100000\n",
      " 34554/50000: episode: 8400, duration: 0.014s, episode steps:   3, steps per second: 213, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 34.328401, mae: 573.051961, accuracy: 0.135417, mean_q: -145.646759, mean_eps: 0.100000\n",
      " 34557/50000: episode: 8401, duration: 0.020s, episode steps:   3, steps per second: 147, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 19.136025, mae: 566.171244, accuracy: 0.156250, mean_q: -144.004593, mean_eps: 0.100000\n",
      " 34561/50000: episode: 8402, duration: 0.019s, episode steps:   4, steps per second: 208, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.750 [0.000, 3.000],  loss: 59.623229, mae: 578.878235, accuracy: 0.171875, mean_q: -142.911686, mean_eps: 0.100000\n",
      " 34564/50000: episode: 8403, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 16.493027, mae: 593.811218, accuracy: 0.125000, mean_q: -139.625748, mean_eps: 0.100000\n",
      " 34568/50000: episode: 8404, duration: 0.016s, episode steps:   4, steps per second: 248, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 38.543605, mae: 576.865189, accuracy: 0.171875, mean_q: -141.554787, mean_eps: 0.100000\n",
      " 34571/50000: episode: 8405, duration: 0.012s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 61.426957, mae: 563.509013, accuracy: 0.114583, mean_q: -143.330622, mean_eps: 0.100000\n",
      " 34574/50000: episode: 8406, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 53.390388, mae: 553.552368, accuracy: 0.135417, mean_q: -147.933533, mean_eps: 0.100000\n",
      " 34577/50000: episode: 8407, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 45.254818, mae: 577.146769, accuracy: 0.177083, mean_q: -137.175512, mean_eps: 0.100000\n",
      " 34580/50000: episode: 8408, duration: 0.012s, episode steps:   3, steps per second: 247, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 32.253633, mae: 589.176921, accuracy: 0.104167, mean_q: -136.053614, mean_eps: 0.100000\n",
      " 34584/50000: episode: 8409, duration: 0.016s, episode steps:   4, steps per second: 256, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 62.077434, mae: 570.357559, accuracy: 0.203125, mean_q: -139.211971, mean_eps: 0.100000\n",
      " 34587/50000: episode: 8410, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 52.543432, mae: 560.024414, accuracy: 0.114583, mean_q: -145.493134, mean_eps: 0.100000\n",
      " 34590/50000: episode: 8411, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 38.899551, mae: 565.846863, accuracy: 0.093750, mean_q: -149.394318, mean_eps: 0.100000\n",
      " 34593/50000: episode: 8412, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 38.250188, mae: 602.015625, accuracy: 0.114583, mean_q: -138.206640, mean_eps: 0.100000\n",
      " 34596/50000: episode: 8413, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 53.753189, mae: 555.568278, accuracy: 0.208333, mean_q: -145.909332, mean_eps: 0.100000\n",
      " 34599/50000: episode: 8414, duration: 0.016s, episode steps:   3, steps per second: 185, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 43.120052, mae: 572.644552, accuracy: 0.093750, mean_q: -142.316182, mean_eps: 0.100000\n",
      " 34602/50000: episode: 8415, duration: 0.038s, episode steps:   3, steps per second:  80, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 81.462547, mae: 583.509277, accuracy: 0.229167, mean_q: -142.801620, mean_eps: 0.100000\n",
      " 34605/50000: episode: 8416, duration: 0.016s, episode steps:   3, steps per second: 184, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 58.182156, mae: 600.573975, accuracy: 0.125000, mean_q: -133.707972, mean_eps: 0.100000\n",
      " 34608/50000: episode: 8417, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 67.593754, mae: 578.381124, accuracy: 0.135417, mean_q: -142.185552, mean_eps: 0.100000\n",
      " 34612/50000: episode: 8418, duration: 0.016s, episode steps:   4, steps per second: 255, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 53.983574, mae: 593.705460, accuracy: 0.117188, mean_q: -142.176064, mean_eps: 0.100000\n",
      " 34615/50000: episode: 8419, duration: 0.012s, episode steps:   3, steps per second: 243, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 74.558139, mae: 598.121155, accuracy: 0.104167, mean_q: -133.573517, mean_eps: 0.100000\n",
      " 34619/50000: episode: 8420, duration: 0.015s, episode steps:   4, steps per second: 260, episode reward: -1238.000, mean reward: -309.500 [-999.000, -45.000], mean action: 1.750 [0.000, 3.000],  loss: 65.743904, mae: 562.969345, accuracy: 0.117188, mean_q: -144.477203, mean_eps: 0.100000\n",
      " 34622/50000: episode: 8421, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 36.356321, mae: 576.182414, accuracy: 0.135417, mean_q: -143.361847, mean_eps: 0.100000\n",
      " 34625/50000: episode: 8422, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 33.138391, mae: 603.101725, accuracy: 0.093750, mean_q: -136.503627, mean_eps: 0.100000\n",
      " 34628/50000: episode: 8423, duration: 0.013s, episode steps:   3, steps per second: 238, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 32.129552, mae: 574.477458, accuracy: 0.114583, mean_q: -144.266123, mean_eps: 0.100000\n",
      " 34631/50000: episode: 8424, duration: 0.012s, episode steps:   3, steps per second: 246, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 30.963114, mae: 592.923075, accuracy: 0.208333, mean_q: -138.182922, mean_eps: 0.100000\n",
      " 34635/50000: episode: 8425, duration: 0.016s, episode steps:   4, steps per second: 251, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 57.192270, mae: 572.410629, accuracy: 0.164062, mean_q: -143.068748, mean_eps: 0.100000\n",
      " 34639/50000: episode: 8426, duration: 0.016s, episode steps:   4, steps per second: 253, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 37.117387, mae: 617.204926, accuracy: 0.171875, mean_q: -136.715485, mean_eps: 0.100000\n",
      " 34642/50000: episode: 8427, duration: 0.015s, episode steps:   3, steps per second: 204, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 68.124621, mae: 588.992594, accuracy: 0.343750, mean_q: -134.423782, mean_eps: 0.100000\n",
      " 34645/50000: episode: 8428, duration: 0.018s, episode steps:   3, steps per second: 168, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 31.107703, mae: 569.883097, accuracy: 0.145833, mean_q: -140.736984, mean_eps: 0.100000\n",
      " 34648/50000: episode: 8429, duration: 0.015s, episode steps:   3, steps per second: 198, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 43.926952, mae: 587.059041, accuracy: 0.135417, mean_q: -146.784190, mean_eps: 0.100000\n",
      " 34651/50000: episode: 8430, duration: 0.013s, episode steps:   3, steps per second: 225, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 44.880122, mae: 595.294312, accuracy: 0.104167, mean_q: -141.226237, mean_eps: 0.100000\n",
      " 34654/50000: episode: 8431, duration: 0.013s, episode steps:   3, steps per second: 224, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 53.001788, mae: 592.948792, accuracy: 0.156250, mean_q: -140.811234, mean_eps: 0.100000\n",
      " 34657/50000: episode: 8432, duration: 0.013s, episode steps:   3, steps per second: 224, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 54.002025, mae: 594.873088, accuracy: 0.166667, mean_q: -140.016998, mean_eps: 0.100000\n",
      " 34661/50000: episode: 8433, duration: 0.016s, episode steps:   4, steps per second: 258, episode reward: -1223.000, mean reward: -305.750 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 38.643576, mae: 582.031097, accuracy: 0.187500, mean_q: -133.948393, mean_eps: 0.100000\n",
      " 34664/50000: episode: 8434, duration: 0.012s, episode steps:   3, steps per second: 244, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 38.731310, mae: 579.084452, accuracy: 0.125000, mean_q: -143.964935, mean_eps: 0.100000\n",
      " 34668/50000: episode: 8435, duration: 0.016s, episode steps:   4, steps per second: 253, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 52.832421, mae: 571.576187, accuracy: 0.156250, mean_q: -145.491058, mean_eps: 0.100000\n",
      " 34671/50000: episode: 8436, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 37.129555, mae: 551.850240, accuracy: 0.145833, mean_q: -140.823608, mean_eps: 0.100000\n",
      " 34674/50000: episode: 8437, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 44.329568, mae: 577.646281, accuracy: 0.156250, mean_q: -137.704158, mean_eps: 0.100000\n",
      " 34677/50000: episode: 8438, duration: 0.013s, episode steps:   3, steps per second: 231, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 41.584574, mae: 605.639404, accuracy: 0.145833, mean_q: -137.533020, mean_eps: 0.100000\n",
      " 34680/50000: episode: 8439, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 57.012245, mae: 588.475220, accuracy: 0.270833, mean_q: -140.986791, mean_eps: 0.100000\n",
      " 34683/50000: episode: 8440, duration: 0.012s, episode steps:   3, steps per second: 241, episode reward: -239.000, mean reward: -79.667 [-136.000, -45.000], mean action: 1.667 [0.000, 3.000],  loss: 39.437864, mae: 622.391337, accuracy: 0.114583, mean_q: -136.095322, mean_eps: 0.100000\n",
      " 34687/50000: episode: 8441, duration: 0.016s, episode steps:   4, steps per second: 246, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 41.751400, mae: 609.019501, accuracy: 0.132812, mean_q: -133.378481, mean_eps: 0.100000\n",
      " 34690/50000: episode: 8442, duration: 0.014s, episode steps:   3, steps per second: 213, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 34.173080, mae: 570.910929, accuracy: 0.125000, mean_q: -143.382253, mean_eps: 0.100000\n",
      " 34693/50000: episode: 8443, duration: 0.020s, episode steps:   3, steps per second: 148, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 42.957045, mae: 595.081340, accuracy: 0.156250, mean_q: -138.996170, mean_eps: 0.100000\n",
      " 34696/50000: episode: 8444, duration: 0.015s, episode steps:   3, steps per second: 202, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 41.627988, mae: 598.828491, accuracy: 0.156250, mean_q: -137.498128, mean_eps: 0.100000\n",
      " 34699/50000: episode: 8445, duration: 0.013s, episode steps:   3, steps per second: 224, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 73.859894, mae: 583.409566, accuracy: 0.093750, mean_q: -138.216115, mean_eps: 0.100000\n",
      " 34702/50000: episode: 8446, duration: 0.015s, episode steps:   3, steps per second: 201, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 43.066914, mae: 567.720846, accuracy: 0.125000, mean_q: -142.326843, mean_eps: 0.100000\n",
      " 34706/50000: episode: 8447, duration: 0.016s, episode steps:   4, steps per second: 246, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 0.750 [0.000, 2.000],  loss: 51.189056, mae: 585.154572, accuracy: 0.179688, mean_q: -142.623837, mean_eps: 0.100000\n",
      " 34709/50000: episode: 8448, duration: 0.013s, episode steps:   3, steps per second: 223, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 34.060615, mae: 587.058634, accuracy: 0.125000, mean_q: -141.426910, mean_eps: 0.100000\n",
      " 34713/50000: episode: 8449, duration: 0.017s, episode steps:   4, steps per second: 239, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 69.330246, mae: 568.651062, accuracy: 0.156250, mean_q: -144.487404, mean_eps: 0.100000\n",
      " 34717/50000: episode: 8450, duration: 0.016s, episode steps:   4, steps per second: 249, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 2.000 [0.000, 3.000],  loss: 44.558791, mae: 583.975677, accuracy: 0.078125, mean_q: -144.104977, mean_eps: 0.100000\n",
      " 34720/50000: episode: 8451, duration: 0.013s, episode steps:   3, steps per second: 234, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 38.950851, mae: 571.348429, accuracy: 0.208333, mean_q: -145.198354, mean_eps: 0.100000\n",
      " 34723/50000: episode: 8452, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 49.261523, mae: 586.229655, accuracy: 0.197917, mean_q: -139.714483, mean_eps: 0.100000\n",
      " 34726/50000: episode: 8453, duration: 0.013s, episode steps:   3, steps per second: 229, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 61.567379, mae: 567.885417, accuracy: 0.083333, mean_q: -145.377523, mean_eps: 0.100000\n",
      " 34729/50000: episode: 8454, duration: 0.014s, episode steps:   3, steps per second: 211, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 63.941479, mae: 613.644938, accuracy: 0.197917, mean_q: -138.396673, mean_eps: 0.100000\n",
      " 34732/50000: episode: 8455, duration: 0.015s, episode steps:   3, steps per second: 203, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 39.890390, mae: 587.050496, accuracy: 0.208333, mean_q: -143.855845, mean_eps: 0.100000\n",
      " 34735/50000: episode: 8456, duration: 0.020s, episode steps:   3, steps per second: 146, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 71.327082, mae: 613.030090, accuracy: 0.093750, mean_q: -135.250743, mean_eps: 0.100000\n",
      " 34738/50000: episode: 8457, duration: 0.018s, episode steps:   3, steps per second: 170, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 33.709499, mae: 605.292969, accuracy: 0.114583, mean_q: -139.354462, mean_eps: 0.100000\n",
      " 34741/50000: episode: 8458, duration: 0.015s, episode steps:   3, steps per second: 194, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 37.441062, mae: 569.122681, accuracy: 0.302083, mean_q: -138.894338, mean_eps: 0.100000\n",
      " 34744/50000: episode: 8459, duration: 0.016s, episode steps:   3, steps per second: 184, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 48.189301, mae: 566.876017, accuracy: 0.125000, mean_q: -147.252004, mean_eps: 0.100000\n",
      " 34747/50000: episode: 8460, duration: 0.015s, episode steps:   3, steps per second: 195, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 37.939699, mae: 574.138407, accuracy: 0.145833, mean_q: -146.562826, mean_eps: 0.100000\n",
      " 34750/50000: episode: 8461, duration: 0.017s, episode steps:   3, steps per second: 175, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 39.497137, mae: 608.395467, accuracy: 0.135417, mean_q: -134.609400, mean_eps: 0.100000\n",
      " 34753/50000: episode: 8462, duration: 0.018s, episode steps:   3, steps per second: 163, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 29.303049, mae: 583.282613, accuracy: 0.125000, mean_q: -144.168152, mean_eps: 0.100000\n",
      " 34757/50000: episode: 8463, duration: 0.027s, episode steps:   4, steps per second: 148, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 38.003901, mae: 578.026825, accuracy: 0.109375, mean_q: -143.136421, mean_eps: 0.100000\n",
      " 34760/50000: episode: 8464, duration: 0.020s, episode steps:   3, steps per second: 149, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 37.732235, mae: 594.181193, accuracy: 0.104167, mean_q: -141.552058, mean_eps: 0.100000\n",
      " 34763/50000: episode: 8465, duration: 0.020s, episode steps:   3, steps per second: 147, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 68.481034, mae: 570.936869, accuracy: 0.197917, mean_q: -144.810486, mean_eps: 0.100000\n",
      " 34767/50000: episode: 8466, duration: 0.023s, episode steps:   4, steps per second: 177, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 51.868184, mae: 594.055161, accuracy: 0.156250, mean_q: -134.732468, mean_eps: 0.100000\n",
      " 34770/50000: episode: 8467, duration: 0.028s, episode steps:   3, steps per second: 106, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 48.414508, mae: 602.348796, accuracy: 0.177083, mean_q: -136.655212, mean_eps: 0.100000\n",
      " 34774/50000: episode: 8468, duration: 0.023s, episode steps:   4, steps per second: 170, episode reward: -1238.000, mean reward: -309.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 58.957170, mae: 584.419434, accuracy: 0.218750, mean_q: -137.317039, mean_eps: 0.100000\n",
      " 34777/50000: episode: 8469, duration: 0.015s, episode steps:   3, steps per second: 195, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 66.002981, mae: 616.695109, accuracy: 0.104167, mean_q: -142.778234, mean_eps: 0.100000\n",
      " 34780/50000: episode: 8470, duration: 0.015s, episode steps:   3, steps per second: 204, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 23.759914, mae: 587.352091, accuracy: 0.145833, mean_q: -141.418457, mean_eps: 0.100000\n",
      " 34783/50000: episode: 8471, duration: 0.014s, episode steps:   3, steps per second: 209, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 41.961817, mae: 577.922363, accuracy: 0.062500, mean_q: -140.881719, mean_eps: 0.100000\n",
      " 34786/50000: episode: 8472, duration: 0.015s, episode steps:   3, steps per second: 206, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 46.929057, mae: 597.130025, accuracy: 0.093750, mean_q: -140.761032, mean_eps: 0.100000\n",
      " 34789/50000: episode: 8473, duration: 0.015s, episode steps:   3, steps per second: 204, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 43.371117, mae: 581.375773, accuracy: 0.125000, mean_q: -147.364085, mean_eps: 0.100000\n",
      " 34792/50000: episode: 8474, duration: 0.028s, episode steps:   3, steps per second: 106, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 25.897445, mae: 613.520732, accuracy: 0.104167, mean_q: -141.780182, mean_eps: 0.100000\n",
      " 34795/50000: episode: 8475, duration: 0.024s, episode steps:   3, steps per second: 123, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 32.584492, mae: 583.122416, accuracy: 0.145833, mean_q: -139.175054, mean_eps: 0.100000\n",
      " 34798/50000: episode: 8476, duration: 0.021s, episode steps:   3, steps per second: 146, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 69.574275, mae: 590.780029, accuracy: 0.114583, mean_q: -145.079437, mean_eps: 0.100000\n",
      " 34801/50000: episode: 8477, duration: 0.019s, episode steps:   3, steps per second: 162, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 35.596333, mae: 590.878988, accuracy: 0.072917, mean_q: -139.450272, mean_eps: 0.100000\n",
      " 34804/50000: episode: 8478, duration: 0.016s, episode steps:   3, steps per second: 189, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 37.597938, mae: 603.107646, accuracy: 0.114583, mean_q: -139.593409, mean_eps: 0.100000\n",
      " 34808/50000: episode: 8479, duration: 0.022s, episode steps:   4, steps per second: 185, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.750 [0.000, 3.000],  loss: 43.395324, mae: 637.891312, accuracy: 0.156250, mean_q: -133.050959, mean_eps: 0.100000\n",
      " 34812/50000: episode: 8480, duration: 0.026s, episode steps:   4, steps per second: 156, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 51.078071, mae: 584.724182, accuracy: 0.179688, mean_q: -140.388367, mean_eps: 0.100000\n",
      " 34815/50000: episode: 8481, duration: 0.014s, episode steps:   3, steps per second: 221, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 34.537447, mae: 567.222168, accuracy: 0.197917, mean_q: -151.986771, mean_eps: 0.100000\n",
      " 34818/50000: episode: 8482, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 43.354983, mae: 597.475260, accuracy: 0.104167, mean_q: -138.387319, mean_eps: 0.100000\n",
      " 34821/50000: episode: 8483, duration: 0.013s, episode steps:   3, steps per second: 237, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 42.857723, mae: 594.166117, accuracy: 0.104167, mean_q: -138.147196, mean_eps: 0.100000\n",
      " 34824/50000: episode: 8484, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 52.173414, mae: 587.821859, accuracy: 0.135417, mean_q: -142.280151, mean_eps: 0.100000\n",
      " 34827/50000: episode: 8485, duration: 0.013s, episode steps:   3, steps per second: 233, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 33.093367, mae: 605.708537, accuracy: 0.145833, mean_q: -136.065236, mean_eps: 0.100000\n",
      " 34830/50000: episode: 8486, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 28.270215, mae: 611.433004, accuracy: 0.187500, mean_q: -134.178686, mean_eps: 0.100000\n",
      " 34833/50000: episode: 8487, duration: 0.013s, episode steps:   3, steps per second: 239, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 46.001742, mae: 569.610046, accuracy: 0.177083, mean_q: -144.652044, mean_eps: 0.100000\n",
      " 34836/50000: episode: 8488, duration: 0.012s, episode steps:   3, steps per second: 251, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 59.108541, mae: 607.039551, accuracy: 0.125000, mean_q: -137.862244, mean_eps: 0.100000\n",
      " 34839/50000: episode: 8489, duration: 0.014s, episode steps:   3, steps per second: 222, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 23.770669, mae: 571.471293, accuracy: 0.114583, mean_q: -144.963725, mean_eps: 0.100000\n",
      " 34842/50000: episode: 8490, duration: 0.014s, episode steps:   3, steps per second: 213, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 37.061915, mae: 581.477844, accuracy: 0.135417, mean_q: -141.951207, mean_eps: 0.100000\n",
      " 34845/50000: episode: 8491, duration: 0.016s, episode steps:   3, steps per second: 190, episode reward: -224.000, mean reward: -74.667 [-134.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 43.213915, mae: 558.948324, accuracy: 0.166667, mean_q: -146.859965, mean_eps: 0.100000\n",
      " 34848/50000: episode: 8492, duration: 0.017s, episode steps:   3, steps per second: 172, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 22.866030, mae: 620.815674, accuracy: 0.104167, mean_q: -137.587743, mean_eps: 0.100000\n",
      " 34851/50000: episode: 8493, duration: 0.022s, episode steps:   3, steps per second: 135, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 29.925814, mae: 581.759237, accuracy: 0.093750, mean_q: -144.295858, mean_eps: 0.100000\n",
      " 34854/50000: episode: 8494, duration: 0.020s, episode steps:   3, steps per second: 152, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 30.920302, mae: 574.257528, accuracy: 0.229167, mean_q: -137.434713, mean_eps: 0.100000\n",
      " 34859/50000: episode: 8495, duration: 0.024s, episode steps:   5, steps per second: 207, episode reward: -2193.000, mean reward: -438.600 [-999.000, -45.000], mean action: 1.400 [0.000, 3.000],  loss: 57.336158, mae: 591.644800, accuracy: 0.112500, mean_q: -142.458618, mean_eps: 0.100000\n",
      " 34862/50000: episode: 8496, duration: 0.013s, episode steps:   3, steps per second: 225, episode reward: -224.000, mean reward: -74.667 [-90.000, -58.000], mean action: 1.667 [0.000, 3.000],  loss: 32.113529, mae: 580.334086, accuracy: 0.135417, mean_q: -137.727089, mean_eps: 0.100000\n",
      " 34865/50000: episode: 8497, duration: 0.013s, episode steps:   3, steps per second: 240, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 46.338514, mae: 561.673136, accuracy: 0.135417, mean_q: -146.394552, mean_eps: 0.100000\n",
      " 34868/50000: episode: 8498, duration: 0.013s, episode steps:   3, steps per second: 236, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 39.448554, mae: 560.972046, accuracy: 0.187500, mean_q: -146.252151, mean_eps: 0.100000\n",
      " 34871/50000: episode: 8499, duration: 0.015s, episode steps:   3, steps per second: 200, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 35.623568, mae: 578.544718, accuracy: 0.145833, mean_q: -147.997467, mean_eps: 0.100000\n",
      " 34874/50000: episode: 8500, duration: 0.016s, episode steps:   3, steps per second: 183, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 44.945180, mae: 571.627645, accuracy: 0.072917, mean_q: -151.350001, mean_eps: 0.100000\n",
      " 34877/50000: episode: 8501, duration: 0.017s, episode steps:   3, steps per second: 179, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 24.437112, mae: 601.126994, accuracy: 0.093750, mean_q: -137.906896, mean_eps: 0.100000\n",
      " 34880/50000: episode: 8502, duration: 0.014s, episode steps:   3, steps per second: 220, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 34.099819, mae: 606.195475, accuracy: 0.093750, mean_q: -139.185176, mean_eps: 0.100000\n",
      " 34883/50000: episode: 8503, duration: 0.014s, episode steps:   3, steps per second: 209, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 28.286389, mae: 604.318542, accuracy: 0.052083, mean_q: -137.422017, mean_eps: 0.100000\n",
      " 34887/50000: episode: 8504, duration: 0.017s, episode steps:   4, steps per second: 231, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 81.953385, mae: 606.439453, accuracy: 0.179688, mean_q: -138.789528, mean_eps: 0.100000\n",
      " 34891/50000: episode: 8505, duration: 0.021s, episode steps:   4, steps per second: 190, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 38.789410, mae: 574.810776, accuracy: 0.289062, mean_q: -138.942978, mean_eps: 0.100000\n",
      " 34894/50000: episode: 8506, duration: 0.022s, episode steps:   3, steps per second: 135, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 50.025838, mae: 611.420593, accuracy: 0.093750, mean_q: -135.466146, mean_eps: 0.100000\n",
      " 34898/50000: episode: 8507, duration: 0.023s, episode steps:   4, steps per second: 170, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.250 [0.000, 3.000],  loss: 40.204092, mae: 597.361862, accuracy: 0.242188, mean_q: -136.039879, mean_eps: 0.100000\n",
      " 34901/50000: episode: 8508, duration: 0.018s, episode steps:   3, steps per second: 169, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 49.365799, mae: 565.902690, accuracy: 0.229167, mean_q: -147.840113, mean_eps: 0.100000\n",
      " 34904/50000: episode: 8509, duration: 0.018s, episode steps:   3, steps per second: 165, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 46.966358, mae: 539.644389, accuracy: 0.125000, mean_q: -152.539841, mean_eps: 0.100000\n",
      " 34907/50000: episode: 8510, duration: 0.018s, episode steps:   3, steps per second: 171, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 48.825213, mae: 583.416809, accuracy: 0.125000, mean_q: -135.754527, mean_eps: 0.100000\n",
      " 34911/50000: episode: 8511, duration: 0.022s, episode steps:   4, steps per second: 179, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 35.154856, mae: 579.893646, accuracy: 0.281250, mean_q: -139.742823, mean_eps: 0.100000\n",
      " 34914/50000: episode: 8512, duration: 0.015s, episode steps:   3, steps per second: 203, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 34.262323, mae: 590.041239, accuracy: 0.177083, mean_q: -142.476237, mean_eps: 0.100000\n",
      " 34917/50000: episode: 8513, duration: 0.013s, episode steps:   3, steps per second: 235, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 39.005905, mae: 554.841675, accuracy: 0.145833, mean_q: -149.152802, mean_eps: 0.100000\n",
      " 34920/50000: episode: 8514, duration: 0.014s, episode steps:   3, steps per second: 219, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 37.102139, mae: 584.175212, accuracy: 0.177083, mean_q: -144.204900, mean_eps: 0.100000\n",
      " 34923/50000: episode: 8515, duration: 0.013s, episode steps:   3, steps per second: 230, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 32.717352, mae: 574.327718, accuracy: 0.218750, mean_q: -143.346471, mean_eps: 0.100000\n",
      " 34926/50000: episode: 8516, duration: 0.013s, episode steps:   3, steps per second: 223, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 33.193963, mae: 577.106893, accuracy: 0.177083, mean_q: -145.043269, mean_eps: 0.100000\n",
      " 34930/50000: episode: 8517, duration: 0.033s, episode steps:   4, steps per second: 123, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 2.000 [0.000, 3.000],  loss: 54.255857, mae: 563.368683, accuracy: 0.132812, mean_q: -144.434448, mean_eps: 0.100000\n",
      " 34933/50000: episode: 8518, duration: 0.016s, episode steps:   3, steps per second: 183, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 53.282026, mae: 592.345276, accuracy: 0.197917, mean_q: -138.486023, mean_eps: 0.100000\n",
      " 34936/50000: episode: 8519, duration: 0.013s, episode steps:   3, steps per second: 230, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 40.233149, mae: 600.227295, accuracy: 0.114583, mean_q: -145.424566, mean_eps: 0.100000\n",
      " 34939/50000: episode: 8520, duration: 0.013s, episode steps:   3, steps per second: 227, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 26.278495, mae: 595.016439, accuracy: 0.145833, mean_q: -136.504003, mean_eps: 0.100000\n",
      " 34942/50000: episode: 8521, duration: 0.014s, episode steps:   3, steps per second: 219, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 52.621890, mae: 571.202393, accuracy: 0.135417, mean_q: -143.236918, mean_eps: 0.100000\n",
      " 34945/50000: episode: 8522, duration: 0.016s, episode steps:   3, steps per second: 184, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 38.947272, mae: 582.625061, accuracy: 0.072917, mean_q: -144.604126, mean_eps: 0.100000\n",
      " 34949/50000: episode: 8523, duration: 0.026s, episode steps:   4, steps per second: 157, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 44.098894, mae: 570.468765, accuracy: 0.148438, mean_q: -139.990528, mean_eps: 0.100000\n",
      " 34952/50000: episode: 8524, duration: 0.017s, episode steps:   3, steps per second: 178, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 42.744673, mae: 595.601318, accuracy: 0.083333, mean_q: -141.360184, mean_eps: 0.100000\n",
      " 34955/50000: episode: 8525, duration: 0.017s, episode steps:   3, steps per second: 175, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 35.810874, mae: 567.544189, accuracy: 0.125000, mean_q: -149.395035, mean_eps: 0.100000\n",
      " 34958/50000: episode: 8526, duration: 0.017s, episode steps:   3, steps per second: 178, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 35.907694, mae: 594.444275, accuracy: 0.135417, mean_q: -140.042414, mean_eps: 0.100000\n",
      " 34962/50000: episode: 8527, duration: 0.020s, episode steps:   4, steps per second: 199, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 36.937844, mae: 588.687057, accuracy: 0.132812, mean_q: -140.680542, mean_eps: 0.100000\n",
      " 34965/50000: episode: 8528, duration: 0.016s, episode steps:   3, steps per second: 183, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 54.676835, mae: 586.979106, accuracy: 0.250000, mean_q: -132.594945, mean_eps: 0.100000\n",
      " 34968/50000: episode: 8529, duration: 0.017s, episode steps:   3, steps per second: 174, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 27.874132, mae: 562.399862, accuracy: 0.187500, mean_q: -145.181707, mean_eps: 0.100000\n",
      " 34971/50000: episode: 8530, duration: 0.013s, episode steps:   3, steps per second: 223, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 31.753457, mae: 552.841777, accuracy: 0.145833, mean_q: -147.993703, mean_eps: 0.100000\n",
      " 34974/50000: episode: 8531, duration: 0.014s, episode steps:   3, steps per second: 213, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 39.829048, mae: 571.697449, accuracy: 0.145833, mean_q: -140.787974, mean_eps: 0.100000\n",
      " 34977/50000: episode: 8532, duration: 0.014s, episode steps:   3, steps per second: 211, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 29.143851, mae: 600.220337, accuracy: 0.239583, mean_q: -139.791031, mean_eps: 0.100000\n",
      " 34980/50000: episode: 8533, duration: 0.016s, episode steps:   3, steps per second: 190, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 48.788312, mae: 592.785400, accuracy: 0.114583, mean_q: -141.151861, mean_eps: 0.100000\n",
      " 34984/50000: episode: 8534, duration: 0.019s, episode steps:   4, steps per second: 207, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 50.452374, mae: 581.482178, accuracy: 0.195312, mean_q: -140.863522, mean_eps: 0.100000\n",
      " 34987/50000: episode: 8535, duration: 0.018s, episode steps:   3, steps per second: 164, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 46.146926, mae: 572.459229, accuracy: 0.104167, mean_q: -141.660629, mean_eps: 0.100000\n",
      " 34992/50000: episode: 8536, duration: 0.031s, episode steps:   5, steps per second: 160, episode reward: -2193.000, mean reward: -438.600 [-999.000, -32.000], mean action: 1.600 [0.000, 3.000],  loss: 57.398554, mae: 579.833423, accuracy: 0.112500, mean_q: -146.001651, mean_eps: 0.100000\n",
      " 34995/50000: episode: 8537, duration: 0.020s, episode steps:   3, steps per second: 152, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 50.954926, mae: 570.173910, accuracy: 0.229167, mean_q: -149.160894, mean_eps: 0.100000\n",
      " 34998/50000: episode: 8538, duration: 0.021s, episode steps:   3, steps per second: 144, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 28.340930, mae: 582.949198, accuracy: 0.125000, mean_q: -143.364268, mean_eps: 0.100000\n",
      " 35001/50000: episode: 8539, duration: 0.019s, episode steps:   3, steps per second: 155, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 71.808973, mae: 583.482849, accuracy: 0.145833, mean_q: -144.200795, mean_eps: 0.100000\n",
      " 35004/50000: episode: 8540, duration: 0.024s, episode steps:   3, steps per second: 123, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 44.044144, mae: 596.452922, accuracy: 0.177083, mean_q: -140.006098, mean_eps: 0.100000\n",
      " 35007/50000: episode: 8541, duration: 0.021s, episode steps:   3, steps per second: 142, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 30.526825, mae: 569.468241, accuracy: 0.062500, mean_q: -141.958206, mean_eps: 0.100000\n",
      " 35010/50000: episode: 8542, duration: 0.017s, episode steps:   3, steps per second: 179, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 32.758809, mae: 552.240479, accuracy: 0.229167, mean_q: -150.717570, mean_eps: 0.100000\n",
      " 35013/50000: episode: 8543, duration: 0.014s, episode steps:   3, steps per second: 211, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 52.022333, mae: 596.121867, accuracy: 0.104167, mean_q: -142.198853, mean_eps: 0.100000\n",
      " 35016/50000: episode: 8544, duration: 0.014s, episode steps:   3, steps per second: 208, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 46.324308, mae: 582.144185, accuracy: 0.145833, mean_q: -140.121740, mean_eps: 0.100000\n",
      " 35019/50000: episode: 8545, duration: 0.014s, episode steps:   3, steps per second: 210, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 22.084116, mae: 591.124776, accuracy: 0.166667, mean_q: -142.625509, mean_eps: 0.100000\n",
      " 35022/50000: episode: 8546, duration: 0.014s, episode steps:   3, steps per second: 218, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 33.301667, mae: 593.887410, accuracy: 0.114583, mean_q: -137.854701, mean_eps: 0.100000\n",
      " 35026/50000: episode: 8547, duration: 0.017s, episode steps:   4, steps per second: 233, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 37.187259, mae: 605.089798, accuracy: 0.101562, mean_q: -138.797314, mean_eps: 0.100000\n",
      " 35029/50000: episode: 8548, duration: 0.014s, episode steps:   3, steps per second: 211, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 41.280975, mae: 592.938212, accuracy: 0.229167, mean_q: -141.109589, mean_eps: 0.100000\n",
      " 35032/50000: episode: 8549, duration: 0.015s, episode steps:   3, steps per second: 206, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 32.577419, mae: 566.510640, accuracy: 0.187500, mean_q: -140.301168, mean_eps: 0.100000\n",
      " 35035/50000: episode: 8550, duration: 0.015s, episode steps:   3, steps per second: 198, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 25.975953, mae: 599.243551, accuracy: 0.177083, mean_q: -138.701523, mean_eps: 0.100000\n",
      " 35039/50000: episode: 8551, duration: 0.018s, episode steps:   4, steps per second: 218, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 2.000 [0.000, 3.000],  loss: 27.788928, mae: 574.352158, accuracy: 0.148438, mean_q: -145.817410, mean_eps: 0.100000\n",
      " 35042/50000: episode: 8552, duration: 0.015s, episode steps:   3, steps per second: 201, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 24.633240, mae: 597.211324, accuracy: 0.187500, mean_q: -138.929194, mean_eps: 0.100000\n",
      " 35045/50000: episode: 8553, duration: 0.018s, episode steps:   3, steps per second: 169, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 48.327405, mae: 591.091492, accuracy: 0.135417, mean_q: -140.074493, mean_eps: 0.100000\n",
      " 35048/50000: episode: 8554, duration: 0.015s, episode steps:   3, steps per second: 195, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 55.164573, mae: 591.552226, accuracy: 0.125000, mean_q: -142.352061, mean_eps: 0.100000\n",
      " 35051/50000: episode: 8555, duration: 0.014s, episode steps:   3, steps per second: 215, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 28.668367, mae: 561.207865, accuracy: 0.135417, mean_q: -146.181366, mean_eps: 0.100000\n",
      " 35054/50000: episode: 8556, duration: 0.014s, episode steps:   3, steps per second: 222, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 33.031963, mae: 596.655436, accuracy: 0.197917, mean_q: -141.683945, mean_eps: 0.100000\n",
      " 35057/50000: episode: 8557, duration: 0.015s, episode steps:   3, steps per second: 198, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 35.159695, mae: 602.921916, accuracy: 0.156250, mean_q: -132.866872, mean_eps: 0.100000\n",
      " 35060/50000: episode: 8558, duration: 0.018s, episode steps:   3, steps per second: 163, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 31.409025, mae: 609.934774, accuracy: 0.156250, mean_q: -138.075241, mean_eps: 0.100000\n",
      " 35063/50000: episode: 8559, duration: 0.018s, episode steps:   3, steps per second: 170, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 33.042943, mae: 599.072795, accuracy: 0.156250, mean_q: -137.623077, mean_eps: 0.100000\n",
      " 35066/50000: episode: 8560, duration: 0.020s, episode steps:   3, steps per second: 150, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 40.478226, mae: 566.905924, accuracy: 0.187500, mean_q: -140.346680, mean_eps: 0.100000\n",
      " 35069/50000: episode: 8561, duration: 0.019s, episode steps:   3, steps per second: 158, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 20.166827, mae: 585.368591, accuracy: 0.114583, mean_q: -144.910416, mean_eps: 0.100000\n",
      " 35073/50000: episode: 8562, duration: 0.020s, episode steps:   4, steps per second: 196, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.250 [0.000, 2.000],  loss: 32.024076, mae: 582.276962, accuracy: 0.164062, mean_q: -138.142513, mean_eps: 0.100000\n",
      " 35076/50000: episode: 8563, duration: 0.015s, episode steps:   3, steps per second: 194, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 79.025546, mae: 572.894145, accuracy: 0.187500, mean_q: -137.777954, mean_eps: 0.100000\n",
      " 35079/50000: episode: 8564, duration: 0.021s, episode steps:   3, steps per second: 144, episode reward: -224.000, mean reward: -74.667 [-108.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 24.892198, mae: 574.834005, accuracy: 0.187500, mean_q: -148.188207, mean_eps: 0.100000\n",
      " 35083/50000: episode: 8565, duration: 0.024s, episode steps:   4, steps per second: 170, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 0.750 [0.000, 2.000],  loss: 32.964799, mae: 565.854507, accuracy: 0.164062, mean_q: -143.676121, mean_eps: 0.100000\n",
      " 35086/50000: episode: 8566, duration: 0.018s, episode steps:   3, steps per second: 164, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 51.170431, mae: 610.522644, accuracy: 0.083333, mean_q: -129.244929, mean_eps: 0.100000\n",
      " 35089/50000: episode: 8567, duration: 0.021s, episode steps:   3, steps per second: 141, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 31.129630, mae: 579.059163, accuracy: 0.156250, mean_q: -145.160497, mean_eps: 0.100000\n",
      " 35092/50000: episode: 8568, duration: 0.019s, episode steps:   3, steps per second: 158, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 51.079148, mae: 558.824788, accuracy: 0.145833, mean_q: -146.769424, mean_eps: 0.100000\n",
      " 35095/50000: episode: 8569, duration: 0.015s, episode steps:   3, steps per second: 203, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 53.500731, mae: 580.371460, accuracy: 0.156250, mean_q: -140.705083, mean_eps: 0.100000\n",
      " 35098/50000: episode: 8570, duration: 0.015s, episode steps:   3, steps per second: 206, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 32.252406, mae: 566.741486, accuracy: 0.208333, mean_q: -143.080653, mean_eps: 0.100000\n",
      " 35101/50000: episode: 8571, duration: 0.016s, episode steps:   3, steps per second: 193, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 60.236811, mae: 594.525228, accuracy: 0.177083, mean_q: -142.083257, mean_eps: 0.100000\n",
      " 35104/50000: episode: 8572, duration: 0.014s, episode steps:   3, steps per second: 214, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 51.065567, mae: 590.525228, accuracy: 0.166667, mean_q: -140.054286, mean_eps: 0.100000\n",
      " 35107/50000: episode: 8573, duration: 0.015s, episode steps:   3, steps per second: 203, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 34.748707, mae: 591.852132, accuracy: 0.177083, mean_q: -140.416667, mean_eps: 0.100000\n",
      " 35111/50000: episode: 8574, duration: 0.019s, episode steps:   4, steps per second: 209, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 47.049512, mae: 612.831573, accuracy: 0.171875, mean_q: -136.523155, mean_eps: 0.100000\n",
      " 35114/50000: episode: 8575, duration: 0.015s, episode steps:   3, steps per second: 197, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 34.295958, mae: 557.428914, accuracy: 0.104167, mean_q: -152.194158, mean_eps: 0.100000\n",
      " 35117/50000: episode: 8576, duration: 0.016s, episode steps:   3, steps per second: 185, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 22.347291, mae: 562.380636, accuracy: 0.208333, mean_q: -147.374542, mean_eps: 0.100000\n",
      " 35120/50000: episode: 8577, duration: 0.020s, episode steps:   3, steps per second: 147, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 30.897847, mae: 573.302327, accuracy: 0.125000, mean_q: -143.761775, mean_eps: 0.100000\n",
      " 35123/50000: episode: 8578, duration: 0.014s, episode steps:   3, steps per second: 215, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 37.728276, mae: 593.029399, accuracy: 0.145833, mean_q: -142.482814, mean_eps: 0.100000\n",
      " 35126/50000: episode: 8579, duration: 0.013s, episode steps:   3, steps per second: 223, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 49.198521, mae: 618.420369, accuracy: 0.187500, mean_q: -133.484014, mean_eps: 0.100000\n",
      " 35129/50000: episode: 8580, duration: 0.014s, episode steps:   3, steps per second: 208, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 25.935211, mae: 580.243530, accuracy: 0.250000, mean_q: -136.579763, mean_eps: 0.100000\n",
      " 35132/50000: episode: 8581, duration: 0.015s, episode steps:   3, steps per second: 198, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 68.236181, mae: 601.051697, accuracy: 0.145833, mean_q: -140.919281, mean_eps: 0.100000\n",
      " 35135/50000: episode: 8582, duration: 0.015s, episode steps:   3, steps per second: 199, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 33.586886, mae: 600.553223, accuracy: 0.114583, mean_q: -137.648565, mean_eps: 0.100000\n",
      " 35138/50000: episode: 8583, duration: 0.016s, episode steps:   3, steps per second: 193, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 46.176597, mae: 607.718709, accuracy: 0.145833, mean_q: -131.599045, mean_eps: 0.100000\n",
      " 35141/50000: episode: 8584, duration: 0.015s, episode steps:   3, steps per second: 201, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 30.913715, mae: 638.352824, accuracy: 0.114583, mean_q: -133.485565, mean_eps: 0.100000\n",
      " 35144/50000: episode: 8585, duration: 0.017s, episode steps:   3, steps per second: 178, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 39.896615, mae: 611.517802, accuracy: 0.104167, mean_q: -139.970225, mean_eps: 0.100000\n",
      " 35147/50000: episode: 8586, duration: 0.017s, episode steps:   3, steps per second: 178, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 45.208750, mae: 604.176351, accuracy: 0.166667, mean_q: -130.944702, mean_eps: 0.100000\n",
      " 35150/50000: episode: 8587, duration: 0.017s, episode steps:   3, steps per second: 176, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 39.416314, mae: 609.847432, accuracy: 0.062500, mean_q: -141.375249, mean_eps: 0.100000\n",
      " 35153/50000: episode: 8588, duration: 0.017s, episode steps:   3, steps per second: 180, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 39.110802, mae: 583.741109, accuracy: 0.156250, mean_q: -143.943420, mean_eps: 0.100000\n",
      " 35156/50000: episode: 8589, duration: 0.015s, episode steps:   3, steps per second: 195, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 51.970512, mae: 580.048991, accuracy: 0.208333, mean_q: -135.453044, mean_eps: 0.100000\n",
      " 35159/50000: episode: 8590, duration: 0.019s, episode steps:   3, steps per second: 155, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 26.335636, mae: 566.683228, accuracy: 0.093750, mean_q: -145.891978, mean_eps: 0.100000\n",
      " 35163/50000: episode: 8591, duration: 0.022s, episode steps:   4, steps per second: 185, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 50.895439, mae: 573.228149, accuracy: 0.257812, mean_q: -139.708637, mean_eps: 0.100000\n",
      " 35166/50000: episode: 8592, duration: 0.018s, episode steps:   3, steps per second: 171, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 84.475904, mae: 585.377401, accuracy: 0.250000, mean_q: -141.084752, mean_eps: 0.100000\n",
      " 35169/50000: episode: 8593, duration: 0.029s, episode steps:   3, steps per second: 105, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 19.590570, mae: 587.773234, accuracy: 0.083333, mean_q: -142.961110, mean_eps: 0.100000\n",
      " 35172/50000: episode: 8594, duration: 0.020s, episode steps:   3, steps per second: 147, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 48.587538, mae: 594.902344, accuracy: 0.135417, mean_q: -135.113047, mean_eps: 0.100000\n",
      " 35175/50000: episode: 8595, duration: 0.022s, episode steps:   3, steps per second: 137, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 37.182807, mae: 607.044800, accuracy: 0.177083, mean_q: -139.771708, mean_eps: 0.100000\n",
      " 35178/50000: episode: 8596, duration: 0.016s, episode steps:   3, steps per second: 190, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 46.321915, mae: 605.049113, accuracy: 0.104167, mean_q: -141.378367, mean_eps: 0.100000\n",
      " 35181/50000: episode: 8597, duration: 0.013s, episode steps:   3, steps per second: 226, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 34.498886, mae: 566.008728, accuracy: 0.208333, mean_q: -138.286825, mean_eps: 0.100000\n",
      " 35184/50000: episode: 8598, duration: 0.014s, episode steps:   3, steps per second: 217, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 22.452548, mae: 586.322286, accuracy: 0.135417, mean_q: -142.980825, mean_eps: 0.100000\n",
      " 35187/50000: episode: 8599, duration: 0.014s, episode steps:   3, steps per second: 216, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 36.359340, mae: 596.942403, accuracy: 0.135417, mean_q: -136.272995, mean_eps: 0.100000\n",
      " 35190/50000: episode: 8600, duration: 0.014s, episode steps:   3, steps per second: 211, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 25.952571, mae: 591.514262, accuracy: 0.197917, mean_q: -135.867315, mean_eps: 0.100000\n",
      " 35193/50000: episode: 8601, duration: 0.017s, episode steps:   3, steps per second: 179, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 47.206627, mae: 591.013997, accuracy: 0.166667, mean_q: -144.785370, mean_eps: 0.100000\n",
      " 35196/50000: episode: 8602, duration: 0.017s, episode steps:   3, steps per second: 178, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 37.469495, mae: 564.773234, accuracy: 0.250000, mean_q: -140.187291, mean_eps: 0.100000\n",
      " 35199/50000: episode: 8603, duration: 0.014s, episode steps:   3, steps per second: 220, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 27.408564, mae: 546.637573, accuracy: 0.166667, mean_q: -143.794357, mean_eps: 0.100000\n",
      " 35202/50000: episode: 8604, duration: 0.014s, episode steps:   3, steps per second: 220, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 33.054995, mae: 590.717367, accuracy: 0.177083, mean_q: -139.958333, mean_eps: 0.100000\n",
      " 35206/50000: episode: 8605, duration: 0.017s, episode steps:   4, steps per second: 234, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 31.029137, mae: 568.964600, accuracy: 0.187500, mean_q: -141.513103, mean_eps: 0.100000\n",
      " 35209/50000: episode: 8606, duration: 0.014s, episode steps:   3, steps per second: 215, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 44.416065, mae: 576.485901, accuracy: 0.145833, mean_q: -146.582072, mean_eps: 0.100000\n",
      " 35212/50000: episode: 8607, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 32.139787, mae: 606.639343, accuracy: 0.104167, mean_q: -136.774633, mean_eps: 0.100000\n",
      " 35215/50000: episode: 8608, duration: 0.013s, episode steps:   3, steps per second: 225, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 31.422576, mae: 592.510213, accuracy: 0.177083, mean_q: -142.076584, mean_eps: 0.100000\n",
      " 35218/50000: episode: 8609, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 58.341942, mae: 589.277730, accuracy: 0.218750, mean_q: -139.570592, mean_eps: 0.100000\n",
      " 35222/50000: episode: 8610, duration: 0.016s, episode steps:   4, steps per second: 244, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 63.161940, mae: 577.828796, accuracy: 0.125000, mean_q: -140.687969, mean_eps: 0.100000\n",
      " 35225/50000: episode: 8611, duration: 0.013s, episode steps:   3, steps per second: 222, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 27.771063, mae: 562.184835, accuracy: 0.166667, mean_q: -147.361572, mean_eps: 0.100000\n",
      " 35228/50000: episode: 8612, duration: 0.013s, episode steps:   3, steps per second: 232, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 51.379911, mae: 590.974141, accuracy: 0.177083, mean_q: -140.471786, mean_eps: 0.100000\n",
      " 35231/50000: episode: 8613, duration: 0.013s, episode steps:   3, steps per second: 228, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 38.313641, mae: 578.261292, accuracy: 0.104167, mean_q: -138.709188, mean_eps: 0.100000\n",
      " 35234/50000: episode: 8614, duration: 0.014s, episode steps:   3, steps per second: 214, episode reward: -224.000, mean reward: -74.667 [-116.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 21.404754, mae: 574.324259, accuracy: 0.145833, mean_q: -149.985621, mean_eps: 0.100000\n",
      " 35238/50000: episode: 8615, duration: 0.018s, episode steps:   4, steps per second: 218, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 0.750 [0.000, 2.000],  loss: 72.735761, mae: 586.802933, accuracy: 0.148438, mean_q: -139.243826, mean_eps: 0.100000\n",
      " 35241/50000: episode: 8616, duration: 0.015s, episode steps:   3, steps per second: 198, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 21.253781, mae: 602.882914, accuracy: 0.156250, mean_q: -139.492126, mean_eps: 0.100000\n",
      " 35244/50000: episode: 8617, duration: 0.016s, episode steps:   3, steps per second: 193, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 27.776558, mae: 576.449666, accuracy: 0.177083, mean_q: -143.308548, mean_eps: 0.100000\n",
      " 35247/50000: episode: 8618, duration: 0.014s, episode steps:   3, steps per second: 210, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 19.199604, mae: 591.513529, accuracy: 0.145833, mean_q: -140.076691, mean_eps: 0.100000\n",
      " 35250/50000: episode: 8619, duration: 0.014s, episode steps:   3, steps per second: 217, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 31.124070, mae: 591.515747, accuracy: 0.187500, mean_q: -138.380890, mean_eps: 0.100000\n",
      " 35254/50000: episode: 8620, duration: 0.018s, episode steps:   4, steps per second: 227, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 29.083058, mae: 607.233887, accuracy: 0.179688, mean_q: -137.750219, mean_eps: 0.100000\n",
      " 35257/50000: episode: 8621, duration: 0.015s, episode steps:   3, steps per second: 204, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 35.325392, mae: 589.843363, accuracy: 0.135417, mean_q: -144.008677, mean_eps: 0.100000\n",
      " 35260/50000: episode: 8622, duration: 0.014s, episode steps:   3, steps per second: 209, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 41.374786, mae: 594.312337, accuracy: 0.166667, mean_q: -137.710576, mean_eps: 0.100000\n",
      " 35263/50000: episode: 8623, duration: 0.015s, episode steps:   3, steps per second: 196, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 30.902269, mae: 582.209086, accuracy: 0.187500, mean_q: -137.366714, mean_eps: 0.100000\n",
      " 35266/50000: episode: 8624, duration: 0.016s, episode steps:   3, steps per second: 193, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 27.530746, mae: 581.189331, accuracy: 0.135417, mean_q: -138.220856, mean_eps: 0.100000\n",
      " 35269/50000: episode: 8625, duration: 0.015s, episode steps:   3, steps per second: 205, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 28.495548, mae: 598.522034, accuracy: 0.156250, mean_q: -143.818141, mean_eps: 0.100000\n",
      " 35272/50000: episode: 8626, duration: 0.016s, episode steps:   3, steps per second: 191, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 17.492160, mae: 607.346313, accuracy: 0.093750, mean_q: -138.036972, mean_eps: 0.100000\n",
      " 35275/50000: episode: 8627, duration: 0.014s, episode steps:   3, steps per second: 213, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 18.413590, mae: 565.369934, accuracy: 0.114583, mean_q: -143.361501, mean_eps: 0.100000\n",
      " 35278/50000: episode: 8628, duration: 0.015s, episode steps:   3, steps per second: 197, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 40.901805, mae: 587.219238, accuracy: 0.229167, mean_q: -139.660548, mean_eps: 0.100000\n",
      " 35281/50000: episode: 8629, duration: 0.014s, episode steps:   3, steps per second: 216, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 32.769607, mae: 581.942342, accuracy: 0.093750, mean_q: -144.289388, mean_eps: 0.100000\n",
      " 35284/50000: episode: 8630, duration: 0.015s, episode steps:   3, steps per second: 196, episode reward: -239.000, mean reward: -79.667 [-121.000, -58.000], mean action: 1.000 [0.000, 2.000],  loss: 36.528353, mae: 583.539205, accuracy: 0.208333, mean_q: -137.327911, mean_eps: 0.100000\n",
      " 35288/50000: episode: 8631, duration: 0.031s, episode steps:   4, steps per second: 129, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 1.500 [0.000, 3.000],  loss: 23.790044, mae: 560.314499, accuracy: 0.210938, mean_q: -145.785610, mean_eps: 0.100000\n",
      " 35291/50000: episode: 8632, duration: 0.016s, episode steps:   3, steps per second: 188, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 40.486172, mae: 608.856038, accuracy: 0.156250, mean_q: -132.774310, mean_eps: 0.100000\n",
      " 35294/50000: episode: 8633, duration: 0.016s, episode steps:   3, steps per second: 184, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 39.379778, mae: 593.270935, accuracy: 0.229167, mean_q: -139.823441, mean_eps: 0.100000\n",
      " 35297/50000: episode: 8634, duration: 0.016s, episode steps:   3, steps per second: 190, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 20.941659, mae: 619.200399, accuracy: 0.093750, mean_q: -134.815964, mean_eps: 0.100000\n",
      " 35300/50000: episode: 8635, duration: 0.016s, episode steps:   3, steps per second: 185, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 28.742427, mae: 596.529419, accuracy: 0.156250, mean_q: -139.734553, mean_eps: 0.100000\n",
      " 35304/50000: episode: 8636, duration: 0.020s, episode steps:   4, steps per second: 201, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 32.855704, mae: 581.926117, accuracy: 0.093750, mean_q: -144.643635, mean_eps: 0.100000\n",
      " 35307/50000: episode: 8637, duration: 0.017s, episode steps:   3, steps per second: 175, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 34.616113, mae: 552.509176, accuracy: 0.072917, mean_q: -144.048411, mean_eps: 0.100000\n",
      " 35310/50000: episode: 8638, duration: 0.018s, episode steps:   3, steps per second: 168, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 20.657433, mae: 585.862793, accuracy: 0.072917, mean_q: -141.217509, mean_eps: 0.100000\n",
      " 35313/50000: episode: 8639, duration: 0.017s, episode steps:   3, steps per second: 178, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 35.101168, mae: 575.674866, accuracy: 0.166667, mean_q: -145.141602, mean_eps: 0.100000\n",
      " 35316/50000: episode: 8640, duration: 0.020s, episode steps:   3, steps per second: 150, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 29.693095, mae: 597.203756, accuracy: 0.187500, mean_q: -146.208140, mean_eps: 0.100000\n",
      " 35319/50000: episode: 8641, duration: 0.026s, episode steps:   3, steps per second: 115, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 23.967214, mae: 568.808146, accuracy: 0.104167, mean_q: -148.934402, mean_eps: 0.100000\n",
      " 35322/50000: episode: 8642, duration: 0.031s, episode steps:   3, steps per second:  98, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 63.635127, mae: 589.568115, accuracy: 0.093750, mean_q: -136.842896, mean_eps: 0.100000\n",
      " 35325/50000: episode: 8643, duration: 0.019s, episode steps:   3, steps per second: 157, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 26.026155, mae: 558.812012, accuracy: 0.093750, mean_q: -148.787755, mean_eps: 0.100000\n",
      " 35328/50000: episode: 8644, duration: 0.018s, episode steps:   3, steps per second: 171, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 39.676263, mae: 567.852437, accuracy: 0.218750, mean_q: -140.654383, mean_eps: 0.100000\n",
      " 35332/50000: episode: 8645, duration: 0.021s, episode steps:   4, steps per second: 187, episode reward: -1194.000, mean reward: -298.500 [-999.000, -58.000], mean action: 1.500 [0.000, 3.000],  loss: 41.093750, mae: 553.981003, accuracy: 0.179688, mean_q: -148.234673, mean_eps: 0.100000\n",
      " 35335/50000: episode: 8646, duration: 0.016s, episode steps:   3, steps per second: 183, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 50.975545, mae: 624.305379, accuracy: 0.177083, mean_q: -132.863764, mean_eps: 0.100000\n",
      " 35338/50000: episode: 8647, duration: 0.017s, episode steps:   3, steps per second: 176, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 32.993304, mae: 611.993001, accuracy: 0.083333, mean_q: -135.602885, mean_eps: 0.100000\n",
      " 35341/50000: episode: 8648, duration: 0.017s, episode steps:   3, steps per second: 175, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 31.064386, mae: 574.205241, accuracy: 0.093750, mean_q: -146.318649, mean_eps: 0.100000\n",
      " 35344/50000: episode: 8649, duration: 0.017s, episode steps:   3, steps per second: 178, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 31.814115, mae: 578.656250, accuracy: 0.114583, mean_q: -144.239858, mean_eps: 0.100000\n",
      " 35347/50000: episode: 8650, duration: 0.017s, episode steps:   3, steps per second: 178, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 48.751477, mae: 564.607727, accuracy: 0.208333, mean_q: -144.842214, mean_eps: 0.100000\n",
      " 35350/50000: episode: 8651, duration: 0.016s, episode steps:   3, steps per second: 186, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 76.889220, mae: 561.860433, accuracy: 0.166667, mean_q: -143.944214, mean_eps: 0.100000\n",
      " 35353/50000: episode: 8652, duration: 0.016s, episode steps:   3, steps per second: 189, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 52.886486, mae: 613.057719, accuracy: 0.145833, mean_q: -132.697962, mean_eps: 0.100000\n",
      " 35356/50000: episode: 8653, duration: 0.017s, episode steps:   3, steps per second: 175, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 38.612312, mae: 600.762410, accuracy: 0.072917, mean_q: -140.350149, mean_eps: 0.100000\n",
      " 35359/50000: episode: 8654, duration: 0.020s, episode steps:   3, steps per second: 147, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 27.375741, mae: 584.971293, accuracy: 0.177083, mean_q: -141.967265, mean_eps: 0.100000\n",
      " 35362/50000: episode: 8655, duration: 0.017s, episode steps:   3, steps per second: 177, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 46.731829, mae: 592.613790, accuracy: 0.093750, mean_q: -140.168650, mean_eps: 0.100000\n",
      " 35366/50000: episode: 8656, duration: 0.019s, episode steps:   4, steps per second: 205, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 2.000 [0.000, 3.000],  loss: 35.810423, mae: 589.029587, accuracy: 0.117188, mean_q: -144.525032, mean_eps: 0.100000\n",
      " 35369/50000: episode: 8657, duration: 0.016s, episode steps:   3, steps per second: 190, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 48.501952, mae: 564.622498, accuracy: 0.166667, mean_q: -143.341853, mean_eps: 0.100000\n",
      " 35372/50000: episode: 8658, duration: 0.016s, episode steps:   3, steps per second: 193, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 27.946301, mae: 574.606649, accuracy: 0.208333, mean_q: -139.495051, mean_eps: 0.100000\n",
      " 35375/50000: episode: 8659, duration: 0.016s, episode steps:   3, steps per second: 183, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.000 [0.000, 2.000],  loss: 25.403730, mae: 553.129557, accuracy: 0.166667, mean_q: -145.656062, mean_eps: 0.100000\n",
      " 35378/50000: episode: 8660, duration: 0.016s, episode steps:   3, steps per second: 191, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 1.333 [0.000, 3.000],  loss: 23.711685, mae: 583.521139, accuracy: 0.166667, mean_q: -145.359278, mean_eps: 0.100000\n",
      " 35382/50000: episode: 8661, duration: 0.020s, episode steps:   4, steps per second: 205, episode reward: -1194.000, mean reward: -298.500 [-999.000, -32.000], mean action: 0.750 [0.000, 2.000],  loss: 25.071334, mae: 590.834839, accuracy: 0.187500, mean_q: -142.071911, mean_eps: 0.100000\n",
      " 35385/50000: episode: 8662, duration: 0.016s, episode steps:   3, steps per second: 189, episode reward: -195.000, mean reward: -65.000 [-103.000, -32.000], mean action: 1.667 [0.000, 3.000],  loss: 49.436324, mae: 561.864095, accuracy: 0.114583, mean_q: -146.131760, mean_eps: 0.100000\n",
      " 35388/50000: episode: 8663, duration: 0.016s, episode steps:   3, steps per second: 193, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 1.333 [0.000, 3.000],  loss: 34.288047, mae: 567.838420, accuracy: 0.177083, mean_q: -145.749252, mean_eps: 0.100000\n",
      " 35391/50000: episode: 8664, duration: 0.018s, episode steps:   3, steps per second: 171, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 29.169952, mae: 550.614665, accuracy: 0.166667, mean_q: -149.155045, mean_eps: 0.100000\n",
      " 35394/50000: episode: 8665, duration: 0.015s, episode steps:   3, steps per second: 198, episode reward: -195.000, mean reward: -65.000 [-92.000, -45.000], mean action: 1.000 [0.000, 2.000],  loss: 27.687004, mae: 581.213765, accuracy: 0.177083, mean_q: -134.949554, mean_eps: 0.100000\n",
      " 35397/50000: episode: 8666, duration: 0.014s, episode steps:   3, steps per second: 218, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 34.401042, mae: 557.116577, accuracy: 0.208333, mean_q: -151.382365, mean_eps: 0.100000\n",
      " 35400/50000: episode: 8667, duration: 0.018s, episode steps:   3, steps per second: 163, episode reward: -195.000, mean reward: -65.000 [-77.000, -58.000], mean action: 2.000 [1.000, 3.000],  loss: 54.592951, mae: 623.506449, accuracy: 0.135417, mean_q: -129.419284, mean_eps: 0.100000\n",
      " 35403/50000: episode: 8668, duration: 0.014s, episode steps:   3, steps per second: 212, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 26.779284, mae: 549.738342, accuracy: 0.177083, mean_q: -148.766734, mean_eps: 0.100000\n",
      " 35406/50000: episode: 8669, duration: 0.014s, episode steps:   3, steps per second: 221, episode reward: -195.000, mean reward: -65.000 [-118.000, -32.000], mean action: 2.000 [1.000, 3.000],  loss: 40.486390, mae: 548.075460, accuracy: 0.208333, mean_q: -151.040980, mean_eps: 0.100000\n",
      " 35410/50000: episode: 8670, duration: 0.016s, episode steps:   4, steps per second: 249, episode reward: -1194.000, mean reward: -298.500 [-999.000, -45.000], mean action: 1.500 [0.000, 3.000],  loss: 43.305323, mae: 587.834778, accuracy: 0.093750, mean_q: -141.958359, mean_eps: 0.100000\n",
      "done, took 150.045 seconds\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import Sequential\n",
    "from keras.layers import Input, Flatten, Dense\n",
    "\n",
    "import rl\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
    "\n",
    "# setup experience replay buffer\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "\n",
    "# setup the Linear annealed policy with the EpsGreedyQPolicy as the inner policy\n",
    "policy =  LinearAnnealedPolicy(inner_policy=  EpsGreedyQPolicy(),   # policy used to select actions\n",
    "                               attr='eps',                          # attribute in the inner policy to vary             \n",
    "                               value_max=1.0,                       # maximum value of attribute that is varying\n",
    "                               value_min=0.1,                       # minimum value of attribute that is varying\n",
    "                               value_test=0.05,                     # test if the value selected is < 0.05\n",
    "                               nb_steps=10000)                      # the number of steps between value_max and value_min\n",
    "\n",
    "#Feed-Forward Neural Network Model for Deep Q Learning (DQN)\n",
    "model = Sequential()\n",
    "#Input is 1 observation vector, and the number of observations in that vector \n",
    "model.add(Input(shape=(1,8)))  \n",
    "model.add(Flatten())\n",
    "#Hidden layers with 24 nodes each\n",
    "model.add(Dense(24, activation='relu'))\n",
    "model.add(Dense(24, activation='relu'))\n",
    "#Output is the number of actions in the action space\n",
    "model.add(Dense(env.action_space.n, activation='linear')) \n",
    "\n",
    "\n",
    "#Feed-Forward Neural Network Architecture Summary\n",
    "print(model.summary())\n",
    "\n",
    "#Defining DQN Agent for DQN Model\n",
    "dqn = DQNAgent(model=model,                     # Q-Network model\n",
    "               nb_actions=env.action_space.n,   # number of actions\n",
    "               memory=memory,                   # experience replay memory\n",
    "               nb_steps_warmup=25,              # how many steps are waited before starting experience replay\n",
    "               target_model_update=1e-2,        # how often the target network is updated\n",
    "               policy=policy)                   # the action selection policy\n",
    "\n",
    "# Finally, we configure and compile our agent. \n",
    "#We can use built-in tensorflow.keras Adam optimizer and evaluation metrics            \n",
    "from tensorflow.keras.optimizers import Adam\n",
    "dqn.compile(Adam(learning_rate=1e-3), metrics=['mae','accuracy'])\n",
    "\n",
    "#Finally fit and train the agent\n",
    "history = dqn.fit(env, nb_steps=50000, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import rl\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Input\n",
    "from keras.optimizers import Adam\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95    # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Input(shape=(1,self.state_size)))  \n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.choice(self.action_size)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        if(len(self.memory) < 100):\n",
    "            return\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = reward + self.gamma * np.amax(self.model.predict(next_state)[0])\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "# create the environment\n",
    "env = TSPEnv(4)\n",
    "\n",
    "# create the agent with the number of actions in the environment\n",
    "state_size = env.observation_space.shape[0]\n",
    "print(state_size)\n",
    "action_size = env.action_space.n\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "# training loop\n",
    "EPISODES = 1000\n",
    "for episode in range(EPISODES):\n",
    "    state = env.reset()\n",
    "    \n",
    "    state = np.ravel(state)\n",
    "    print(state)\n",
    "    for time in range(500):\n",
    "        # choose an action\n",
    "        action = agent.act(state)\n",
    "\n",
    "        # take the action in the environment\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        # remember the experience and update the agent\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        agent.replay(32)\n",
    "        \n",
    "        # update the current state\n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            print(\"episode: {}/{}, score: {}, e: {:.2}\"\n",
    "                  .format(episode, EPISODES, time, agent.epsilon))\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "env = TSPEnv(4,False)\n",
    "\n",
    "\n",
    "print(env.observation_space.shape[0])\n",
    "agent = DQNAgent(8, 4)\n",
    "samples = []\n",
    "state = env.reset()\n",
    "agent._build_model()\n",
    "\n",
    "\n",
    "\n",
    "for i in range(0,env.n_cities):\n",
    "    action = agent.act(state)\n",
    " \n",
    "\n",
    "# create the agent with 4 actions (corresponding to each city)\n",
    "\n",
    "\n",
    "# choose an action\n",
    "\n",
    "\n",
    "# receive a reward and new state\n",
    "reward = 10\n",
    "visited_places = [True, True, False, False]\n",
    "distance_to_next_city = [5, 15, 25, 35]\n",
    "next_state = np.concatenate((visited_places, distance_to_next_city))\n",
    "done = False\n",
    "\n",
    "# remember the experience and update the agent\n",
    "agent.remember(state, action, reward, next_state, done)\n",
    "#agent.replay(batch_size=32)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tensorflow DQNAgent\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 32)\n",
    "        self.fc2 = nn.Linear(32,action_size)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.model = DQN(state_size, action_size)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "    def get_action(self, state):\n",
    "        state = torch.tensor(state, dtype=torch.float32)\n",
    "        q_values = self.model(state)\n",
    "        action = q_values.argmax().item()\n",
    "        return action\n",
    "    \n",
    "    def update(self, stateDistance, action, next_state, reward, done):\n",
    "        stateDistance = torch.tensor(stateDistance, dtype=torch.float32)\n",
    "        stateVisited = torch.tensor(stateVisited, dtype=torch.int32)\n",
    "        next_state = torch.tensor(next_state, dtype=torch.float32)\n",
    "        reward = torch.tensor(reward, dtype=torch.float32)\n",
    "        done = torch.tensor(done, dtype=torch.float32)\n",
    "        \n",
    "        q_values = self.model(state)\n",
    "        next_q_values = self.model(next_state)\n",
    "        target = q_values.clone()\n",
    "        print(f'Target: {target}')\n",
    "        print(f'Action: {action}')\n",
    "        target[action] = reward + (1 - done) * next_q_values.max()\n",
    "        loss = self.loss_fn(q_values, target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "\n",
    "# state[distance / visited] = got from step\n",
    "# \n",
    "# reward = got from step\n",
    "\n",
    "\n",
    "\n",
    "observation_space = np.array([[1,0,0,0], [0,32,58,60]])\n",
    "state_size = observation_space.shape[1]\n",
    "print(state_size)\n",
    "action_size = 2\n",
    "\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "\n",
    "distance = np.array([[0,32,58,60], [1,0,0,0]])\n",
    "distanceTensor = torch.from_numpy(distance)\n",
    "\n",
    "print(distanceTensor)\n",
    "print(distanceTensor[0])\n",
    "print(distanceTensor[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "num_episodes = 2000\n",
    "\n",
    "state = env.reset() # get the initial state of the environment\n",
    "print(f'State in Training: {state}')\n",
    "# action = agent.get_action(state) # use the agent to select an action\n",
    "action = 0\n",
    "print(f'Action in Training: {action}')\n",
    "next_state, reward, done, info = env.step(action) # take the action in the environment\n",
    "\n",
    "# state = env.reset()\n",
    "# # agent.get_action(0)\n",
    "# # state = env.reset()\n",
    "# agent.get_action(1)\n",
    "# state = env.reset()\n",
    "# agent.get_action(2)\n",
    "\n",
    "\n",
    "# agent.update(state, action, next_state, reward, done) # update the Q-values\n",
    "# state = next_state # set the current state to the next state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "import torch\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the DQN model\n",
    "class DQN(nn.Module):\n",
    "  def __init__(self,observation, action, network_arch):\n",
    "    super().__init__()\n",
    "    self.num_states = network_arch['num_states']\n",
    "    self.hidden_units = network_arch['num_hidden_units']\n",
    "    self.num_actions = network_arch['num_actions']\n",
    "\n",
    "    self.fc1 = nn.Linear(in_features = self.num_states, out_features = self.hidden_units)\n",
    "    # self.relu1 = nn.ReLU()\n",
    "    # self.fc2 = nn.Linear(4, 4)\n",
    "    # self.relu2 = nn.ReLU()\n",
    "    self.fc3 = nn.Linear(in_features = self.hidden_units, out_features = self.num_actions)\n",
    "    self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "\n",
    "  def forward(self, state):\n",
    "    x = self.fc1(state)\n",
    "    # x = self.fc2(x)\n",
    "    action = self.fc3(x)\n",
    "    return action\n",
    "\n",
    "\n",
    "# Define the DQNAgent\n",
    "class DQNAgent:\n",
    "  def __init__(self, env, epsilon=0.20, epsilon_decay=0.995, epsilon_min=0.01, \n",
    "               alpha=1e-3, alpha_decay=0.01, gamma=0.99, memory_size=10000, \n",
    "               batch_size=64):\n",
    "\n",
    "    network_arch = {\n",
    "    'num_states' : 8,\n",
    "    'num_hidden_units': 20,\n",
    "    'num_actions':3\n",
    "    }\n",
    "\n",
    "    self.env = env\n",
    "    self.epsilon = epsilon\n",
    "    self.epsilon_decay = epsilon_decay\n",
    "    self.epsilon_min = epsilon_min\n",
    "    self.alpha = alpha\n",
    "    self.alpha_decay = alpha_decay\n",
    "    self.gamma = gamma\n",
    "    self.memory = deque(maxlen=memory_size)\n",
    "    self.memory_size = memory_size\n",
    "    self.batch_size = batch_size\n",
    "    self.mem_cntr = 0\n",
    "\n",
    "    # Define the model and the target model\n",
    "    self.model = DQN(env.observation_space.shape[0], env.action_space.n, network_arch).to(device)\n",
    "    self.target_model = DQN(env.observation_space.shape[0], env.action_space.n, network_arch).to(device)\n",
    "    self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    # Define the optimizer\n",
    "    self.optimizer = optim.Adam(self.model.parameters(), lr=alpha)\n",
    "\n",
    "    # Define the loss function\n",
    "    self.loss_fn = nn.MSELoss()\n",
    "\n",
    "\n",
    "  def remember(self, state, action, reward, next_state, done):\n",
    "    if(reward >= -3000):\n",
    "      self.memory.append((state, action, reward, next_state, done))\n",
    "    else:\n",
    "      return\n",
    "  \n",
    "  def step(self, state, action, reward, next_state, done):\n",
    "        # Save experience in replay memory\n",
    "        self.memory.append(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Learn every UPDATE_EVERY time steps.\n",
    "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
    "        if self.t_step == 0:\n",
    "            # If enough samples are available in memory, get random subset and learn\n",
    "            if len(self.memory) > BATCH_SIZE:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, GAMMA)\n",
    "\n",
    "  #Select action\n",
    "  def act(self, state, eps=0.):\n",
    "        \"\"\"Returns actions for given state as per current policy.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state (array_like): current state\n",
    "            eps (float): epsilon, for epsilon-greedy action selection\n",
    "        \"\"\"\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state)\n",
    "        self.qnetwork_local.train()\n",
    "\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "\n",
    "  def update(self):\n",
    "    # Don't update if there are not enough samples in the memory\n",
    "    if len(self.memory) < self.batch_size:\n",
    "      return\n",
    "\n",
    "    # Sample a batch from the memory\n",
    "    samples = random.sample(self.memory, self.batch_size)\n",
    "  \n",
    "\n",
    "    # Split the batch into separate variables\n",
    "    states, actions, rewards, next_states, dones = zip(*samples)\n",
    "\n",
    "    # Convert variables to tensors and move them to the device\n",
    "    states = torch.from_numpy(np.vstack(states)).float().to(device)\n",
    "    print(f'States shape: {states.shape}')\n",
    "    actions = torch.from_numpy(np.vstack(actions)).long().to(device)\n",
    "    print(f'Action Shapes: {actions.shape}')\n",
    "    rewards = torch.from_numpy(np.vstack(rewards)).float().to(device)\n",
    "    next_states = torch.from_numpy(np.vstack(next_states)).float().to(device)\n",
    "    dones = torch.from_numpy(np.vstack(dones)).float().to(device)\n",
    "\n",
    "    # Calculate the Q values for the current states\n",
    "    q_values = self.model(states)\n",
    "    # q_values = q_values.gather(1, actions)\n",
    "  \n",
    "\n",
    "    # Calculate the Q values for the next states\n",
    "    # next_q_values = self.target_model(next_states).max(1)[0].unsqueeze(1)\n",
    "    next_q_values = self.target_model(next_states).max(dim=1)[0].unsqueeze(1)\n",
    "\n",
    "    # Calculate the target Q values\n",
    "    target_q_values = rewards + (self.gamma * next_q_values * (1 - dones))\n",
    "\n",
    "    # Calculate the loss\n",
    "    loss = self.loss_fn(q_values, target_q_values)\n",
    "\n",
    "    # Perform backpropagation\n",
    "    self.optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    self.optimizer.step()\n",
    "\n",
    "    # Update the target model\n",
    "    for target_param, param in zip(self.target_model.parameters(), self.model.parameters()):\n",
    "      target_param.data.copy_(param.data * (1 - self.alpha_decay) + target_param.data * self.alpha_decay)\n",
    "\n",
    "    # Update the epsilon value\n",
    "    self.epsilon = max(self.epsilon * self.epsilon_decay, self.epsilon_min)\n",
    "  \n",
    "  def SaveAgent(self, name):\n",
    "    torch.save(self.model.state_dict(),'model episode'+name+'pt')\n",
    "    print('Model saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Instantiate the environment\n",
    "env = TSPEnv(4)\n",
    "\n",
    "# Instantiate the agent\n",
    "agent = DQNAgent(env)\n",
    "\n",
    "# Set the number of episodes to run\n",
    "n_episodes = 200\n",
    "\n",
    "#lists for learning evaluation\n",
    "scores = []\n",
    "\n",
    "\n",
    "# Run the episodes\n",
    "for episode in range(n_episodes):\n",
    "  # Reset the environment and get the initial state\n",
    "  state = env.reset()\n",
    "\n",
    "  # Set the initial reward to 0\n",
    "  total_reward = 0\n",
    "  total_steps = 0\n",
    "  Route = []\n",
    "\n",
    "  while True:\n",
    "    # Take an action\n",
    "    # print(state.shape)\n",
    "    # print(state)\n",
    "    action = agent.act(state)\n",
    "\n",
    "    # Step the environment\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "    # Remember the experience\n",
    "    agent.remember(state, action, reward, next_state, done)\n",
    "\n",
    "    # Update the state and the reward\n",
    "    state = next_state\n",
    "    total_reward += reward\n",
    "    total_steps += 1\n",
    "\n",
    "    # Update the agent\n",
    "    agent.update()\n",
    "\n",
    "    # If the episode is done, break the loop\n",
    "    if done:\n",
    "      break\n",
    "  \n",
    "  scores.append((episode, total_reward, total_steps,env.visited_cities, env._total_distance, env.xy))\n",
    "  #env.plotCities()\n",
    "  # Print the total reward for the episode\n",
    "  print(f\"Episode: {episode+1}, Reward: {total_reward}, Steps needed: {total_steps}, Total Distance: {env._total_distance}, Visited Cities: {env.visited_cities}\")\n",
    "\n",
    "  if episode > 1000:\n",
    "    agent.SaveAgent(str(episode))\n",
    "  if episode > 5000:\n",
    "    agent.SaveAgent(str(episode))\n",
    "  if episode > 10000:\n",
    "    agent.SaveAgent(str(episode))\n",
    "\n",
    "  # Open a file handle in write mode\n",
    "  with open('my_file.txt', 'w') as f:\n",
    "      # Write the list to the file, one item per line\n",
    "      f.writelines([f\"{item}\\n\" for item in scores])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5a312273ffa6240539e020f48a5de0b17350fba060a017bafd7c66c880b86767"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
