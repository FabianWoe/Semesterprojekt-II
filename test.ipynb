{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "\n",
    "# Define the environment\n",
    "class TSPEnv(gym.Env):\n",
    "  def __init__(self, n_cities=100, show_debug_data = False):\n",
    "    self.n_cities = n_cities\n",
    "    self.xy = (np.random.rand(self.n_cities,2)*100).round(2)\n",
    "    self.x=self.xy[:,0]\n",
    "    self.y=self.xy[:,1]\n",
    "    self.step_counter = 0\n",
    "    self.show_debug_data = show_debug_data\n",
    "\n",
    "    #print(f'genrated stops xy: {self.xy}')\n",
    "    self.distance_matrix = cdist(self.xy,self.xy,'euclidean').round(0)\n",
    "\n",
    "    # self.distance_matrix = np.array([[0, 2448, 1434, 1260, 2045],\n",
    "    #                   [2448, 0, 2546, 959, 2367],\n",
    "    #                   [1434, 2546, 0, 2408, 1745],\n",
    "    #                   [1260, 959, 2408, 0, 2295],\n",
    "    #                   [2045, 2367, 1745, 2295, 0]])\n",
    "    self.current_city = np.random.randint(n_cities)\n",
    "    self.visited_cities = [self.current_city]\n",
    "    self.remaining_cities = [i for i in range(n_cities) if i != self.current_city]\n",
    "    # Define the action space\n",
    "    self.action_space = gym.spaces.Discrete(n_cities)\n",
    "\n",
    "    # Define the observation space\n",
    "    self.observation_space = gym.spaces.Box(low=0, high=1, shape=(n_cities,), dtype=np.float32)\n",
    "\n",
    "    print(f'Start in Init: {self.current_city}')\n",
    "\n",
    "  def reset(self):\n",
    "    self.step_counter = 0\n",
    "    self.current_city = np.random.randint(self.n_cities)\n",
    "    self.visited_cities = [self.current_city]\n",
    "    self.remaining_cities = [i for i in range(self.n_cities) if i != self.current_city]\n",
    "    return self._get_observation()\n",
    "\n",
    "  def step(self, action):\n",
    "    self.step_counter += 1\n",
    "    if action < len(self.remaining_cities):\n",
    "      next_city = self.remaining_cities[action]\n",
    "      reward = -self.distance_matrix[self.current_city][next_city]\n",
    "      self.remaining_cities.remove(next_city)\n",
    "      self.visited_cities.append(next_city)\n",
    "      self.current_city = next_city\n",
    "      done = len(self.remaining_cities) == 0\n",
    "      \n",
    "      if(self.show_debug_data):\n",
    "        print(f'Action in step: {action}')\n",
    "        print(f'Reward in step: {reward}')\n",
    "        print(f'Current city in step: {self.current_city}')\n",
    "        print(f'Remaining city in step: {self.remaining_cities}')\n",
    "        print(f'Visited city in step: {self.visited_cities}')\n",
    "        print(f'Stepcounter in step: {self.step_counter}')\n",
    "      return self._get_observation(), reward, done, {}\n",
    "    else:\n",
    "      return self._get_observation(), 0, False, {}\n",
    "\n",
    "\n",
    "  def _get_observation(self):\n",
    "    observation = np.zeros(self.n_cities)\n",
    "    observation[self.current_city] = 1\n",
    "    return observation\n",
    "\n",
    "  \n",
    "  def _test_distance(self,CurrentCity, NextCity):\n",
    "    return -self.distance_matrix[CurrentCity][NextCity]\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TSPEnv()\n",
    "\n",
    "env._test_distance(1,3)\n",
    "\n",
    "env.step(0)\n",
    "\n",
    "env.step(1)\n",
    "env.step(2)\n",
    "env.step(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the DQN model\n",
    "class DQN(nn.Module):\n",
    "  def __init__(self, input_dim, output_dim):\n",
    "    super().__init__()\n",
    "    self.fc1 = nn.Linear(input_dim, 512)\n",
    "    self.relu1 = nn.ReLU()\n",
    "    self.fc2 = nn.Linear(512, 256)\n",
    "    self.relu2 = nn.ReLU()\n",
    "    self.fc3 = nn.Linear(256, output_dim)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.fc1(x)\n",
    "    x = self.relu1(x)\n",
    "    x = self.fc2(x)\n",
    "    x = self.relu2(x)\n",
    "    x = self.fc3(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "# Define the DQNAgent\n",
    "class DQNAgent:\n",
    "  def __init__(self, env, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01, \n",
    "               alpha=1e-3, alpha_decay=0.01, gamma=0.99, memory_size=10000, \n",
    "               batch_size=64):\n",
    "    self.env = env\n",
    "    self.epsilon = epsilon\n",
    "    self.epsilon_decay = epsilon_decay\n",
    "    self.epsilon_min = epsilon_min\n",
    "    self.alpha = alpha\n",
    "    self.alpha_decay = alpha_decay\n",
    "    self.gamma = gamma\n",
    "    self.memory = deque(maxlen=memory_size)\n",
    "    self.batch_size = batch_size\n",
    "\n",
    "    # Define the model and the target model\n",
    "    self.model = DQN(env.observation_space.shape[0], env.action_space.n).to(device)\n",
    "    self.target_model = DQN(env.observation_space.shape[0], env.action_space.n).to(device)\n",
    "    self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    # Define the optimizer\n",
    "    self.optimizer = optim.Adam(self.model.parameters(), lr=alpha)\n",
    "\n",
    "    # Define the loss function\n",
    "    self.loss_fn = nn.MSELoss()\n",
    "\n",
    "  def remember(self, state, action, reward, next_state, done):\n",
    "    self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "  def act(self, state):\n",
    "    if np.random.rand() <= self.epsilon:\n",
    "      return self.env.action_space.sample()\n",
    "    else:\n",
    "      state = torch.from_numpy(state).float().to(device)\n",
    "      q_values = self.model(state)\n",
    "      return q_values.argmax().item()\n",
    "\n",
    "  def update(self):\n",
    "    # Don't update if there are not enough samples in the memory\n",
    "    if len(self.memory) < self.batch_size:\n",
    "      return\n",
    "\n",
    "    # Sample a batch from the memory\n",
    "    samples = random.sample(self.memory, self.batch_size)\n",
    "\n",
    "    # Split the batch into separate variables\n",
    "    states, actions, rewards, next_states, dones = zip(*samples)\n",
    "\n",
    "    # Convert variables to tensors and move them to the device\n",
    "    states = torch.from_numpy(np.vstack(states)).float().to(device)\n",
    "    actions = torch.from_numpy(np.vstack(actions)).long().to(device)\n",
    "    rewards = torch.from_numpy(np.vstack(rewards)).float().to(device)\n",
    "    next_states = torch.from_numpy(np.vstack(next_states)).float().to(device)\n",
    "    dones = torch.from_numpy(np.vstack(dones)).float().to(device)\n",
    "\n",
    "    # Calculate the Q values for the current states\n",
    "    q_values = self.model(states)\n",
    "    q_values = q_values.gather(1, actions)\n",
    "\n",
    "    # Calculate the Q values for the next states\n",
    "    next_q_values = self.target_model(next_states).max(1)[0].unsqueeze(1)\n",
    "\n",
    "    # Calculate the target Q values\n",
    "    target_q_values = rewards + (self.gamma * next_q_values * (1 - dones))\n",
    "\n",
    "    # Calculate the loss\n",
    "    loss = self.loss_fn(q_values, target_q_values)\n",
    "\n",
    "    # Perform backpropagation\n",
    "    self.optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    self.optimizer.step()\n",
    "\n",
    "    # Update the target model\n",
    "    for target_param, param in zip(self.target_model.parameters(), self.model.parameters()):\n",
    "      target_param.data.copy_(param.data * (1 - self.alpha_decay) + target_param.data * self.alpha_decay)\n",
    "\n",
    "    # Update the epsilon value\n",
    "    self.epsilon = max(self.epsilon * self.epsilon_decay, self.epsilon_min)\n",
    "  \n",
    "  def SaveAgent(self):\n",
    "    torch.save(self.model.state_dict(),'model.pt')\n",
    "    print('Model saved')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start in Init: 49\n",
      "Episode: 1, Reward: -2364.0, Steps needed: 348\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Instantiate the environment\n",
    "env = TSPEnv(50)\n",
    "\n",
    "# Instantiate the agent\n",
    "agent = DQNAgent(env)\n",
    "\n",
    "# Set the number of episodes to run\n",
    "n_episodes = 1\n",
    "\n",
    "#lists for learning evaluation\n",
    "scores = []\n",
    "\n",
    "\n",
    "# Run the episodes\n",
    "for episode in range(n_episodes):\n",
    "  # Reset the environment and get the initial state\n",
    "  state = env.reset()\n",
    "\n",
    "  # Set the initial reward to 0\n",
    "  total_reward = 0\n",
    "  total_steps = 0\n",
    "\n",
    "  while True:\n",
    "    # Take an action\n",
    "    action = agent.act(state)\n",
    "\n",
    "    # Step the environment\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "    # Remember the experience\n",
    "    agent.remember(state, action, reward, next_state, done)\n",
    "\n",
    "    # Update the state and the reward\n",
    "    state = next_state\n",
    "    total_reward += reward\n",
    "    total_steps += 1\n",
    "\n",
    "    # Update the agent\n",
    "    agent.update()\n",
    "\n",
    "    # If the episode is done, break the loop\n",
    "    if done:\n",
    "      break\n",
    "  \n",
    "  scores.append((episode, total_reward, total_steps))\n",
    "  # Print the total reward for the episode\n",
    "  print(f\"Episode: {episode+1}, Reward: {total_reward}, Steps needed: {total_steps}\")\n",
    "\n",
    "agent.SaveAgent()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Episode', 0, -2369.0, 185), ('Episode', 1, -2595.0, 22739), ('Episode', 2, -2337.0, 15193), ('Episode', 3, -2550.0, 9175), ('Episode', 4, -2330.0, 11747), ('Episode', 5, -2863.0, 9510), ('Episode', 6, -2555.0, 8727), ('Episode', 7, -2586.0, 13904), ('Episode', 8, -2451.0, 8929), ('Episode', 9, -2707.0, 17729)]\n"
     ]
    }
   ],
   "source": [
    "print(scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5a312273ffa6240539e020f48a5de0b17350fba060a017bafd7c66c880b86767"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
