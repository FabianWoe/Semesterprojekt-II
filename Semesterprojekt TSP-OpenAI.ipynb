{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabian.woellenweber\\AppData\\Local\\Temp\\ipykernel_22044\\1146908951.py:11: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
      "  plt.style.use(\"seaborn-dark\")\n"
     ]
    }
   ],
   "source": [
    "## Create the TSP Environment\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "import pygame\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.collections import PatchCollection\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"seaborn-dark\")\n",
    "\n",
    "class TSPEnvironment(gym.Env):\n",
    "    def __init__ (self, n_stops = 100, version=True):\n",
    "        print(f\"TSP-Environment initialized with {n_stops} random stops\")\n",
    "\n",
    "        #True (V1) = Discrete Space / False = Array space\n",
    "        self.version = version\n",
    "\n",
    "        # Initialization\n",
    "        #Number of stops\n",
    "        self.n_stops = n_stops\n",
    "        #Coordinates of stops\n",
    "        self.xy = []\n",
    "        self._visitedStops = []\n",
    "        self._notVisitedStops = list(range(0,self.n_stops))\n",
    "        \n",
    "        #if(self.version):\n",
    "        self.action_space = spaces.Discrete(n_stops)\n",
    "        #else:\n",
    "            #self.action_space = spaces.Box(np.array(range(0,self.n_stops)))\n",
    "        \n",
    "        if(self.version):\n",
    "            self.observation_space = spaces.Box(low= 0, high = self.n_stops)\n",
    "        else:\n",
    "            self.observation_space = spaces.Box(low=np.zeros(self.n_stops), high=np.ones(self.n_stops))\n",
    "        #self.action_space = self._notVisitedStops\n",
    "        self.episode_length = 0\n",
    "        self.step_count = 0\n",
    "        self.distances = np.array\n",
    "\n",
    "        self.array_visitedStops = np.zeros(n_stops)\n",
    "        print(f'Shape Array:{self.array_visitedStops.shape}')\n",
    "\n",
    "        \n",
    "\n",
    "        #set starting point (state)\n",
    "        \n",
    "        #Generate stops\n",
    "        self._generate_stops()\n",
    "        #self._generate_q_values()\n",
    "\n",
    "\n",
    "    def _generate_stops(self):\n",
    "        self.xy = (np.random.rand(self.n_stops,2)*100).round(2)\n",
    "        self.x=self.xy[:,0]\n",
    "        self.y=self.xy[:,1]\n",
    "\n",
    "        #print(f'genrated stops xy: {self.xy}')\n",
    "        self.distances = cdist(self.xy,self.xy,'euclidean').round(0)\n",
    "        \n",
    "        #pick random StartPoint\n",
    "        self._visitedStops.append(np.random.randint(0,self.n_stops))\n",
    "        print(f'Starting Point: {self._visitedStops}')\n",
    "\n",
    "\n",
    "    #return stops[-1]\n",
    "    #Gibt die aktuelle Position des Agenten zurÃ¼ck\n",
    "    def _get_state(self):\n",
    "        if( len(self._visitedStops)>0):\n",
    "            return self._visitedStops[-1]\n",
    "        else:\n",
    "            return 0 \n",
    "\n",
    "\n",
    "    def render(self):\n",
    "        pass\n",
    "\n",
    "    #Resets StartingPoint\n",
    "    def reset(self):\n",
    "        self._visitedStops.clear()\n",
    "        self.array_visitedStops = np.zeros(self.n_stops)\n",
    "        #self._notVisitedStops = list(range(0,self.n_stops))\n",
    "\n",
    "        first_stop = np.random.randint(self.n_stops)\n",
    "        #self._notVisitedStops.remove(first_stop)\n",
    "        self._visitedStops.append(first_stop)\n",
    "        self.array_visitedStops[first_stop] = True\n",
    "        self.step_count = 1\n",
    "\n",
    "        if(self.version):\n",
    "            return self._get_state()\n",
    "        else:\n",
    "            return self.array_visitedStops\n",
    "\n",
    "    def step(self,destination):\n",
    "        done = False\n",
    "        self.step_count +=1\n",
    "        reward = -self.n_stops*1000\n",
    "\n",
    "        self.episode_length += 1\n",
    "        if(self.episode_length < 1000):\n",
    "            if(np.random.rand(1,1) < 0.1):\n",
    "                destination = np.random.randint(0,self.n_stops)\n",
    "\n",
    "        #Validize Step\n",
    "        if(self._get_state() != destination & destination not in self._visitedStops):\n",
    "            #Get reward for such a move\n",
    "            reward = -self.distances[self._get_state(), destination]\n",
    "            \n",
    "            # Append state (new position)\n",
    "            self._visitedStops.append(destination)\n",
    "            self.array_visitedStops[destination] = True\n",
    "\n",
    "        print(f'State in step: {self._get_state()}')\n",
    "        print(f'Destination in step: {destination}')\n",
    "        print(f'Length visited stops: {len(self._visitedStops)}')\n",
    "        print(f'Visited Stops in step: {self._visitedStops}')\n",
    "        print(f'Reward in step: {reward}')\n",
    "        print(f'Stepcounter: {self.step_count}')\n",
    "        \n",
    "        #exploration = np.random.random_sample()\n",
    "        #print(f'Exploration: {exploration}')\n",
    "        #if(exploration >= 0.05):\n",
    "            #destination = np.random.randint(self.action_space.n)\n",
    "            #print(f'Random Destination: {destination}')\n",
    "\n",
    "        if(self.step_count >= self.n_stops*5):\n",
    "            done = True\n",
    "            reward = -2000000\n",
    "            print('Too much steps')\n",
    "\n",
    "        if(len(self._visitedStops) == self.n_stops):\n",
    "            if(len(self._visitedStops) <= self.step_count*2):\n",
    "                reward += 500\n",
    "            reward += 200    \n",
    "            done = True\n",
    "            print(f'Done = True')\n",
    "            print(f'Length visited stops: {len(self._visitedStops)}')\n",
    "            print(f'Visited Stops in step: {self._visitedStops}')\n",
    "            \n",
    "        info = {}\n",
    "\n",
    "        #print(f'Agent position: {self._get_state()}')\n",
    "\n",
    "        if(self.version):\n",
    "            return self._get_state(), reward, done, {}\n",
    "        else:\n",
    "            return self.array_visitedStops, reward, done, {}\n",
    "\n",
    "\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 10 \n",
    "\n",
    "env = TSPEnvironment(episodes, False)\n",
    "print(env.array_visitedStops)\n",
    "print(f'Step: {env.step(2)}')\n",
    "print(f'Step: {env.step(3)}')\n",
    "print(env.array_visitedStops.shape)\n",
    "print(f'Reset: {env.reset()}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 10 \n",
    "\n",
    "env = TSPEnvironment(episodes)\n",
    "\n",
    "\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    Loops = 0\n",
    "    \n",
    "\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score+=reward\n",
    "        Loops += 1\n",
    "    print('Epsiode: {} Score: {} Episodes: {}'.format(episode,score.round(2),Loops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSP-Environment initialized with 10 random stops\n",
      "Shape Array:(10,)\n",
      "Starting Point: [8]\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_15 (Dense)            (None, 512)               5632      \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 512)               262656    \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 273,418\n",
      "Trainable params: 273,418\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fabian.woellenweber\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gym\\spaces\\box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
      "c:\\Users\\fabian.woellenweber\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected dense_15_input to have 2 dimensions, but got array with shape (1, 1, 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [10], line 61\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[39m# Train the agent\u001b[39;00m\n\u001b[0;32m     59\u001b[0m agent\u001b[39m.\u001b[39mcompile(Adam(lr\u001b[39m=\u001b[39m\u001b[39m1e-3\u001b[39m), metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mmae\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> 61\u001b[0m agent\u001b[39m.\u001b[39;49mfit(env, nb_steps\u001b[39m=\u001b[39;49m\u001b[39m5000\u001b[39;49m, visualize\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, verbose\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\fabian.woellenweber\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\rl\\core.py:168\u001b[0m, in \u001b[0;36mAgent.fit\u001b[1;34m(self, env, nb_steps, action_repetition, callbacks, verbose, visualize, nb_max_start_steps, start_step_policy, log_interval, nb_max_episode_steps)\u001b[0m\n\u001b[0;32m    165\u001b[0m callbacks\u001b[39m.\u001b[39mon_step_begin(episode_step)\n\u001b[0;32m    166\u001b[0m \u001b[39m# This is were all of the work happens. We first perceive and compute the action\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[39m# (forward step) and then use the reward to improve (backward step).\u001b[39;00m\n\u001b[1;32m--> 168\u001b[0m action \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(observation)\n\u001b[0;32m    169\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    170\u001b[0m     action \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessor\u001b[39m.\u001b[39mprocess_action(action)\n",
      "File \u001b[1;32mc:\\Users\\fabian.woellenweber\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\rl\\agents\\dqn.py:224\u001b[0m, in \u001b[0;36mDQNAgent.forward\u001b[1;34m(self, observation)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, observation):\n\u001b[0;32m    222\u001b[0m     \u001b[39m# Select an action.\u001b[39;00m\n\u001b[0;32m    223\u001b[0m     state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemory\u001b[39m.\u001b[39mget_recent_state(observation)\n\u001b[1;32m--> 224\u001b[0m     q_values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_q_values(state)\n\u001b[0;32m    225\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining:\n\u001b[0;32m    226\u001b[0m         action \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy\u001b[39m.\u001b[39mselect_action(q_values\u001b[39m=\u001b[39mq_values)\n",
      "File \u001b[1;32mc:\\Users\\fabian.woellenweber\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\rl\\agents\\dqn.py:68\u001b[0m, in \u001b[0;36mAbstractDQNAgent.compute_q_values\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_q_values\u001b[39m(\u001b[39mself\u001b[39m, state):\n\u001b[1;32m---> 68\u001b[0m     q_values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_batch_q_values([state])\u001b[39m.\u001b[39mflatten()\n\u001b[0;32m     69\u001b[0m     \u001b[39massert\u001b[39;00m q_values\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnb_actions,)\n\u001b[0;32m     70\u001b[0m     \u001b[39mreturn\u001b[39;00m q_values\n",
      "File \u001b[1;32mc:\\Users\\fabian.woellenweber\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\rl\\agents\\dqn.py:63\u001b[0m, in \u001b[0;36mAbstractDQNAgent.compute_batch_q_values\u001b[1;34m(self, state_batch)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_batch_q_values\u001b[39m(\u001b[39mself\u001b[39m, state_batch):\n\u001b[0;32m     62\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocess_state_batch(state_batch)\n\u001b[1;32m---> 63\u001b[0m     q_values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mpredict_on_batch(batch)\n\u001b[0;32m     64\u001b[0m     \u001b[39massert\u001b[39;00m q_values\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m (\u001b[39mlen\u001b[39m(state_batch), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnb_actions)\n\u001b[0;32m     65\u001b[0m     \u001b[39mreturn\u001b[39;00m q_values\n",
      "File \u001b[1;32mc:\\Users\\fabian.woellenweber\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training_v1.py:1304\u001b[0m, in \u001b[0;36mModel.predict_on_batch\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m   1299\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[0;32m   1300\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`predict_on_batch` is not supported for models distributed \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1301\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mwith tf.distribute.Strategy.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1302\u001b[0m     )\n\u001b[0;32m   1303\u001b[0m \u001b[39m# Validate and standardize user data.\u001b[39;00m\n\u001b[1;32m-> 1304\u001b[0m inputs, _, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_standardize_user_data(\n\u001b[0;32m   1305\u001b[0m     x, extract_tensors_from_dataset\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[0;32m   1306\u001b[0m )\n\u001b[0;32m   1307\u001b[0m \u001b[39m# If `self._distribution_strategy` is True, then we are in a replica\u001b[39;00m\n\u001b[0;32m   1308\u001b[0m \u001b[39m# context at this point.\u001b[39;00m\n\u001b[0;32m   1309\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrun_eagerly \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_distribution_strategy:\n",
      "File \u001b[1;32mc:\\Users\\fabian.woellenweber\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training_v1.py:2649\u001b[0m, in \u001b[0;36mModel._standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2640\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   2641\u001b[0m     \u001b[39mnot\u001b[39;00m run_eagerly\n\u001b[0;32m   2642\u001b[0m     \u001b[39mand\u001b[39;00m is_build_called\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2645\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39many\u001b[39m(_is_symbolic_tensor(v) \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m all_inputs)\n\u001b[0;32m   2646\u001b[0m ):\n\u001b[0;32m   2647\u001b[0m     \u001b[39mreturn\u001b[39;00m [], [], \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 2649\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_standardize_tensors(\n\u001b[0;32m   2650\u001b[0m     x,\n\u001b[0;32m   2651\u001b[0m     y,\n\u001b[0;32m   2652\u001b[0m     sample_weight,\n\u001b[0;32m   2653\u001b[0m     run_eagerly\u001b[39m=\u001b[39;49mrun_eagerly,\n\u001b[0;32m   2654\u001b[0m     dict_inputs\u001b[39m=\u001b[39;49mdict_inputs,\n\u001b[0;32m   2655\u001b[0m     is_dataset\u001b[39m=\u001b[39;49mis_dataset,\n\u001b[0;32m   2656\u001b[0m     class_weight\u001b[39m=\u001b[39;49mclass_weight,\n\u001b[0;32m   2657\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m   2658\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\fabian.woellenweber\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training_v1.py:2690\u001b[0m, in \u001b[0;36mModel._standardize_tensors\u001b[1;34m(self, x, y, sample_weight, run_eagerly, dict_inputs, is_dataset, class_weight, batch_size)\u001b[0m\n\u001b[0;32m   2687\u001b[0m \u001b[39m# Standardize the inputs.\u001b[39;00m\n\u001b[0;32m   2688\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(x, (tf\u001b[39m.\u001b[39mcompat\u001b[39m.\u001b[39mv1\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset, tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset)):\n\u001b[0;32m   2689\u001b[0m     \u001b[39m# TODO(fchollet): run static checks with dataset output shape(s).\u001b[39;00m\n\u001b[1;32m-> 2690\u001b[0m     x \u001b[39m=\u001b[39m training_utils_v1\u001b[39m.\u001b[39;49mstandardize_input_data(\n\u001b[0;32m   2691\u001b[0m         x,\n\u001b[0;32m   2692\u001b[0m         feed_input_names,\n\u001b[0;32m   2693\u001b[0m         feed_input_shapes,\n\u001b[0;32m   2694\u001b[0m         check_batch_axis\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,  \u001b[39m# Don't enforce the batch size.\u001b[39;49;00m\n\u001b[0;32m   2695\u001b[0m         exception_prefix\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39minput\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   2696\u001b[0m     )\n\u001b[0;32m   2698\u001b[0m \u001b[39m# Get typespecs for the input data and sanitize it if necessary.\u001b[39;00m\n\u001b[0;32m   2699\u001b[0m \u001b[39m# TODO(momernick): This should be capable of doing full input validation\u001b[39;00m\n\u001b[0;32m   2700\u001b[0m \u001b[39m# at all times - validate that this is so and refactor the\u001b[39;00m\n\u001b[0;32m   2701\u001b[0m \u001b[39m# standardization code.\u001b[39;00m\n\u001b[0;32m   2702\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(x, tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset):\n",
      "File \u001b[1;32mc:\\Users\\fabian.woellenweber\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training_utils_v1.py:714\u001b[0m, in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    712\u001b[0m shape \u001b[39m=\u001b[39m shapes[i]\n\u001b[0;32m    713\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(data_shape) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(shape):\n\u001b[1;32m--> 714\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    715\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mError when checking \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    716\u001b[0m         \u001b[39m+\u001b[39m exception_prefix\n\u001b[0;32m    717\u001b[0m         \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m: expected \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    718\u001b[0m         \u001b[39m+\u001b[39m names[i]\n\u001b[0;32m    719\u001b[0m         \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m to have \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    720\u001b[0m         \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mlen\u001b[39m(shape))\n\u001b[0;32m    721\u001b[0m         \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m dimensions, but got array \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    722\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mwith shape \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(data_shape)\n\u001b[0;32m    723\u001b[0m     )\n\u001b[0;32m    724\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m check_batch_axis:\n\u001b[0;32m    725\u001b[0m     data_shape \u001b[39m=\u001b[39m data_shape[\u001b[39m1\u001b[39m:]\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected dense_15_input to have 2 dimensions, but got array with shape (1, 1, 10)"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from rl.agents import DQNAgent, DDPGAgent, ContinuousDQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy, BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from tensorflow.keras.layers import Lambda\n",
    "\n",
    "# Learning factors...\n",
    "GeneratedStops = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "# create replay memory using deque\n",
    "#memory = deque(maxlen=2000)\n",
    "\n",
    "# Create gym environment\n",
    "env = TSPEnvironment(GeneratedStops, False)\n",
    "\n",
    "\n",
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space.shape[0]\n",
    "\n",
    "\n",
    "# The following function creates a neural network which is used as an \n",
    "# approximate Q function\n",
    "# Input: state \n",
    "# Output: Q Value of each action\n",
    "def build_model(state_size, action_size):\n",
    "    model = tf.keras.Sequential()\n",
    "    #model.add(Input(shape=(3,)))\n",
    "    model.add(Dense(512 ,input_dim=state_size, activation='relu'))\n",
    "    model.add(Dense(512 ,activation='relu'))\n",
    "    model.add(Dense(action_size, activation='linear'))\n",
    "    model.summary()\n",
    "    model.compile(loss='mse', optimizer=Adam(learning_rate=learning_rate))\n",
    "    return model\n",
    "\n",
    "# Create the TSP agent\n",
    "def build_agent(model, action_size):\n",
    "    # Use Epsilon-Greedy policy for exploration\n",
    "    policy = BoltzmannQPolicy()\n",
    "    # Create memory for storing transitions\n",
    "    memory = SequentialMemory(limit=50000, window_length=1)\n",
    "    # Create the DQN agent\n",
    "    agent = DQNAgent(model, memory=memory, policy=policy, nb_actions=action_size, nb_steps_warmup=100, target_model_update=1e-2)\n",
    "    \n",
    "    return agent\n",
    "\n",
    "\n",
    "# Create the TSP model\n",
    "model = build_model(state_size, action_size)\n",
    "model.output\n",
    "# Create the TSP agent\n",
    "agent = build_agent(model, action_size)\n",
    "# Train the agent\n",
    "agent.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "\n",
    "agent.fit(env, nb_steps=5000, visualize=False, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores=agent.test(env, nb_episodes=1000, visualize=False)\n",
    "print(np.mean(scores.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TSPEnvironment(GeneratedStops, False)\n",
    "\n",
    "\n",
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space.shape\n",
    "\n",
    "print(action_size)\n",
    "print(state_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5 (tags/v3.10.5:f377153, Jun  6 2022, 16:14:13) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5a312273ffa6240539e020f48a5de0b17350fba060a017bafd7c66c880b86767"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
