{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import Env\n",
    "from gym.spaces import Discrete, box\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'QAgent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mDeliveryQAgent\u001b[39;00m(QAgent):\n\u001b[0;32m      3\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m,\u001b[39m*\u001b[39margs,\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m      4\u001b[0m         \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39m*\u001b[39margs,\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'QAgent' is not defined"
     ]
    }
   ],
   "source": [
    "class DeliveryQAgent(QAgent):\n",
    "\n",
    "    def __init__(self,*args,**kwargs):\n",
    "        super().__init__(*args,**kwargs)\n",
    "        self.reset_memory()\n",
    "\n",
    "    def act(self,s):\n",
    "\n",
    "        # Get Q Vector\n",
    "        q = np.copy(self.Q[s,:])\n",
    "\n",
    "        # Avoid already visited states\n",
    "        q[self.states_memory] = -np.inf\n",
    "\n",
    "        if np.random.rand() > self.epsilon:\n",
    "            a = np.argmax(q)\n",
    "        else:\n",
    "            a = np.random.choice([x for x in range(self.actions_size) if x not in self.states_memory])\n",
    "\n",
    "        return a\n",
    "\n",
    "\n",
    "    def remember_state(self,s):\n",
    "        self.states_memory.append(s)\n",
    "\n",
    "    def reset_memory(self):\n",
    "        self.states_memory = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabian.woellenweber\\AppData\\Local\\Temp\\ipykernel_28404\\1267304800.py:11: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
      "  plt.style.use(\"seaborn-dark\")\n"
     ]
    }
   ],
   "source": [
    "## Create the TSP Environment\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "import pygame\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.collections import PatchCollection\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"seaborn-dark\")\n",
    "\n",
    "class TSPEnvironment(gym.Env):\n",
    "    def __init__ (self, n_stops = 100):\n",
    "        print(f\"TSP-Environment initialized with {n_stops} random stops\")\n",
    "\n",
    "        # Initialization\n",
    "        #Number of stops\n",
    "        self.n_stops = n_stops\n",
    "        #Coordinates of stops\n",
    "        self.xy = []\n",
    "        \n",
    "        self.action_space = spaces.Discrete(n_stops)\n",
    "        self.observation_space = spaces.Discrete(n_stops)\n",
    "        self.episode_length = n_stops\n",
    "        self._visitedStops = []\n",
    "\n",
    "        #set starting point (state)\n",
    "        \n",
    "        #Generate stops\n",
    "        self._generate_stops()\n",
    "        self._generate_q_values()\n",
    "                \n",
    "        \n",
    "\n",
    "    def _get_obs(self):\n",
    "        return {\"agent\": self._agent_location, \"target\": self._target_location}\n",
    "\n",
    "    def _get_info(self):\n",
    "        return {\"distance\": np.linalg.normal(self._agent_location - self._target_location, ord=1)} \n",
    "\n",
    "    def _generate_stops(self):\n",
    "        self.xy = (np.random.rand(100,2)*100).round(2)\n",
    "        self.x=self.xy[:,0]\n",
    "        self.y=self.xy[:,1]\n",
    "\n",
    "        #print(f'genrated stops xy: {self.xy}')\n",
    "        \n",
    "        #pick random StartPoint\n",
    "        self._visitedStops.append(np.random.randint(0,self.n_stops))\n",
    "        print(f'Starting Point: {self._visitedStops}')\n",
    "\n",
    "\n",
    "    def _generate_q_values(self,box_size = 0.2):\n",
    "        self.q_stops = -cdist(self.xy,self.xy,'euclidean').round(2)\n",
    "        #print(f'Distance: \\n {self.q_stops}')\n",
    "\n",
    "\n",
    "    def render(self):\n",
    "        pass\n",
    "\n",
    "    #Resets StartingPoint\n",
    "    def reset(self):\n",
    "        self._visitedStops = []\n",
    "\n",
    "        first_stop = np.random.randint(self.n_stops)\n",
    "        self._visitedStops.append(first_stop)\n",
    "        return first_stop\n",
    "\n",
    "    def step(self,destination):\n",
    "        #Get reward for such a move\n",
    "        reward = self._get_reward(self._get_state(), destination)\n",
    "\n",
    "        #set new position of agent\n",
    "        #self.state = destination\n",
    "        if(destination not in self._visitedStops):\n",
    "            self._visitedStops.append(destination)\n",
    "\n",
    "        print(f'Visited Stops: {self._visitedStops}')\n",
    "\n",
    "        done = len(self._visitedStops) == self.n_stops\n",
    "        if(done):\n",
    "            reward+=200\n",
    "            \n",
    "        info = {}\n",
    "\n",
    "        #print(f'Agent position: {self._get_state()}')\n",
    "\n",
    "        return self._get_state(), reward, done, info\n",
    "\n",
    "\n",
    "    #return stops[-1]\n",
    "    #Gibt die aktuelle Position des Agenten zurÃ¼ck\n",
    "    def _get_state(self):\n",
    "        if( len(self._visitedStops)>0):\n",
    "            return self._visitedStops[-1]\n",
    "        else:\n",
    "            return 0 \n",
    "\n",
    "    def _get_reward(self, state, destination):\n",
    "        return self.q_stops[self._get_state(), destination]\n",
    "\n",
    "    def _get_xy(self):\n",
    "        return self.xy[self._get_state()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSP-Environment initialized with 10 random stops\n",
      "Starting Point: [2]\n",
      "Visited Stops: [8]\n",
      "Visited Stops: [8, 6]\n",
      "Visited Stops: [8, 6, 7]\n",
      "Visited Stops: [8, 6, 7, 0]\n",
      "Visited Stops: [8, 6, 7, 0]\n",
      "Visited Stops: [8, 6, 7, 0, 3]\n",
      "Visited Stops: [8, 6, 7, 0, 3, 1]\n",
      "Visited Stops: [8, 6, 7, 0, 3, 1]\n",
      "Visited Stops: [8, 6, 7, 0, 3, 1]\n",
      "Visited Stops: [8, 6, 7, 0, 3, 1, 5]\n",
      "Visited Stops: [8, 6, 7, 0, 3, 1, 5, 9]\n",
      "Visited Stops: [8, 6, 7, 0, 3, 1, 5, 9]\n",
      "Visited Stops: [8, 6, 7, 0, 3, 1, 5, 9, 4]\n",
      "Visited Stops: [8, 6, 7, 0, 3, 1, 5, 9, 4]\n",
      "Visited Stops: [8, 6, 7, 0, 3, 1, 5, 9, 4]\n",
      "Visited Stops: [8, 6, 7, 0, 3, 1, 5, 9, 4]\n",
      "Visited Stops: [8, 6, 7, 0, 3, 1, 5, 9, 4]\n",
      "Visited Stops: [8, 6, 7, 0, 3, 1, 5, 9, 4, 2]\n",
      "Epsiode: 1 Score: -424.01 Episodes: 18\n",
      "Visited Stops: [5, 3]\n",
      "Visited Stops: [5, 3, 6]\n",
      "Visited Stops: [5, 3, 6, 8]\n",
      "Visited Stops: [5, 3, 6, 8, 1]\n",
      "Visited Stops: [5, 3, 6, 8, 1]\n",
      "Visited Stops: [5, 3, 6, 8, 1]\n",
      "Visited Stops: [5, 3, 6, 8, 1, 7]\n",
      "Visited Stops: [5, 3, 6, 8, 1, 7]\n",
      "Visited Stops: [5, 3, 6, 8, 1, 7]\n",
      "Visited Stops: [5, 3, 6, 8, 1, 7]\n",
      "Visited Stops: [5, 3, 6, 8, 1, 7, 2]\n",
      "Visited Stops: [5, 3, 6, 8, 1, 7, 2]\n",
      "Visited Stops: [5, 3, 6, 8, 1, 7, 2]\n",
      "Visited Stops: [5, 3, 6, 8, 1, 7, 2]\n",
      "Visited Stops: [5, 3, 6, 8, 1, 7, 2]\n",
      "Visited Stops: [5, 3, 6, 8, 1, 7, 2]\n",
      "Visited Stops: [5, 3, 6, 8, 1, 7, 2]\n",
      "Visited Stops: [5, 3, 6, 8, 1, 7, 2, 0]\n",
      "Visited Stops: [5, 3, 6, 8, 1, 7, 2, 0, 9]\n",
      "Visited Stops: [5, 3, 6, 8, 1, 7, 2, 0, 9]\n",
      "Visited Stops: [5, 3, 6, 8, 1, 7, 2, 0, 9]\n",
      "Visited Stops: [5, 3, 6, 8, 1, 7, 2, 0, 9]\n",
      "Visited Stops: [5, 3, 6, 8, 1, 7, 2, 0, 9]\n",
      "Visited Stops: [5, 3, 6, 8, 1, 7, 2, 0, 9, 4]\n",
      "Epsiode: 2 Score: -895.81 Episodes: 24\n",
      "Visited Stops: [0, 4]\n",
      "Visited Stops: [0, 4, 2]\n",
      "Visited Stops: [0, 4, 2, 5]\n",
      "Visited Stops: [0, 4, 2, 5]\n",
      "Visited Stops: [0, 4, 2, 5]\n",
      "Visited Stops: [0, 4, 2, 5, 8]\n",
      "Visited Stops: [0, 4, 2, 5, 8]\n",
      "Visited Stops: [0, 4, 2, 5, 8]\n",
      "Visited Stops: [0, 4, 2, 5, 8]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3]\n",
      "Visited Stops: [0, 4, 2, 5, 8, 6, 7, 9, 3, 1]\n",
      "Epsiode: 3 Score: -3539.76 Episodes: 101\n",
      "Visited Stops: [1]\n",
      "Visited Stops: [1, 9]\n",
      "Visited Stops: [1, 9, 6]\n",
      "Visited Stops: [1, 9, 6, 4]\n",
      "Visited Stops: [1, 9, 6, 4]\n",
      "Visited Stops: [1, 9, 6, 4, 2]\n",
      "Visited Stops: [1, 9, 6, 4, 2]\n",
      "Visited Stops: [1, 9, 6, 4, 2]\n",
      "Visited Stops: [1, 9, 6, 4, 2, 3]\n",
      "Visited Stops: [1, 9, 6, 4, 2, 3]\n",
      "Visited Stops: [1, 9, 6, 4, 2, 3]\n",
      "Visited Stops: [1, 9, 6, 4, 2, 3, 0]\n",
      "Visited Stops: [1, 9, 6, 4, 2, 3, 0]\n",
      "Visited Stops: [1, 9, 6, 4, 2, 3, 0]\n",
      "Visited Stops: [1, 9, 6, 4, 2, 3, 0, 5]\n",
      "Visited Stops: [1, 9, 6, 4, 2, 3, 0, 5]\n",
      "Visited Stops: [1, 9, 6, 4, 2, 3, 0, 5]\n",
      "Visited Stops: [1, 9, 6, 4, 2, 3, 0, 5]\n",
      "Visited Stops: [1, 9, 6, 4, 2, 3, 0, 5]\n",
      "Visited Stops: [1, 9, 6, 4, 2, 3, 0, 5]\n",
      "Visited Stops: [1, 9, 6, 4, 2, 3, 0, 5]\n",
      "Visited Stops: [1, 9, 6, 4, 2, 3, 0, 5]\n",
      "Visited Stops: [1, 9, 6, 4, 2, 3, 0, 5]\n",
      "Visited Stops: [1, 9, 6, 4, 2, 3, 0, 5, 8]\n",
      "Visited Stops: [1, 9, 6, 4, 2, 3, 0, 5, 8]\n",
      "Visited Stops: [1, 9, 6, 4, 2, 3, 0, 5, 8]\n",
      "Visited Stops: [1, 9, 6, 4, 2, 3, 0, 5, 8]\n",
      "Visited Stops: [1, 9, 6, 4, 2, 3, 0, 5, 8]\n",
      "Visited Stops: [1, 9, 6, 4, 2, 3, 0, 5, 8]\n",
      "Visited Stops: [1, 9, 6, 4, 2, 3, 0, 5, 8]\n",
      "Visited Stops: [1, 9, 6, 4, 2, 3, 0, 5, 8]\n",
      "Visited Stops: [1, 9, 6, 4, 2, 3, 0, 5, 8]\n",
      "Visited Stops: [1, 9, 6, 4, 2, 3, 0, 5, 8]\n",
      "Visited Stops: [1, 9, 6, 4, 2, 3, 0, 5, 8]\n",
      "Visited Stops: [1, 9, 6, 4, 2, 3, 0, 5, 8]\n",
      "Visited Stops: [1, 9, 6, 4, 2, 3, 0, 5, 8]\n",
      "Visited Stops: [1, 9, 6, 4, 2, 3, 0, 5, 8]\n",
      "Visited Stops: [1, 9, 6, 4, 2, 3, 0, 5, 8]\n",
      "Visited Stops: [1, 9, 6, 4, 2, 3, 0, 5, 8, 7]\n",
      "Epsiode: 4 Score: -1112.01 Episodes: 39\n",
      "Visited Stops: [8, 3]\n",
      "Visited Stops: [8, 3, 4]\n",
      "Visited Stops: [8, 3, 4, 7]\n",
      "Visited Stops: [8, 3, 4, 7, 6]\n",
      "Visited Stops: [8, 3, 4, 7, 6]\n",
      "Visited Stops: [8, 3, 4, 7, 6, 5]\n",
      "Visited Stops: [8, 3, 4, 7, 6, 5]\n",
      "Visited Stops: [8, 3, 4, 7, 6, 5]\n",
      "Visited Stops: [8, 3, 4, 7, 6, 5, 9]\n",
      "Visited Stops: [8, 3, 4, 7, 6, 5, 9]\n",
      "Visited Stops: [8, 3, 4, 7, 6, 5, 9, 0]\n",
      "Visited Stops: [8, 3, 4, 7, 6, 5, 9, 0]\n",
      "Visited Stops: [8, 3, 4, 7, 6, 5, 9, 0]\n",
      "Visited Stops: [8, 3, 4, 7, 6, 5, 9, 0]\n",
      "Visited Stops: [8, 3, 4, 7, 6, 5, 9, 0, 1]\n",
      "Visited Stops: [8, 3, 4, 7, 6, 5, 9, 0, 1]\n",
      "Visited Stops: [8, 3, 4, 7, 6, 5, 9, 0, 1]\n",
      "Visited Stops: [8, 3, 4, 7, 6, 5, 9, 0, 1]\n",
      "Visited Stops: [8, 3, 4, 7, 6, 5, 9, 0, 1]\n",
      "Visited Stops: [8, 3, 4, 7, 6, 5, 9, 0, 1]\n",
      "Visited Stops: [8, 3, 4, 7, 6, 5, 9, 0, 1]\n",
      "Visited Stops: [8, 3, 4, 7, 6, 5, 9, 0, 1]\n",
      "Visited Stops: [8, 3, 4, 7, 6, 5, 9, 0, 1]\n",
      "Visited Stops: [8, 3, 4, 7, 6, 5, 9, 0, 1]\n",
      "Visited Stops: [8, 3, 4, 7, 6, 5, 9, 0, 1]\n",
      "Visited Stops: [8, 3, 4, 7, 6, 5, 9, 0, 1]\n",
      "Visited Stops: [8, 3, 4, 7, 6, 5, 9, 0, 1]\n",
      "Visited Stops: [8, 3, 4, 7, 6, 5, 9, 0, 1]\n",
      "Visited Stops: [8, 3, 4, 7, 6, 5, 9, 0, 1]\n",
      "Visited Stops: [8, 3, 4, 7, 6, 5, 9, 0, 1]\n",
      "Visited Stops: [8, 3, 4, 7, 6, 5, 9, 0, 1]\n",
      "Visited Stops: [8, 3, 4, 7, 6, 5, 9, 0, 1]\n",
      "Visited Stops: [8, 3, 4, 7, 6, 5, 9, 0, 1, 2]\n",
      "Epsiode: 5 Score: -1345.02 Episodes: 33\n",
      "Visited Stops: [0, 9]\n",
      "Visited Stops: [0, 9, 6]\n",
      "Visited Stops: [0, 9, 6, 5]\n",
      "Visited Stops: [0, 9, 6, 5]\n",
      "Visited Stops: [0, 9, 6, 5]\n",
      "Visited Stops: [0, 9, 6, 5]\n",
      "Visited Stops: [0, 9, 6, 5, 7]\n",
      "Visited Stops: [0, 9, 6, 5, 7]\n",
      "Visited Stops: [0, 9, 6, 5, 7]\n",
      "Visited Stops: [0, 9, 6, 5, 7, 2]\n",
      "Visited Stops: [0, 9, 6, 5, 7, 2]\n",
      "Visited Stops: [0, 9, 6, 5, 7, 2, 3]\n",
      "Visited Stops: [0, 9, 6, 5, 7, 2, 3, 4]\n",
      "Visited Stops: [0, 9, 6, 5, 7, 2, 3, 4]\n",
      "Visited Stops: [0, 9, 6, 5, 7, 2, 3, 4, 1]\n",
      "Visited Stops: [0, 9, 6, 5, 7, 2, 3, 4, 1]\n",
      "Visited Stops: [0, 9, 6, 5, 7, 2, 3, 4, 1]\n",
      "Visited Stops: [0, 9, 6, 5, 7, 2, 3, 4, 1]\n",
      "Visited Stops: [0, 9, 6, 5, 7, 2, 3, 4, 1]\n",
      "Visited Stops: [0, 9, 6, 5, 7, 2, 3, 4, 1]\n",
      "Visited Stops: [0, 9, 6, 5, 7, 2, 3, 4, 1]\n",
      "Visited Stops: [0, 9, 6, 5, 7, 2, 3, 4, 1]\n",
      "Visited Stops: [0, 9, 6, 5, 7, 2, 3, 4, 1]\n",
      "Visited Stops: [0, 9, 6, 5, 7, 2, 3, 4, 1]\n",
      "Visited Stops: [0, 9, 6, 5, 7, 2, 3, 4, 1]\n",
      "Visited Stops: [0, 9, 6, 5, 7, 2, 3, 4, 1]\n",
      "Visited Stops: [0, 9, 6, 5, 7, 2, 3, 4, 1]\n",
      "Visited Stops: [0, 9, 6, 5, 7, 2, 3, 4, 1, 8]\n",
      "Epsiode: 6 Score: -856.66 Episodes: 28\n",
      "Visited Stops: [7, 6]\n",
      "Visited Stops: [7, 6, 4]\n",
      "Visited Stops: [7, 6, 4, 2]\n",
      "Visited Stops: [7, 6, 4, 2, 3]\n",
      "Visited Stops: [7, 6, 4, 2, 3, 8]\n",
      "Visited Stops: [7, 6, 4, 2, 3, 8, 5]\n",
      "Visited Stops: [7, 6, 4, 2, 3, 8, 5, 9]\n",
      "Visited Stops: [7, 6, 4, 2, 3, 8, 5, 9]\n",
      "Visited Stops: [7, 6, 4, 2, 3, 8, 5, 9]\n",
      "Visited Stops: [7, 6, 4, 2, 3, 8, 5, 9]\n",
      "Visited Stops: [7, 6, 4, 2, 3, 8, 5, 9]\n",
      "Visited Stops: [7, 6, 4, 2, 3, 8, 5, 9]\n",
      "Visited Stops: [7, 6, 4, 2, 3, 8, 5, 9, 1]\n",
      "Visited Stops: [7, 6, 4, 2, 3, 8, 5, 9, 1]\n",
      "Visited Stops: [7, 6, 4, 2, 3, 8, 5, 9, 1]\n",
      "Visited Stops: [7, 6, 4, 2, 3, 8, 5, 9, 1]\n",
      "Visited Stops: [7, 6, 4, 2, 3, 8, 5, 9, 1]\n",
      "Visited Stops: [7, 6, 4, 2, 3, 8, 5, 9, 1, 0]\n",
      "Epsiode: 7 Score: -535.84 Episodes: 18\n",
      "Visited Stops: [3, 5]\n",
      "Visited Stops: [3, 5, 0]\n",
      "Visited Stops: [3, 5, 0]\n",
      "Visited Stops: [3, 5, 0, 8]\n",
      "Visited Stops: [3, 5, 0, 8]\n",
      "Visited Stops: [3, 5, 0, 8, 9]\n",
      "Visited Stops: [3, 5, 0, 8, 9, 4]\n",
      "Visited Stops: [3, 5, 0, 8, 9, 4]\n",
      "Visited Stops: [3, 5, 0, 8, 9, 4, 1]\n",
      "Visited Stops: [3, 5, 0, 8, 9, 4, 1, 6]\n",
      "Visited Stops: [3, 5, 0, 8, 9, 4, 1, 6]\n",
      "Visited Stops: [3, 5, 0, 8, 9, 4, 1, 6]\n",
      "Visited Stops: [3, 5, 0, 8, 9, 4, 1, 6]\n",
      "Visited Stops: [3, 5, 0, 8, 9, 4, 1, 6]\n",
      "Visited Stops: [3, 5, 0, 8, 9, 4, 1, 6, 7]\n",
      "Visited Stops: [3, 5, 0, 8, 9, 4, 1, 6, 7]\n",
      "Visited Stops: [3, 5, 0, 8, 9, 4, 1, 6, 7]\n",
      "Visited Stops: [3, 5, 0, 8, 9, 4, 1, 6, 7]\n",
      "Visited Stops: [3, 5, 0, 8, 9, 4, 1, 6, 7]\n",
      "Visited Stops: [3, 5, 0, 8, 9, 4, 1, 6, 7]\n",
      "Visited Stops: [3, 5, 0, 8, 9, 4, 1, 6, 7]\n",
      "Visited Stops: [3, 5, 0, 8, 9, 4, 1, 6, 7]\n",
      "Visited Stops: [3, 5, 0, 8, 9, 4, 1, 6, 7]\n",
      "Visited Stops: [3, 5, 0, 8, 9, 4, 1, 6, 7]\n",
      "Visited Stops: [3, 5, 0, 8, 9, 4, 1, 6, 7]\n",
      "Visited Stops: [3, 5, 0, 8, 9, 4, 1, 6, 7, 2]\n",
      "Epsiode: 8 Score: -994.61 Episodes: 26\n",
      "Visited Stops: [0]\n",
      "Visited Stops: [0, 6]\n",
      "Visited Stops: [0, 6, 7]\n",
      "Visited Stops: [0, 6, 7, 9]\n",
      "Visited Stops: [0, 6, 7, 9]\n",
      "Visited Stops: [0, 6, 7, 9, 2]\n",
      "Visited Stops: [0, 6, 7, 9, 2, 1]\n",
      "Visited Stops: [0, 6, 7, 9, 2, 1]\n",
      "Visited Stops: [0, 6, 7, 9, 2, 1, 8]\n",
      "Visited Stops: [0, 6, 7, 9, 2, 1, 8, 4]\n",
      "Visited Stops: [0, 6, 7, 9, 2, 1, 8, 4]\n",
      "Visited Stops: [0, 6, 7, 9, 2, 1, 8, 4]\n",
      "Visited Stops: [0, 6, 7, 9, 2, 1, 8, 4, 5]\n",
      "Visited Stops: [0, 6, 7, 9, 2, 1, 8, 4, 5]\n",
      "Visited Stops: [0, 6, 7, 9, 2, 1, 8, 4, 5, 3]\n",
      "Epsiode: 9 Score: -428.07 Episodes: 15\n",
      "Visited Stops: [2, 5]\n",
      "Visited Stops: [2, 5, 8]\n",
      "Visited Stops: [2, 5, 8]\n",
      "Visited Stops: [2, 5, 8]\n",
      "Visited Stops: [2, 5, 8]\n",
      "Visited Stops: [2, 5, 8, 4]\n",
      "Visited Stops: [2, 5, 8, 4, 9]\n",
      "Visited Stops: [2, 5, 8, 4, 9]\n",
      "Visited Stops: [2, 5, 8, 4, 9]\n",
      "Visited Stops: [2, 5, 8, 4, 9, 3]\n",
      "Visited Stops: [2, 5, 8, 4, 9, 3, 7]\n",
      "Visited Stops: [2, 5, 8, 4, 9, 3, 7, 6]\n",
      "Visited Stops: [2, 5, 8, 4, 9, 3, 7, 6]\n",
      "Visited Stops: [2, 5, 8, 4, 9, 3, 7, 6, 1]\n",
      "Visited Stops: [2, 5, 8, 4, 9, 3, 7, 6, 1, 0]\n",
      "Epsiode: 10 Score: -410.91 Episodes: 15\n"
     ]
    }
   ],
   "source": [
    "episodes = 10 \n",
    "\n",
    "env = TSPEnvironment(episodes)\n",
    "\n",
    "\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    Loops = 0\n",
    "    \n",
    "\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score+=reward\n",
    "        Loops += 1\n",
    "    print('Epsiode: {} Score: {} Episodes: {}'.format(episode,score.round(2),Loops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSP-Environment initialized with 50 random stops\n",
      "Starting Point: [20]\n",
      "Model: \"sequential_41\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_119 (Dense)           (None, 256)               13056     \n",
      "                                                                 \n",
      " dense_120 (Dense)           (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_121 (Dense)           (None, 50)                12850     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 91,698\n",
      "Trainable params: 91,698\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Keras symbolic inputs/outputs do not implement `__len__`. You may be trying to pass Keras symbolic inputs/outputs to a TF API that does not register dispatching, preventing Keras from automatically converting the API call to a lambda layer in the Functional Model. This error will also get raised if you try asserting a symbolic input/output directly.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [46], line 72\u001b[0m\n\u001b[0;32m     70\u001b[0m model\u001b[39m.\u001b[39moutput\n\u001b[0;32m     71\u001b[0m \u001b[39m# Create the TSP agent\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m agent \u001b[39m=\u001b[39m build_agent(model, action_size)\n\u001b[0;32m     73\u001b[0m \u001b[39m# Train the agent\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[39m#agent.compile(Adam(lr=1e-3), metrics=['mae'])\u001b[39;00m\n",
      "Cell \u001b[1;32mIn [46], line 62\u001b[0m, in \u001b[0;36mbuild_agent\u001b[1;34m(model, action_size)\u001b[0m\n\u001b[0;32m     60\u001b[0m memory \u001b[39m=\u001b[39m SequentialMemory(limit\u001b[39m=\u001b[39m\u001b[39m50000\u001b[39m, window_length\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     61\u001b[0m \u001b[39m# Create the DQN agent\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m agent \u001b[39m=\u001b[39m DQNAgent(model, memory\u001b[39m=\u001b[39;49mmemory, policy\u001b[39m=\u001b[39;49mpolicy, nb_actions\u001b[39m=\u001b[39;49maction_size, nb_steps_warmup\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, target_model_update\u001b[39m=\u001b[39;49m\u001b[39m1e-2\u001b[39;49m)\n\u001b[0;32m     64\u001b[0m \u001b[39mreturn\u001b[39;00m agent\n",
      "File \u001b[1;32mc:\\Users\\fabian.woellenweber\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\rl\\agents\\dqn.py:108\u001b[0m, in \u001b[0;36mDQNAgent.__init__\u001b[1;34m(self, model, policy, test_policy, enable_double_dqn, enable_dueling_network, dueling_type, *args, **kwargs)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[39msuper\u001b[39m(DQNAgent, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    107\u001b[0m \u001b[39m# Validate (important) input.\u001b[39;00m\n\u001b[1;32m--> 108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(model\u001b[39m.\u001b[39moutput, \u001b[39m'\u001b[39m\u001b[39m__len__\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39;49m(model\u001b[39m.\u001b[39;49moutput) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    109\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mModel \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m has more than one output. DQN expects a model that has a single output.\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(model))\n\u001b[0;32m    110\u001b[0m \u001b[39mif\u001b[39;00m model\u001b[39m.\u001b[39moutput\u001b[39m.\u001b[39m_keras_shape \u001b[39m!=\u001b[39m (\u001b[39mNone\u001b[39;00m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnb_actions):\n",
      "File \u001b[1;32mc:\\Users\\fabian.woellenweber\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\keras_tensor.py:244\u001b[0m, in \u001b[0;36mKerasTensor.__len__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__len__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 244\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m    245\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mKeras symbolic inputs/outputs do not \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    246\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mimplement `__len__`. You may be \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    247\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtrying to pass Keras symbolic inputs/outputs \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    248\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mto a TF API that does not register dispatching, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    249\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpreventing Keras from automatically \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    250\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mconverting the API call to a lambda layer \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    251\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39min the Functional Model. This error will also get raised \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    252\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mif you try asserting a symbolic input/output directly.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    253\u001b[0m     )\n",
      "\u001b[1;31mTypeError\u001b[0m: Keras symbolic inputs/outputs do not implement `__len__`. You may be trying to pass Keras symbolic inputs/outputs to a TF API that does not register dispatching, preventing Keras from automatically converting the API call to a lambda layer in the Functional Model. This error will also get raised if you try asserting a symbolic input/output directly."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from rl.agents import DQNAgent, DDPGAgent, ContinuousDQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy, BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from tensorflow.keras.layers import Lambda\n",
    "\n",
    "\n",
    "# Learning factors...\n",
    "EPISODES = 50\n",
    "discount_factor = 0.99\n",
    "learning_rate = 0.001\n",
    "epsilon_start = 1.0\n",
    "epsilon_decay_steps = 0.999\n",
    "epsilon_end = 0.01\n",
    "batch_size = 64\n",
    "train_start = 1000\n",
    "\n",
    "# create replay memory using deque\n",
    "#memory = deque(maxlen=2000)\n",
    "\n",
    "# Create gym environment\n",
    "env = TSPEnvironment(EPISODES)\n",
    "\n",
    "state_size = env.observation_space.n\n",
    "action_size = env.action_space.n\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "loss_fn = tf.keras.losses.mean_squared_error\n",
    "\n",
    "# Create a replay buffer to store transitions\n",
    "replay_buffer = []\n",
    "\n",
    "# Define the update frequency and batch size\n",
    "update_frequency = 10\n",
    "batch_size = 10\n",
    "\n",
    "# The following function creates a neural network which is used as an \n",
    "# approximate Q function\n",
    "# Input: state \n",
    "# Output: Q Value of each action\n",
    "def build_model(state_size, action_size):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_dim=state_size, activation='relu'))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(action_size, activation='linear'))\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "# Create the TSP agent\n",
    "def build_agent(model, action_size):\n",
    "    # Use Epsilon-Greedy policy for exploration\n",
    "    policy = BoltzmannQPolicy()\n",
    "    # Create memory for storing transitions\n",
    "    memory = SequentialMemory(limit=50000, window_length=1)\n",
    "    # Create the DQN agent\n",
    "    agent = DQNAgent(model, memory=memory, policy=policy, nb_actions=action_size, nb_steps_warmup=10, target_model_update=1e-2)\n",
    "    \n",
    "    return agent\n",
    "\n",
    "\n",
    "\n",
    "# Create the TSP model\n",
    "model = build_model(state_size, action_size)\n",
    "model.output\n",
    "# Create the TSP agent\n",
    "agent = build_agent(model, action_size)\n",
    "# Train the agent\n",
    "#agent.compile(Adam(lr=1e-3), metrics=['mae'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSP-Environment initialized with 99 random stops\n",
      "Starting Point: [85]\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_24 (Dense)            (None, 32)                3200      \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 99)                3267      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,523\n",
      "Trainable params: 7,523\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_27 (Dense)            (None, 32)                3200      \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 99)                3267      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,523\n",
      "Trainable params: 7,523\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Visited Stops: [75, 49]\n",
      "Visited Stops: [75, 49, 9]\n",
      "Visited Stops: [75, 49, 9, 21]\n",
      "Visited Stops: [75, 49, 9, 21, 68]\n",
      "Visited Stops: [75, 49, 9, 21, 68, 95]\n",
      "Visited Stops: [75, 49, 9, 21, 68, 95, 55]\n",
      "Visited Stops: [75, 49, 9, 21, 68, 95, 55, 78]\n",
      "Visited Stops: [75, 49, 9, 21, 68, 95, 55, 78, 60]\n",
      "Visited Stops: [75, 49, 9, 21, 68, 95, 55, 78, 60, 6]\n",
      "Visited Stops: [75, 49, 9, 21, 68, 95, 55, 78, 60, 6, 17]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "random_sample() takes at most 1 positional argument (2 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [12], line 91\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[39m# If the replay buffer has reached a certain size, sample a batch of transitions and update the model\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(replay_buffer) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m batch_size:\n\u001b[0;32m     90\u001b[0m     \u001b[39m# Sample a batch of transitions\u001b[39;00m\n\u001b[1;32m---> 91\u001b[0m     transitions \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mrandom\u001b[39m.\u001b[39;49msample(replay_buffer,batch_size)\n\u001b[0;32m     92\u001b[0m     states, actions, rewards, next_states \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mtransitions)\n\u001b[0;32m     94\u001b[0m     \u001b[39m# Convert the states and next_states to tensors\u001b[39;00m\n",
      "File \u001b[1;32mmtrand.pyx:4708\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.sample\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mmtrand.pyx:374\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.random_sample\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: random_sample() takes at most 1 positional argument (2 given)"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "## Initialize important variables\n",
    "env_name = 'TSPEnv'\n",
    "bRender = False\n",
    "load_model = False\n",
    "\n",
    "# Learning factors...\n",
    "EPISODES = 99\n",
    "discount_factor = 0.99\n",
    "learning_rate = 0.001\n",
    "epsilon_start = 1.0\n",
    "epsilon_decay_steps = 0.999\n",
    "epsilon_end = 0.01\n",
    "batch_size = 64\n",
    "train_start = 1000\n",
    "\n",
    "# create replay memory using deque\n",
    "#memory = deque(maxlen=2000)\n",
    "\n",
    "# Create gym environment\n",
    "env = TSPEnvironment(EPISODES)\n",
    "\n",
    "state_size = env.observation_space.n\n",
    "action_size = env.action_space.n\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "loss_fn = tf.keras.losses.mean_squared_error\n",
    "\n",
    "# Create a replay buffer to store transitions\n",
    "replay_buffer = []\n",
    "\n",
    "# Define the update frequency and batch size\n",
    "update_frequency = 10\n",
    "batch_size = 10\n",
    "\n",
    "# The following function creates a neural network which is used as an \n",
    "# approximate Q function\n",
    "# Input: state \n",
    "# Output: Q Value of each action\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, input_dim=state_size, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(32, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(action_size, activation='linear', kernel_initializer='he_uniform'))\n",
    "    model.summary()\n",
    "    model.compile(loss='mse', optimizer=Adam(learning_rate=learning_rate))\n",
    "    return model\n",
    "\n",
    "# after some time interval update the target model to be same with model\n",
    "# This is done using the following function\n",
    "def update_target_model():\n",
    "    target_model.set_weights(model.get_weights())\n",
    "\n",
    "# create main model and target model\n",
    "model = build_model()\n",
    "target_model = build_model()\n",
    "\n",
    "# We call that function in the beginning to make sure model and target_model have the same weights initally\n",
    "update_target_model()\n",
    "\n",
    "# Run the TSP loop\n",
    "num_episodes = 50\n",
    "for episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    state = env.reset()\n",
    "\n",
    "    # Calculate the epsilon for this episode\n",
    "    epsilon = epsilon_end + (epsilon_start - epsilon_end) * np.exp(-episode / epsilon_decay_steps)\n",
    "\n",
    "    while True:\n",
    "        # Select an action using the DQN model and an epsilon-greedy policy\n",
    "        if np.random.rand() < epsilon:\n",
    "            # Explore: choose a random action\n",
    "            action = np.random.randint(env.action_space.n)\n",
    "        else:\n",
    "            # Exploit: choose the action with the highest Q-value\n",
    "            action = np.argmax(model(np.expand_dims(state, axis=0)).numpy())\n",
    "\n",
    "        # Take the action and observe the reward and next state\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # Store the transition in the replay buffer\n",
    "        replay_buffer.append((state, action, reward, next_state))\n",
    "\n",
    "        # If the replay buffer has reached a certain size, sample a batch of transitions and update the model\n",
    "        if len(replay_buffer) >= batch_size:\n",
    "            # Sample a batch of transitions\n",
    "            transitions = np.random.sample(replay_buffer,batch_size)\n",
    "            states, actions, rewards, next_states = zip(*transitions)\n",
    "\n",
    "            # Convert the states and next_states to tensors\n",
    "            states = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "            next_states = tf.convert_to_tensor(next_states, dtype=tf.float32)\n",
    "            \n",
    "            print(f'States: {states}')\n",
    "            print(f'States: {next_states}')\n",
    "\n",
    "            # Compute the Q-values for the current states and the next states\n",
    "            current_q_values = model(states)\n",
    "            next_q_values = target_model(next_states)\n",
    "\n",
    "            # Compute the expected Q-values\n",
    "            expected_q_values = rewards + np.max(next_q_values.numpy(), axis=1)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = loss_fn(expected_q_values, current_q_values)\n",
    "\n",
    "            # Update the model\n",
    "            optimizer.minimize(loss, model.trainable_variables)\n",
    "\n",
    "            # Clear the replay buffer\n",
    "            replay_buffer = []\n",
    "\n",
    "        # Update the target model every `update_frequency` steps\n",
    "        if episode % update_frequency == 0:\n",
    "            update_target_model()\n",
    "\n",
    "        # Update the state\n",
    "        state = next_state\n",
    "\n",
    "        # If the episode is done, break out of the inner loop\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5a312273ffa6240539e020f48a5de0b17350fba060a017bafd7c66c880b86767"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
