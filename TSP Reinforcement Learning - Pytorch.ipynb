{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "\n",
    "\n",
    "# Define the environment\n",
    "class TSPEnv(gym.Env):\n",
    "  def __init__(self, n_cities=100, show_debug_data = False):\n",
    "    self.n_cities = n_cities\n",
    "    self.xy = (np.random.rand(self.n_cities,2)*100).round(2)\n",
    "    self.x=self.xy[:,0]\n",
    "    self.y=self.xy[:,1]\n",
    "    self.step_counter = 0\n",
    "    self.show_debug_data = show_debug_data\n",
    "    self._array_visited = np.zeros(self.n_cities)\n",
    "    self._total_distance = 0\n",
    "\n",
    "    #print(f'genrated stops xy: {self.xy}')\n",
    "    self.distance_matrix = cdist(self.xy,self.xy,'euclidean').round(0)\n",
    "\n",
    "\n",
    "    self.current_city = np.random.randint(n_cities)\n",
    "    self.visited_cities = [self.current_city]\n",
    "    self._array_visited[self.current_city] = 1\n",
    "    self.remaining_cities = [i for i in range(n_cities)]\n",
    "    self.cities_list = [i for i in range(n_cities)]\n",
    "    self.remaining_cities.remove(self.current_city)\n",
    "    # Define the action space\n",
    "    self.action_space = gym.spaces.Discrete(n_cities)\n",
    "\n",
    "    # Define the observation space\n",
    "    #self.observation_space = gym.spaces.Box(low=0, high=1, shape=(n_cities,), dtype=np.int32)\n",
    "    self.observation_space = gym.spaces.MultiBinary(n_cities)\n",
    "\n",
    "    if(self.show_debug_data):\n",
    "        print(f'Current city in init: {self.current_city}')\n",
    "        print(f'Remaining city in init: {self.remaining_cities}')\n",
    "        print(f'Visited city in init: {self.visited_cities}')\n",
    "        print(f'Stepcounter in init: {self.step_counter}')\n",
    "        print(f'Observation in init: {self._array_visited}')\n",
    "\n",
    "  def reset(self):\n",
    "    self.step_counter = 0\n",
    "    self._total_distance = 0\n",
    "    self._array_visited = np.zeros(self.n_cities)\n",
    "    self.current_city = np.random.randint(self.n_cities)\n",
    "    self._array_visited[self.current_city] = 1\n",
    "    self.visited_cities = [self.current_city]\n",
    "    self.remaining_cities = [i for i in range(self.n_cities)]\n",
    "    self.remaining_cities.remove(self.current_city)\n",
    "    \n",
    "    if(self.show_debug_data):\n",
    "      print(f'Observation in Reset: {self._array_visited}')\n",
    "    return self._get_observation()\n",
    "\n",
    "  def step(self, action):\n",
    "    done = False\n",
    "    self.step_counter += 1\n",
    "    reward = -999999999\n",
    "\n",
    "    if(self.show_debug_data):\n",
    "      print(f'Action in Step(top): {action}')\n",
    "      print(f'Remaining-City Step(top): {self.remaining_cities}')\n",
    "      print(f'Observation in step: {self._array_visited}')\n",
    "    \n",
    "    if (action in self.remaining_cities):\n",
    "      if(self.show_debug_data):\n",
    "        print('Action True')\n",
    "      \n",
    "      reward = -self.distance_matrix[self.current_city][action]\n",
    "      self._total_distance += reward\n",
    "\n",
    "      self.remaining_cities.remove(action)\n",
    "      self.visited_cities.append(action)\n",
    "      self._array_visited[action] = 1\n",
    "      self.current_city = action\n",
    "\n",
    "      if (len(self.remaining_cities) == 0):\n",
    "        startingpoint = self.visited_cities[0]\n",
    "        self.visited_cities.append(startingpoint)\n",
    "        reward += -self.distance_matrix[self.current_city][startingpoint]\n",
    "        done = True\n",
    "      \n",
    "      if(self.show_debug_data):\n",
    "        print(f'Action in step: {action}')\n",
    "        print(f'Reward in step: {reward}')\n",
    "        print(f'Current city in step: {self.current_city}')\n",
    "        print(f'Remaining city in step: {self.remaining_cities}')\n",
    "        print(f'Visited city in step: {self.visited_cities}')\n",
    "        print(f'Stepcounter in step: {self.step_counter}')\n",
    "        print(f'Observation in step: {self._array_visited}')\n",
    "      \n",
    "      return self._get_observation(), reward, done, {}\n",
    "    else:\n",
    "        return self._get_observation(), reward, False, {}\n",
    "\n",
    "\n",
    "\n",
    "  def _get_observation(self):\n",
    "    # observation = np.zeros(self.n_cities)\n",
    "    # observation[self.current_city] = 1\n",
    "    # return observation\n",
    "    return self._array_visited\n",
    "\n",
    "  \n",
    "  def _test_distance(self,CurrentCity, NextCity):\n",
    "    return -self.distance_matrix[CurrentCity][NextCity]\n",
    "    \n",
    "  def plotCities(self):\n",
    "    fig, ax = plt.subplots(1, figsize=(7,7))\n",
    "    fig.suptitle = \"Delivery Stops\"\n",
    "    plt.scatter(self.x,self.y)\n",
    "    xcoord = []\n",
    "    ycoord = []\n",
    "    for i in range(0,len(self.visited_cities)):\n",
    "      xcoord.append(self.x[self.visited_cities[i]])\n",
    "      ycoord.append(self.y[self.visited_cities[i]])\n",
    "      if(i == 0):\n",
    "        ax.annotate(\"Anfang\", xy=(xcoord[i], ycoord[i]), xytext=(xcoord[i]+0.5, ycoord[i]))\n",
    "      ax.annotate(str(i), xy=(xcoord[i], ycoord[i]), xytext=(xcoord[i]+0.5, ycoord[i]))\n",
    "\n",
    "    plt.plot(xcoord, ycoord)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TSPEnv(10,False)\n",
    "for i in range(0,env.n_cities):\n",
    "    env.step(i)\n",
    "print(env.visited_cities)    \n",
    "env.plotCities()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the DQN model\n",
    "class DQN(nn.Module):\n",
    "  def __init__(self, input_dim, output_dim):\n",
    "    super().__init__()\n",
    "    self.fc1 = nn.Linear(input_dim, 512)\n",
    "    self.relu1 = nn.ReLU()\n",
    "    self.fc2 = nn.Linear(512, 256)\n",
    "    self.relu2 = nn.ReLU()\n",
    "    self.fc3 = nn.Linear(256, output_dim)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.fc1(x)\n",
    "    x = self.relu1(x)\n",
    "    x = self.fc2(x)\n",
    "    x = self.relu2(x)\n",
    "    x = self.fc3(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "# Define the DQNAgent\n",
    "class DQNAgent:\n",
    "  def __init__(self, env, epsilon=0.01, epsilon_decay=0.995, epsilon_min=0.01, \n",
    "               alpha=1e-3, alpha_decay=0.01, gamma=0.99, memory_size=10000, \n",
    "               batch_size=64):\n",
    "    self.env = env\n",
    "    self.epsilon = epsilon\n",
    "    self.epsilon_decay = epsilon_decay\n",
    "    self.epsilon_min = epsilon_min\n",
    "    self.alpha = alpha\n",
    "    self.alpha_decay = alpha_decay\n",
    "    self.gamma = gamma\n",
    "    self.memory = deque(maxlen=memory_size)\n",
    "    self.batch_size = batch_size\n",
    "\n",
    "    # Define the model and the target model\n",
    "    self.model = DQN(env.observation_space.shape[0], env.action_space.n).to(device)\n",
    "    self.target_model = DQN(env.observation_space.shape[0], env.action_space.n).to(device)\n",
    "    self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    # Define the optimizer\n",
    "    self.optimizer = optim.Adam(self.model.parameters(), lr=alpha)\n",
    "\n",
    "    # Define the loss function\n",
    "    self.loss_fn = nn.MSELoss()\n",
    "\n",
    "  def remember(self, state, action, reward, next_state, done):\n",
    "    self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "  def act(self, state):\n",
    "    if np.random.rand() <= self.epsilon:\n",
    "      return self.env.action_space.sample()\n",
    "    else:\n",
    "      state = torch.from_numpy(state).float().to(device)\n",
    "      q_values = self.model(state)\n",
    "      return q_values.argmax().item()\n",
    "\n",
    "  def update(self):\n",
    "    # Don't update if there are not enough samples in the memory\n",
    "    if len(self.memory) < self.batch_size:\n",
    "      return\n",
    "\n",
    "    # Sample a batch from the memory\n",
    "    samples = random.sample(self.memory, self.batch_size)\n",
    "\n",
    "    # Split the batch into separate variables\n",
    "    states, actions, rewards, next_states, dones = zip(*samples)\n",
    "\n",
    "    # Convert variables to tensors and move them to the device\n",
    "    states = torch.from_numpy(np.vstack(states)).float().to(device)\n",
    "    actions = torch.from_numpy(np.vstack(actions)).long().to(device)\n",
    "    rewards = torch.from_numpy(np.vstack(rewards)).float().to(device)\n",
    "    next_states = torch.from_numpy(np.vstack(next_states)).float().to(device)\n",
    "    dones = torch.from_numpy(np.vstack(dones)).float().to(device)\n",
    "\n",
    "    # Calculate the Q values for the current states\n",
    "    q_values = self.model(states)\n",
    "    q_values = q_values.gather(1, actions)\n",
    "\n",
    "    # Calculate the Q values for the next states\n",
    "    next_q_values = self.target_model(next_states).max(1)[0].unsqueeze(1)\n",
    "\n",
    "    # Calculate the target Q values\n",
    "    target_q_values = rewards + (self.gamma * next_q_values * (1 - dones))\n",
    "\n",
    "    # Calculate the loss\n",
    "    loss = self.loss_fn(q_values, target_q_values)\n",
    "\n",
    "    # Perform backpropagation\n",
    "    self.optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    self.optimizer.step()\n",
    "\n",
    "    # Update the target model\n",
    "    for target_param, param in zip(self.target_model.parameters(), self.model.parameters()):\n",
    "      target_param.data.copy_(param.data * (1 - self.alpha_decay) + target_param.data * self.alpha_decay)\n",
    "\n",
    "    # Update the epsilon value\n",
    "    self.epsilon = max(self.epsilon * self.epsilon_decay, self.epsilon_min)\n",
    "  \n",
    "  def SaveAgent(self):\n",
    "    torch.save(self.model.state_dict(),'model.pt')\n",
    "    print('Model saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1, Reward: -81000000322.0, Steps needed: 90, Total Distance: -392.0, Visited Cities: [1, 5, 9, 4, 6, 2, 0, 8, 3, 7, 1]\n",
      "Episode: 2, Reward: -351999999996.0, Steps needed: 361, Total Distance: -309.0, Visited Cities: [3, 7, 2, 9, 4, 0, 8, 6, 1, 5, 3]\n",
      "Episode: 3, Reward: -275000000275.0, Steps needed: 284, Total Distance: -491.0, Visited Cities: [9, 8, 3, 5, 1, 0, 6, 7, 4, 2, 9]\n",
      "Episode: 4, Reward: -338000000214.0, Steps needed: 347, Total Distance: -499.0, Visited Cities: [5, 2, 1, 8, 9, 7, 6, 0, 3, 4, 5]\n",
      "Episode: 5, Reward: -767999999909.0, Steps needed: 777, Total Distance: -613.0, Visited Cities: [9, 4, 7, 0, 3, 8, 2, 5, 6, 1, 9]\n",
      "Episode: 6, Reward: -753999999691.0, Steps needed: 763, Total Distance: -412.0, Visited Cities: [4, 1, 7, 5, 3, 6, 9, 0, 2, 8, 4]\n",
      "Episode: 7, Reward: -622999999863.0, Steps needed: 632, Total Distance: -442.0, Visited Cities: [3, 6, 8, 0, 2, 7, 5, 9, 1, 4, 3]\n",
      "Episode: 8, Reward: -1578999998912.0, Steps needed: 1588, Total Distance: -453.0, Visited Cities: [4, 6, 3, 2, 7, 9, 8, 1, 5, 0, 4]\n",
      "Episode: 9, Reward: -1042999999523.0, Steps needed: 1052, Total Distance: -525.0, Visited Cities: [1, 0, 3, 4, 9, 2, 7, 8, 6, 5, 1]\n",
      "Episode: 10, Reward: -430000000036.0, Steps needed: 439, Total Distance: -422.0, Visited Cities: [4, 1, 9, 8, 0, 2, 6, 7, 5, 3, 4]\n",
      "Episode: 11, Reward: -1188999999339.0, Steps needed: 1198, Total Distance: -490.0, Visited Cities: [0, 5, 3, 7, 9, 8, 1, 6, 2, 4, 0]\n",
      "Episode: 12, Reward: -2542999997949.0, Steps needed: 2552, Total Distance: -443.0, Visited Cities: [6, 4, 8, 2, 1, 5, 7, 0, 9, 3, 6]\n",
      "Episode: 13, Reward: -864999999724.0, Steps needed: 874, Total Distance: -522.0, Visited Cities: [2, 3, 9, 8, 7, 0, 4, 1, 6, 5, 2]\n",
      "Episode: 14, Reward: -2586999997850.0, Steps needed: 2596, Total Distance: -397.0, Visited Cities: [8, 5, 4, 7, 2, 1, 3, 0, 9, 6, 8]\n",
      "Episode: 15, Reward: -1224999999299.0, Steps needed: 1234, Total Distance: -481.0, Visited Cities: [6, 4, 7, 0, 8, 1, 5, 3, 9, 2, 6]\n",
      "Episode: 16, Reward: -1690999998731.0, Steps needed: 1700, Total Distance: -369.0, Visited Cities: [5, 2, 1, 7, 0, 8, 9, 3, 6, 4, 5]\n",
      "Episode: 17, Reward: -1623999998979.0, Steps needed: 1633, Total Distance: -519.0, Visited Cities: [8, 5, 4, 7, 9, 3, 0, 6, 2, 1, 8]\n",
      "Episode: 18, Reward: -1924999998770.0, Steps needed: 1934, Total Distance: -614.0, Visited Cities: [2, 4, 1, 6, 7, 9, 5, 8, 3, 0, 2]\n",
      "Episode: 19, Reward: -1861999998683.0, Steps needed: 1871, Total Distance: -493.0, Visited Cities: [9, 2, 4, 0, 6, 5, 1, 7, 8, 3, 9]\n",
      "Episode: 20, Reward: -2143999998307.0, Steps needed: 2153, Total Distance: -407.0, Visited Cities: [4, 5, 6, 7, 1, 9, 8, 0, 2, 3, 4]\n",
      "Episode: 21, Reward: -2871999997668.0, Steps needed: 2881, Total Distance: -521.0, Visited Cities: [9, 0, 5, 6, 3, 8, 1, 2, 7, 4, 9]\n",
      "Episode: 22, Reward: -1641999998874.0, Steps needed: 1651, Total Distance: -452.0, Visited Cities: [1, 8, 0, 6, 3, 5, 7, 4, 2, 9, 1]\n",
      "Episode: 23, Reward: -4399999996123.0, Steps needed: 4409, Total Distance: -482.0, Visited Cities: [6, 4, 3, 9, 8, 1, 2, 5, 7, 0, 6]\n",
      "Episode: 24, Reward: -2068999998492.0, Steps needed: 2078, Total Distance: -489.0, Visited Cities: [8, 7, 2, 1, 9, 6, 5, 0, 4, 3, 8]\n",
      "Episode: 25, Reward: -2021999998603.0, Steps needed: 2031, Total Distance: -552.0, Visited Cities: [9, 8, 3, 4, 2, 0, 1, 6, 5, 7, 9]\n",
      "Episode: 26, Reward: -1239999999298.0, Steps needed: 1249, Total Distance: -479.0, Visited Cities: [9, 0, 5, 3, 1, 8, 6, 7, 4, 2, 9]\n",
      "Episode: 27, Reward: -1257999999278.0, Steps needed: 1267, Total Distance: -478.0, Visited Cities: [1, 8, 0, 7, 3, 4, 5, 9, 2, 6, 1]\n",
      "Episode: 28, Reward: -2551999997974.0, Steps needed: 2561, Total Distance: -476.0, Visited Cities: [7, 4, 1, 2, 0, 9, 8, 3, 6, 5, 7]\n",
      "Episode: 29, Reward: -1731999998809.0, Steps needed: 1741, Total Distance: -512.0, Visited Cities: [3, 7, 6, 5, 4, 9, 0, 1, 8, 2, 3]\n",
      "Episode: 30, Reward: -4395999996151.0, Steps needed: 4405, Total Distance: -525.0, Visited Cities: [7, 5, 8, 2, 9, 6, 1, 4, 0, 3, 7]\n",
      "Episode: 31, Reward: -1050999999497.0, Steps needed: 1060, Total Distance: -451.0, Visited Cities: [0, 2, 5, 3, 1, 9, 6, 4, 8, 7, 0]\n",
      "Episode: 32, Reward: -1187999999264.0, Steps needed: 1197, Total Distance: -441.0, Visited Cities: [7, 3, 5, 6, 8, 4, 2, 0, 9, 1, 7]\n",
      "Episode: 33, Reward: -1757999998872.0, Steps needed: 1767, Total Distance: -559.0, Visited Cities: [9, 2, 3, 4, 8, 1, 6, 7, 0, 5, 9]\n",
      "Episode: 34, Reward: -2534999998007.0, Steps needed: 2544, Total Distance: -499.0, Visited Cities: [6, 7, 5, 3, 8, 1, 0, 9, 4, 2, 6]\n",
      "Episode: 35, Reward: -1820999998792.0, Steps needed: 1830, Total Distance: -558.0, Visited Cities: [1, 9, 3, 8, 7, 5, 2, 0, 6, 4, 1]\n",
      "Episode: 36, Reward: -1270999999195.0, Steps needed: 1280, Total Distance: -416.0, Visited Cities: [5, 3, 4, 9, 0, 6, 2, 1, 8, 7, 5]\n",
      "Episode: 37, Reward: -2418999998088.0, Steps needed: 2428, Total Distance: -449.0, Visited Cities: [1, 4, 0, 8, 7, 2, 3, 9, 5, 6, 1]\n",
      "Episode: 38, Reward: -3738999996857.0, Steps needed: 3748, Total Distance: -584.0, Visited Cities: [3, 4, 0, 2, 6, 7, 8, 5, 9, 1, 3]\n",
      "Episode: 39, Reward: -883999999773.0, Steps needed: 893, Total Distance: -586.0, Visited Cities: [9, 7, 6, 4, 1, 0, 2, 3, 8, 5, 9]\n",
      "Episode: 40, Reward: -1743999998843.0, Steps needed: 1753, Total Distance: -494.0, Visited Cities: [7, 9, 1, 6, 5, 3, 2, 0, 4, 8, 7]\n",
      "Episode: 41, Reward: -3787999996742.0, Steps needed: 3797, Total Distance: -503.0, Visited Cities: [2, 8, 5, 9, 3, 1, 6, 0, 4, 7, 2]\n",
      "Episode: 42, Reward: -2151999998360.0, Steps needed: 2161, Total Distance: -453.0, Visited Cities: [2, 5, 7, 8, 1, 3, 4, 6, 0, 9, 2]\n",
      "Episode: 43, Reward: -1780999998784.0, Steps needed: 1790, Total Distance: -498.0, Visited Cities: [5, 8, 7, 1, 0, 6, 9, 3, 4, 2, 5]\n",
      "Episode: 44, Reward: -2422999998157.0, Steps needed: 2432, Total Distance: -541.0, Visited Cities: [5, 8, 0, 7, 9, 2, 4, 1, 6, 3, 5]\n",
      "Episode: 45, Reward: -4670999995864.0, Steps needed: 4680, Total Distance: -477.0, Visited Cities: [6, 2, 3, 8, 5, 7, 4, 9, 0, 1, 6]\n",
      "Episode: 46, Reward: -1652999998830.0, Steps needed: 1662, Total Distance: -392.0, Visited Cities: [0, 9, 2, 1, 8, 4, 6, 7, 3, 5, 0]\n",
      "Episode: 47, Reward: -3195999997344.0, Steps needed: 3205, Total Distance: -461.0, Visited Cities: [2, 4, 1, 3, 5, 0, 9, 6, 7, 8, 2]\n",
      "Episode: 48, Reward: -1526999998969.0, Steps needed: 1536, Total Distance: -463.0, Visited Cities: [8, 3, 9, 1, 7, 2, 5, 0, 6, 4, 8]\n",
      "Episode: 49, Reward: -1772999998696.0, Steps needed: 1782, Total Distance: -458.0, Visited Cities: [1, 4, 8, 9, 6, 5, 2, 0, 3, 7, 1]\n",
      "Episode: 50, Reward: -1392999999043.0, Steps needed: 1402, Total Distance: -369.0, Visited Cities: [2, 4, 7, 1, 3, 6, 0, 9, 8, 5, 2]\n",
      "Episode: 51, Reward: -1472999999163.0, Steps needed: 1482, Total Distance: -552.0, Visited Cities: [8, 7, 2, 0, 3, 6, 4, 5, 9, 1, 8]\n",
      "Episode: 52, Reward: -1230999999180.0, Steps needed: 1240, Total Distance: -373.0, Visited Cities: [4, 6, 3, 9, 5, 1, 7, 2, 8, 0, 4]\n",
      "Episode: 53, Reward: -1627999999024.0, Steps needed: 1637, Total Distance: -588.0, Visited Cities: [7, 8, 1, 4, 0, 5, 2, 9, 3, 6, 7]\n",
      "Episode: 54, Reward: -1284999999320.0, Steps needed: 1294, Total Distance: -532.0, Visited Cities: [7, 8, 1, 3, 0, 6, 2, 4, 5, 9, 7]\n",
      "Episode: 55, Reward: -505999999937.0, Steps needed: 515, Total Distance: -410.0, Visited Cities: [8, 6, 1, 7, 0, 9, 2, 3, 5, 4, 8]\n",
      "Episode: 56, Reward: -1199999999290.0, Steps needed: 1209, Total Distance: -406.0, Visited Cities: [8, 4, 9, 3, 0, 6, 5, 7, 2, 1, 8]\n",
      "Episode: 57, Reward: -3105999997424.0, Steps needed: 3115, Total Distance: -444.0, Visited Cities: [5, 9, 3, 1, 6, 0, 4, 7, 2, 8, 5]\n",
      "Episode: 58, Reward: -1595999998883.0, Steps needed: 1605, Total Distance: -474.0, Visited Cities: [8, 9, 1, 4, 3, 5, 6, 7, 2, 0, 8]\n",
      "Episode: 59, Reward: -4987999995412.0, Steps needed: 4997, Total Distance: -373.0, Visited Cities: [2, 0, 4, 6, 8, 9, 3, 1, 5, 7, 2]\n",
      "Episode: 60, Reward: -1812999998732.0, Steps needed: 1822, Total Distance: -521.0, Visited Cities: [9, 7, 1, 0, 3, 4, 8, 5, 2, 6, 9]\n",
      "Episode: 61, Reward: -3238999997416.0, Steps needed: 3248, Total Distance: -589.0, Visited Cities: [7, 6, 8, 1, 9, 2, 5, 0, 3, 4, 7]\n",
      "Episode: 62, Reward: -2073999998486.0, Steps needed: 2083, Total Distance: -469.0, Visited Cities: [5, 6, 4, 1, 2, 7, 8, 3, 9, 0, 5]\n",
      "Episode: 63, Reward: -2247999998351.0, Steps needed: 2257, Total Distance: -555.0, Visited Cities: [4, 2, 6, 1, 0, 5, 7, 9, 8, 3, 4]\n",
      "Episode: 64, Reward: -3754999996715.0, Steps needed: 3764, Total Distance: -443.0, Visited Cities: [7, 3, 0, 5, 9, 8, 6, 4, 1, 2, 7]\n",
      "Episode: 65, Reward: -1744999998866.0, Steps needed: 1754, Total Distance: -553.0, Visited Cities: [1, 2, 0, 3, 9, 4, 5, 8, 7, 6, 1]\n",
      "Episode: 66, Reward: -1168999999465.0, Steps needed: 1178, Total Distance: -590.0, Visited Cities: [3, 0, 7, 6, 9, 5, 8, 1, 2, 4, 3]\n",
      "Episode: 67, Reward: -2220999998258.0, Steps needed: 2230, Total Distance: -458.0, Visited Cities: [9, 6, 3, 5, 7, 2, 4, 0, 1, 8, 9]\n",
      "Episode: 68, Reward: -2867999997578.0, Steps needed: 2877, Total Distance: -408.0, Visited Cities: [4, 8, 9, 2, 1, 3, 6, 7, 5, 0, 4]\n",
      "Episode: 69, Reward: -1508999998977.0, Steps needed: 1518, Total Distance: -419.0, Visited Cities: [2, 4, 9, 1, 7, 3, 8, 6, 0, 5, 2]\n",
      "Episode: 70, Reward: -1297999999197.0, Steps needed: 1307, Total Distance: -490.0, Visited Cities: [8, 6, 1, 9, 5, 3, 4, 7, 2, 0, 8]\n",
      "Episode: 71, Reward: -4316999996190.0, Steps needed: 4326, Total Distance: -426.0, Visited Cities: [0, 8, 5, 3, 6, 9, 7, 4, 1, 2, 0]\n",
      "Episode: 72, Reward: -638999999889.0, Steps needed: 648, Total Distance: -499.0, Visited Cities: [2, 0, 9, 5, 8, 6, 7, 4, 1, 3, 2]\n",
      "Episode: 73, Reward: -2626999998016.0, Steps needed: 2636, Total Distance: -584.0, Visited Cities: [2, 0, 3, 5, 8, 1, 9, 6, 7, 4, 2]\n",
      "Episode: 74, Reward: -2697999997729.0, Steps needed: 2707, Total Distance: -422.0, Visited Cities: [8, 7, 1, 6, 4, 2, 3, 5, 9, 0, 8]\n",
      "Episode: 75, Reward: -2074999998371.0, Steps needed: 2084, Total Distance: -406.0, Visited Cities: [6, 3, 2, 1, 5, 9, 4, 7, 0, 8, 6]\n",
      "Episode: 76, Reward: -1532999998980.0, Steps needed: 1542, Total Distance: -434.0, Visited Cities: [6, 2, 4, 0, 3, 8, 9, 7, 1, 5, 6]\n",
      "Episode: 77, Reward: -2157999998369.0, Steps needed: 2167, Total Distance: -500.0, Visited Cities: [7, 0, 4, 5, 9, 8, 1, 6, 3, 2, 7]\n",
      "Episode: 78, Reward: -984999999453.0, Steps needed: 994, Total Distance: -426.0, Visited Cities: [3, 8, 9, 4, 0, 5, 2, 6, 7, 1, 3]\n",
      "Episode: 79, Reward: -2984999997518.0, Steps needed: 2994, Total Distance: -476.0, Visited Cities: [2, 0, 8, 1, 5, 4, 6, 3, 9, 7, 2]\n",
      "Episode: 80, Reward: -1346999999235.0, Steps needed: 1356, Total Distance: -570.0, Visited Cities: [1, 4, 9, 7, 0, 5, 2, 8, 6, 3, 1]\n",
      "Episode: 81, Reward: -2068999998401.0, Steps needed: 2078, Total Distance: -406.0, Visited Cities: [1, 5, 7, 6, 3, 2, 0, 4, 8, 9, 1]\n",
      "Episode: 82, Reward: -1249999999255.0, Steps needed: 1259, Total Distance: -426.0, Visited Cities: [2, 3, 9, 4, 7, 5, 1, 0, 6, 8, 2]\n",
      "Episode: 83, Reward: -1606999998886.0, Steps needed: 1616, Total Distance: -488.0, Visited Cities: [8, 7, 3, 6, 9, 5, 4, 2, 1, 0, 8]\n",
      "Episode: 84, Reward: -1664999998732.0, Steps needed: 1674, Total Distance: -392.0, Visited Cities: [0, 6, 1, 7, 5, 4, 9, 3, 2, 8, 0]\n",
      "Episode: 85, Reward: -4001999996483.0, Steps needed: 4011, Total Distance: -404.0, Visited Cities: [0, 9, 8, 1, 3, 4, 6, 7, 5, 2, 0]\n",
      "Episode: 86, Reward: -2419999998121.0, Steps needed: 2429, Total Distance: -468.0, Visited Cities: [7, 8, 2, 5, 1, 3, 0, 4, 6, 9, 7]\n",
      "Episode: 87, Reward: -996999999485.0, Steps needed: 1006, Total Distance: -403.0, Visited Cities: [5, 2, 3, 9, 8, 4, 0, 1, 7, 6, 5]\n",
      "Episode: 88, Reward: -2818999997644.0, Steps needed: 2828, Total Distance: -424.0, Visited Cities: [3, 8, 2, 9, 4, 0, 6, 7, 1, 5, 3]\n",
      "Episode: 89, Reward: -1768999998791.0, Steps needed: 1778, Total Distance: -519.0, Visited Cities: [6, 9, 5, 1, 8, 7, 3, 4, 2, 0, 6]\n",
      "Episode: 90, Reward: -1290999999218.0, Steps needed: 1300, Total Distance: -425.0, Visited Cities: [8, 2, 6, 7, 5, 9, 0, 4, 3, 1, 8]\n",
      "Episode: 91, Reward: -1435999999184.0, Steps needed: 1445, Total Distance: -541.0, Visited Cities: [5, 9, 7, 1, 0, 3, 4, 2, 8, 6, 5]\n",
      "Episode: 92, Reward: -2693999997928.0, Steps needed: 2703, Total Distance: -550.0, Visited Cities: [8, 1, 6, 9, 2, 0, 7, 5, 4, 3, 8]\n",
      "Episode: 93, Reward: -1406999999021.0, Steps needed: 1416, Total Distance: -409.0, Visited Cities: [9, 3, 5, 1, 2, 6, 0, 8, 7, 4, 9]\n",
      "Episode: 94, Reward: -2451999997900.0, Steps needed: 2461, Total Distance: -333.0, Visited Cities: [4, 0, 8, 1, 7, 3, 5, 2, 6, 9, 4]\n",
      "Episode: 95, Reward: -2144999998376.0, Steps needed: 2154, Total Distance: -449.0, Visited Cities: [3, 6, 9, 1, 7, 2, 4, 0, 5, 8, 3]\n",
      "Episode: 96, Reward: -3255999997301.0, Steps needed: 3265, Total Distance: -493.0, Visited Cities: [7, 4, 8, 5, 3, 9, 2, 1, 0, 6, 7]\n",
      "Episode: 97, Reward: -1273999999250.0, Steps needed: 1283, Total Distance: -483.0, Visited Cities: [6, 9, 4, 1, 8, 5, 2, 3, 7, 0, 6]\n",
      "Episode: 98, Reward: -1575999998880.0, Steps needed: 1585, Total Distance: -413.0, Visited Cities: [2, 8, 7, 3, 5, 1, 4, 9, 0, 6, 2]\n",
      "Episode: 99, Reward: -3513999997128.0, Steps needed: 3523, Total Distance: -613.0, Visited Cities: [3, 6, 0, 5, 8, 1, 9, 7, 4, 2, 3]\n",
      "Episode: 100, Reward: -1701999998771.0, Steps needed: 1711, Total Distance: -454.0, Visited Cities: [4, 0, 5, 2, 8, 6, 7, 1, 3, 9, 4]\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Instantiate the environment\n",
    "env = TSPEnv(10)\n",
    "\n",
    "# Instantiate the agent\n",
    "agent = DQNAgent(env)\n",
    "\n",
    "# Set the number of episodes to run\n",
    "n_episodes = 1000\n",
    "\n",
    "#lists for learning evaluation\n",
    "scores = []\n",
    "\n",
    "\n",
    "# Run the episodes\n",
    "for episode in range(n_episodes):\n",
    "  # Reset the environment and get the initial state\n",
    "  state = env.reset()\n",
    "\n",
    "  # Set the initial reward to 0\n",
    "  total_reward = 0\n",
    "  total_steps = 0\n",
    "  Route = []\n",
    "\n",
    "  while True:\n",
    "    # Take an action\n",
    "    action = agent.act(state)\n",
    "\n",
    "    # Step the environment\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "    # Remember the experience\n",
    "    agent.remember(state, action, reward, next_state, done)\n",
    "\n",
    "    # Update the state and the reward\n",
    "    state = next_state\n",
    "    total_reward += reward\n",
    "    total_steps += 1\n",
    "\n",
    "    # Update the agent\n",
    "    agent.update()\n",
    "\n",
    "    # If the episode is done, break the loop\n",
    "    if done:\n",
    "      break\n",
    "  \n",
    "  scores.append((episode, total_reward, total_steps,env.visited_cities, env._total_distance, env.xy))\n",
    "  #env.plotCities()\n",
    "  # Print the total reward for the episode\n",
    "  print(f\"Episode: {episode+1}, Reward: {total_reward}, Steps needed: {total_steps}, Total Distance: {env._total_distance}, Visited Cities: {env.visited_cities}\")\n",
    "\n",
    "agent.SaveAgent()\n",
    "\n",
    "# Open a file handle in write mode\n",
    "with open('my_file.txt', 'w') as f:\n",
    "    # Write the list to the file, one item per line\n",
    "    f.writelines([f\"{item}\\n\" for item in scores])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5a312273ffa6240539e020f48a5de0b17350fba060a017bafd7c66c880b86767"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
